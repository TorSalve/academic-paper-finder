<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Perceiving affordances in virtual reality: influence of person and env"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surface. Participants were asked to judge..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds"/>

    <meta name="dc.source" content="Virtual Reality 2012 17:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-10-02"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2012 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surface. Participants were asked to judge whether a virtual slanted surface supported upright stance. The objective was to evaluate whether this perception was possible in virtual reality (VR) and comparable to previous works conducted in real environments. We found that the perception of affordances for standing on a slanted surface in virtual reality is possible and comparable (with an underestimation) to previous studies conducted in real environments. We also found that participants were able to extract and to use virtual information about friction in order to judge whether a slanted surface supported an upright stance. Finally, results revealed that the person&#8217;s position on the slanted surface is involved in the perception of affordances for standing on virtual grounds. Taken together, our results show quantitatively that the perception of affordances can be effective in virtual environments and influenced by both environmental and person properties. Such a perceptual evaluation of affordances in VR could guide VE designers to improve their designs and to better understand the effect of these designs on VE users."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-10-02"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="17"/>

    <meta name="prism.endingPage" content="28"/>

    <meta name="prism.copyright" content="2012 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-012-0216-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-012-0216-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-012-0216-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-012-0216-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2013/03"/>

    <meta name="citation_online_date" content="2012/10/02"/>

    <meta name="citation_firstpage" content="17"/>

    <meta name="citation_lastpage" content="28"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-012-0216-3"/>

    <meta name="DOI" content="10.1007/s10055-012-0216-3"/>

    <meta name="citation_doi" content="10.1007/s10055-012-0216-3"/>

    <meta name="description" content="We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surfa"/>

    <meta name="dc.creator" content="Tony Regia-Corte"/>

    <meta name="dc.creator" content="Maud Marchal"/>

    <meta name="dc.creator" content="Gabriel Cirio"/>

    <meta name="dc.creator" content="Anatole L&#233;cuyer"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=The effect of anxiety on perceiving the reachability of passing objects; citation_author=RJ Bootsma, FC Bakker, FEJ Snippenberg, CW Tdlohreg; citation_volume=4; citation_publication_date=1992; citation_pages=1-16; citation_doi=10.1080/10407413.1992.10530790; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=An outline of a theory of affordances; citation_author=A Chemero; citation_volume=15; citation_publication_date=2003; citation_pages=181-195; citation_doi=10.1207/S15326969ECO1502_5; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Perception of a stepping-across affordance; citation_author=S Cornus, G Montagne, M Laurent; citation_volume=11; citation_issue=4; citation_publication_date=1999; citation_pages=249-267; citation_doi=10.1207/s15326969eco1104_1; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Haptic and visual perception of an affordance for upright posture; citation_author=P Fitzpatrick, C Carello, RC Schmidt, D Corey; citation_volume=6; citation_issue=4; citation_publication_date=1994; citation_pages=265-287; citation_doi=10.1207/s15326969eco0604_2; citation_id=CR4"/>

    <meta name="citation_reference" content="Fitzpatrick P, Metta G, Natale L, Rao A, Sandini, G (2003) Learning about objects through action&#8212;initial steps towards artificial cognition. In: Proceedings of the 2003 IEEE international conference on robotics and automation (ICRA), pp. 3140&#8211;3145"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The reality of experience: Gibsons way; citation_author=J Flash, J Holden; citation_volume=7; citation_issue=1; citation_publication_date=1998; citation_pages=90-95; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_title=The ecological approach to visual perception; citation_publication_date=1979; citation_id=CR6; citation_author=J Gibson; citation_publisher=Houghton Mifflin"/>

    <meta name="citation_reference" content="Gross D (2004) Affordances in the design of virtual environments. PhD thesis, University of Central Florida"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Evoking affordances in virtual environments via sensori-stimuli substitution; citation_author=D Gross, K Stanney, L Cohn; citation_volume=14; citation_issue=4; citation_publication_date=2005; citation_pages=482-491; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Assess; citation_title=The anxiety thermometer: a validation study; citation_author=ILD Houtman, FC Bakker; citation_volume=53; citation_publication_date=1989; citation_pages=575-582; citation_doi=10.1207/s15327752jpa5303_14; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=The effect of gap depth on the perception of whether a gap is crossable; citation_author=Y Jiang, LS Mark; citation_volume=56; citation_publication_date=1994; citation_pages=691-700; citation_doi=10.3758/BF03208362; citation_id=CR10"/>

    <meta name="citation_reference" content="Jiang Y, Mark LS, Anderson D, Domm A (1993) The effect of viewing location and direction of gaze in determining whether a gap is crossable. In: Valenti SS, Pittenger JB (eds) Studies in perception and action II. Lawrence Erlbaum Associates, New Jersey, pp 333&#8211;337"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=What is an affordance?; citation_author=KS Jones; citation_volume=15; citation_publication_date=2003; citation_pages=107-114; citation_doi=10.1207/S15326969ECO1502_1; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Perceiving walk-on-able slopes; citation_author=J Kinsella-Shaw, B Shaw, M Turvey; citation_volume=4; citation_issue=4; citation_publication_date=1992; citation_pages=223-239; citation_doi=10.1207/s15326969eco0404_2; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Mov Sci; citation_title=Visual and haptic perception of postural affordances in children and adults; citation_author=G Klevberg, D Anderson; citation_volume=21; citation_issue=2; citation_publication_date=2002; citation_pages=169-186; citation_doi=10.1016/S0167-9457(02)00100-8; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Afforded actions as a behavioral assessment of physical presence in virtual environments; citation_author=J-C Lepecq, L Bringoux, J-M Pergandi, T Coyle, D Mestre; citation_volume=13; citation_issue=3; citation_publication_date=2009; citation_pages=141-151; citation_doi=10.1007/s10055-009-0118-1; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Q J Exp Psychol; citation_title=Kinetic potential influences visual and remote haptic perception of affordances for standing on an inclined surface; citation_author=EA Malek, JB Wagman; citation_volume=61; citation_publication_date=2008; citation_pages=1813-1826; citation_doi=10.1080/17470210701712978; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Eyeheight-scaled information about affordances: a study of sitting and stair climbing; citation_author=L Mark; citation_volume=13; citation_issue=3; citation_publication_date=1987; citation_pages=361-370; citation_doi=10.1037/0096-1523.13.3.361; citation_id=CR17"/>

    <meta name="citation_reference" content="McGrenere J, Ho W (2000) Affordances: clarifying and evolving a concept. In: Fels S, Poulin P (eds) Proceedings of the graphics interface, Toronto, pp 179&#8211;186"/>

    <meta name="citation_reference" content="citation_title=Direct perception; citation_publication_date=1981; citation_id=CR19; citation_author=CF Michaels; citation_author=C Carello; citation_publisher=Prentice-Hall"/>

    <meta name="citation_reference" content="citation_title=The psychology of everyday things; citation_publication_date=1988; citation_id=CR20; citation_author=DA Norman; citation_publisher=Basic Books"/>

    <meta name="citation_reference" content="citation_journal_title=Interactions; citation_title=Affordance, conventions, and design; citation_author=DA Norman; citation_volume=6; citation_publication_date=1999; citation_pages=38-42; citation_doi=10.1145/301153.301168; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Estimating psychometric functions in forced-choice situations: significant biases found in threshold and slope estimations when small samples are used; citation_author=JK O&#8217;Regan, R Humbert; citation_volume=46; citation_publication_date=1989; citation_pages=434-442; citation_doi=10.3758/BF03210858; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=The relevance of action in perceiving affordances: perception of catchableness of fly balls; citation_author=R Oudejans, C Michaels, F Bakker, M Dolne; citation_volume=22; citation_issue=4; citation_publication_date=1996; citation_pages=879-891; citation_doi=10.1037/0096-1523.22.4.879; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Catching balls: how to get the hand to the right place at the right time; citation_author=L Peper, RJ Bootsma, DR Mestre, FC Bakker; citation_volume=20; citation_publication_date=1994; citation_pages=591-612; citation_doi=10.1037/0096-1523.20.3.591; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=The role of anxiety in perceiving and realizing affordances; citation_author=JR Pijpers, RRD Oudejans, FC Bakker, PJ Beek; citation_volume=18; citation_issue=3; citation_publication_date=2006; citation_pages=131-161; citation_doi=10.1207/s15326969eco1803_1; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Brain Res; citation_title=Perception of affordances for standing on an inclined surface depends on height of center of mass; citation_author=T Regia-Corte, J Wagman; citation_volume=191; citation_issue=1; citation_publication_date=2008; citation_pages=25-35; citation_doi=10.1007/s00221-008-1492-8; citation_id=CR26"/>

    <meta name="citation_reference" content="Regia-Corte T, Marchal M, L&#233;cuyer A (2010) Can you stand on virtual grounds? A study on postural affordances in virtual reality. In: Proceedings of IEEE international conference on virtual reality (IEEE VR&#8217;10), pp 207&#8211;210"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Judging and actualizing intrapersonal and interpersonal affordances; citation_author=M Richardson, K Marsh, R Baron; citation_volume=33; citation_issue=4; citation_publication_date=2007; citation_pages=845-859; citation_doi=10.1037/0096-1523.33.4.845; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Adapt Behav; citation_title=To afford or not to afford: a new formalization of affordances towards affordance-based robot control; citation_author=E Sahin, M Cakmak, MR Dogar, E Ugur, G Ucoluk; citation_volume=15; citation_issue=4; citation_publication_date=2007; citation_pages=447-472; citation_doi=10.1177/1059712307084689; citation_id=CR29"/>

    <meta name="citation_reference" content="Steinicke F, Bruder G, Hinrichs K, Lappe M, Ries B, Interrante V (2009) Transitional environments enhance distance perception in immersive virtual reality systems. In: Proceedings of the 6th symposium on applied perception in graphics and visualization (APGV&#8217;09), ACM, New York, pp 19&#8211;26"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Affordances as properties of the animal-environment system; citation_author=T Stoffregen; citation_volume=15; citation_issue=2; citation_publication_date=2003; citation_pages=115-134; citation_doi=10.1207/S15326969ECO1502_2; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Affordances and prospective control: an outline of the ontology; citation_author=MT Turvey; citation_volume=4; citation_publication_date=1992; citation_pages=173-187; citation_doi=10.1207/s15326969eco0403_3; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Adapt Behav; citation_title=Traversability: a case study for learning and perceiving affordances in robots; citation_author=E Ugur, E Sahin; citation_volume=18; citation_issue=3&#8211;4; citation_publication_date=2010; citation_pages=258-284; citation_doi=10.1177/1059712310370625; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Auton Syst; citation_title=Goal emulation and planning in perceptual space using learned affordances; citation_author=E Ugur, E Oztop, E Sahin; citation_volume=59; citation_issue=7&#8211;8; citation_publication_date=2011; citation_pages=580-595; citation_doi=10.1016/j.robot.2011.04.005; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Perceiving affordances: visual guidance of stair climbing; citation_author=W Warren; citation_volume=10; citation_issue=5; citation_publication_date=1984; citation_pages=683-703; citation_doi=10.1037/0096-1523.10.5.683; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Visual guidance of walking through apertures: body-scaled information for affordances; citation_author=W Warren, S Whang; citation_volume=13; citation_issue=3; citation_publication_date=1987; citation_pages=371-383; citation_doi=10.1037/0096-1523.13.3.371; citation_id=CR37"/>

    <meta name="citation_author" content="Tony Regia-Corte"/>

    <meta name="citation_author_email" content="tony.regiacorte@gmail.com"/>

    <meta name="citation_author_institution" content="INRIA Rennes, Campus Universitaire de Beaulieu, Rennes, France"/>

    <meta name="citation_author" content="Maud Marchal"/>

    <meta name="citation_author_email" content="maud.marchal@inria.fr"/>

    <meta name="citation_author_institution" content="INRIA Rennes, Campus Universitaire de Beaulieu, Rennes, France"/>

    <meta name="citation_author" content="Gabriel Cirio"/>

    <meta name="citation_author_email" content="gabriel.cirio@inria.fr"/>

    <meta name="citation_author_institution" content="INRIA Rennes, Campus Universitaire de Beaulieu, Rennes, France"/>

    <meta name="citation_author" content="Anatole L&#233;cuyer"/>

    <meta name="citation_author_email" content="anatole.lecuyer@inria.fr"/>

    <meta name="citation_author_institution" content="INRIA Rennes, Campus Universitaire de Beaulieu, Rennes, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-012-0216-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-012-0216-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds"/>
        <meta property="og:description" content="We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surface. Participants were asked to judge whether a virtual slanted surface supported upright stance. The objective was to evaluate whether this perception was possible in virtual reality (VR) and comparable to previous works conducted in real environments. We found that the perception of affordances for standing on a slanted surface in virtual reality is possible and comparable (with an underestimation) to previous studies conducted in real environments. We also found that participants were able to extract and to use virtual information about friction in order to judge whether a slanted surface supported an upright stance. Finally, results revealed that the person’s position on the slanted surface is involved in the perception of affordances for standing on virtual grounds. Taken together, our results show quantitatively that the perception of affordances can be effective in virtual environments and influenced by both environmental and person properties. Such a perceptual evaluation of affordances in VR could guide VE designers to improve their designs and to better understand the effect of these designs on VE users."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-012-0216-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Perception of affordances, Visual perception, Posture, Slanted surface, Virtual environments, Head-mounted display","kwrd":["Perception_of_affordances","Visual_perception","Posture","Slanted_surface","Virtual_environments","Head-mounted_display"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-012-0216-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-012-0216-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=216;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-012-0216-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0216-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0216-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-10-02" itemprop="datePublished">02 October 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tony-Regia_Corte" data-author-popup="auth-Tony-Regia_Corte" data-corresp-id="c1">Tony Regia-Corte<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="INRIA Rennes, Campus Universitaire de Beaulieu" /><meta itemprop="address" content="INRIA Rennes, Campus Universitaire de Beaulieu, 35042, Rennes, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Maud-Marchal" data-author-popup="auth-Maud-Marchal">Maud Marchal</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="INRIA Rennes, Campus Universitaire de Beaulieu" /><meta itemprop="address" content="INRIA Rennes, Campus Universitaire de Beaulieu, 35042, Rennes, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gabriel-Cirio" data-author-popup="auth-Gabriel-Cirio">Gabriel Cirio</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="INRIA Rennes, Campus Universitaire de Beaulieu" /><meta itemprop="address" content="INRIA Rennes, Campus Universitaire de Beaulieu, 35042, Rennes, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Anatole-L_cuyer" data-author-popup="auth-Anatole-L_cuyer">Anatole Lécuyer</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="INRIA Rennes, Campus Universitaire de Beaulieu" /><meta itemprop="address" content="INRIA Rennes, Campus Universitaire de Beaulieu, 35042, Rennes, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">17</span>–<span itemprop="pageEnd">28</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">699 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-012-0216-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surface. Participants were asked to judge whether a virtual slanted surface supported upright stance. The objective was to evaluate whether this perception was possible in virtual reality (VR) and comparable to previous works conducted in real environments. We found that the perception of affordances for standing on a slanted surface in virtual reality is possible and comparable (with an underestimation) to previous studies conducted in real environments. We also found that participants were able to extract and to use virtual information about friction in order to judge whether a slanted surface supported an upright stance. Finally, results revealed that the person’s position on the slanted surface is involved in the perception of affordances for standing on virtual grounds. Taken together, our results show quantitatively that the perception of affordances can be effective in virtual environments and influenced by both environmental and person properties. Such a perceptual evaluation of affordances in VR could guide VE designers to improve their designs and to better understand the effect of these designs on VE users.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In order to successfully engage in an intended behavior, for example, to negotiate a cluttered environment, a perceiver–actor must be able to perceive the different action possibilities offered by this environment. Particular information must be available for the perceiver-actor to determine whether an action is possible or not. Such possibilities for action are known as affordances (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Gibson J (1979) The ecological approach to visual perception. Houghton Mifflin, Boston" href="/article/10.1007/s10055-012-0216-3#ref-CR6" id="ref-link-section-d62685e323">1979</a>). In this context, a horizontal and rigid surface would afford walkability, a large aperture would afford passability, and so forth. Thus, the environment is full of things that have different affordances for the organism acting in it. For the psychologist J. J. Gibson, the affordance is directly perceivable by the organism because there is information in the environment that uniquely specifies that affordance for this organism (Michaels and Carello <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Michaels CF, Carello C (1981) Direct perception. Prentice-Hall, Englewood Cliffs, NJ" href="/article/10.1007/s10055-012-0216-3#ref-CR19" id="ref-link-section-d62685e326">1981</a>). In other words, Gibson’s affordances introduce the idea of the actor–environment mutuality; the actor and the environment make an inseparable pair. This idea was different from the contemporary view of the time that the meaning of objects was created internally with further “mental calculation” of the otherwise meaningless perceptual data. Indeed, Gibson’s work was focussed on direct perception, a form of perception that does not require mediation or internal processing by an actor (see Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Jones KS (2003) What is an affordance? Ecol Psychol 15:107–114" href="/article/10.1007/s10055-012-0216-3#ref-CR12" id="ref-link-section-d62685e329">2003</a>; Chemero <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chemero A (2003) An outline of a theory of affordances. Ecol Psychol 15:181–195" href="/article/10.1007/s10055-012-0216-3#ref-CR2" id="ref-link-section-d62685e332">2003</a>).</p><p>The concept of affordances and J. J. Gibson’s view of studying organism and environment together as a system has been one of founding pillars of ecological psychology. Although introduced in psychology, the concept influenced studies in other fields as autonomous robotics (Sahin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sahin E, Cakmak M, Dogar MR, Ugur E, Ucoluk G (2007) To afford or not to afford: a new formalization of affordances towards affordance-based robot control. Adapt Behav 15(4):447–472" href="/article/10.1007/s10055-012-0216-3#ref-CR29" id="ref-link-section-d62685e338">2007</a>; Ugur and Sahin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ugur E, Sahin E (2010) Traversability: a case study for learning and perceiving affordances in robots. Adapt Behav 18(3–4):258–284" href="/article/10.1007/s10055-012-0216-3#ref-CR34" id="ref-link-section-d62685e341">2010</a>; Ugur et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ugur E, Oztop E, Sahin E (2011) Goal emulation and planning in perceptual space using learned affordances. Robot Auton Syst 59(7–8):580–595" href="/article/10.1007/s10055-012-0216-3#ref-CR35" id="ref-link-section-d62685e344">2011</a>; Fitzpatrick et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Fitzpatrick P, Metta G, Natale L, Rao A, Sandini, G (2003) Learning about objects through action—initial steps towards artificial cognition. In: Proceedings of the 2003 IEEE international conference on robotics and automation (ICRA), pp. 3140–3145" href="/article/10.1007/s10055-012-0216-3#ref-CR30" id="ref-link-section-d62685e347">2003</a>) and human–computer interaction (Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution. Presence Teleoper Virtual Environ, 14(4):482–491" href="/article/10.1007/s10055-012-0216-3#ref-CR8" id="ref-link-section-d62685e350">2005</a>; Norman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Norman DA (1988) The psychology of everyday things. Basic Books, New York" href="/article/10.1007/s10055-012-0216-3#ref-CR20" id="ref-link-section-d62685e354">1988</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Norman DA (1999) Affordance, conventions, and design. Interactions 6:38–42" href="/article/10.1007/s10055-012-0216-3#ref-CR21" id="ref-link-section-d62685e357">1999</a>; McGrenere and Ho <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="McGrenere J, Ho W (2000) Affordances: clarifying and evolving a concept. In: Fels S, Poulin P (eds) Proceedings of the graphics interface, Toronto, pp 179–186" href="/article/10.1007/s10055-012-0216-3#ref-CR18" id="ref-link-section-d62685e360">2000</a>). Regarding the human–computer interaction field, design principles have largely focused on static representations and thus have yet to fully incorporate theories of perception appropriate for the dynamic multimodal interactions inherent to virtual environment (VE) interaction. In other words, there is a need to integrate a comprehensive theory of perception into VE design. Theories of direct perception, in particular affordance theory, may prove particularly relevant to VE system design because affordance theory provides an explanation of the interaction of an organism with its environment (see Gross <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Gross D (2004) Affordances in the design of virtual environments. PhD thesis, University of Central Florida" href="/article/10.1007/s10055-012-0216-3#ref-CR7" id="ref-link-section-d62685e363">2004</a>; Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution. Presence Teleoper Virtual Environ, 14(4):482–491" href="/article/10.1007/s10055-012-0216-3#ref-CR8" id="ref-link-section-d62685e366">2005</a>).</p><p>The aim of the present study is to evaluate the perception of affordances when people are inside VE. In order to test the perception of affordances in such a condition, we have chosen to consider the perception of affordances for standing on a slanted surface. This perception is basic and fundamental in the interactions with our environment. In this paper, we begin with a review on the affordances in real and virtual worlds: The concept of affordance is explained, and the previous works on the perception of affordances in the context of virtual reality (VR) and postural activities are described. Regarding our experiments, participants were asked to judge whether a virtual slanted surface supported upright stance. In Experiment 1, we evaluated whether this perception was possible in VR and comparable to previous works conducted in real environments. The other experiments considered two dimensions involved in this perception: (a) the properties of the VE and (b) the properties of the person in the VE. The first dimension (environment) was investigated in Experiment 2 by manipulating the texture of the slanted surface (wooden texture vs. ice texture). The second dimension (person) was investigated in Experiment 3 by manipulating the person’s position on the slanted surface. Finally, results were analyzed in relation to previous works, and different practical implications were suggested for several domains.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Affordances in real and virtual worlds</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">The concept of affordance</h3><p>Gibson’s work, mainly centered on the field of visual perception, is at the origin of the ecological approach to perception and action as opposed to the cognitive approach found in psychology. A fundamental tenet of the ecological approach is the claim that affordances are perceived directly (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Gibson J (1979) The ecological approach to visual perception. Houghton Mifflin, Boston" href="/article/10.1007/s10055-012-0216-3#ref-CR6" id="ref-link-section-d62685e383">1979</a>). In other words, the perception of affordances does not require mediation or internal processing by the perceiver. The direct perception of the affordance is possible because there is invariant information in the environment that uniquely specifies that affordance. A growing body of research has demonstrated that participants are capable of perceiving affordances to control their actions in various activities including stair climbing (Mark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Mark L (1987) Eyeheight-scaled information about affordances: a study of sitting and stair climbing. J Exp Psychol Hum Percept Perform 13(3):361–370" href="/article/10.1007/s10055-012-0216-3#ref-CR17" id="ref-link-section-d62685e386">1987</a>; Warren <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="Warren W (1984) Perceiving affordances: visual guidance of stair climbing. J Exp Psychol Hum Percept Perform 10(5):683–703" href="/article/10.1007/s10055-012-0216-3#ref-CR36" id="ref-link-section-d62685e389">1984</a>), sitting on surfaces (Mark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Mark L (1987) Eyeheight-scaled information about affordances: a study of sitting and stair climbing. J Exp Psychol Hum Percept Perform 13(3):361–370" href="/article/10.1007/s10055-012-0216-3#ref-CR17" id="ref-link-section-d62685e392">1987</a>), walking through apertures (Warren and Whang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Warren W, Whang S (1987) Visual guidance of walking through apertures: body-scaled information for affordances. J Exp Psychol Hum Percept Perform 13(3):371–383" href="/article/10.1007/s10055-012-0216-3#ref-CR37" id="ref-link-section-d62685e395">1987</a>), and walking up slopes (Kinsella-Shaw et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Kinsella-Shaw J, Shaw B, Turvey M (1992) Perceiving walk-on-able slopes. Ecol Psychol 4(4):223–239" href="/article/10.1007/s10055-012-0216-3#ref-CR13" id="ref-link-section-d62685e399">1992</a>). Although these results allow a better understanding of the perception of affordances, there is still debate between researchers whether the affordance is an inherent property of the environment (Turvey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Turvey MT (1992) Affordances and prospective control: an outline of the ontology. Ecol Psychol 4:173–187" href="/article/10.1007/s10055-012-0216-3#ref-CR33" id="ref-link-section-d62685e402">1992</a>) or an emergent property of the animal–environment system (Stoffregen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stoffregen T (2003) Affordances as properties of the animal-environment system. Ecol Psychol 15(2):115–134" href="/article/10.1007/s10055-012-0216-3#ref-CR32" id="ref-link-section-d62685e405">2003</a>). However, in both of these theoretical views, there is an agreement on the fact that the perception of affordances involves that the environmental properties (height, width, weight, distance, etc.) are not evaluated on an extrinsic scale (i.e. in physical units) but are measured on an intrinsic scale according to certain relevant properties of the perceiver–actor, such as its own height, width and running speed (Oudejans et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Oudejans R, Michaels C, Bakker F, Dolne M (1996) The relevance of action in perceiving affordances: perception of catchableness of fly balls. J Exp Psychol Hum Percept Perform 22(4):879–891" href="/article/10.1007/s10055-012-0216-3#ref-CR23" id="ref-link-section-d62685e408">1996</a>). Indeed, the aforementioned studies have demonstrated that perception of affordances is based on body-scaled information. In other words, actors perceive the properties of the environment in relation to themselves. In a study of the perception of stair climbing, Warren (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="Warren W (1984) Perceiving affordances: visual guidance of stair climbing. J Exp Psychol Hum Percept Perform 10(5):683–703" href="/article/10.1007/s10055-012-0216-3#ref-CR36" id="ref-link-section-d62685e411">1984</a>) asked observers to view stairs of different heights and judge which ones they could ascend in normal fashion. Warren found that observers’ judgments were consistent and accurate with respect to their actual stair-climbing capabilities; each person’s maximum climbable riser height was a constant proportion (.88) of leg length. Studies of other actions identified similar invariant relationships between the critical action boundary and a relevant body part across actors of different sizes: sitting (Mark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Mark L (1987) Eyeheight-scaled information about affordances: a study of sitting and stair climbing. J Exp Psychol Hum Percept Perform 13(3):361–370" href="/article/10.1007/s10055-012-0216-3#ref-CR17" id="ref-link-section-d62685e414">1987</a>) and passing through apertures (Warren and Whang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Warren W, Whang S (1987) Visual guidance of walking through apertures: body-scaled information for affordances. J Exp Psychol Hum Percept Perform 13(3):371–383" href="/article/10.1007/s10055-012-0216-3#ref-CR37" id="ref-link-section-d62685e418">1987</a>).</p><h3 class="c-article__sub-heading" id="Sec4">Affordances and virtual reality</h3><p>Several researchers consider that the Gibson’s ecological framework is a promising functional approach for defining the reality of experience in relation to the problem of designing virtual environments (Flash and Holden <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Flash J, Holden J (1998) The reality of experience: Gibsons way. Presence Teleoper Virtual Environ 7(1):90–95" href="/article/10.1007/s10055-012-0216-3#ref-CR5" id="ref-link-section-d62685e429">1998</a>; Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution. Presence Teleoper Virtual Environ, 14(4):482–491" href="/article/10.1007/s10055-012-0216-3#ref-CR8" id="ref-link-section-d62685e432">2005</a>). For example, the perception of affordances could be a potential tool for sensorimotor assessment of physical presence, that is, the feeling of being physically located in a virtual place (Lepecq et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepecq J-C, Bringoux L, Pergandi J-M, Coyle T, Mestre D (2009) Afforded actions as a behavioral assessment of physical presence in virtual environments. Virtual Real 13(3):141–151" href="/article/10.1007/s10055-012-0216-3#ref-CR15" id="ref-link-section-d62685e435">2009</a>). Therefore, Lepecq et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepecq J-C, Bringoux L, Pergandi J-M, Coyle T, Mestre D (2009) Afforded actions as a behavioral assessment of physical presence in virtual environments. Virtual Real 13(3):141–151" href="/article/10.1007/s10055-012-0216-3#ref-CR15" id="ref-link-section-d62685e438">2009</a>) investigated the walk through a virtual aperture of variable widths. In the case of presence, the subject’s body orientation, while walking, was hypothesized to be adapted to the width of the aperture and to their own shoulder width. The results of this study indicated that the locomotor postural patterns of subjects having to walk through a virtual aperture strongly resemble those of subjects who have to walk through a real aperture (see Warren and Whang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Warren W, Whang S (1987) Visual guidance of walking through apertures: body-scaled information for affordances. J Exp Psychol Hum Percept Perform 13(3):371–383" href="/article/10.1007/s10055-012-0216-3#ref-CR37" id="ref-link-section-d62685e441">1987</a>). For most subjects, a behavioral transition from frontal walking to body rotation was observed as the width of the virtual aperture decreased. Finally, researchers have designed a conceptual model in order to evoke affordances in VE via sensory-stimuli substitution. Such a model can potentially guide VE designers in generating more ecologically valid designs (Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution. Presence Teleoper Virtual Environ, 14(4):482–491" href="/article/10.1007/s10055-012-0216-3#ref-CR8" id="ref-link-section-d62685e445">2005</a>).</p><h3 class="c-article__sub-heading" id="Sec5">Affordances for standing on surfaces</h3><p>In the field of postural activities, different studies have shown that the stance can be an example of affordance, that is, a given environment can afford stance for a given organism (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Gibson J (1979) The ecological approach to visual perception. Houghton Mifflin, Boston" href="/article/10.1007/s10055-012-0216-3#ref-CR6" id="ref-link-section-d62685e456">1979</a>). In a pioneering study, Fitzpatrick et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e459">1994</a>) examined perception of affordance for supporting upright stance. The participants were asked to judge visually or haptically (i.e. by probing the surface with a hand-held rod while blindfolded) whether a wooden slanted surface supports upright stance. In the experiments, participants stood at a distance of one meter from an inclined board, and either looked at the surface or explored the surface with the hand-held rod. Although participants were less confident and took longer to make haptic judgments in comparison to visual judgments, the perceptual boundary between supporting and not supporting did not differ for haptic and visual judgments (29.8 and 29.6°, respectively). The results also showed that the profiles of the responses time and confident judgments were similar for both perceptual systems: the exploration time increased and confidence decreased at the perceptual boundary. Moreover, this perceptual boundary was within a few degrees of the actual (behavioral) boundary on this behavior (approximately 30°). In a second experiment using ascending and descending methods of limits for the presentation of angles, the results also revealed that the perceptual boundaries occurred at steeper angles of inclination on descending trials than on ascending trials. This finding demonstrates a phenomenon known as enhanced contrast and suggests that perception of affordances in this task is a dynamical process (Richardson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Richardson M, Marsh K, Baron R (2007) Judging and actualizing intrapersonal and interpersonal affordances. J Exp Psychol Hum Percept Perform 33(4):845–859" href="/article/10.1007/s10055-012-0216-3#ref-CR28" id="ref-link-section-d62685e462">2007</a>). In a more recent study using the same experimental paradigm, researchers have shown that the perception of affordance for supporting upright stance depended on height of center of mass (Regia-Corte and Wagman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Regia-Corte T, Wagman J (2008) Perception of affordances for standing on an inclined surface depends on height of center of mass. Exp Brain Res 191(1):25–35" href="/article/10.1007/s10055-012-0216-3#ref-CR26" id="ref-link-section-d62685e465">2008</a>; see also Malek and Wagman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Malek EA, Wagman JB (2008) Kinetic potential influences visual and remote haptic perception of affordances for standing on an inclined surface. Q J Exp Psychol 61:1813–1826" href="/article/10.1007/s10055-012-0216-3#ref-CR16" id="ref-link-section-d62685e468">2008</a>). In this study, participants performed the task while wearing a backpack apparatus to which masses were attached in different configurations. The developmental dimension was also examined in a study evaluating how children and adults perceived affordances for upright stance. The overall superiority of the adults relative to the children indicated clearly that there are developmental changes in the ability to perceive affordances (Klevberg and Anderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Klevberg G, Anderson D (2002) Visual and haptic perception of postural affordances in children and adults. Hum Mov Sci 21(2):169–186" href="/article/10.1007/s10055-012-0216-3#ref-CR14" id="ref-link-section-d62685e472">2002</a>).</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Objective of the study</h2><div class="c-article-section__content" id="Sec6-content"><p>The purpose of this article was to study the perception of affordances in VR. In order to investigate this topic empirically, we have chosen to focus our analysis on the perception of affordances for standing on a slanted surface. During the different experiments, participants were asked to judge whether a virtual slanted surface supported upright stance. In Experiment 1, we evaluated whether this perception was possible in VR. In the other experiments, we analyzed this perception more precisely by considering the influence of VE and person properties. Therefore, we examined the influence of the texture of the slanted surface in Experiment 2 and the influence of the person’s position in Experiment 3.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Experiment 1: Can we perceive affordances for standing on a slanted surface in virtual reality?</h2><div class="c-article-section__content" id="Sec7-content"><p>The aim of Experiment 1 was to assess the perception of affordances for standing on a slanted surface in VR and to establish a comparison with previous studies conducted in real environments. Fitzpatrick et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e491">1994</a>) investigated this perception in real environments. In their study, participants reported whether they would be able to stand on a wooden slanted surface. This perception was also evaluated by considering the time taken to reach this determination and the participant’s confidence in making this determination. Results showed that the perceptual reports varied as function of inclination of the slanted surface, and the boundary between inclinations that were perceived to afford standing on and those that were not (i.e. the critical angle) was within a few degrees of the actual boundary for this behavior (an inclination of approximately 30°; see also Klevberg and Anderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Klevberg G, Anderson D (2002) Visual and haptic perception of postural affordances in children and adults. Hum Mov Sci 21(2):169–186" href="/article/10.1007/s10055-012-0216-3#ref-CR14" id="ref-link-section-d62685e494">2002</a>; Regia-Corte and Wagman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Regia-Corte T, Wagman J (2008) Perception of affordances for standing on an inclined surface depends on height of center of mass. Exp Brain Res 191(1):25–35" href="/article/10.1007/s10055-012-0216-3#ref-CR26" id="ref-link-section-d62685e497">2008</a>; Malek and Wagman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Malek EA, Wagman JB (2008) Kinetic potential influences visual and remote haptic perception of affordances for standing on an inclined surface. Q J Exp Psychol 61:1813–1826" href="/article/10.1007/s10055-012-0216-3#ref-CR16" id="ref-link-section-d62685e500">2008</a>). Moreover, results indicated that participants took longer to answer and were less confident of their responses when the slanted surface was close to the critical angle. Consequently, the hypothesis of our experiment was that if the participant is able to perceive affordances for standing on a slanted surface in VR, we should observe: (a) an effect of inclination of the slanted surface, that is, a perceptual discrimination for the inclinations that appear to support upright stance and those that do not, and (b) a pattern of results for response time and confidence judgement similar to the one of Fitzpatrick et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e503">1994</a>).</p><h3 class="c-article__sub-heading" id="Sec8">Participants</h3><p>Twelve participants (3 females and 9 males) aged from 23 to 44 (M = 27.5, SD = 5.41) took part in this experiment. All of them were right-handed, and none of them had known perception disorders. They were all naive to the purpose of the experiment.</p><h3 class="c-article__sub-heading" id="Sec9">Experimental apparatus</h3><p>The experiment was conducted in a closed room with dim light. We used the eMagin Z800 Head Mounted Display as display device, at 60 Hz and with stereoscopy enabled. The participant was upright in front of a table with the laptop computer running the application (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig1">1</a>) and was wearing an opaque fabric on top of the HMD to avoid seeing the surrounding real world. The participant’s head was tracked by an ART ARTtrack2 infrared tracking system with 9 surrounding cameras for 360° tracking. The available tracking space was a cylinder with a 3 m diameter and a 2.5 m height.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The experimental apparatus. <i>Left</i> the participant wearing the HMD and the tracking equipment. <i>Right</i> the participant equipped and wearing the opaque fabric in front of the table with the laptop computer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec10">The virtual environment</h3><p>In the virtual environment (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig2">2</a>), the participant was inside a room (width: 8.5 m × height: 4 m × length: 8.5 m) and stood upright 1 m from a slanted surface (width: 0.76 m × length: 1.56 m × thickness: 0.02 m). There were no contextual cues in the room. The floor of the room was displayed with a gray carpet and the walls and the ceiling with a brown paint. A wooden texture was used for the slanted surface. The participant’s virtual eye height (i.e. the position of the camera) corresponded to the actual participant’s eye height.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The virtual environment was made up of a room with a wooden slanted surface. <i>Left</i> the participant’s view. <i>Right</i> a side view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">Procedure</h3><p>The participant’s task in this experiment was to determine whether a wooden surface with a given inclination would support normal upright posture. Normal upright posture was defined as standing with the feet flat (i.e. not on the toes) without bending at the hip or knees. Before the experiment, each participant was briefed about the task and was instructed to stay upright during experiment. The participant was allowed to move the head in order to explore the virtual environment. Once equipped with the HMD and the opaque fabric (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig1">1</a>), the participant was led in front of the table with the laptop. The experimenter pressed the Enter key to start the presentation of the virtual environment with the slanted surface and pressed again the Enter key when participant began responding in order to measure his/her response time. The experimenter recorded the perceptual response (i.e. “yes” the surface would support upright posture or “no”, it would not). Participants also reported their confidence in their judgments on a scale ranging from very uncertain/not at all confident (1) to absolutely certain/very confident (7). The response time began with the presentation of the virtual environment and stopped when the participant began answering. Participants could view the surface for as long as they wished to determine whether they would be able to stand on the slanted surface. After recording the responses, the experimenter pressed the Enter key then a black screen appeared and the application displayed the next trial. The method of constant stimuli was used for the measure of the critical inclination. Seven angles of inclination 12°, 17°, 22°, 27°, 33°, 39°, and 45° were presented during the experiment. Each angle was randomly presented six times, resulting in 42 trials per participant. The duration of the experiment was approximately 15 min.</p><h3 class="c-article__sub-heading" id="Sec12">Results</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Analysis on the percent of “yes” responses</h4><p>For each participant, the percentage of trials that received a “yes” response was calculated for each of 7 angles of inclination. An alpha level of 0.05 was adopted. A 7 (angle of inclination) repeated-measures ANOVA on percentage of “yes” responses revealed a significant main effect, <i>F</i>(6,66) = 103.8, <i>p</i> &lt; 0.001 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig3">3</a>). The mean percentage of “yes” responses for the seven slopes was 100, 86.11, 45.83, 19.44, 5.56, 4.17, and 1.39, indicating that participants made a distinction between those inclines that appeared to support upright stance and those that did not.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Mean percentage of “yes” responses (the surface would support upright stance) as a function of angle of inclination of the slanted surface. <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>To get an accurate measure of the critical angle, the percentage of “yes” responses for each angle of inclination was analyzed using a logistic function expressed by the following equation (Bootsma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Bootsma RJ, Bakker FC, Van Snippenberg FEJ, Tdlohreg CW (1992) The effect of anxiety on perceiving the reachability of passing objects. Ecol Psychol 4:1–16" href="/article/10.1007/s10055-012-0216-3#ref-CR1" id="ref-link-section-d62685e642">1992</a>; O’Regan and Humbert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="O’Regan JK, Humbert R (1989) Estimating psychometric functions in forced-choice situations: significant biases found in threshold and slope estimations when small samples are used. Percept Psychophys 46:434–442" href="/article/10.1007/s10055-012-0216-3#ref-CR22" id="ref-link-section-d62685e645">1989</a>; Peper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Peper L, Bootsma RJ, Mestre DR, Bakker FC (1994) Catching balls: how to get the hand to the right place at the right time. J Exp Psychol Hum Percept Perform 20:591–612" href="/article/10.1007/s10055-012-0216-3#ref-CR24" id="ref-link-section-d62685e648">1994</a>; Cornus et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cornus S, Montagne G, Laurent M (1999) Perception of a stepping-across affordance. Ecol Psychol 11(4):249–267" href="/article/10.1007/s10055-012-0216-3#ref-CR3" id="ref-link-section-d62685e651">1999</a>):</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \% \,{\text{of}}\,  {\text{``yes''}}\,{\text{responses}} = \frac{100}{{\left( {1 + e^{ - k(c - x)} } \right)}} $$</span></div></div>
                  <p>In the logistic equation, 100 % was the maximum percentage of “yes” responses (i.e. the participants always judged to be able to stand on the slanted surface), <i>x</i>, the angle of inclination in degrees. <i>C</i> was the 50 % point, that is, the angle of the slanted surface at which the participant changed his or her judgment from “yes, I can stand on the slanted surface” to “no, I can’t”. In other words, this point was the critical angle for standing on the slanted surface with an upright posture. <i>K</i> was the slope approaching that point. The analysis revealed that the 50 % point occurred at an angle of inclination of 21.98° (<i>k</i> = 0.32; <i>r</i>
                    <sup>2</sup> = 0.84) with lower and upper fiducial limits of 21.06° and 22.91°.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Analysis on response time</h4><p>For each participant, the mean response time (in s) was computed on the 6 trials for each angle of inclination. A 7 (angle of inclination) repeated-measures ANOVA on the mean response time showed a significant main effect, <i>F</i>(6,66) = 9.23, <i>p</i> &lt; 0.001 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig4">4</a>), indicating that participants took longer to explore surfaces close to the transition point between supporting and not supporting upright posture. Mean response times for the seven angles of inclination were 3.15, 4.93, 5.29, 4.14, 3.47, 2.84, and 2.48 s, respectively.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Mean response time (in s) as a function of angle of inclination. <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>As in Fitzpatrick et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e728">1994</a>), the increase in response time near the transition point and its decrease on either side of the transition would be confirmed by a significant polynomial regression with a positive coefficient on the x term (i.e. angle) and a negative coefficient on the <i>x</i>
                    <sup>2</sup> term (angle<sup>2</sup>). Thus, a polynomial regression was conducted on the mean response time. The resulting equation was <i>y</i> = 1.65 + 0.2439*<i>x</i> − 0.0052*<i>x</i>
                    <sup>2</sup>, <i>r</i>
                    <sup>2</sup> = 0.22, <i>F</i>(2,81) = 11.23, <i>p</i> &lt; 0.001. This result confirmed that more time was needed for inclinations close to the perceptual transition.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Analysis on confidence judgment</h4><p>For each participant, the mean confidence judgment was computed on the 6 trials for each angle of inclination. A 7 (angle of inclination) repeated-measures ANOVA on the mean confidence judgment also revealed a significant main effect, <i>F</i>(6,66) = 21.78, <i>p</i> &lt; 0.001 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig5">5</a>), indicating that participants were less confident of their responses close to the transition point. Mean confidence judgments for the seven angles of inclination were 5.83, 4.51, 3.69, 4.72, 5.94, 6.40, and 6.78, respectively.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Mean confidence rating (1 indicates not confident; 7 indicates very confident) as a function of angle of inclination. <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>As in Fitzpatrick et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e804">1994</a>), the decrease in confidence near the transition point and its increase on either side of the transition would be confirmed by a negative x term and a positive x<sup>2</sup> term in a significant polynomial regression. Results confirmed that participants were least confident in their perceptual responses the closer the presented angle was to the transition point: <i>y</i> = 7.47 − 0.248*<i>x</i> + 0.0054*<i>x</i>
                    <sup>2</sup>, <i>r</i>
                    <sup>2</sup> = 0.39, <i>F</i>(2,81) = 25.79, <i>p</i> &lt; 0.001.</p><h3 class="c-article__sub-heading" id="Sec16">Summary of results</h3><p>As in the Fitzpatrick et al.’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287" href="/article/10.1007/s10055-012-0216-3#ref-CR4" id="ref-link-section-d62685e841">1994</a>) study conducted in real environment, results showed that participants were able to discriminate the inclinations that appeared to support upright stance and those that did not in VR. Moreover, the analysis revealed that the 50 % point (or the critical angle for an upright posture) occurred at an angle of inclination of 21.98°, and the pattern of results for the response time and the confidence judgment was consistent with this result by showing that participants took longer to answer and were less confident of their responses when the inclination was close to the critical angle. Thus, these results revealed that the perception of affordances for standing on a slanted surface in VR is possible and comparable to previous studies conducted in real environments.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Experiment 2: Influence of VE properties in perceiving affordances for standing on a slanted surface in virtual reality</h2><div class="c-article-section__content" id="Sec17-content"><p>The aim of Experiment 2 was to evaluate the perception of affordances for standing on a slanted surface by considering the properties of the VE. In this experiment, we considered the texture of the slanted surface as pertinent property (see our previous study, Regia-Corte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Regia-Corte T, Marchal M, Lécuyer A (2010) Can you stand on virtual grounds? A study on postural affordances in virtual reality. In: Proceedings of IEEE international conference on virtual reality (IEEE VR’10), pp 207–210" href="/article/10.1007/s10055-012-0216-3#ref-CR27" id="ref-link-section-d62685e853">2010</a>). To prevent an object from slipping down a slope, frictional force must be strong enough to overcome the pull of gravity. The amount of frictional force that is created depends on the coefficient of friction between the object and the surface of the slope. Thus, two contrasted textures (high-friction: wooden vs. low-friction: ice) were used for the slanted surface. The hypothesis of this experiment was that if the texture is involved in the perception of affordances for standing on a slanted surface in VR, we should observe an effect of the texture on the perceptual boundary (or critical angle): with a perceptual boundary lower with the ice texture than with the wooden texture.</p><h3 class="c-article__sub-heading" id="Sec18">Participants</h3><p>Twelve participants (2 females and 10 males) aged from 20 to 29 (M = 24.9, SD = 2.8) took part in this experiment. All of them were right-handed, and none of them had known perception disorders. They were all naive to the purpose of the experiment.</p><h3 class="c-article__sub-heading" id="Sec19">Experimental apparatus</h3><p>We used the same experimental apparatus as in Experiment 1.</p><h3 class="c-article__sub-heading" id="Sec20">The virtual environment</h3><p>We used the same virtual environment as in Experiment 1 except that two different textures were used for the slanted surface: a wooden texture or an ice texture (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig6">6</a>). In this experiment, the participant controlled the inclination of the slanted surface with the keyboard of the laptop computer.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The virtual environment was made up of a room with a slanted surface. Two different textures were used for the slanted surface: a wooden texture (<i>left</i>) and an ice texture (<i>right</i>). The participant controlled the inclination of the slanted surface with the keyboard of the laptop computer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec21">Procedure</h3><p>The task in this experiment was to adjust the angle of inclination of the virtual slanted surface until the participant felt that it was just barely possible for him/her to stand on that surface with a normal upright posture. Before the experiment, each participant was briefed about the task and was instructed to stay upright during experiment. The participant was allowed to move the head in order to explore the virtual environment. Once equipped with the HMD and the opaque fabric, the participant was led in front of the table with the laptop and the participant’s right hand was placed on the keyboard. The participant used his/her right-hand fingers to press the computer keys. The participant could adjust the angle of the slanted surface with three keys: the up arrow to increase the inclination, the down arrow to decrease the inclination, and the Enter key to validate the response. The resolution for one press on the up-down arrows was 0.25°, and a continuous press on the keys was possible to adjust the inclination (5°/s).</p><p>The method of adjustment was used for the measure of the critical inclination. For each trial, the angle of inclination of the surface was initially set at either the lowest angle of inclination (i.e. 0°) or the highest angle of inclination (i.e. 90°), and the participants adjusted the angle of inclination until they felt that the surface was set at the steepest angle that would support upright posture. Participants could view the surface for as long as they wished to determine whether they would be able to stand on the slanted surface. Once participants were satisfied with position of the surface, they pressed the Enter key to validate the response then a confirmation message appeared with a black screen and asked to press again the Enter key to confirm the response or to press the Space bar to return to the task. When the response was confirmed, the value of the inclination was recorded, and the application displayed the next trial. During the experiment, two different textures were used for the slanted surface: a wooden texture and an ice texture. No information was communicated to the participant about the texture of the slanted surface. Participants completed all the two texture conditions (wooden and ice), and the order of the conditions was counterbalanced across participants. Thus, half of the participants completed the wooden texture condition first, and the other half of the participants completed the ice texture condition first. In each condition, participants completed two ascending trials (in which the angle of inclination was initially set at 0°) and two descending trials (in which the angle of inclination was initially set at 90°). Ascending and descending trials alternated within a given condition, and the order of the sequence (i.e. whether an ascending or a descending trial was presented first in a given condition) was counterbalanced across participants. Thus, half of the participants completed the ascending trial first, and the other half of the participants completed the descending trial first. In this experiment, participants completed a total of 8 trials (2 texture conditions × 2 directions × 2 trials per condition). The duration of the experiment was approximately 10 min.</p><h3 class="c-article__sub-heading" id="Sec22">Results</h3><p>The mean angle of inclination chosen by the participants was considered as the perceptual boundary. For the analysis, an alpha level of 0.05 was adopted. A 2 (texture: wooden vs. ice) × 2 (direction: ascending vs. descending) repeated-measures ANOVA was conducted on these perceptual boundaries. The ANOVA revealed a significant effect of texture, <i>F</i>(1,11) = 8.07, <i>p</i> = 0.016 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig7">7</a>), and the perceptual boundary with the ice texture (M = 22.13°, SD = 8.52°) was significantly lower than with the wooden texture (M = 27.60°, SD = 10.57°).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mean perceptual boundary (or critical angle in degrees for standing on the slanted surface) as a function of the texture condition (wooden and ice). <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The ANOVA also revealed a significant effect of direction, <i>F</i>(1,11) = 6.83, <i>p</i> = 0.024 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig8">8</a>), and the perceptual boundary occurred at a steeper angle of inclination when the surface was descending (M = 26.09°, SD = 9.89°) than when the surface was ascending (M = 23.65°, SD = 8.34°). The interaction between texture and direction was not significant (<i>F</i>(1,11) = 1.38, <i>p</i> = 0.26).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Mean perceptual boundary (or critical angle in degrees for standing on the slanted surface) as a function of the direction condition (<i>Ascending</i> and <i>Descending</i>). <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec23">Summary of results</h3><p>In this experiment, the texture of the slanted surface was manipulated. Results showed that the perceptual boundary with the ice texture (22.13°) was significantly lower than with the wooden texture (27.60°). This result revealed that the virtual information about friction was detected and used in VE. Thus, participants were able to differentiate visually a low-friction texture (ice) from a high-friction texture (wooden). In other words, this result indicated that the texture of the slanted surface was involved in perceiving affordances for standing on this surface in VR. Furthermore, as in the previous works conducted in real environments, our results also revealed that the perceptual boundaries occurred at steeper angles of inclination on descending trials than on ascending trials. This finding demonstrates a phenomenon known as enhanced contrast (Richardson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Richardson M, Marsh K, Baron R (2007) Judging and actualizing intrapersonal and interpersonal affordances. J Exp Psychol Hum Percept Perform 33(4):845–859" href="/article/10.1007/s10055-012-0216-3#ref-CR28" id="ref-link-section-d62685e1009">2007</a>) and suggests that perception of affordances in this task is a dynamical process. Finally, this last result reinforces the similarity observed between the perception of affordances in VE and in real environments.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Experiment 3: Influence of person properties in perceiving affordances for standing on a slanted surface in virtual reality</h2><div class="c-article-section__content" id="Sec24-content"><p>The aim of Experiment 3 was to evaluate the perception of affordances for standing on a slanted surface by considering the properties of the person in the VE. In this experiment, we considered the person’s position on the slanted surface as pertinent property. The person’s position is the location on the slanted surface considered by the person during his/her perceptual judgement. This person’s property was not analyzed in previous studies conducted in real environments. For our experiment, it is important to notice that the different locations on the slanted surface involve different aspects for the participant. Thus, for example, a high location is more dangerous for the physical integrity than a low location. Consequently, in Experiment 3, the perception of whether a slanted surface supported upright stance was investigated by using a postural zone differently positioned on the slanted surface. When this postural zone was displayed on the surface, the participant had to consider this information of position during his/her perceptual judgement. In other words, the participant had to visualize himself or herself being inside the postural zone when it was displayed. Thus, three postural zone conditions (No zone vs. Low zone vs. High zone) were used during experiment. The hypothesis of this experiment was that if the person’s position is involved in the perception of affordances for standing on a slanted surface in VR, we should observe an effect of the postural zone on the perceptual boundary (or critical angle): with a perceptual boundary lower in the High zone condition (dangerous) than in the Low zone condition (not dangerous).</p><h3 class="c-article__sub-heading" id="Sec25">Participants</h3><p>The participants of this experiment were the same as for the Experiment 2.</p><h3 class="c-article__sub-heading" id="Sec26">Experimental apparatus</h3><p>We used the same experimental apparatus as in the Experiments 1 and 2.</p><h3 class="c-article__sub-heading" id="Sec27">The virtual environment</h3><p>We used the same virtual environment as in the Experiments 1 and 2 except that in this experiment, it was possible to display a postural zone on the slanted surface and to change its position in relation to the bottom of the slanted surface (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig9">9</a>). This postural zone was delimited by a white rectangle (width: 60 cm × height: 30 cm). Three different zone conditions were used during the experiment: a No zone condition (where no postural zone was displayed), a Low zone condition (positioned at 20 cm from the bottom) and a High zone condition (positioned at 1.36 m from the bottom). A wooden texture was used for the slanted surface. The participant controlled the inclination of the slanted surface with the keyboard of the laptop computer.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The virtual environment was made up of a room with a wooden slanted surface. Three different zone conditions were used during the experiment: a No zone condition (<i>left</i>), a Low zone condition (<i>center</i>), and a High zone condition (<i>right</i>). The participant controlled the inclination of the slanted surface with the keyboard of the laptop computer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec28">Procedure</h3><p>The task and the method in this experiment were the same that in the Experiment 2 except that when the postural zone was displayed on the slanted surface, the participant had to consider this zone for the adjustment of his/her critical angle for an upright posture. During the experiment, three different zone conditions were used for the slanted surface: No zone, Low zone and High zone. Participants completed all the three zone conditions, and the order of the conditions was counterbalanced across participants. A full counterbalancing with six possible orders was used. Participants were randomly assigned to each of the six possible orders of conditions. In each zone condition, participants completed two ascending trials (in which the angle of inclination was initially set at 0°) and two descending trials (in which the angle of inclination was initially set at 90°). Ascending and descending trials alternated within a given condition, and the order of the sequence (i.e. whether an ascending or a descending trial was presented first in a given condition) was counterbalanced across participants. Thus, half of the participants completed the ascending trial first, and the other half of the participants completed the descending trial first. In this experiment, participants completed a total of 12 trials (3 zone conditions × 2 directions × 2 trials per condition). The duration of the experiment was approximately 12 min.</p><h3 class="c-article__sub-heading" id="Sec29">Results</h3><p>The mean angle of inclination chosen by the participants was considered as the perceptual boundary. For the analysis, an alpha level of 0.05 was adopted. A 3 (zone condition: No zone vs. Low zone vs. High zone) × 2 (direction: ascending vs. descending) repeated-measures ANOVA was conducted on these perceptual boundaries. The ANOVA revealed a significant effect of the zone condition, <i>F</i>(2,22) = 6.74, <i>p</i> = 0.005 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0216-3#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0216-3/MediaObjects/10055_2012_216_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Mean perceptual boundary (or critical angle in degrees for standing on the slanted surface) as a function of the zone condition (No zone, Low zone, and High zone). <i>Bars</i> represent SD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0216-3/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For the different comparison analysis, a correction for experiment-wise error was realized by using Bonferroni-adjusted alpha level (<i>p</i> = 0.05 divided by the number of tests). Thus, in order to compare the three zone conditions (No zone, Low zone and High zone), the alpha level was adjusted to <i>p</i> = 0.0167. Follow-up t test revealed that the perceptual boundary in the Low zone condition (M = 27.04°, SD = 9.02°) was significantly higher than in the High zone condition (M = 21.10°, SD = 9.52°), <i>t</i>(11) = 3.86, <i>p</i> = 0.002. By contrast, the analysis indicated that the perceptual boundary in the No zone condition (M = 24.41°, SD = 8.26°) was not significantly different from the perceptual boundaries in the Low zone condition (<i>t</i>(11) = − 1.82, <i>p</i> = 0.096) and in the High zone condition (<i>t</i>(11) = 1.79, <i>p</i> = 0.10).</p><p>The ANOVA also indicated a tendency for the direction, <i>F</i>(1,11) = 4.77, <i>p</i> = 0.052, and the perceptual boundary occurred at a marginally steeper angle of inclination when the surface was descending (M = 25.46°, SD = 9.08°) than when the surface was ascending (M = 22.91°, SD = 8.05°). The interaction between zone condition and direction was not significant (<i>F</i>(1,11) = 2.30, <i>p</i> = 0.12).</p><h3 class="c-article__sub-heading" id="Sec30">Summary of results</h3><p>The analysis revealed that the postural zone on the slanted surface had an effect in perceiving affordances for an upright posture: the perceptual boundary in the High zone condition was significantly lower than in the Low zone condition. Thus, these results indicated that the person’s position on the slanted surface was involved in the perception of affordances for standing on this surface in VR. The absence of significant differences between the No zone condition and the two other conditions (Low zone and High zone) could be explained by the fact that when no postural zone was displayed on the slanted surface, participants were free to consider different postural positions on the slanted surface (i.e. low, high, or middle) during their perceptual judgments. Finally, consistent with Experiment 2 and previous studies, our results also indicated a tendency for which the perceptual boundaries occurred at steeper angles of inclination on descending trials than on ascending trials (enhanced contrast).</p></div></div></section><section aria-labelledby="Sec31"><div class="c-article-section" id="Sec31-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec31">General discussion</h2><div class="c-article-section__content" id="Sec31-content"><p>This paper analyzed the perception of affordances for standing on a slanted surface in VR. During the different experiments, participants were asked to judge whether a virtual slanted surface supported upright stance. Interestingly, participants showed a natural ability to perceive affordances in VR although they have no prior experience with the virtual slanted surface displayed. These results are interesting because they are consistent with the previous research conducted in real environments but also because they reveal several specificities.</p><p>The aim of Experiment 1 was to evaluate whether the perception of affordances for standing on a slanted surface was possible in VR and comparable to previous works conducted in real environments. In this experiment, participants reported whether they would be able to stand on a virtual wooden surface with an upright posture. Results showed that participants were able to discriminate the inclinations that appeared to support upright stance and those that did not in VR. Response time and confidence judgment were consistent with this result by showing an increase in response time and a decrease in confidence judgment when the inclination was close to the critical angle. However, the observation of results indicated that the critical angle for an upright posture in VR (21.98°) appeared to be lower in comparison with those of the previous studies conducted in real environments (approximately 30°). This underestimation is an interesting and paradoxical result. Indeed, we can imagine that people inside VE are aware to be in an unrealistic world where their physical integrity is not involved and where it is possible to risk dangerous behaviors. However, this underestimation indicated, on the contrary, that participants were more careful in VE. One possible explanation for this paradoxical result would be the presence of a time effect on the perception: at the beginning, the VE as a new environment involves a safety first effect with an underestimation of action possibilities. But during time and practice inside the VE, participants become more adapted and confident with a virtual perception reaching the real perception. And finally, participants adopt risky and dangerous behaviors leading to an overestimation of action possibilities. Thus, it would be interesting for the future research to consider the time factor in order to test this hypothesis. It is important to notice that previous studies have shown that distances appear to be compressed in immersive virtual environments presented via head-mounted display systems, relative to in the real world (Steinicke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Steinicke F, Bruder G, Hinrichs K, Lappe M, Ries B, Interrante V (2009) Transitional environments enhance distance perception in immersive virtual reality systems. In: Proceedings of the 6th symposium on applied perception in graphics and visualization (APGV’09), ACM, New York, pp 19–26" href="/article/10.1007/s10055-012-0216-3#ref-CR31" id="ref-link-section-d62685e1181">2009</a>). Thus, the underestimation observed in our study could indicate that the perception of affordances in VR would be also affected by the effect of compression. On the other hand, it is possible that some properties of the HMD configuration were involved in this perception. These topics could be investigated more precisely in future experimental work.</p><p>The aim of Experiment 2 was to evaluate the role of VE properties in perceiving affordances by manipulating the texture of the slanted surface. In this experiment, the participant adjusted the angle of inclination of the virtual slanted surface until he/she felt that it was just barely possible for him/her to stand on that surface with a normal upright posture. The analysis showed that the perceptual boundary with the ice texture (22.13°) was significantly lower than with the wooden texture (27.60°). Thus, this result revealed that the virtual information about friction was detected and used in VE. Participants were able to differentiate visually a low-friction texture (ice) from a high-friction texture (wooden). In other words, this result indicated that the texture of the slanted surface was involved in perceiving affordances for standing on this surface in VR. It suggests that participants can be influenced by the properties of the virtual environment (here the visual textures) and can extract and use such information when perceiving affordances of virtual objects. Furthermore, as in the previous works conducted in real environments, our results also revealed that the perceptual boundaries occurred at steeper angles of inclination on descending trials than on ascending trials. This finding demonstrates a phenomenon known as enhanced contrast (Richardson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Richardson M, Marsh K, Baron R (2007) Judging and actualizing intrapersonal and interpersonal affordances. J Exp Psychol Hum Percept Perform 33(4):845–859" href="/article/10.1007/s10055-012-0216-3#ref-CR28" id="ref-link-section-d62685e1187">2007</a>) and suggests that perception of affordances in this task is a dynamical process. This last result reinforces the similarity observed between the perception of affordances in VE and in real environments.</p><p>The comparison of results for Experiments 1 and 2 indicated an important difference between perceptual boundaries. Indeed, the results of Experiment 2 showed higher critical angles for both conditions compared with the result in Experiment 1. Thus, even the critical angle for the ice texture was slightly steeper than the critical angle for the wooden texture in Experiment 1. However, such a comparison is not really pertinent in our case since the psychophysical methods used in each experiment were not the same (method of stimuli constant in Experiment 1 and method of adjustment in Experiment 2). Finally, this difference and the incoherence observed can be attributed to the technical aspect resulting from the use of each method, such as the verbal responses in Experiment 1 and the possibility for the participant to move the slanted surface in Experiment 2.</p><p>The aim of Experiment 3 was to evaluate the role of person properties in perceiving affordances by manipulating the person’s position on the slanted surface. In this experiment, the task and the method were the same as in the Experiment 2 except that a postural zone was displayed on the slanted surface, and the participant had to consider this zone for the adjustment of his/her critical angle for an upright posture. Three postural zone conditions were used during experiment: No zone, Low zone, and High zone. The analysis revealed that the postural zone on the slanted surface had an effect in perceiving affordances for an upright posture: The perceptual boundary in the High zone condition (21.10°) was significantly lower than in the Low zone condition (27.04°). Thus, these results indicated that the person’s position on the slanted surface was involved in the perception of affordances for standing on this surface in VR. These results might first be related to previous studies conducted in order to evaluate the role of the person’s emotional state (e.g. anxiety) in the perception of affordances. For example, Pijpers et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Pijpers JR, Oudejans RRD, Bakker FC, Beek PJ (2006) The role of anxiety in perceiving and realizing affordances. Ecol Psychol 18(3):131–161" href="/article/10.1007/s10055-012-0216-3#ref-CR25" id="ref-link-section-d62685e1196">2006</a>) used a climber wall and determined perceived and actual maximal overhead reaching height under different anxiety conditions, which were created by placing the same climbing route high and low on the wall. Anxiety was found to reduce both perceived and actual maximal reaching height. On the other hand, Jiang and colleagues (Jiang and Mark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Jiang Y, Mark LS (1994) The effect of gap depth on the perception of whether a gap is crossable. Percept Psychophys 56:691–700" href="/article/10.1007/s10055-012-0216-3#ref-CR10" id="ref-link-section-d62685e1199">1994</a>; Jiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Jiang Y, Mark LS, Anderson D, Domm A (1993) The effect of viewing location and direction of gaze in determining whether a gap is crossable. In: Valenti SS, Pittenger JB (eds) Studies in perception and action II. Lawrence Erlbaum Associates, New Jersey, pp 333–337" href="/article/10.1007/s10055-012-0216-3#ref-CR11" id="ref-link-section-d62685e1202">1993</a>) found that when individuals had to judge whether they could step over a gap, their estimates of crossable gap width decreased as gap depth increased. This finding seems to refer to a process similar to that addressed in the Pijpers et al.’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Pijpers JR, Oudejans RRD, Bakker FC, Beek PJ (2006) The role of anxiety in perceiving and realizing affordances. Ecol Psychol 18(3):131–161" href="/article/10.1007/s10055-012-0216-3#ref-CR25" id="ref-link-section-d62685e1205">2006</a>) study in that increased gap depth led to increased anxiety, which in turn affected the perception of gap-crossing capability. Consequently, these studies indicate that the use of postural zones in our experiment may have modified the person’s emotional state (i.e. anxiety or vertigo) which in turn affected the perception of affordances for standing on the surface. Hence, we can suppose that the lower perceptual boundary observed in the High zone condition in comparison with the one in the Low zone condition could be explained by the fact that more anxiety was felt in the High zone condition than in the Low zone condition. Future research could investigate this point by using physiological measures and an “anxiety thermometer” (see Houtman and Bakker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Houtman ILD, Bakker FC (1989) The anxiety thermometer: a validation study. J Pers Assess 53:575–582" href="/article/10.1007/s10055-012-0216-3#ref-CR9" id="ref-link-section-d62685e1208">1989</a>) during the experiment.</p><p>Regarding the practical implications of our study, the results suggest various applications. People with motor impairments or balance disorders might improve their postural ability with specific activities in VR where they are confronted with different affordances. On the other hand, other practical implications would be in the context of urban projects, where the immersion in the 3D representations of buildings would allow to localize the uncomfortable affordances. The results observed in the present work call for additional investigations devoted to evaluate the perception of different affordances in VR (walking up slopes, stair climbing, gap crossing, and object reaching). It would also be interesting to conduct these investigations by considering different perceptual modalities (vision, haptic, and audition) and their interactions.</p></div></div></section><section aria-labelledby="Sec32"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Conclusion</h2><div class="c-article-section__content" id="Sec32-content"><p>The aim of this work was to evaluate the perception of affordances in VR taking as an example standing on a slanted surface. Therefore, we have conducted different experiments where participants judged whether a virtual slanted surface supports upright stance. Results indicated that the perception of affordances for standing on a slanted surface in VR is possible and comparable (with an underestimation) to previous studies conducted in real environments. Participants were also able to differentiate visually a low-friction texture (ice) from a high-friction texture (wooden) and to use this virtual information about friction in the perception of affordances for standing on a slanted surface. Finally, our study indicated that the person’s position is an important factor involved in the perception of affordances for standing on a slanted surface in VR. Taken together, our results show quantitatively that the perception of affordances can be effective in virtual environments and influenced by both environmental and person properties. They introduce and validate the paradigm of postural affordance of standing on a slanted surface for future studies on affordances in VR. Thus, future research might evaluate the influence of other parameters such as the type of display or some characteristics of participants.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RJ. Bootsma, FC. Bakker, FEJ. Snippenberg, CW. Tdlohreg, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Bootsma RJ, Bakker FC, Van Snippenberg FEJ, Tdlohreg CW (1992) The effect of anxiety on perceiving the reachab" /><p class="c-article-references__text" id="ref-CR1">Bootsma RJ, Bakker FC, Van Snippenberg FEJ, Tdlohreg CW (1992) The effect of anxiety on perceiving the reachability of passing objects. Ecol Psychol 4:1–16</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10407413.1992.10530790" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20anxiety%20on%20perceiving%20the%20reachability%20of%20passing%20objects&amp;journal=Ecol%20Psychol&amp;volume=4&amp;pages=1-16&amp;publication_year=1992&amp;author=Bootsma%2CRJ&amp;author=Bakker%2CFC&amp;author=Snippenberg%2CFEJ&amp;author=Tdlohreg%2CCW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Chemero, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Chemero A (2003) An outline of a theory of affordances. Ecol Psychol 15:181–195" /><p class="c-article-references__text" id="ref-CR2">Chemero A (2003) An outline of a theory of affordances. Ecol Psychol 15:181–195</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15326969ECO1502_5" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20outline%20of%20a%20theory%20of%20affordances&amp;journal=Ecol%20Psychol&amp;volume=15&amp;pages=181-195&amp;publication_year=2003&amp;author=Chemero%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Cornus, G. Montagne, M. Laurent, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cornus S, Montagne G, Laurent M (1999) Perception of a stepping-across affordance. Ecol Psychol 11(4):249–267" /><p class="c-article-references__text" id="ref-CR3">Cornus S, Montagne G, Laurent M (1999) Perception of a stepping-across affordance. Ecol Psychol 11(4):249–267</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15326969eco1104_1" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20of%20a%20stepping-across%20affordance&amp;journal=Ecol%20Psychol&amp;volume=11&amp;issue=4&amp;pages=249-267&amp;publication_year=1999&amp;author=Cornus%2CS&amp;author=Montagne%2CG&amp;author=Laurent%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Fitzpatrick, C. Carello, RC. Schmidt, D. Corey, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright" /><p class="c-article-references__text" id="ref-CR4">Fitzpatrick P, Carello C, Schmidt RC, Corey D (1994) Haptic and visual perception of an affordance for upright posture. Ecol Psychol 6(4):265–287</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15326969eco0604_2" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20and%20visual%20perception%20of%20an%20affordance%20for%20upright%20posture&amp;journal=Ecol%20Psychol&amp;volume=6&amp;issue=4&amp;pages=265-287&amp;publication_year=1994&amp;author=Fitzpatrick%2CP&amp;author=Carello%2CC&amp;author=Schmidt%2CRC&amp;author=Corey%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fitzpatrick P, Metta G, Natale L, Rao A, Sandini, G (2003) Learning about objects through action—initial steps" /><p class="c-article-references__text" id="ref-CR30">Fitzpatrick P, Metta G, Natale L, Rao A, Sandini, G (2003) Learning about objects through action—initial steps towards artificial cognition. In: Proceedings of the 2003 IEEE international conference on robotics and automation (ICRA), pp. 3140–3145</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Flash, J. Holden, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Flash J, Holden J (1998) The reality of experience: Gibsons way. Presence Teleoper Virtual Environ 7(1):90–95" /><p class="c-article-references__text" id="ref-CR5">Flash J, Holden J (1998) The reality of experience: Gibsons way. Presence Teleoper Virtual Environ 7(1):90–95</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20reality%20of%20experience%3A%20Gibsons%20way&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=7&amp;issue=1&amp;pages=90-95&amp;publication_year=1998&amp;author=Flash%2CJ&amp;author=Holden%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Gibson, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Gibson J (1979) The ecological approach to visual perception. Houghton Mifflin, Boston" /><p class="c-article-references__text" id="ref-CR6">Gibson J (1979) The ecological approach to visual perception. Houghton Mifflin, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ecological%20approach%20to%20visual%20perception&amp;publication_year=1979&amp;author=Gibson%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gross D (2004) Affordances in the design of virtual environments. PhD thesis, University of Central Florida" /><p class="c-article-references__text" id="ref-CR7">Gross D (2004) Affordances in the design of virtual environments. PhD thesis, University of Central Florida</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Gross, K. Stanney, L. Cohn, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution" /><p class="c-article-references__text" id="ref-CR8">Gross D, Stanney K, Cohn L (2005) Evoking affordances in virtual environments via sensori-stimuli substitution. Presence Teleoper Virtual Environ, 14(4):482–491</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evoking%20affordances%20in%20virtual%20environments%20via%20sensori-stimuli%20substitution&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=14&amp;issue=4&amp;pages=482-491&amp;publication_year=2005&amp;author=Gross%2CD&amp;author=Stanney%2CK&amp;author=Cohn%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ILD. Houtman, FC. Bakker, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Houtman ILD, Bakker FC (1989) The anxiety thermometer: a validation study. J Pers Assess 53:575–582" /><p class="c-article-references__text" id="ref-CR9">Houtman ILD, Bakker FC (1989) The anxiety thermometer: a validation study. J Pers Assess 53:575–582</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327752jpa5303_14" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20anxiety%20thermometer%3A%20a%20validation%20study&amp;journal=J%20Pers%20Assess&amp;volume=53&amp;pages=575-582&amp;publication_year=1989&amp;author=Houtman%2CILD&amp;author=Bakker%2CFC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Jiang, LS. Mark, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Jiang Y, Mark LS (1994) The effect of gap depth on the perception of whether a gap is crossable. Percept Psych" /><p class="c-article-references__text" id="ref-CR10">Jiang Y, Mark LS (1994) The effect of gap depth on the perception of whether a gap is crossable. Percept Psychophys 56:691–700</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03208362" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20gap%20depth%20on%20the%20perception%20of%20whether%20a%20gap%20is%20crossable&amp;journal=Percept%20Psychophys&amp;volume=56&amp;pages=691-700&amp;publication_year=1994&amp;author=Jiang%2CY&amp;author=Mark%2CLS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang Y, Mark LS, Anderson D, Domm A (1993) The effect of viewing location and direction of gaze in determinin" /><p class="c-article-references__text" id="ref-CR11">Jiang Y, Mark LS, Anderson D, Domm A (1993) The effect of viewing location and direction of gaze in determining whether a gap is crossable. In: Valenti SS, Pittenger JB (eds) Studies in perception and action II. Lawrence Erlbaum Associates, New Jersey, pp 333–337</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KS. Jones, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Jones KS (2003) What is an affordance? Ecol Psychol 15:107–114" /><p class="c-article-references__text" id="ref-CR12">Jones KS (2003) What is an affordance? Ecol Psychol 15:107–114</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15326969ECO1502_1" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20is%20an%20affordance%3F&amp;journal=Ecol%20Psychol&amp;volume=15&amp;pages=107-114&amp;publication_year=2003&amp;author=Jones%2CKS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kinsella-Shaw, B. Shaw, M. Turvey, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Kinsella-Shaw J, Shaw B, Turvey M (1992) Perceiving walk-on-able slopes. Ecol Psychol 4(4):223–239" /><p class="c-article-references__text" id="ref-CR13">Kinsella-Shaw J, Shaw B, Turvey M (1992) Perceiving walk-on-able slopes. Ecol Psychol 4(4):223–239</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15326969eco0404_2" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceiving%20walk-on-able%20slopes&amp;journal=Ecol%20Psychol&amp;volume=4&amp;issue=4&amp;pages=223-239&amp;publication_year=1992&amp;author=Kinsella-Shaw%2CJ&amp;author=Shaw%2CB&amp;author=Turvey%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Klevberg, D. Anderson, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Klevberg G, Anderson D (2002) Visual and haptic perception of postural affordances in children and adults. Hum" /><p class="c-article-references__text" id="ref-CR14">Klevberg G, Anderson D (2002) Visual and haptic perception of postural affordances in children and adults. Hum Mov Sci 21(2):169–186</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0167-9457%2802%2900100-8" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20and%20haptic%20perception%20of%20postural%20affordances%20in%20children%20and%20adults&amp;journal=Hum%20Mov%20Sci&amp;volume=21&amp;issue=2&amp;pages=169-186&amp;publication_year=2002&amp;author=Klevberg%2CG&amp;author=Anderson%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J-C. Lepecq, L. Bringoux, J-M. Pergandi, T. Coyle, D. Mestre, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Lepecq J-C, Bringoux L, Pergandi J-M, Coyle T, Mestre D (2009) Afforded actions as a behavioral assessment of " /><p class="c-article-references__text" id="ref-CR15">Lepecq J-C, Bringoux L, Pergandi J-M, Coyle T, Mestre D (2009) Afforded actions as a behavioral assessment of physical presence in virtual environments. Virtual Real 13(3):141–151</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0118-1" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Afforded%20actions%20as%20a%20behavioral%20assessment%20of%20physical%20presence%20in%20virtual%20environments&amp;journal=Virtual%20Real&amp;volume=13&amp;issue=3&amp;pages=141-151&amp;publication_year=2009&amp;author=Lepecq%2CJ-C&amp;author=Bringoux%2CL&amp;author=Pergandi%2CJ-M&amp;author=Coyle%2CT&amp;author=Mestre%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EA. Malek, JB. Wagman, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Malek EA, Wagman JB (2008) Kinetic potential influences visual and remote haptic perception of affordances for" /><p class="c-article-references__text" id="ref-CR16">Malek EA, Wagman JB (2008) Kinetic potential influences visual and remote haptic perception of affordances for standing on an inclined surface. Q J Exp Psychol 61:1813–1826</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F17470210701712978" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Kinetic%20potential%20influences%20visual%20and%20remote%20haptic%20perception%20of%20affordances%20for%20standing%20on%20an%20inclined%20surface&amp;journal=Q%20J%20Exp%20Psychol&amp;volume=61&amp;pages=1813-1826&amp;publication_year=2008&amp;author=Malek%2CEA&amp;author=Wagman%2CJB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Mark, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Mark L (1987) Eyeheight-scaled information about affordances: a study of sitting and stair climbing. J Exp Psy" /><p class="c-article-references__text" id="ref-CR17">Mark L (1987) Eyeheight-scaled information about affordances: a study of sitting and stair climbing. J Exp Psychol Hum Percept Perform 13(3):361–370</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=932892" aria-label="View reference 18 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.13.3.361" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Eyeheight-scaled%20information%20about%20affordances%3A%20a%20study%20of%20sitting%20and%20stair%20climbing&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=13&amp;issue=3&amp;pages=361-370&amp;publication_year=1987&amp;author=Mark%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McGrenere J, Ho W (2000) Affordances: clarifying and evolving a concept. In: Fels S, Poulin P (eds) Proceeding" /><p class="c-article-references__text" id="ref-CR18">McGrenere J, Ho W (2000) Affordances: clarifying and evolving a concept. In: Fels S, Poulin P (eds) Proceedings of the graphics interface, Toronto, pp 179–186</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="CF. Michaels, C. Carello, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Michaels CF, Carello C (1981) Direct perception. Prentice-Hall, Englewood Cliffs, NJ" /><p class="c-article-references__text" id="ref-CR19">Michaels CF, Carello C (1981) Direct perception. Prentice-Hall, Englewood Cliffs, NJ</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Direct%20perception&amp;publication_year=1981&amp;author=Michaels%2CCF&amp;author=Carello%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Norman, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Norman DA (1988) The psychology of everyday things. Basic Books, New York" /><p class="c-article-references__text" id="ref-CR20">Norman DA (1988) The psychology of everyday things. Basic Books, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20psychology%20of%20everyday%20things&amp;publication_year=1988&amp;author=Norman%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Norman, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Norman DA (1999) Affordance, conventions, and design. Interactions 6:38–42" /><p class="c-article-references__text" id="ref-CR21">Norman DA (1999) Affordance, conventions, and design. Interactions 6:38–42</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F301153.301168" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affordance%2C%20conventions%2C%20and%20design&amp;journal=Interactions&amp;volume=6&amp;pages=38-42&amp;publication_year=1999&amp;author=Norman%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JK. O’Regan, R. Humbert, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="O’Regan JK, Humbert R (1989) Estimating psychometric functions in forced-choice situations: significant biases" /><p class="c-article-references__text" id="ref-CR22">O’Regan JK, Humbert R (1989) Estimating psychometric functions in forced-choice situations: significant biases found in threshold and slope estimations when small samples are used. Percept Psychophys 46:434–442</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03210858" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimating%20psychometric%20functions%20in%20forced-choice%20situations%3A%20significant%20biases%20found%20in%20threshold%20and%20slope%20estimations%20when%20small%20samples%20are%20used&amp;journal=Percept%20Psychophys&amp;volume=46&amp;pages=434-442&amp;publication_year=1989&amp;author=O%E2%80%99Regan%2CJK&amp;author=Humbert%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Oudejans, C. Michaels, F. Bakker, M. Dolne, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Oudejans R, Michaels C, Bakker F, Dolne M (1996) The relevance of action in perceiving affordances: perception" /><p class="c-article-references__text" id="ref-CR23">Oudejans R, Michaels C, Bakker F, Dolne M (1996) The relevance of action in perceiving affordances: perception of catchableness of fly balls. J Exp Psychol Hum Percept Perform 22(4):879–891</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.22.4.879" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20relevance%20of%20action%20in%20perceiving%20affordances%3A%20perception%20of%20catchableness%20of%20fly%20balls&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=22&amp;issue=4&amp;pages=879-891&amp;publication_year=1996&amp;author=Oudejans%2CR&amp;author=Michaels%2CC&amp;author=Bakker%2CF&amp;author=Dolne%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Peper, RJ. Bootsma, DR. Mestre, FC. Bakker, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Peper L, Bootsma RJ, Mestre DR, Bakker FC (1994) Catching balls: how to get the hand to the right place at the" /><p class="c-article-references__text" id="ref-CR24">Peper L, Bootsma RJ, Mestre DR, Bakker FC (1994) Catching balls: how to get the hand to the right place at the right time. J Exp Psychol Hum Percept Perform 20:591–612</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.20.3.591" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Catching%20balls%3A%20how%20to%20get%20the%20hand%20to%20the%20right%20place%20at%20the%20right%20time&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=20&amp;pages=591-612&amp;publication_year=1994&amp;author=Peper%2CL&amp;author=Bootsma%2CRJ&amp;author=Mestre%2CDR&amp;author=Bakker%2CFC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JR. Pijpers, RRD. Oudejans, FC. Bakker, PJ. Beek, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Pijpers JR, Oudejans RRD, Bakker FC, Beek PJ (2006) The role of anxiety in perceiving and realizing affordance" /><p class="c-article-references__text" id="ref-CR25">Pijpers JR, Oudejans RRD, Bakker FC, Beek PJ (2006) The role of anxiety in perceiving and realizing affordances. Ecol Psychol 18(3):131–161</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15326969eco1803_1" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20anxiety%20in%20perceiving%20and%20realizing%20affordances&amp;journal=Ecol%20Psychol&amp;volume=18&amp;issue=3&amp;pages=131-161&amp;publication_year=2006&amp;author=Pijpers%2CJR&amp;author=Oudejans%2CRRD&amp;author=Bakker%2CFC&amp;author=Beek%2CPJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Regia-Corte, J. Wagman, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Regia-Corte T, Wagman J (2008) Perception of affordances for standing on an inclined surface depends on height" /><p class="c-article-references__text" id="ref-CR26">Regia-Corte T, Wagman J (2008) Perception of affordances for standing on an inclined surface depends on height of center of mass. Exp Brain Res 191(1):25–35</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00221-008-1492-8" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20of%20affordances%20for%20standing%20on%20an%20inclined%20surface%20depends%20on%20height%20of%20center%20of%20mass&amp;journal=Exp%20Brain%20Res&amp;volume=191&amp;issue=1&amp;pages=25-35&amp;publication_year=2008&amp;author=Regia-Corte%2CT&amp;author=Wagman%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Regia-Corte T, Marchal M, Lécuyer A (2010) Can you stand on virtual grounds? A study on postural affordances i" /><p class="c-article-references__text" id="ref-CR27">Regia-Corte T, Marchal M, Lécuyer A (2010) Can you stand on virtual grounds? A study on postural affordances in virtual reality. In: Proceedings of IEEE international conference on virtual reality (IEEE VR’10), pp 207–210</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Richardson, K. Marsh, R. Baron, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Richardson M, Marsh K, Baron R (2007) Judging and actualizing intrapersonal and interpersonal affordances. J E" /><p class="c-article-references__text" id="ref-CR28">Richardson M, Marsh K, Baron R (2007) Judging and actualizing intrapersonal and interpersonal affordances. J Exp Psychol Hum Percept Perform 33(4):845–859</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.33.4.845" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Judging%20and%20actualizing%20intrapersonal%20and%20interpersonal%20affordances&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=33&amp;issue=4&amp;pages=845-859&amp;publication_year=2007&amp;author=Richardson%2CM&amp;author=Marsh%2CK&amp;author=Baron%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Sahin, M. Cakmak, MR. Dogar, E. Ugur, G. Ucoluk, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Sahin E, Cakmak M, Dogar MR, Ugur E, Ucoluk G (2007) To afford or not to afford: a new formalization of afford" /><p class="c-article-references__text" id="ref-CR29">Sahin E, Cakmak M, Dogar MR, Ugur E, Ucoluk G (2007) To afford or not to afford: a new formalization of affordances towards affordance-based robot control. Adapt Behav 15(4):447–472</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F1059712307084689" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=To%20afford%20or%20not%20to%20afford%3A%20a%20new%20formalization%20of%20affordances%20towards%20affordance-based%20robot%20control&amp;journal=Adapt%20Behav&amp;volume=15&amp;issue=4&amp;pages=447-472&amp;publication_year=2007&amp;author=Sahin%2CE&amp;author=Cakmak%2CM&amp;author=Dogar%2CMR&amp;author=Ugur%2CE&amp;author=Ucoluk%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Steinicke F, Bruder G, Hinrichs K, Lappe M, Ries B, Interrante V (2009) Transitional environments enhance dist" /><p class="c-article-references__text" id="ref-CR31">Steinicke F, Bruder G, Hinrichs K, Lappe M, Ries B, Interrante V (2009) Transitional environments enhance distance perception in immersive virtual reality systems. In: Proceedings of the 6th symposium on applied perception in graphics and visualization (APGV’09), ACM, New York, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Stoffregen, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Stoffregen T (2003) Affordances as properties of the animal-environment system. Ecol Psychol 15(2):115–134" /><p class="c-article-references__text" id="ref-CR32">Stoffregen T (2003) Affordances as properties of the animal-environment system. Ecol Psychol 15(2):115–134</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15326969ECO1502_2" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affordances%20as%20properties%20of%20the%20animal-environment%20system&amp;journal=Ecol%20Psychol&amp;volume=15&amp;issue=2&amp;pages=115-134&amp;publication_year=2003&amp;author=Stoffregen%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MT. Turvey, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Turvey MT (1992) Affordances and prospective control: an outline of the ontology. Ecol Psychol 4:173–187" /><p class="c-article-references__text" id="ref-CR33">Turvey MT (1992) Affordances and prospective control: an outline of the ontology. Ecol Psychol 4:173–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15326969eco0403_3" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affordances%20and%20prospective%20control%3A%20an%20outline%20of%20the%20ontology&amp;journal=Ecol%20Psychol&amp;volume=4&amp;pages=173-187&amp;publication_year=1992&amp;author=Turvey%2CMT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Ugur, E. Sahin, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ugur E, Sahin E (2010) Traversability: a case study for learning and perceiving affordances in robots. Adapt B" /><p class="c-article-references__text" id="ref-CR34">Ugur E, Sahin E (2010) Traversability: a case study for learning and perceiving affordances in robots. Adapt Behav 18(3–4):258–284</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F1059712310370625" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Traversability%3A%20a%20case%20study%20for%20learning%20and%20perceiving%20affordances%20in%20robots&amp;journal=Adapt%20Behav&amp;volume=18&amp;issue=3%E2%80%934&amp;pages=258-284&amp;publication_year=2010&amp;author=Ugur%2CE&amp;author=Sahin%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Ugur, E. Oztop, E. Sahin, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ugur E, Oztop E, Sahin E (2011) Goal emulation and planning in perceptual space using learned affordances. Rob" /><p class="c-article-references__text" id="ref-CR35">Ugur E, Oztop E, Sahin E (2011) Goal emulation and planning in perceptual space using learned affordances. Robot Auton Syst 59(7–8):580–595</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.robot.2011.04.005" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Goal%20emulation%20and%20planning%20in%20perceptual%20space%20using%20learned%20affordances&amp;journal=Robot%20Auton%20Syst&amp;volume=59&amp;issue=7%E2%80%938&amp;pages=580-595&amp;publication_year=2011&amp;author=Ugur%2CE&amp;author=Oztop%2CE&amp;author=Sahin%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Warren, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="Warren W (1984) Perceiving affordances: visual guidance of stair climbing. J Exp Psychol Hum Percept Perform 1" /><p class="c-article-references__text" id="ref-CR36">Warren W (1984) Perceiving affordances: visual guidance of stair climbing. J Exp Psychol Hum Percept Perform 10(5):683–703</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.10.5.683" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceiving%20affordances%3A%20visual%20guidance%20of%20stair%20climbing&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=10&amp;issue=5&amp;pages=683-703&amp;publication_year=1984&amp;author=Warren%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Warren, S. Whang, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Warren W, Whang S (1987) Visual guidance of walking through apertures: body-scaled information for affordances" /><p class="c-article-references__text" id="ref-CR37">Warren W, Whang S (1987) Visual guidance of walking through apertures: body-scaled information for affordances. J Exp Psychol Hum Percept Perform 13(3):371–383</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.13.3.371" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20guidance%20of%20walking%20through%20apertures%3A%20body-scaled%20information%20for%20affordances&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=13&amp;issue=3&amp;pages=371-383&amp;publication_year=1987&amp;author=Warren%2CW&amp;author=Whang%2CS">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-012-0216-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors would like to thank Mr. Laurent Bonnet for his help on the design of the virtual reality setup and experimental benchmark. This work was supported by the European community under FP7 FET—Open grant agreement no. 222107 NIW—Natural Interactive Walking.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">INRIA Rennes, Campus Universitaire de Beaulieu, 35042, Rennes, France</p><p class="c-article-author-affiliation__authors-list">Tony Regia-Corte, Maud Marchal, Gabriel Cirio &amp; Anatole Lécuyer</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Tony-Regia_Corte"><span class="c-article-authors-search__title u-h3 js-search-name">Tony Regia-Corte</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tony+Regia-Corte&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tony+Regia-Corte" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tony+Regia-Corte%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Maud-Marchal"><span class="c-article-authors-search__title u-h3 js-search-name">Maud Marchal</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Maud+Marchal&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Maud+Marchal" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Maud+Marchal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gabriel-Cirio"><span class="c-article-authors-search__title u-h3 js-search-name">Gabriel Cirio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gabriel+Cirio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gabriel+Cirio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gabriel+Cirio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Anatole-L_cuyer"><span class="c-article-authors-search__title u-h3 js-search-name">Anatole Lécuyer</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Anatole+L%C3%A9cuyer&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Anatole+L%C3%A9cuyer" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Anatole+L%C3%A9cuyer%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-012-0216-3/email/correspondent/c1/new">Tony Regia-Corte</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Perceiving%20affordances%20in%20virtual%20reality%3A%20influence%20of%20person%20and%20environmental%20properties%20in%20perception%20of%20standing%20on%20virtual%20grounds&amp;author=Tony%20Regia-Corte%20et%20al&amp;contentID=10.1007%2Fs10055-012-0216-3&amp;publication=1359-4338&amp;publicationDate=2012-10-02&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Regia-Corte, T., Marchal, M., Cirio, G. <i>et al.</i> Perceiving affordances in virtual reality: influence of person and environmental properties in perception of standing on virtual grounds.
                    <i>Virtual Reality</i> <b>17, </b>17–28 (2013). https://doi.org/10.1007/s10055-012-0216-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-012-0216-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-02-28">28 February 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-09-20">20 September 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-10-02">02 October 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-03">March 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-012-0216-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-012-0216-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Perception of affordances</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual perception</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Posture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Slanted surface</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Head-mounted display</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0216-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=216;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

