<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An augmented reality interface to contextual information"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In this paper, we report on a prototype augmented reality (AR) platform for accessing abstract information in real-world pervasive computing environments. Using this platform, objects, people, and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An augmented reality interface to contextual information"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-12-16"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In this paper, we report on a prototype augmented reality (AR) platform for accessing abstract information in real-world pervasive computing environments. Using this platform, objects, people, and the environment serve as contextual channels to more information. The user&#8217;s interest with respect to the environment is inferred from eye movement patterns, speech, and other implicit feedback signals, and these data are used for information filtering. The results of proactive context-sensitive information retrieval are augmented onto the view of a handheld or head-mounted display or uttered as synthetic speech. The augmented information becomes part of the user&#8217;s context, and if the user shows interest in the AR content, the system detects this and provides progressively more information. In this paper, we describe the first use of the platform to develop a pilot application, Virtual Laboratory Guide, and early evaluation results of this application."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-12-16"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="161"/>

    <meta name="prism.endingPage" content="173"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0183-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0183-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0183-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0183-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An augmented reality interface to contextual information"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2010/12/16"/>

    <meta name="citation_firstpage" content="161"/>

    <meta name="citation_lastpage" content="173"/>

    <meta name="citation_article_type" content="SI: Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0183-5"/>

    <meta name="DOI" content="10.1007/s10055-010-0183-5"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0183-5"/>

    <meta name="description" content="In this paper, we report on a prototype augmented reality (AR) platform for accessing abstract information in real-world pervasive computing environments. "/>

    <meta name="dc.creator" content="Antti Ajanki"/>

    <meta name="dc.creator" content="Mark Billinghurst"/>

    <meta name="dc.creator" content="Hannes Gamper"/>

    <meta name="dc.creator" content="Toni J&#228;rvenp&#228;&#228;"/>

    <meta name="dc.creator" content="Melih Kandemir"/>

    <meta name="dc.creator" content="Samuel Kaski"/>

    <meta name="dc.creator" content="Markus Koskela"/>

    <meta name="dc.creator" content="Mikko Kurimo"/>

    <meta name="dc.creator" content="Jorma Laaksonen"/>

    <meta name="dc.creator" content="Kai Puolam&#228;ki"/>

    <meta name="dc.creator" content="Teemu Ruokolainen"/>

    <meta name="dc.creator" content="Timo Tossavainen"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=User Model User-Adapt Interact; citation_title=Can eyes reveal interest?&#8212;implicit queries from gaze patterns; citation_author=A Ajanki, DR Hardoon, S Kaski, K Puolam&#228;ki, J Shawe-Taylor; citation_volume=19; citation_issue=4; citation_publication_date=2009; citation_pages=307-339; citation_doi=10.1007/s11257-009-9066-4; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A survey of augmented reality; citation_author=R Azuma; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=355-385; citation_id=CR2"/>

    <meta name="citation_reference" content="Baillot Y, Brown D, Julier S (2001) Authoring of physical models using mobile computers. In: Proceedings of the 5th IEEE international symposium on wearable computer. IEEE Computer Society, Washington, DC, USA, pp 39&#8211;46"/>

    <meta name="citation_reference" content="Bee N, Andr&#233; E (2008) Writing with your eye: a dwell time free writing system adapted to the nature of human eye gaze. In: PIT &#8217;08: proceedings of the 4th IEEE tutorial and research workshop on perception and interactive technologies for speech-based systems. Springer, Berlin, pp 111&#8211;122, doi:
                    10.1007/978-3-540-69369-7_13
                    
                  
                        "/>

    <meta name="citation_reference" content="Brzezowski S, Dunn CM, Vetter M (1996) Integrated portable system for suspect identification and tracking. In: DePersia AT, Yeager S, Ortiz S (eds) SPIE: surveillance and assessment technologies for law enforcement. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, USA, pp 24&#8211;35"/>

    <meta name="citation_reference" content="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for simulation set-up and control. In: Proceedings of the fifth conference on applied natural language processing, Washington, DC. pp 20&#8211;24"/>

    <meta name="citation_reference" content="citation_journal_title=Inf Retr; citation_title=Introduction to special issue on contextual information retrieval systems; citation_author=F Crestani, I Ruthven; citation_volume=10; citation_issue=2; citation_publication_date=2007; citation_pages=111-113; citation_doi=10.1007/s10791-007-9022-z; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Anal Mach Intell IEEE Trans; citation_title=Monoslam: real-time single camera slam; citation_author=A Davison, I Reid, N Molton, O Stasse; citation_volume=29; citation_issue=6; citation_publication_date=2007; citation_pages=1052-1067; citation_doi=10.1109/TPAMI.2007.1049; citation_id=CR8"/>

    <meta name="citation_reference" content="Farringdon J, Oni V (2000) Visual augmented memory (VAM). In: Proceedings of 4th international symposium on wearable computers. pp 167&#8211;168. doi:
                    10.1109/ISWC.2000.888484
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Personal Ubiquitous Comput; citation_title=A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment; citation_author=S Feiner, B MacIntyre, T H&#246;llerer, A Webster; citation_volume=1; citation_issue=4; citation_publication_date=1997; citation_pages=208-217; citation_id=CR10"/>

    <meta name="citation_reference" content="Hahn D, Beutler F, Hanebeck U (2005) Visual scene augmentation for enhanced human perception. In: International conference on informatics in control, automation &amp; robotics (ICINCO 2005). INSTICC Press, Barcelona, Spain, pp 146&#8211;153"/>

    <meta name="citation_reference" content="citation_journal_title=Trends Cogn Sci; citation_title=Eye movements in natural behavior; citation_author=M Hayhoe, D Ballard; citation_volume=9; citation_issue=4; citation_publication_date=2005; citation_pages=188-194; citation_doi=10.1016/j.tics.2005.02.009; citation_id=CR12"/>

    <meta name="citation_reference" content="Hendricksen K, Indulska J, Rakotonirainy A (2002) Modeling context information in pervasive computing systems. In: Proceedings of the first international conference on pervasive computing, pp 167&#8211;180"/>

    <meta name="citation_reference" content="Henrysson A, Ollila M (2004) Umar: ubiquitous mobile augmented reality. In: MUM &#8217;04: proceedings of the 3rd international conference on mobile and ubiquitous multimedia. ACM, New York, NY, USA, pp 41&#8211;45, doi:
                    10.1145/1052380.1052387
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Processing; citation_title=Importance of high-order n-gram models in morph-based speech recognition; citation_author=T Hirsim&#228;ki, J Pylkk&#246;nen, M Kurimo; citation_volume=17; citation_issue=4; citation_publication_date=2009; citation_pages=724-732; citation_doi=10.1109/TASL.2008.2012323; citation_id=CR15"/>

    <meta name="citation_reference" content="Hyrskykari A, Majaranta P, Aaltonen A, R&#228;ih&#228; KJ (2000) Design issues of idict: a gaze-assisted translation aid. In: Proceedings of ETRA 2000, eye tracking research and applications symposium. ACM Press, pp 9&#8211;14, 
                    http://www.cs.uta.fi/~curly/publications/ETRA2000-Hyrskykari.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="Iordanoglou C, Jonsson K, Kittler J, Matas J (2000) Wearable face recognition aid. In: Proceedings. 2000 IEEE international conference on acoustics, speech, and signal processing (ICASSP &#8217;00). pp 2365&#8211;2368"/>

    <meta name="citation_reference" content="ISO/IEC (2002) Information technology&#8212;multimedia content description interface&#8212;part 3: visual. 15938&#8211;3:2002(E)"/>

    <meta name="citation_reference" content="J&#228;rvenp&#228;&#228; T, Aaltonen V (2008) Photonics in multimedia II. In: Proceedings of SPIE, vol 7001, SPIE, Bellingham, WA, chap Compact near-to-eye display with integrated gaze tracker. pp 700106&#8211;1&#8211;700106&#8211;8"/>

    <meta name="citation_reference" content="Joachims T, Granka L, Pan B, Hembrooke H, Gay G (2005) Accurately interpreting clickthrough data as implicit feedback. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 154&#8211;161"/>

    <meta name="citation_reference" content="Julier S, Lanzagorta M, Baillot Y, Rosenblum L, Feiner S, Hollerer T, Sestito S (2000) Information filtering for mobile augmented reality. In: Augmented reality, 2000. (ISAR 2000). Proceedings. IEEE and ACM international symposium on. pp 3&#8211;11, doi:
                    10.1109/ISAR.2000.880917
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Information filtering for mobile augmented reality; citation_author=S Julier, Y Baillot, D Brown, M Lanzagorta; citation_volume=22; citation_publication_date=2002; citation_pages=12-15; citation_id=CR22"/>

    <meta name="citation_reference" content="Kandemir M, Saarinen VM, Kaski S (2010) Inferring object relevance from gaze in dynamic scenes. In: ETRA 2010: proceedings of the 2010 symposium on eye-tracking research &amp; applications. ACM, New York, NY, USA, pp 105&#8211;108"/>

    <meta name="citation_reference" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR&#8217;07). IEEE computer society, Washington, DC, USA, pp 1&#8211;10"/>

    <meta name="citation_reference" content="Klein G, Murray D (2008) Compositing for small cameras. In: Proceedings of seventh IEEE and ACM international symposium on mixed and augmented reality (ISMAR&#8217;08). IEEE Computer Society, Washington, DC, USA, pp 57&#8211;60"/>

    <meta name="citation_reference" content="Kozma L, Klami A, Kaski S (2009) GaZIR: gaze-based zooming interface for image retrieval. In: Proceedings of 11th conference on multimodal interfaces and the sixth workshop on machine learning for multimodal interaction (ICMI-MLMI). ACM, New York, NY, USA, pp 305&#8211;312"/>

    <meta name="citation_reference" content="citation_journal_title=Prog Retin Eye Res; citation_title=Eye movements and the control of actions in everyday life; citation_author=MF Land; citation_volume=25; citation_issue=3; citation_publication_date=2006; citation_pages=296-324; citation_doi=10.1016/j.preteyeres.2006.01.002; citation_id=CR27"/>

    <meta name="citation_reference" content="Lee R, Kwon YJ, Sumiya K (2009) Layer-based media integration for mobile mixed-reality applications. In: International conference on next generation mobile applications, services and technologies. IEEE Computer Society, Los Alamitos, CA, pp 58&#8211;63"/>

    <meta name="citation_reference" content="Lowe D (1999) Object recognition from local scale-invariant features. In: Computer vision, 1999. The proceedings of the seventh IEEE international conference on, vol 2. pp 1150&#8211;1157, doi:
                    10.1109/ICCV.1999.790410
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Psychol J; citation_title=Hands free interaction with virtual information in a real environment: eye gaze as an interaction tool in an augmented reality system; citation_author=S Nilsson, T Gustafsson, P Carleberg; citation_volume=7; citation_issue=2; citation_publication_date=2009; citation_pages=175-196; citation_id=CR30"/>

    <meta name="citation_reference" content="Oyekoya O, Stentiford F (2006) Perceptual image retrieval using eye movements. In: International workshop on intelligent computing in pattern analysis/synthesis. Advances in machine vision, image processing, and pattern analysis. Springer, Xi&#8217;an, China, pp 281&#8211;289"/>

    <meta name="citation_reference" content="Park H, Lee S, Choi J (2008) Wearable augmented reality system using gaze interaction. In: Proceedings of the 2008 7th IEEE/ACM international symposium on mixed and augmented reality, vol 00. IEEE Computer Society, pp 175&#8211;176"/>

    <meta name="citation_reference" content="Pentland A (1998) Wearable Intelligence. Exploring Intelligence; Scientific American Presents"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Looking at people: sensing for ubiquitous and wearable computing; citation_author=A Pentland; citation_volume=22; citation_issue=1; citation_publication_date=2000; citation_pages=107-119; citation_doi=10.1109/34.824823; citation_id=CR34"/>

    <meta name="citation_reference" content="Pfeiffer T, Latoschik ME, Wachsmuth I (2008) Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments. J Virtual Real Broadcast 5(16). urn:nbn:de:0009-6-16605, ISSN 1860-2037"/>

    <meta name="citation_reference" content="Puolam&#228;ki K, Saloj&#228;rvi J, Savia E, Simola J, Kaski S (2005) Combining eye movements and collaborative filtering for proactive information retrieval. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 146&#8211;153"/>

    <meta name="citation_reference" content="Pylv&#228;n&#228;inen T, J&#228;rvenp&#228;&#228; T, Nummela V (2008) Gaze tracking for near to eye displays. In: Proceedings of the 18th international conference on artificial reality and telexistence (ICAT 2008), Yokohama, Japan. pp 5&#8211;11"/>

    <meta name="citation_reference" content="Qvarfordt P, Zhai S (2005) Conversing with the user based on eye-gaze patterns. In: CHI &#8217;05: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, NY, USA, pp 221&#8211;230, doi:
                    10.1145/1054972.1055004
                    
                  
                        "/>

    <meta name="citation_reference" content="Rauhala M, Gunnarsson AS, Henrysson A (2006) A novel interface to sensor networks using handheld augmented reality. In: MobileHCI &#8217;06: proceedings of the 8th conference on human&#8211;computer interaction with mobile devices and services. ACM, New York, NY, USA, pp 145&#8211;148, doi:
                    10.1145/1152215.1152245
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Inf Manage; citation_title=Speech recognition in the human-computer interface; citation_author=CM Rebman, MW Aiken, CG Cegielski; citation_volume=40; citation_issue=6; citation_publication_date=2003; citation_pages=509-519; citation_doi=10.1016/S0378-7206(02)00067-8; citation_id=CR40"/>

    <meta name="citation_reference" content="Rekimoto J, Ayatsuka Y, Hayashi K (1998) Augment-able reality: situated communication through physical and digital spaces. In: Proceedings of the 2nd IEEE international symposium on wearable computers. IEEE Computer Society, Washington, DC, USA, pp 68&#8211;75"/>

    <meta name="citation_reference" content="Singletary BA, Starner TE (2001) Symbiotic interfaces for wearable face recognition. In: Proceedings of HCI international 2001 workshop on wearable computing, New Orleans, LA. pp 813&#8211;817"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoperators Virtual Environ; citation_title=Augmented reality through wearable computing; citation_author=T Starner, S Mann, B Rhodes, J Levine, J Healey, D Kirsch, RW Picard, A Pentland; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=452-460; citation_id=CR43"/>

    <meta name="citation_reference" content="Sun Y, Prendinger H, Shi Y, Chen F, Chung V, Ishizuka M (2008) The hinge between input and output: understanding the multimodal input fusion results in an agent-based multimodal presentation system. In: Conference on human factors in computing systems (CHI &#8217;08), Florence, Italy. pp 3483&#8211;3488"/>

    <meta name="citation_reference" content="Tanriverdi V, Jacob R (2000) Interacting with eye movements in virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, p 272"/>

    <meta name="citation_reference" content="Tomasi C, Kanade T (1991) Detection and tracking of point features. Tech. Rep. CMU-CS-91-132, Carnegie Mellon University"/>

    <meta name="citation_reference" content="Turpin A, Scholer F (2006) User performance versus precision measures for simple search tasks. In: SIGIR &#8217;06: proceedings of the international ACM SIGIR conference on research and development in information retrieval. ACM, New York, NY, pp 11&#8211;18"/>

    <meta name="citation_reference" content="Vertegaal R (2002) Designing attentive interfaces. In: Proceedings of the 2002 symposium on eye tracking research &amp; applications. ACM, p 30"/>

    <meta name="citation_reference" content="Viola P, Jones M (2001) Rapid object detection using a boosted cascade of simple features. In: IEEE Computer Society conference on computer vision and pattern recognition (CVPR&#8217;01), Kauai, Hawaii, pp 511&#8211;518"/>

    <meta name="citation_reference" content="Wang H, Tan CC, Li Q (2008) Snoogle: a search engine for the physical world. In: Proceedings of the 27th conference on computer communications (IEEE INFOCOM), pp 1382&#8211;1390"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Fast hands-free writing by gaze direction; citation_author=DJ Ward, DJC MacKay; citation_volume=418; citation_issue=6900; citation_publication_date=2002; citation_pages=838; citation_doi=10.1038/418838a; citation_id=CR51"/>

    <meta name="citation_reference" content="Yamagishi J, Usabaev B, King S, Watts O, Dines J, Tian J, Hu R, Guan Y, Oura K, Tokuda K, Karhila R, Kurimo M (2009) Thousands of voices for HMM-based speech synthesis. In: Proceedings of the 10th annual conference of the international speech communication association, INTERSPEECH 2009, ISCA, Brighton, UK"/>

    <meta name="citation_reference" content="Yap KK, Srinivasan V, Motani M (2005) MAX: human-centric search of the physical world. In: Proceedings of the 3rd international conference on embedded networked sensor systems. ACM, pp 166&#8211;179"/>

    <meta name="citation_author" content="Antti Ajanki"/>

    <meta name="citation_author_email" content="antti.ajanki@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Mark Billinghurst"/>

    <meta name="citation_author_email" content="mark.billinghurst@canterbury.ac.nz"/>

    <meta name="citation_author_institution" content="The Human Interface Technology Laboratory New Zealand (HIT Lab NZ), University of Canterbury, Christchurch, New Zealand"/>

    <meta name="citation_author" content="Hannes Gamper"/>

    <meta name="citation_author_email" content="hannes.gamper@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Media Technology, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Toni J&#228;rvenp&#228;&#228;"/>

    <meta name="citation_author_email" content="toni.jarvenpaa@nokia.com"/>

    <meta name="citation_author_institution" content="Nokia Research Center, Tampere, Finland"/>

    <meta name="citation_author" content="Melih Kandemir"/>

    <meta name="citation_author_email" content="melih.kandemir@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Samuel Kaski"/>

    <meta name="citation_author_email" content="samuel.kaski@tkk.fi"/>

    <meta name="citation_author_institution" content="Aalto University and University of Helsinki, Helsinki Institute for Information Technology HIIT, Helsinki, Finland"/>

    <meta name="citation_author" content="Markus Koskela"/>

    <meta name="citation_author_email" content="markus.koskela@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Mikko Kurimo"/>

    <meta name="citation_author_email" content="mikko.kurimo@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Jorma Laaksonen"/>

    <meta name="citation_author_email" content="jorma.laaksonen@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Kai Puolam&#228;ki"/>

    <meta name="citation_author_email" content="kai.puolamaki@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Media Technology, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Teemu Ruokolainen"/>

    <meta name="citation_author_email" content="teemu.ruokolainen@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Information and Computer Science, Aalto University, Espoo, Finland"/>

    <meta name="citation_author" content="Timo Tossavainen"/>

    <meta name="citation_author_email" content="timo.tossavainen@tkk.fi"/>

    <meta name="citation_author_institution" content="Department of Media Technology, Aalto University, Espoo, Finland"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0183-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0183-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An augmented reality interface to contextual information"/>
        <meta property="og:description" content="In this paper, we report on a prototype augmented reality (AR) platform for accessing abstract information in real-world pervasive computing environments. Using this platform, objects, people, and the environment serve as contextual channels to more information. The user’s interest with respect to the environment is inferred from eye movement patterns, speech, and other implicit feedback signals, and these data are used for information filtering. The results of proactive context-sensitive information retrieval are augmented onto the view of a handheld or head-mounted display or uttered as synthetic speech. The augmented information becomes part of the user’s context, and if the user shows interest in the AR content, the system detects this and provides progressively more information. In this paper, we describe the first use of the platform to develop a pilot application, Virtual Laboratory Guide, and early evaluation results of this application."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An augmented reality interface to contextual information | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0183-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Gaze tracking, Information retrieval, Machine learning, Pattern recognition","kwrd":["Augmented_reality","Gaze_tracking","Information_retrieval","Machine_learning","Pattern_recognition"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0183-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0183-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=183;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0183-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An augmented reality interface to contextual information
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0183-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0183-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-12-16" itemprop="datePublished">16 December 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An augmented reality interface to contextual information</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Antti-Ajanki" data-author-popup="auth-Antti-Ajanki" data-corresp-id="c1">Antti Ajanki<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mark-Billinghurst" data-author-popup="auth-Mark-Billinghurst">Mark Billinghurst</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canterbury" /><meta itemprop="address" content="grid.21006.35, 0000000121791970, The Human Interface Technology Laboratory New Zealand (HIT Lab NZ), University of Canterbury, Christchurch, New Zealand" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hannes-Gamper" data-author-popup="auth-Hannes-Gamper">Hannes Gamper</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Media Technology, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Toni-J_rvenp__" data-author-popup="auth-Toni-J_rvenp__">Toni Järvenpää</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nokia Research Center" /><meta itemprop="address" content="grid.6533.3, 0000000123048515, Nokia Research Center, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Melih-Kandemir" data-author-popup="auth-Melih-Kandemir">Melih Kandemir</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Samuel-Kaski" data-author-popup="auth-Samuel-Kaski">Samuel Kaski</a></span><sup class="u-js-hide"><a href="#Aff5">5</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Helsinki Institute for Information Technology HIIT" /><meta itemprop="address" content="grid.500231.5, 0000 0004 0530 9461, Aalto University and University of Helsinki, Helsinki Institute for Information Technology HIIT, Helsinki, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Markus-Koskela" data-author-popup="auth-Markus-Koskela">Markus Koskela</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mikko-Kurimo" data-author-popup="auth-Mikko-Kurimo">Mikko Kurimo</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jorma-Laaksonen" data-author-popup="auth-Jorma-Laaksonen">Jorma Laaksonen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kai-Puolam_ki" data-author-popup="auth-Kai-Puolam_ki">Kai Puolamäki</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Media Technology, Aalto University, Espoo, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Teemu-Ruokolainen" data-author-popup="auth-Teemu-Ruokolainen">Teemu Ruokolainen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Information and Computer Science, Aalto University, Espoo, Finland" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Timo-Tossavainen" data-author-popup="auth-Timo-Tossavainen">Timo Tossavainen</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Aalto University" /><meta itemprop="address" content="grid.5373.2, 0000000108389418, Department of Media Technology, Aalto University, Espoo, Finland" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">161</span>–<span itemprop="pageEnd">173</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1524 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">36 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0183-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In this paper, we report on a prototype augmented reality (AR) platform for accessing abstract information in real-world pervasive computing environments. Using this platform, objects, people, and the environment serve as contextual channels to more information. The user’s interest with respect to the environment is inferred from eye movement patterns, speech, and other implicit feedback signals, and these data are used for information filtering. The results of proactive context-sensitive information retrieval are augmented onto the view of a handheld or head-mounted display or uttered as synthetic speech. The augmented information becomes part of the user’s context, and if the user shows interest in the AR content, the system detects this and provides progressively more information. In this paper, we describe the first use of the platform to develop a pilot application, Virtual Laboratory Guide, and early evaluation results of this application.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In pervasive computing systems, there is often a need to provide users with a way to access and search through ubiquitous information associated with real-world objects and locations. Technology such as augmented reality (AR) allows virtual information to be overlaid on the users’ environment (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" href="/article/10.1007/s10055-010-0183-5#ref-CR2" id="ref-link-section-d51036e533">1997</a>) and can be used as a way to view contextual information. However, there are interesting research questions that need to be addressed in terms of how to know when to present information to the user and how to allow the user to interact with it. As Hendricksen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Hendricksen K, Indulska J, Rakotonirainy A (2002) Modeling context information in pervasive computing systems. In: Proceedings of the first international conference on pervasive computing, pp 167–180" href="/article/10.1007/s10055-010-0183-5#ref-CR13" id="ref-link-section-d51036e536">2002</a>) point out, pervasive computing applications need to place few demands on the user’s attention and be sensitive to context.</p><p>We are interested in the problem of how to efficiently retrieve and present contextual information in real-world environments where (i) it is hard to formulate explicit search queries and (ii) the temporal and spatial context provides potentially useful search cues. In other words, the user may not have an explicit query in mind or may not even be searching, and the information relevant to him or her is likely to be related to objects in the surrounding environment or other current context cues.</p><p>The scenario is that the user wears data glasses and sensors measuring his or her actions, including gaze patterns and further, the visual focus of attention. Using the implicit measurements about the user’s interactions with the environment, we can infer which of the potential search cues (objects, people) are relevant for the user at the current point of time, and augment retrieved information in the user’s view (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Information retrieval (IR) is done in a loop where relevance of already augmented information (Augmented Reality, AR) and of the objects in the scene is inferred from user’s observed interaction with them, then more information is retrieved given any contextual cues and the inferred relevance, and new information is augmented</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>This new augmented information forms part of the user’s visual context, and once the user’s interaction with the new context is measured, more fine-grained inferences about relevance can be made, and the search refined. Retrieval with such a system could be described as retrieval by zooming through augmented reality, analogously to text entry by zooming through predicted alternative textual continuations (Dasher system, Ward and MacKay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ward DJ, MacKay DJC (2002) Fast hands-free writing by gaze direction. Nature 418(6900):838" href="/article/10.1007/s10055-010-0183-5#ref-CR51" id="ref-link-section-d51036e568">2002</a>) or image retrieval by zooming deeper into an image collection (Gazir system, Kozma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kozma L, Klami A, Kaski S (2009) GaZIR: gaze-based zooming interface for image retrieval. In: Proceedings of 11th conference on multimodal interfaces and the sixth workshop on machine learning for multimodal interaction (ICMI-MLMI). ACM, New York, NY, USA, pp 305–312" href="/article/10.1007/s10055-010-0183-5#ref-CR26" id="ref-link-section-d51036e571">2009</a>).</p><p>To realize this scenario, several elements are needed. First, objects and people should be recognized as potential cues with pattern recognition methods. The relevance of these cues needs to be inferred from gaze patterns and other implicit feedback using machine learning methods. Second, context-sensitive information retrieval needs to operate proactively given the relevant cues. Finally, the retrieved information needs to be overlaid on the user’s view with AR techniques and modern display devices. All this should operate such that the users are distracted as little as possible from their real-work tasks.</p><p>In this paper, we present a hardware and software platform we have developed which meets these needs, and a demonstration prototype application created using the platform. This application is a <i>Virtual Laboratory Guide</i>, which will help a visitor to a university department find out about (i) teachers and teaching and (ii) researchers and research projects. The Virtual Laboratory Guide presents context-sensitive virtual information about the persons, offices, and artifacts that appear as needed and disappear when not attended to. As far as we know, the implementation of gaze-based relevance estimation for contextual information filtering, the use of augmented audio reality, and their evaluations are the main novel contributions of this paper.</p><p>In the remainder of this paper, we first review earlier related work and describe the lessons learned from this which our research builds on. Then, we discuss the hardware and software platform we developed and finally present the Virtual Laboratory Guide application and a user evaluation of the technology.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>In developing the wearable pervasive information retrieval system described above, our work builds on earlier research on augmented reality and contextual information retrieval.</p><h3 class="c-article__sub-heading" id="Sec3">Mobile augmented reality</h3><p>Augmented Reality (AR) involves the seamless overlay of virtual imagery on the real world (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" href="/article/10.1007/s10055-010-0183-5#ref-CR2" id="ref-link-section-d51036e599">1997</a>). In recent years, wearable computers (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Feiner S, MacIntyre B, Höllerer T, Webster A (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. Personal Ubiquitous Comput 1(4):208–217" href="/article/10.1007/s10055-010-0183-5#ref-CR10" id="ref-link-section-d51036e602">1997</a>) and even mobile phones (Henrysson and Ollila <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Henrysson A, Ollila M (2004) Umar: ubiquitous mobile augmented reality. In: MUM ’04: proceedings of the 3rd international conference on mobile and ubiquitous multimedia. ACM, New York, NY, USA, pp 41–45, doi:&#xA;                    10.1145/1052380.1052387&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR14" id="ref-link-section-d51036e605">2004</a>) have been used to provide a mobile AR experience. Using these platforms, researchers have explored how AR interfaces can be used to provide an intuitive user experience for pervasive computing applications. For example, Rauhala et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rauhala M, Gunnarsson AS, Henrysson A (2006) A novel interface to sensor networks using handheld augmented reality. In: MobileHCI ’06: proceedings of the 8th conference on human–computer interaction with mobile devices and services. ACM, New York, NY, USA, pp 145–148, doi:&#xA;                    10.1145/1152215.1152245&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR39" id="ref-link-section-d51036e608">2006</a>) have developed a mobile phone-based AR interface which communicates with ubiquitous sensors to show the temperature distribution of building walls.</p><p>Previous researchers have used wearable and mobile augmented reality systems to display contextual cues about the surrounding environment. For example, the Touring Machine (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Feiner S, MacIntyre B, Höllerer T, Webster A (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. Personal Ubiquitous Comput 1(4):208–217" href="/article/10.1007/s10055-010-0183-5#ref-CR10" id="ref-link-section-d51036e614">1997</a>) added virtual tags to real university buildings showing which departments were in the buildings. A layer-based integration model for encompassing diverse types of media and functionality in a unified mobile AR browsing system is proposed in Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee R, Kwon YJ, Sumiya K (2009) Layer-based media integration for mobile mixed-reality applications. In: International conference on next generation mobile applications, services and technologies. IEEE Computer Society, Los Alamitos, CA, pp 58–63" href="/article/10.1007/s10055-010-0183-5#ref-CR28" id="ref-link-section-d51036e617">2009</a>). A similar effect is created using the commercially available Layar
<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> or Wikitude
<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> applications for mobile phones, both of which provide virtual information tags on the real world.</p><p>These interfaces highlight the need to filter information according to the user’s interest and present it in an uncluttered way so that it is easy to interact with. In most cases, mobile AR interfaces require explicit user input specifying the topics of interest to the user. In our research, we want to develop a system that uses unobtrusive implicit input from the user to present relevant contextual information.</p><h3 class="c-article__sub-heading" id="Sec4">Mobile face recognition</h3><p>Gaze and face recognition provides important implicit cues about where the user is looking and whom he is interested in. Starner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Starner T, Mann S, Rhodes B, Levine J, Healey J, Kirsch D, Picard RW, Pentland A (1997) Augmented reality through wearable computing. Presence Teleoperators Virtual Environ 6(4):452–460" href="/article/10.1007/s10055-010-0183-5#ref-CR43" id="ref-link-section-d51036e654">1997</a>) and Pentland (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Pentland A (1998) Wearable Intelligence. Exploring Intelligence; Scientific American Presents" href="/article/10.1007/s10055-010-0183-5#ref-CR33" id="ref-link-section-d51036e657">1998</a>) describe on a conceptual level how wearable computers can be used as an ideal platform for mobile augmented reality and how they can enable many applications, including face recognition. Pentland (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Pentland A (2000) Looking at people: sensing for ubiquitous and wearable computing. IEEE Trans Pattern Anal Mach Intell 22(1):107–119. doi:&#xA;                    10.1109/34.824823&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR34" id="ref-link-section-d51036e660">2000</a>) similarly points out how recognizing people is a key goal in computer vision research and provides an overview of previous work in person identification.</p><p>Several groups have produced prototype wearable face recognition applications for identifying people (Starner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Starner T, Mann S, Rhodes B, Levine J, Healey J, Kirsch D, Picard RW, Pentland A (1997) Augmented reality through wearable computing. Presence Teleoperators Virtual Environ 6(4):452–460" href="/article/10.1007/s10055-010-0183-5#ref-CR43" id="ref-link-section-d51036e666">1997</a>; Farringdon and Oni <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Farringdon J, Oni V (2000) Visual augmented memory (VAM). In: Proceedings of 4th international symposium on wearable computers. pp 167–168. doi:&#xA;                    10.1109/ISWC.2000.888484&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR9" id="ref-link-section-d51036e669">2000</a>; Brzezowski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Brzezowski S, Dunn CM, Vetter M (1996) Integrated portable system for suspect identification and tracking. In: DePersia AT, Yeager S, Ortiz S (eds) SPIE: surveillance and assessment technologies for law enforcement. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, USA, pp 24–35" href="/article/10.1007/s10055-010-0183-5#ref-CR5" id="ref-link-section-d51036e672">1996</a>; Iordanoglou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Iordanoglou C, Jonsson K, Kittler J, Matas J (2000) Wearable face recognition aid. In: Proceedings. 2000 IEEE international conference on acoustics, speech, and signal processing (ICASSP ’00). pp 2365–2368" href="/article/10.1007/s10055-010-0183-5#ref-CR17" id="ref-link-section-d51036e675">2000</a>). For example, Singletary and Starner (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Singletary BA, Starner TE (2001) Symbiotic interfaces for wearable face recognition. In: Proceedings of HCI international 2001 workshop on wearable computing, New Orleans, LA. pp 813–817" href="/article/10.1007/s10055-010-0183-5#ref-CR42" id="ref-link-section-d51036e678">2001</a>) have demonstrated a wearable face recognition system that uses social engagement cues to trigger the recognition process. They were able to achieve more than 90% recognition accuracy on tests with 300 sample faces. These systems have shown that it is possible to perform face recognition on a wearable platform; however, there has been little research on the use of AR and face recognition to trigger and present contextual cues in a pervasive computing setting. Our research uses face recognition to trigger contextual cues for finding context-sensitive information associated with the faces seen; the information is then shown using an AR interface.</p><h3 class="c-article__sub-heading" id="Sec5">Feedback in contextual information retrieval</h3><p>Information retrieval (IR) is an established field of study in which the fundamental question is how to find documents from a large corpus that best match the given query. The most visible everyday application of IR is Internet search. The query is typically a list of textual keywords or a collection of example items.</p><p>Object location search engines such as MAX (Yap et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Yap KK, Srinivasan V, Motani M (2005) MAX: human-centric search of the physical world. In: Proceedings of the 3rd international conference on embedded networked sensor systems. ACM, pp 166–179" href="/article/10.1007/s10055-010-0183-5#ref-CR53" id="ref-link-section-d51036e691">2005</a>) or Snoogle (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wang H, Tan CC, Li Q (2008) Snoogle: a search engine for the physical world. In: Proceedings of the 27th conference on computer communications (IEEE INFOCOM), pp 1382–1390" href="/article/10.1007/s10055-010-0183-5#ref-CR50" id="ref-link-section-d51036e694">2008</a>) let the user search for physical locations of objects that have been tagged with RFID transponders. However, they require that the user explicitly enters the object ID or terms describing the object as a query to the search engine.</p><p>Formulating a good search query can be difficult because of the cognitive effort required (Turpin and Scholer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Turpin A, Scholer F (2006) User performance versus precision measures for simple search tasks. In: SIGIR ’06: proceedings of the international ACM SIGIR conference on research and development in information retrieval. ACM, New York, NY, pp 11–18" href="/article/10.1007/s10055-010-0183-5#ref-CR47" id="ref-link-section-d51036e700">2006</a>) or limitations of mobile input devices. A common way of decreasing the dependency on search queries is to utilize feedback from the user to guide the search. Feedback can be explicit, like in search engines which ask the user to rate documents in an initial result set, or it can be implicit feedback that is collected by observing the context and behavior of user.</p><p>In general, the use of context instead of or in addition to query-based IR is called contextual information retrieval. A simple example, again, is an Internet search engine that customizes the search result based on the location of the user (e.g., show only nearby restaurants). Contextual information retrieval takes into account the task the user is currently involved in, such as shopping or medical diagnosis, or the context expressed within a given domain, such as locations of the restaurants—see Crestani and Ruthven (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Crestani F, Ruthven I (2007) Introduction to special issue on contextual information retrieval systems. Inf Retr 10(2):111–113" href="/article/10.1007/s10055-010-0183-5#ref-CR7" id="ref-link-section-d51036e706">2007</a>) for a recent overview.</p><p>An obvious case where context sensitiveness is useful is when the user wants to retrieve something closely related to the current situation, such as contact information of a conversation partner. Context can also substitute typed queries when the user does not remember the correct keywords or when the ability to type search terms is limited because of restrictions of mobile devices. Even when the conventional query is available, it can also be helpful to restrict the search space and thus allow the search engine to return correct results with fewer iterations.</p><h3 class="c-article__sub-heading" id="Sec6">Implicit speech-based input</h3><p>An interface could monitor the user’s speech or discussion in order to extract additional information about the user’s interest by using automatic speech recognition. Speech recognition in human–computer interfaces has been a subject of extensive study (for a review, see Rebman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Rebman CM Jr., Aiken MW, Cegielski CG (2003) Speech recognition in the human-computer interface. Inf Manage 40(6):509–519. doi:&#xA;                    10.1016/S0378-7206(02)00067-8&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR40" id="ref-link-section-d51036e721">2003</a>). For example, Hahn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hahn D, Beutler F, Hanebeck U (2005) Visual scene augmentation for enhanced human perception. In: International conference on informatics in control, automation &amp; robotics (ICINCO 2005). INSTICC Press, Barcelona, Spain, pp 146–153" href="/article/10.1007/s10055-010-0183-5#ref-CR11" id="ref-link-section-d51036e724">2005</a>) has augmented the observed sound information by a model of attention based on measuring the head posture, Cohen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for simulation set-up and control. In: Proceedings of the fifth conference on applied natural language processing, Washington, DC. pp 20–24" href="/article/10.1007/s10055-010-0183-5#ref-CR6" id="ref-link-section-d51036e727">1997</a>) by recognizing the gestures, and Sun et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Sun Y, Prendinger H, Shi Y, Chen F, Chung V, Ishizuka M (2008) The hinge between input and output: understanding the multimodal input fusion results in an agent-based multimodal presentation system. In: Conference on human factors in computing systems (CHI ’08), Florence, Italy. pp 3483–3488" href="/article/10.1007/s10055-010-0183-5#ref-CR44" id="ref-link-section-d51036e730">2008</a>) by measuring gaze on a computer display. However, as far as we are aware, the idea of combining gaze-based and speech-based implicit input about the interests and context in interaction with persons and objects in the real world is novel.</p><h3 class="c-article__sub-heading" id="Sec7">Relevance estimation from gaze</h3><p>Studies of eye movements during natural behavior, such as driving a car or playing ball games, have shown that eye movements are highly task dependent and that the gaze is mostly directed toward objects that are relevant for the task (Hayhoe and Ballard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hayhoe M, Ballard D (2005) Eye movements in natural behavior. Trends Cogn Sci 9(4):188–194" href="/article/10.1007/s10055-010-0183-5#ref-CR12" id="ref-link-section-d51036e741">2005</a>; Land <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Land MF (2006) Eye movements and the control of actions in everyday life. Prog Retin Eye Res 25(3):296–324" href="/article/10.1007/s10055-010-0183-5#ref-CR27" id="ref-link-section-d51036e744">2006</a>). Tanriverdi and Jacob (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Tanriverdi V, Jacob R (2000) Interacting with eye movements in virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, p 272" href="/article/10.1007/s10055-010-0183-5#ref-CR45" id="ref-link-section-d51036e747">2000</a>) studied the use of gaze as a measure of interest in virtual environments. Eye tracking has been used as source of implicit feedback for inferring relevance in text (Joachims et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Joachims T, Granka L, Pan B, Hembrooke H, Gay G (2005) Accurately interpreting clickthrough data as implicit feedback. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 154–161" href="/article/10.1007/s10055-010-0183-5#ref-CR20" id="ref-link-section-d51036e750">2005</a>; Puolamäki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Puolamäki K, Salojärvi J, Savia E, Simola J, Kaski S (2005) Combining eye movements and collaborative filtering for proactive information retrieval. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 146–153" href="/article/10.1007/s10055-010-0183-5#ref-CR36" id="ref-link-section-d51036e753">2005</a>), and image (Kozma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kozma L, Klami A, Kaski S (2009) GaZIR: gaze-based zooming interface for image retrieval. In: Proceedings of 11th conference on multimodal interfaces and the sixth workshop on machine learning for multimodal interaction (ICMI-MLMI). ACM, New York, NY, USA, pp 305–312" href="/article/10.1007/s10055-010-0183-5#ref-CR26" id="ref-link-section-d51036e757">2009</a>; Oyekoya and Stentiford <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Oyekoya O, Stentiford F (2006) Perceptual image retrieval using eye movements. In: International workshop on intelligent computing in pattern analysis/synthesis. Advances in machine vision, image processing, and pattern analysis. Springer, Xi’an, China, pp 281–289" href="/article/10.1007/s10055-010-0183-5#ref-CR31" id="ref-link-section-d51036e760">2006</a>) retrieval applications on a conventional desktop computer. In a recent work, Ajanki et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ajanki A, Hardoon DR, Kaski S, Puolamäki K, Shawe-Taylor J (2009) Can eyes reveal interest?—implicit queries from gaze patterns. User Model User-Adapt Interact 19(4):307–339. doi:&#xA;                    10.1007/s11257-009-9066-4&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR1" id="ref-link-section-d51036e763">2009</a>) constructed implicit queries for textual search from reading patterns on documents. These results indicate that gaze direction is a useful information source for inferring the focus of attention and that relevance information can be extracted even without any conscious effort from the user. The Virtual Laboratory Guide implements an attentive interface that, in the taxonomy of Vertegaal (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vertegaal R (2002) Designing attentive interfaces. In: Proceedings of the 2002 symposium on eye tracking research &amp; applications. ACM, p 30" href="/article/10.1007/s10055-010-0183-5#ref-CR48" id="ref-link-section-d51036e766">2002</a>), monitors the user implicitly and tries to model the behavior of the user in order to reduce his attentive load.</p><p>Gaze-controlled augmented reality user interfaces are an emerging research field. So far, the research has been concentrated on the problem of explicitly selecting objects with gaze (Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Park H, Lee S, Choi J (2008) Wearable augmented reality system using gaze interaction. In: Proceedings of the 2008 7th IEEE/ACM international symposium on mixed and augmented reality, vol 00. IEEE Computer Society, pp 175–176" href="/article/10.1007/s10055-010-0183-5#ref-CR32" id="ref-link-section-d51036e772">2008</a>; Pfeiffer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pfeiffer T, Latoschik ME, Wachsmuth I (2008) Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments. J Virtual Real Broadcast 5(16). urn:nbn:de:0009-6-16605, ISSN 1860-2037" href="/article/10.1007/s10055-010-0183-5#ref-CR35" id="ref-link-section-d51036e775">2008</a>; Nilsson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Nilsson S, Gustafsson T, Carleberg P (2009) Hands free interaction with virtual information in a real environment: eye gaze as an interaction tool in an augmented reality system. Psychol J 7(2):175–196" href="/article/10.1007/s10055-010-0183-5#ref-CR30" id="ref-link-section-d51036e778">2009</a>). The conceptual idea of using gaze to monitor the user’s interest implicitly in a mobile AR system has been presented previously (Nilsson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Nilsson S, Gustafsson T, Carleberg P (2009) Hands free interaction with virtual information in a real environment: eye gaze as an interaction tool in an augmented reality system. Psychol J 7(2):175–196" href="/article/10.1007/s10055-010-0183-5#ref-CR30" id="ref-link-section-d51036e781">2009</a>; Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Park H, Lee S, Choi J (2008) Wearable augmented reality system using gaze interaction. In: Proceedings of the 2008 7th IEEE/ACM international symposium on mixed and augmented reality, vol 00. IEEE Computer Society, pp 175–176" href="/article/10.1007/s10055-010-0183-5#ref-CR32" id="ref-link-section-d51036e784">2008</a>), but until now a working system has not been demonstrated or evaluated.</p><p>Gaze tracking provides an alternative, hands-free means of input entry for a ubiquitous device. One way to give gaze-based input is to make selections by explicit looking. This has been successfully applied for text entry (Ward and MacKay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ward DJ, MacKay DJC (2002) Fast hands-free writing by gaze direction. Nature 418(6900):838" href="/article/10.1007/s10055-010-0183-5#ref-CR51" id="ref-link-section-d51036e790">2002</a>; Bee and André <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bee N, André E (2008) Writing with your eye: a dwell time free writing system adapted to the nature of human eye gaze. In: PIT ’08: proceedings of the 4th IEEE tutorial and research workshop on perception and interactive technologies for speech-based systems. Springer, Berlin, pp 111–122, doi:&#xA;                    10.1007/978-3-540-69369-7_13&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR4" id="ref-link-section-d51036e793">2008</a>; Hyrskykari et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Hyrskykari A, Majaranta P, Aaltonen A, Räihä KJ (2000) Design issues of idict: a gaze-assisted translation aid. In: Proceedings of ETRA 2000, eye tracking research and applications symposium. ACM Press, pp 9–14, &#xA;                    http://www.cs.uta.fi/~curly/publications/ETRA2000-Hyrskykari.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR16" id="ref-link-section-d51036e796">2000</a>).</p><p>However, giving explicit commands by using gaze is not the best solution in a ubiquitous environment, since it demands the full attention of the user and it suffers from the Midas touch effect: each glance activates an action whether it is intended or not, distracting the user. Hence, in our system, we use a relevance inference engine to infer the user preference implicitly from gaze patterns (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0183-5#Sec13">3.4)</a>
                        </p><h3 class="c-article__sub-heading" id="Sec8">Information filtering</h3><p>In their papers, Julier et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Julier S, Lanzagorta M, Baillot Y, Rosenblum L, Feiner S, Hollerer T, Sestito S (2000) Information filtering for mobile augmented reality. In: Augmented reality, 2000. (ISAR 2000). Proceedings. IEEE and ACM international symposium on. pp 3–11, doi:&#xA;                    10.1109/ISAR.2000.880917&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR21" id="ref-link-section-d51036e813">2000</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Julier S, Baillot Y, Brown D, Lanzagorta M (2002) Information filtering for mobile augmented reality. IEEE Comput Graph Appl 22:12–15. &#xA;                    http://doi.ieeecomputersociety.org/10.1109/MCG.2002.1028721&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR22" id="ref-link-section-d51036e816">2002</a>) have presented the idea of using real-world context as a search cue in information retrieval and implemented a system that filters information based on physical location, for selecting what is displayed to the user by means of AR. The main purpose of information filtering is to prioritize and reduce the amount of information presented in order to show only what is relevant to the user.</p><p>Already in the Touring Machine (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Feiner S, MacIntyre B, Höllerer T, Webster A (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. Personal Ubiquitous Comput 1(4):208–217" href="/article/10.1007/s10055-010-0183-5#ref-CR10" id="ref-link-section-d51036e822">1997</a>), there existed a logic to show more information and menu choices for objects that had remained in the center of the user’s view for a long enough time. This kind of contextual user feedback is, however, more explicit than implicit by nature. With our gaze tracking hardware, we have been able to detect the implicit targets of the user’s attention and to use that data in information filtering. As described in the previous section, there have been studies on using gaze as a form of relevance feedback, but to our best knowledge, the current work is the first one to use implicit gaze data for contextual information filtering in an AR setup and to evaluate its usefulness with a user study.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Components of contextual information access system</h2><div class="c-article-section__content" id="Sec9-content"><p>In this section, we give an overview of the system and describe its main operational parts.</p><h3 class="c-article__sub-heading" id="Sec10">System architecture</h3><p>We have implemented a pilot software system that can be used in the online studies of contextual information access with several alternative hardware platforms. The general overview of the system architecture is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig2">2</a>. The technologies integrated into the system include relevance estimation from gaze or pointing patterns; contextual information retrieval; speech, face, object, and location recognition; object tracking; and augmented reality rendering.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An overview of the system architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The front-end user interface of the pilot system is currently operational on two different display hardware platforms: a Sony Vaio ultra-mobile PC (UMPC) and a prototype wearable near-eye-camera-see-through display and a miniaturized gaze tracker. See Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0183-5#Sec15">3.6</a> for a more detailed description of the devices.</p><p>All the front-ends communicate wirelessly with common back-end information servers, which are responsible for tasks requiring either more computational resources than what is available on the front-end or dynamic data repositories and access to external data sources. In the current setup, the back-end consists of three separate server programs. The first server maintains a context-sensitive database of relevant information to the current application setup and can be queried for context-dependent textual annotations for recognized objects and people. The server also collects background information from external data sources such as the web. The second server is a content-based image and video database engine, which handles image queries and also provides face, object, and scene recognition results. The third server performs speech recognition of the recorded audio and outputs a stream of recognized words.</p><h3 class="c-article__sub-heading" id="Sec11">Context-sensitive information retrieval</h3><p>Our IR database contains annotations for people and objects. Each annotation has an associated context description that is compared to the measured context to decide which annotations are most relevant and should be shown. In the current system, the context consists of two types of features: presence features that indicate which objects and people have been seen recently and topic features, which are the estimated relevances of the available topics (such as <i>research</i> and <i>teaching</i> in the pilot application). Other features, such as time and location, can be added in the future. In the current system, the context features of the annotations are fixed, but later they will be inferred and stored online.</p><p>The current context is inferred from video feed and by observing user actions. The binary presence features are set to one if the corresponding entity has been recognized during the last 2 min, otherwise they are zero. The purpose of the topic relevance features is to track which topics the user prefers. They are periodically adjusted toward the topics of the currently visible annotations. The speech recognizer output is used as additional evidence for topic relevance. If the speech recognizer has been activated during the last 2 min, the topic of the utterance is inferred by comparing the recognized words to topic-specific keyword lists, and the topic feature of the winning topic is set to one and the others to zero.</p><h3 class="c-article__sub-heading" id="Sec12">Extracting contextual cues</h3><p>The <i>face recognition</i> system detects and recognizes human faces and uses them as context cues for associating information to them, such as people’s research activities, publications, teaching, office hours and links to their web pages, and Facebook and LinkedIn profiles. Due to real-time performance requirements, face detection is performed in the front-end of the system (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig2">2</a>) using the OpenCV library. For face detection, we utilize the Viola and Jones face detector (Viola and Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Viola P, Jones M (2001) Rapid object detection using a boosted cascade of simple features. In: IEEE Computer Society conference on computer vision and pattern recognition (CVPR’01), Kauai, Hawaii, pp 511–518" href="/article/10.1007/s10055-010-0183-5#ref-CR49" id="ref-link-section-d51036e899">2001</a>), which is based on a cascade of Haar classifiers and AdaBoost. For missed faces (e.g., due to occlusion, changes in lighting, excessive rotation or camera movement), we initiate an optical flow tracking algorithm (Tomasi and Kanade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Tomasi C, Kanade T (1991) Detection and tracking of point features. Tech. Rep. CMU-CS-91-132, Carnegie Mellon University" href="/article/10.1007/s10055-010-0183-5#ref-CR46" id="ref-link-section-d51036e902">1991</a>), which continues to track the approximate location of the face until either the face detector is again able to detect the face or the tracked keypoints become too dispersed and the tracking is lost. The system supports tracking of multiple persons maintaining identities of the detected faces across frames.</p><p>The detected faces are transmitted to the image database engine (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig2">2</a>) for recognition using the MPEG-7 Face Recognition descriptor (ISO/IEC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="ISO/IEC (2002) Information technology—multimedia content description interface—part 3: visual. 15938–3:2002(E)" href="/article/10.1007/s10055-010-0183-5#ref-CR18" id="ref-link-section-d51036e911">2002</a>). Before extracting the descriptor, the face images are normalized using region-wise histogram equalization and gamma correction. During tracking, we obtain an increasing number of face images of the same subject that can be used as a set in the recognition stage. Given the set we perform online recognition with a <i>k</i>-nn classifier.</p><p>The system can also detect two-dimensional <i>AR markers</i> (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0183-5#Sec14">3.5)</a> which help in recognizing objects and indoor locations. Markers attached to static objects provide the basis for location recognition. In addition, markers attached to movable objects will be associated with a wider area or a set of places where the objects are likely to be encountered.</p><p>The system uses <i>speech recognition</i> to gather contextual cues from the user’s speech. The recognizer takes speech as an input and gives its transcript hypotheses as an output. The speech transcript is then used to determine the underlying topic of discussion. In the current pilot system, we have a fixed set of potential topics, and the decision between topics is made according to keywords detected from the transcripts.</p><p>We use an online large-vocabulary speech recognizer developed at Aalto University (Hirsimäki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Hirsimäki T, Pylkkönen J, Kurimo M (2009) Importance of high-order n-gram models in morph-based speech recognition. IEEE Trans Audio Speech Lang Processing 17(4):724–732" href="/article/10.1007/s10055-010-0183-5#ref-CR15" id="ref-link-section-d51036e936">2009</a>). The system utilizes triphone Hidden Markov models as context-dependent and gender- and speaker-independent phoneme models trained using sentences selected from the Internet and read by a few hundred native Finnish speakers. As a statistical language model, the system has a large 6-gram model of data-driven morpheme-like units.</p><h3 class="c-article__sub-heading" id="Sec13">Inferring object relevance</h3><p>Real-world scenes typically contain several objects or people, each of which can be considered as a potential source of augmentable information. Distraction and information overload is minimized, and best user experience is achieved when information is displayed only for the objects that are interesting for the user at a particular time. For this, a mechanism that estimates the degree of interestingness (relevance) of an object in real time is required.</p><p>In the pilot system, the relevance of an object is estimated by the proportion of the total time an object, or related augmented annotations have been under visual attention within a fixed-length time window. This estimate is defined as <i>gaze intensity</i> by Qvarfordt and Zhai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Qvarfordt P, Zhai S (2005) Conversing with the user based on eye-gaze patterns. In: CHI ’05: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, NY, USA, pp 221–230, doi:&#xA;                    10.1145/1054972.1055004&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR38" id="ref-link-section-d51036e953">2005</a>). For the HMD, where gaze tracking is available, looking at an object is used as an indicator of attention. For the UMPC, pointing of the display device toward an object is the indicator.</p><h3 class="c-article__sub-heading" id="Sec14">The augmented reality interface</h3><p>The objective of the application is to provide the user contextual information non-disruptively. This is accomplished by minimizing the amount of information shown and by displaying only information that is assumed to be relevant for the user in the given context. The augmentation is designed to be as unobtrusive as possible. All of the visual elements are as simple and minimalistic as feasible and are rendered transparently so that they do not fully occlude other objects.</p><p>The user interface of our system is implemented using AR overlay of the virtual objects on top of video of the real world. This is accomplished through a combination of face tracking and 2D marker tracking.</p><p>By attaching a marker to an item of interest, we can detect the item’s presence and annotate it appropriately (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig3">3</a> for an example annotation on a marker).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Screenshots from the Virtual Laboratory Guide. <i>Top left</i> The guide shows augmented information about a recognized person. <i>Top right</i> The guide has recognized that the user is interested in research and shows in the pop-ups mainly research-related information. <i>Bottom left</i> The user of the guide is looking at a specific section of the poster. <i>Bottom right</i> The guide is displaying information about reservations to a meeting room</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The AR implementation in the pilot application uses a monocular video see-through display, where augmentations are rendered over the video from a single camera. The camera captures the video with 640 × 480 resolution at a frame rate of 15 frames per second (FPS). The augmented video is generally displayed at a rate of slightly less than 10 FPS due to the heavy processing involved with respect to the computational power of the mobile devices. We use the ALVAR augmented reality library,<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> developed by VTT Technical Research Centre of Finland, for calibrating the camera and for detecting 2D fiducial markers and determining camera pose relative to the markers. For 3D rendering, we use the OpenSceneGraph library<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup>.</p><p>The application displays information about people who are recognized using face recognition techniques. Since it is difficult to determine the exact pose of a face relative to a camera, we use a 2.5D approach to place augmented annotations relative to faces. We estimate the distance of a person from the camera based on the size of the detected face in the image and place the augmented information at the corresponding distance facing the viewer. This ensures that the augmentations are consistent with the other depth cues present in the image and helps in associating annotations with persons in the scene. Another cue for association is that the annotation related to a person moves when the person moves. The 2.5D approach is also used in text labels for readability.</p><p>In an alternative usage mode the annotations are output as synthetic speech. The synthetic voice for the augmented audio was produced by adapting an average English voice toward the voice of one of our own researchers working in the lab. Using the modeling and adaptation framework (Yamagishi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Yamagishi J, Usabaev B, King S, Watts O, Dines J, Tian J, Hu R, Guan Y, Oura K, Tokuda K, Karhila R, Kurimo M (2009) Thousands of voices for HMM-based speech synthesis. In: Proceedings of the 10th annual conference of the international speech communication association, INTERSPEECH 2009, ISCA, Brighton, UK" href="/article/10.1007/s10055-010-0183-5#ref-CR52" id="ref-link-section-d51036e1036">2009</a>) developed at the EMIME project,<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> it is possible to create new personalized voices using just a few samples of the target voice.</p><h3 class="c-article__sub-heading" id="Sec15">Display devices and cameras</h3><p>We have implemented alternative device setups to be able to study the effectiveness of the different ways of presenting and interacting with AR content. Ultimately also the question about cost-effectiveness of the setups is important, but in this initial application our focus is on effectiveness.</p><p>To show the AR content, our system can use two alternative output devices: (i) a near-to-eye display with an integrated gaze tracker and a camera (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig4">4</a> left) and (ii) a handheld UMPC or a standard laptop with a camera (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig4">4</a> right). The near-to-eye display device is a research prototype provided by Nokia Research Center (Järvenpää and Aaltonen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Järvenpää T, Aaltonen V (2008) Photonics in multimedia II. In: Proceedings of SPIE, vol 7001, SPIE, Bellingham, WA, chap Compact near-to-eye display with integrated gaze tracker. pp 700106–1–700106–8" href="/article/10.1007/s10055-010-0183-5#ref-CR19" id="ref-link-section-d51036e1067">2008</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The display devices. On the left, the head-mounted display (HMD), a wearable near-to-eye display with integrated gaze tracker. On the right, the ultra-mobile PC (UMPC), a handheld computer with virtual see-through display</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The integrated display and gaze tracker covers partially the field of vision with augmented video of the physical world. The bi-ocular near-to-eye display produces the same virtual image for both of the eyes. Images are perceived larger than the physical display itself and are ergonomic to view because of very low level of geometrical misalignments between the left and right eye images. The device is capable of displaying close to VGA resolution images with a 30 degrees field-of-view in diagonal. The size and weight of the display system are kept reasonably small by expanding the image from a microdisplay with suitable optics, including diffractive optics serving as light guides. Real see-through is not implemented because of the fairly low display brightness and the gaze tracker eye-camera, which is situated just in front of the right eye. The forward-looking camera is situated in between the eyes, both of which are shown the same monocular augmented video output.</p><p>The integrated gaze tracker works by illuminating the right eye cornea with collimated infrared beams, which are invisible to human eye. The infrared glints reflected from the pupil are detected by the eye-camera. The locations of the reflections and the pupil center are used for reconstructing the camera-eye-geometry and further detecting the gaze angle. Estimation of the direction of the visual axis of the eye relative to the head gear can be done after a robust per-user calibration. The tracking accuracy is measured to be less than one degree of visual angle with a speed of 25 measurements per second (Pylvänäinen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference  2008" title="Pylvänäinen T, Järvenpää T, Nummela V (2008) Gaze tracking for near to eye displays. In: Proceedings of the 18th international conference on artificial reality and telexistence (ICAT 2008), Yokohama, Japan. pp 5–11" href="/article/10.1007/s10055-010-0183-5#ref-CR37" id="ref-link-section-d51036e1093"> 2008</a>).</p><p>In the second alternative hardware setup, the user carries a small handheld or ultra-mobile Sony Vaio computer with a 4.5 inch display for the augmented video (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig4">4</a> right). We have replaced the integrated forward-looking video camera with a separate wide-angle webcam to increase the field of view of the display and to ease the augmentation of several pieces of information on the screen at the same time.</p><h3 class="c-article__sub-heading" id="Sec16">Interaction</h3><p>On platforms where the gaze input is unavailable, we utilize explicit pointing as an alternative input method. In the current setting with the UMPC, the user can indicate his or her interest either by pointing the device toward an object or using the stylus to click on an interesting object on the touch screen display. The augmented reality display has a cursor. On the handheld device, the cursor is at the center of the screen, or at the last stylus click location if the stylus has been used less than a second ago. On the head-mounted device, the cursor follows the gaze. The potential objects of interest can occlude each other, so we assume that the user is interested in the nearest visible object along the ray defined by the cursor or the gaze.</p><p>The gaze and pointing are inputs for the inference engine. The system learns what kind of information is relevant in the current situation by observing which of the objects or the already-shown annotations the user pays attention to. The system shows more information about topics that have recently attracted attention. The shown annotations are considered potential targets of interest for the relevance inference engine, similarly to recognized faces and markers.</p><p>The objects and persons may be relevant in different contexts for different reasons. At first, when the system does not yet know what kind of content is relevant, general or uniformly chosen information about the object is shown in the AR annotations.</p><p>The estimate of interest we use in the system (based on gaze intensity) is above zero only for the objects that have recently been looked at. We assume that the user is only interested in objects she has looked at, even briefly. This prevents the user from being distracted by annotations that pop up for totally irrelevant objects. The user becomes, to some degree, aware of in advance that an annotation will soon be shown next to the object she is looking at. When the user notices something that is interesting he or she pays more attention to it. The system then shows more information about related topics. If, on the other hand, no attention is paid to the annotations, the system infers that the topics shown are not relevant and shows less of them in the future. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig3">3</a> shows example screenshots from the pilot application; in the first screenshot, two topics are displayed for the user to choose from, and in the second, the user has been inferred to be more interested in research-related annotations.</p><p>Speech spoken by the user is fed to the speech recognizer. The recognition output is matched to the index terms of the available topic models, and the output of the topic classification is sent back to the system that uses it as a contextual cue.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Pilot application: virtual laboratory guide</h2><div class="c-article-section__content" id="Sec17-content"><p>As a pilot application for testing the framework, we have implemented an AR guide to a visitor in a university department. The <i>Virtual Laboratory Guide</i> shows relevant information and helps the visitor to navigate through the department.</p><p>The hardware and software platform we are developing is potentially useful in many application scenarios, of which the virtual laboratory guide was chosen as the first one for convenience. The main requirement when setting the system up for a new application is that names or labels for the objects or people of interest need to be given in one way or the other, either when setting up the system or online in a user interface. When online, either pattern recognition or a marker then identifies the label, and the rest can be done with queries in either a closed (current) or an open (future) system. A sample alternative scenario is virtual tourist guide, where the system could complement the existing tourist guide systems by giving a browser access to unlimited information, chosen according to the observed user interest and context, in addition to the existing fixed information. Another sample scenario is a personal assistant in meetings, helping to remember the names and enabling browsing of background material relevant to discussion context, such as emails from and about the participants.</p><p>The current proof-of-concept version of the virtual laboratory guide is implemented on two display devices, namely the head-mounted display with an integrated gaze tracker and the ultra-mobile Sony Vaio computer. Both devices have a forward-looking video camera and a display that shows the location where the user is looking at, or pointing the computer at.</p><p>The task of the system is to infer what information the user would be interested in and to non-disruptively augment the information in the form of annotations onto the display. See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig3">3</a> for screenshots of the system in operation.</p><p>First, the system detects and recognizes the face of a researcher <i>Agent Smith</i> and augments a transparent browser to the display, showing a couple of potentially relevant items about Smith. Based on the user’s gaze or pointing pattern, the system infers if the user is more interested in Smith’s research or courses he teaches, and offers more information retrieved about the appropriate topic. Finally, in the screenshots in the bottom row of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig3">3</a>, the user is focusing her attention to specific markers, and the guide is displaying related information.</p><p>The Virtual Laboratory Guide recognizes the context of use and is able to infer the role of the user. Currently, only two roles have been explicitly implemented, for concreteness: a student or a visiting researcher. Later, the roles will be inferred implicitly from the contexts that have been activated in the retrieval processes. For a student, the system shows teaching-related information, such as information about the office of a lecturer or a teaching assistant. For a visiting researcher, on the other hand, the guide tells about research conducted in the department. The role is inferred based on which annotated items the user finds interesting (i.e., which annotations he or she looks at or points with the cursor).</p><p>The guide needs a database of all recognized persons and objects labeled with markers and textual annotations and images associated to them in different contexts. The database can be completely open, even consisting of the whole Internet; in the pilot application, we use a small constructed database of about 30 persons and objects. The database was constructed manually to keep the setting of the pilot study simple. For people, the database contains their name, research interests, titles of recent publications, and information about lectured courses. For objects, the database contains additional annotations, such as related publications for posters, printer queue names, and names and office hours next to office doors.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">System evaluation</h2><div class="c-article-section__content" id="Sec18-content"><h3 class="c-article__sub-heading" id="Sec19">Usability study</h3><p>A small-scale pilot study was conducted to provide an informal user evaluation of the Virtual Laboratory Guide application and to test the usefulness of our AR platform. The goal of the study was to collect feedback on how useful people find the ability to access virtual information. We also compared usability of alternative user interfaces: a handheld ultra-mobile PC (UMPC) and a head-mounted (HMD) near-to-eye display, both displaying textual annotations. The head-mounted display is a very early prototype which will naturally affect the results. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig4">4</a> shows the display devices. The comparisons are designed to tell which kind of augmented reality interface is preferred in information access tasks.</p><p>The 8 subjects (all men) were university students or researchers aged from 23 to 32 years old. None of them had prior experience with the Virtual Laboratory Guide application, although some of them had used other AR interfaces before. Each subject used the display configurations in a counterbalanced order to remove order effects. Each subject was trained on how to use the display configurations until they felt comfortable with the technology</p><p> In each condition, the subjects were asked to find answers to one research-related question (for example, “Who is funding the research project?”) and one teaching-related question (for example, “What is the next exam date for signal processing?”). The answers to the questions were available through the information augmented on the real objects (posters or course material) or on the persons. When the subject had completed one task, the experiment supervisor changed the topic of the augmented information by speaking an utterance containing some keywords for the desired topic to the speech recognizer. The experiment in each condition was stopped after the subject had found the answers to the information search tasks or after 5 min in case the subject was not able to find the answers.</p><p>After each display condition, the subjects were asked to fill out a subjective survey that had the following six questions:
</p><ul class="u-list-style-dash">
                    <li>
                      <p>How easy was it to use the application?</p>
                    </li>
                    <li>
                      <p>How easy was it to see or hear the AR information?</p>
                    </li>
                    <li>
                      <p>How useful was the application in helping you learn new information?</p>
                    </li>
                    <li>
                      <p>How well do you think you performed in the task?</p>
                    </li>
                    <li>
                      <p>How easy was it to remember the information presented?</p>
                    </li>
                    <li>
                      <p>How much did you enjoy using the application?</p>
                    </li>
                  </ul>
                        <p>Each question was answered on a Likert scale of 1–7, where 1 = <i>Not very easy/useful/much</i> and 7 = <i>Very easy/useful/much</i>. In addition, subjects were asked what they liked best and least about the display condition and were given the opportunity to write any additional comments about the system.</p><h3 class="c-article__sub-heading" id="Sec20">Results</h3><p>In general, users were able to complete the task with either the head-mounted display or the handheld display and found the system a useful tool for presenting context information. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0183-5#Fig5">5</a> shows a graphical depiction of the questionary results.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0183-5/MediaObjects/10055_2010_183_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Results of the usability questionary on 7-point Likert scale in ultra-mobile PC (UMPC) and head-mounted display (HMD) conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0183-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the second question (how easy was it to see the AR information), the subjects rated the UMPC as easier (<i>p</i> = 0.016, Wilcoxon signed rank test). There were no significant differences in the other questions. Most subjects (5 out of 8) preferred the handheld display. In the interview questions, the subjects reported that the most severe weakness in the head-mounted display was the quality of the image which made it difficult to read augmented texts. The issues with the quality of the early prototype head-mounted display were to be expected. On the other hand, subjects felt the handheld display had a screen that was too small and the device was too heavy. Two test subjects said that they found the eye tracking and hands-free nature of the head-mounted display to be beneficial. The test subjects found the correct answers in less than 5 min with over 90% accuracy in both experiments.</p><h3 class="c-article__sub-heading" id="Sec21">Follow-up study</h3><p>In the first experiment, we learned that it is difficult to read text on the wearable display because of the limitations of the prototype display technology. Therefore, we decided to make a short follow-up experiment where we collected feedback on using the head-mounted display with the textual annotations replaced by an augmented audio interface.</p><p>In the second study, 7 subjects (different from the subjects in the first study, 4 men, 3 women, aged from 25 to 29) completed the same task as in the first study using the head-mounted augmented audio interface. We collected written feedback from the subjects.</p><h3 class="c-article__sub-heading" id="Sec22">Results of the follow-up study</h3><p>The users enjoyed the augmented audio interface but still found the wearable eye-tracker uncomfortable to wear. The quality of the synthetic voice got mixed feedback: two test subjects said that the voice was of good quality while two others commented that it was difficult to understand. One test subject noted that it is impossible to skip ahead in speech like one can do while reading. The results of this small-scale study remain inconclusive about the utility of the augmented audio interface in our application. The accuracy of test subjects finding the correct answers was again over 90%.</p><h3 class="c-article__sub-heading" id="Sec23">Discussion on evaluation results</h3><p>We have reported the results of a preliminary usability studies here. A more thorough evaluations of the system’s components are planned. At least studies on how reliably the relevance can be inferred from gaze, what kind of context features are most effective in retrieving relevant information, and how to make the annotations accessible and information easy to browse are needed. A further study is also needed on how relevance inference from gaze and pointing input proposed here compares to other forms of input, such as explicitly selecting the objects or the annotations of interest using a stylus or a 3D mouse, or to standard keyword based queries.</p><p>A more formal user study of the whole system will be completed in the future with a second-generation head-mounted display that is more comfortable and has better screen quality and real see-through. However, the results seem to indicate that—unless the head-mounted display has a good image quality—it would be better to provide users with a handheld display they can look at when needed, not all the time.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Discussion and future work</h2><div class="c-article-section__content" id="Sec24-content"><p>We have proposed a novel AR application which infers the interests of the user based on her behavior, most notably gaze and speech. The system overlays information about people and objects that it predicts to be useful for the user over the video display of the real world and uses augmented audio. The system uses implicit feedback and contextual information to select the information to show in a novel pilot application that combines AR, information retrieval and implicit relevance feedback.</p><p>We have implemented our system on two alternate devices: a wearable display with an eye-tracker and an ultra-portable laptop computer. We have also performed a preliminary evaluation of an application on these hardware platforms. Both devices can be considered as prototype platforms. The current wearable display suffers from fairly low display brightness and virtual see-through, which make using the device somewhat cumbersome. The upcoming releases of the wearable display address these issues. The UMPC, on the other hand, is relatively heavy and likely to remain a niche product. Therefore, we are studying the option of implementing the system on a mobile phone.</p><p>In this paper, we presented a pilot study on the usability of the idea of augmenting retrieved contextual information onto the view, or as augmented audio. Further studies will be performed on relevance inference, face and marker recognition, speech recognition, as well as the other components.</p><p>In the future, we will extend the system with generic object recognition capabilities based on techniques such as SIFT features and their matching (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lowe D (1999) Object recognition from local scale-invariant features. In: Computer vision, 1999. The proceedings of the seventh IEEE international conference on, vol 2. pp 1150–1157, doi:&#xA;                    10.1109/ICCV.1999.790410&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR29" id="ref-link-section-d51036e1302">1999</a>), which will also reduce the need for markers in object recognition. Also, marker-based AR is limited in that a marker must always be visible in the video for the augmentation to work. This problem can be solved for fixed markers, at least partially, using visual tracking techniques (Davison et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Davison A, Reid I, Molton N, Stasse O (2007) Monoslam: real-time single camera slam. Pattern Anal Mach Intell IEEE Trans 29(6):1052–1067. doi:&#xA;                    10.1109/TPAMI.2007.1049&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0183-5#ref-CR8" id="ref-link-section-d51036e1305">2007</a>; Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’07). IEEE computer society, Washington, DC, USA, pp 1–10" href="/article/10.1007/s10055-010-0183-5#ref-CR24" id="ref-link-section-d51036e1308">2007</a>). Another related problem we will look into is implementing indoor localization.</p><p>Output of the speech recognizer will be extended to include multiple recognition hypotheses with confidence measures. The increase of topics from the preliminary two (teaching and research) will enable experiments on online language model adaptation. Furthermore, we will improve usability by implementing techniques such as automatic speech detection.</p><p>The accuracy of the relevance inference engine will also be improved by extending the current model with additional gaze features and the audio-visual content. Recent experiments (Kandemir et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kandemir M, Saarinen VM, Kaski S (2010) Inferring object relevance from gaze in dynamic scenes. In: ETRA 2010: proceedings of the 2010 symposium on eye-tracking research &amp; applications. ACM, New York, NY, USA, pp 105–108" href="/article/10.1007/s10055-010-0183-5#ref-CR23" id="ref-link-section-d51036e1317">2010</a>) have revealed that a more accurate estimate of relevance can be obtained when additional features of the gaze trajectory, in addition to plain gaze duration, are taken into account. Integration of these ideas to the system is currently in progress.</p><p>In the visual augmentation, trade-offs need to be made on how much information to show and about how many objects, in order not to occlude the view unnecessarily. In the current system, the trade-off is done purely based on estimated relevance, but later the user needs to be given some direct control about the amount. It is also possible to estimate how often the user actually pays attention to the augmentations, and try to estimate the suitable short-time trade-off based on that. The balance should at best also take into account the distance and size of the object. Although very small or far-away objects may be equally important to the user, their relevance cannot be estimated equally accurately and it would be a good idea to take into account the uncertainty of the estimated relevance when deciding whether to augment.</p><p>Visual augmentation has some additional detailed issues which need to be considered, most notably occlusions and camera geometry. The current system tracks the objects on the scene, and the tracking is tolerant to short-term occlusions. Full long-term occlusions cannot of course be easily modeled at all, but for long-term partial occlusions there is an interface solution: given that the user is estimated to be interested in either the occluding or partially occluded object, both can be shown in the browser.</p><p>The present rendering method does not attempt to model the camera geometry in detail—for instance, lens distortion is omitted. It neither matches the augmentations with the illumination conditions, so the augmented objects are easily recognizable from the video. Modelling the camera and illumination with greater accuracy will help to make the augmentation more realistic (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Klein G, Murray D (2008) Compositing for small cameras. In: Proceedings of seventh IEEE and ACM international symposium on mixed and augmented reality (ISMAR’08). IEEE Computer Society, Washington, DC, USA, pp 57–60" href="/article/10.1007/s10055-010-0183-5#ref-CR25" id="ref-link-section-d51036e1327">2008</a>).</p><p>The augmented audio naturally can avoid most of the problems of visual augmentation, at the cost that the audio may be disruptive too in some contexts, and it is harder to infer whether the user pays attention to the audio.</p><p>Integration of the enabling technologies in pilot systems will be continued. We plan to use visual location recognition as well as GPS, gyroscopes and accelerometers to provide context information in the future. As we already have a working prototype of the hardware and software framework, new theoretical developments in any of the enabling technologies can be easily integrated into the system and then evaluated empirically in situ. This results from the careful planning of the interoperability and the modular structure of the different software subparts and information servers in the system.</p><p>The software and hardware platforms make it possible to test new scenarios or application ideas on a short notice and study the integration of input modalities, explicit feedback and contextual information retrieval. We are currently in the process of integrating audio output and stereo vision capabilities to the system.</p><p>In the pilot study, the database was build offline before the experiment. Others have proposed authoring voice (Rekimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Rekimoto J, Ayatsuka Y, Hayashi K (1998) Augment-able reality: situated communication through physical and digital spaces. In: Proceedings of the 2nd IEEE international symposium on wearable computers. IEEE Computer Society, Washington, DC, USA, pp 68–75" href="/article/10.1007/s10055-010-0183-5#ref-CR41" id="ref-link-section-d51036e1340">1998</a>) or 3D object annotations (Baillot et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Baillot Y, Brown D, Julier S (2001) Authoring of physical models using mobile computers. In: Proceedings of the 5th IEEE international symposium on wearable computer. IEEE Computer Society, Washington, DC, USA, pp 39–46" href="/article/10.1007/s10055-010-0183-5#ref-CR3" id="ref-link-section-d51036e1343">2001</a>) within an augmented reality application and adding them, together with information about the creation context, to the database to be retrieved later. We are planning to extend our system with this kind of interaction capabilities between the user and the virtual world (annotations). For example, we are planning to apply the platform to an architecture-related application, where we overlay architectural design elements on the display of real world, the idea being that an architect can then manipulate and interact with these virtual reality elements.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p><a href="http://layar.com/">http://layar.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p><a href="http://www.wikitude.org/">http://www.wikitude.org/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p><a href="http://virtual.vtt.fi/virtual/proj2/multimedia/alvar.html">http://virtual.vtt.fi/virtual/proj2/multimedia/alvar.html</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.openscenegraph.org">http://www.openscenegraph.org</a>
                    </p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p><a href="http://emime.org/">http://emime.org/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Ajanki, DR. Hardoon, S. Kaski, K. Puolamäki, J. Shawe-Taylor, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Ajanki A, Hardoon DR, Kaski S, Puolamäki K, Shawe-Taylor J (2009) Can eyes reveal interest?—implicit queries f" /><p class="c-article-references__text" id="ref-CR1">Ajanki A, Hardoon DR, Kaski S, Puolamäki K, Shawe-Taylor J (2009) Can eyes reveal interest?—implicit queries from gaze patterns. User Model User-Adapt Interact 19(4):307–339. doi:<a href="https://doi.org/10.1007/s11257-009-9066-4">10.1007/s11257-009-9066-4</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11257-009-9066-4" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20eyes%20reveal%20interest%3F%E2%80%94implicit%20queries%20from%20gaze%20patterns&amp;journal=User%20Model%20User-Adapt%20Interact&amp;doi=10.1007%2Fs11257-009-9066-4&amp;volume=19&amp;issue=4&amp;pages=307-339&amp;publication_year=2009&amp;author=Ajanki%2CA&amp;author=Hardoon%2CDR&amp;author=Kaski%2CS&amp;author=Puolam%C3%A4ki%2CK&amp;author=Shawe-Taylor%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385" /><p class="c-article-references__text" id="ref-CR2">Azuma R (1997) A survey of augmented reality. Presence 6(4):355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Presence&amp;volume=6&amp;issue=4&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baillot Y, Brown D, Julier S (2001) Authoring of physical models using mobile computers. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR3">Baillot Y, Brown D, Julier S (2001) Authoring of physical models using mobile computers. In: Proceedings of the 5th IEEE international symposium on wearable computer. IEEE Computer Society, Washington, DC, USA, pp 39–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bee N, André E (2008) Writing with your eye: a dwell time free writing system adapted to the nature of human e" /><p class="c-article-references__text" id="ref-CR4">Bee N, André E (2008) Writing with your eye: a dwell time free writing system adapted to the nature of human eye gaze. In: PIT ’08: proceedings of the 4th IEEE tutorial and research workshop on perception and interactive technologies for speech-based systems. Springer, Berlin, pp 111–122, doi:<a href="https://doi.org/10.1007/978-3-540-69369-7_13">10.1007/978-3-540-69369-7_13</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brzezowski S, Dunn CM, Vetter M (1996) Integrated portable system for suspect identification and tracking. In:" /><p class="c-article-references__text" id="ref-CR5">Brzezowski S, Dunn CM, Vetter M (1996) Integrated portable system for suspect identification and tracking. In: DePersia AT, Yeager S, Ortiz S (eds) SPIE: surveillance and assessment technologies for law enforcement. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, USA, pp 24–35</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal intera" /><p class="c-article-references__text" id="ref-CR6">Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for simulation set-up and control. In: Proceedings of the fifth conference on applied natural language processing, Washington, DC. pp 20–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Crestani, I. Ruthven, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Crestani F, Ruthven I (2007) Introduction to special issue on contextual information retrieval systems. Inf Re" /><p class="c-article-references__text" id="ref-CR7">Crestani F, Ruthven I (2007) Introduction to special issue on contextual information retrieval systems. Inf Retr 10(2):111–113</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10791-007-9022-z" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Introduction%20to%20special%20issue%20on%20contextual%20information%20retrieval%20systems&amp;journal=Inf%20Retr&amp;volume=10&amp;issue=2&amp;pages=111-113&amp;publication_year=2007&amp;author=Crestani%2CF&amp;author=Ruthven%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Davison, I. Reid, N. Molton, O. Stasse, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Davison A, Reid I, Molton N, Stasse O (2007) Monoslam: real-time single camera slam. Pattern Anal Mach Intell " /><p class="c-article-references__text" id="ref-CR8">Davison A, Reid I, Molton N, Stasse O (2007) Monoslam: real-time single camera slam. Pattern Anal Mach Intell IEEE Trans 29(6):1052–1067. doi:<a href="https://doi.org/10.1109/TPAMI.2007.1049">10.1109/TPAMI.2007.1049</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2007.1049" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Monoslam%3A%20real-time%20single%20camera%20slam&amp;journal=Pattern%20Anal%20Mach%20Intell%20IEEE%20Trans&amp;doi=10.1109%2FTPAMI.2007.1049&amp;volume=29&amp;issue=6&amp;pages=1052-1067&amp;publication_year=2007&amp;author=Davison%2CA&amp;author=Reid%2CI&amp;author=Molton%2CN&amp;author=Stasse%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Farringdon J, Oni V (2000) Visual augmented memory (VAM). In: Proceedings of 4th international symposium on we" /><p class="c-article-references__text" id="ref-CR9">Farringdon J, Oni V (2000) Visual augmented memory (VAM). In: Proceedings of 4th international symposium on wearable computers. pp 167–168. doi:<a href="https://doi.org/10.1109/ISWC.2000.888484">10.1109/ISWC.2000.888484</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Feiner, B. MacIntyre, T. Höllerer, A. Webster, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Feiner S, MacIntyre B, Höllerer T, Webster A (1997) A touring machine: prototyping 3D mobile augmented reality" /><p class="c-article-references__text" id="ref-CR10">Feiner S, MacIntyre B, Höllerer T, Webster A (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. Personal Ubiquitous Comput 1(4):208–217</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20touring%20machine%3A%20prototyping%203D%20mobile%20augmented%20reality%20systems%20for%20exploring%20the%20urban%20environment&amp;journal=Personal%20Ubiquitous%20Comput&amp;volume=1&amp;issue=4&amp;pages=208-217&amp;publication_year=1997&amp;author=Feiner%2CS&amp;author=MacIntyre%2CB&amp;author=H%C3%B6llerer%2CT&amp;author=Webster%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hahn D, Beutler F, Hanebeck U (2005) Visual scene augmentation for enhanced human perception. In: Internationa" /><p class="c-article-references__text" id="ref-CR11">Hahn D, Beutler F, Hanebeck U (2005) Visual scene augmentation for enhanced human perception. In: International conference on informatics in control, automation &amp; robotics (ICINCO 2005). INSTICC Press, Barcelona, Spain, pp 146–153</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Hayhoe, D. Ballard, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Hayhoe M, Ballard D (2005) Eye movements in natural behavior. Trends Cogn Sci 9(4):188–194" /><p class="c-article-references__text" id="ref-CR12">Hayhoe M, Ballard D (2005) Eye movements in natural behavior. Trends Cogn Sci 9(4):188–194</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.tics.2005.02.009" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Eye%20movements%20in%20natural%20behavior&amp;journal=Trends%20Cogn%20Sci&amp;volume=9&amp;issue=4&amp;pages=188-194&amp;publication_year=2005&amp;author=Hayhoe%2CM&amp;author=Ballard%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hendricksen K, Indulska J, Rakotonirainy A (2002) Modeling context information in pervasive computing systems." /><p class="c-article-references__text" id="ref-CR13">Hendricksen K, Indulska J, Rakotonirainy A (2002) Modeling context information in pervasive computing systems. In: Proceedings of the first international conference on pervasive computing, pp 167–180</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Henrysson A, Ollila M (2004) Umar: ubiquitous mobile augmented reality. In: MUM ’04: proceedings of the 3rd in" /><p class="c-article-references__text" id="ref-CR14">Henrysson A, Ollila M (2004) Umar: ubiquitous mobile augmented reality. In: MUM ’04: proceedings of the 3rd international conference on mobile and ubiquitous multimedia. ACM, New York, NY, USA, pp 41–45, doi:<a href="https://doi.org/10.1145/1052380.1052387">10.1145/1052380.1052387</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Hirsimäki, J. Pylkkönen, M. Kurimo, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Hirsimäki T, Pylkkönen J, Kurimo M (2009) Importance of high-order n-gram models in morph-based speech recogni" /><p class="c-article-references__text" id="ref-CR15">Hirsimäki T, Pylkkönen J, Kurimo M (2009) Importance of high-order n-gram models in morph-based speech recognition. IEEE Trans Audio Speech Lang Processing 17(4):724–732</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2008.2012323" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Importance%20of%20high-order%20n-gram%20models%20in%20morph-based%20speech%20recognition&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Processing&amp;volume=17&amp;issue=4&amp;pages=724-732&amp;publication_year=2009&amp;author=Hirsim%C3%A4ki%2CT&amp;author=Pylkk%C3%B6nen%2CJ&amp;author=Kurimo%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hyrskykari A, Majaranta P, Aaltonen A, Räihä KJ (2000) Design issues of idict: a gaze-assisted translation aid" /><p class="c-article-references__text" id="ref-CR16">Hyrskykari A, Majaranta P, Aaltonen A, Räihä KJ (2000) Design issues of idict: a gaze-assisted translation aid. In: Proceedings of ETRA 2000, eye tracking research and applications symposium. ACM Press, pp 9–14, <a href="http://www.cs.uta.fi/~curly/publications/ETRA2000-Hyrskykari.pdf">http://www.cs.uta.fi/~curly/publications/ETRA2000-Hyrskykari.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iordanoglou C, Jonsson K, Kittler J, Matas J (2000) Wearable face recognition aid. In: Proceedings. 2000 IEEE " /><p class="c-article-references__text" id="ref-CR17">Iordanoglou C, Jonsson K, Kittler J, Matas J (2000) Wearable face recognition aid. In: Proceedings. 2000 IEEE international conference on acoustics, speech, and signal processing (ICASSP ’00). pp 2365–2368</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ISO/IEC (2002) Information technology—multimedia content description interface—part 3: visual. 15938–3:2002(E)" /><p class="c-article-references__text" id="ref-CR18">ISO/IEC (2002) Information technology—multimedia content description interface—part 3: visual. 15938–3:2002(E)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Järvenpää T, Aaltonen V (2008) Photonics in multimedia II. In: Proceedings of SPIE, vol 7001, SPIE, Bellingham" /><p class="c-article-references__text" id="ref-CR19">Järvenpää T, Aaltonen V (2008) Photonics in multimedia II. In: Proceedings of SPIE, vol 7001, SPIE, Bellingham, WA, chap Compact near-to-eye display with integrated gaze tracker. pp 700106–1–700106–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Joachims T, Granka L, Pan B, Hembrooke H, Gay G (2005) Accurately interpreting clickthrough data as implicit f" /><p class="c-article-references__text" id="ref-CR20">Joachims T, Granka L, Pan B, Hembrooke H, Gay G (2005) Accurately interpreting clickthrough data as implicit feedback. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 154–161</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Julier S, Lanzagorta M, Baillot Y, Rosenblum L, Feiner S, Hollerer T, Sestito S (2000) Information filtering f" /><p class="c-article-references__text" id="ref-CR21">Julier S, Lanzagorta M, Baillot Y, Rosenblum L, Feiner S, Hollerer T, Sestito S (2000) Information filtering for mobile augmented reality. In: Augmented reality, 2000. (ISAR 2000). Proceedings. IEEE and ACM international symposium on. pp 3–11, doi:<a href="https://doi.org/10.1109/ISAR.2000.880917">10.1109/ISAR.2000.880917</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Julier, Y. Baillot, D. Brown, M. Lanzagorta, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Julier S, Baillot Y, Brown D, Lanzagorta M (2002) Information filtering for mobile augmented reality. IEEE Com" /><p class="c-article-references__text" id="ref-CR22">Julier S, Baillot Y, Brown D, Lanzagorta M (2002) Information filtering for mobile augmented reality. IEEE Comput Graph Appl 22:12–15. <a href="http://doi.ieeecomputersociety.org/10.1109/MCG.2002.1028721">http://doi.ieeecomputersociety.org/10.1109/MCG.2002.1028721</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20filtering%20for%20mobile%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=22&amp;pages=12-15&amp;publication_year=2002&amp;author=Julier%2CS&amp;author=Baillot%2CY&amp;author=Brown%2CD&amp;author=Lanzagorta%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kandemir M, Saarinen VM, Kaski S (2010) Inferring object relevance from gaze in dynamic scenes. In: ETRA 2010:" /><p class="c-article-references__text" id="ref-CR23">Kandemir M, Saarinen VM, Kaski S (2010) Inferring object relevance from gaze in dynamic scenes. In: ETRA 2010: proceedings of the 2010 symposium on eye-tracking research &amp; applications. ACM, New York, NY, USA, pp 105–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE " /><p class="c-article-references__text" id="ref-CR24">Klein G, Murray D (2007) Parallel tracking and mapping for small AR workspaces. In: Proceedings of sixth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’07). IEEE computer society, Washington, DC, USA, pp 1–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2008) Compositing for small cameras. In: Proceedings of seventh IEEE and ACM international " /><p class="c-article-references__text" id="ref-CR25">Klein G, Murray D (2008) Compositing for small cameras. In: Proceedings of seventh IEEE and ACM international symposium on mixed and augmented reality (ISMAR’08). IEEE Computer Society, Washington, DC, USA, pp 57–60</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kozma L, Klami A, Kaski S (2009) GaZIR: gaze-based zooming interface for image retrieval. In: Proceedings of 1" /><p class="c-article-references__text" id="ref-CR26">Kozma L, Klami A, Kaski S (2009) GaZIR: gaze-based zooming interface for image retrieval. In: Proceedings of 11th conference on multimodal interfaces and the sixth workshop on machine learning for multimodal interaction (ICMI-MLMI). ACM, New York, NY, USA, pp 305–312</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MF. Land, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Land MF (2006) Eye movements and the control of actions in everyday life. Prog Retin Eye Res 25(3):296–324" /><p class="c-article-references__text" id="ref-CR27">Land MF (2006) Eye movements and the control of actions in everyday life. Prog Retin Eye Res 25(3):296–324</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.preteyeres.2006.01.002" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Eye%20movements%20and%20the%20control%20of%20actions%20in%20everyday%20life&amp;journal=Prog%20Retin%20Eye%20Res&amp;volume=25&amp;issue=3&amp;pages=296-324&amp;publication_year=2006&amp;author=Land%2CMF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee R, Kwon YJ, Sumiya K (2009) Layer-based media integration for mobile mixed-reality applications. In: Inter" /><p class="c-article-references__text" id="ref-CR28">Lee R, Kwon YJ, Sumiya K (2009) Layer-based media integration for mobile mixed-reality applications. In: International conference on next generation mobile applications, services and technologies. IEEE Computer Society, Los Alamitos, CA, pp 58–63</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lowe D (1999) Object recognition from local scale-invariant features. In: Computer vision, 1999. The proceedin" /><p class="c-article-references__text" id="ref-CR29">Lowe D (1999) Object recognition from local scale-invariant features. In: Computer vision, 1999. The proceedings of the seventh IEEE international conference on, vol 2. pp 1150–1157, doi:<a href="https://doi.org/10.1109/ICCV.1999.790410">10.1109/ICCV.1999.790410</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Nilsson, T. Gustafsson, P. Carleberg, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Nilsson S, Gustafsson T, Carleberg P (2009) Hands free interaction with virtual information in a real environm" /><p class="c-article-references__text" id="ref-CR30">Nilsson S, Gustafsson T, Carleberg P (2009) Hands free interaction with virtual information in a real environment: eye gaze as an interaction tool in an augmented reality system. Psychol J 7(2):175–196</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hands%20free%20interaction%20with%20virtual%20information%20in%20a%20real%20environment%3A%20eye%20gaze%20as%20an%20interaction%20tool%20in%20an%20augmented%20reality%20system&amp;journal=Psychol%20J&amp;volume=7&amp;issue=2&amp;pages=175-196&amp;publication_year=2009&amp;author=Nilsson%2CS&amp;author=Gustafsson%2CT&amp;author=Carleberg%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oyekoya O, Stentiford F (2006) Perceptual image retrieval using eye movements. In: International workshop on i" /><p class="c-article-references__text" id="ref-CR31">Oyekoya O, Stentiford F (2006) Perceptual image retrieval using eye movements. In: International workshop on intelligent computing in pattern analysis/synthesis. Advances in machine vision, image processing, and pattern analysis. Springer, Xi’an, China, pp 281–289</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park H, Lee S, Choi J (2008) Wearable augmented reality system using gaze interaction. In: Proceedings of the " /><p class="c-article-references__text" id="ref-CR32">Park H, Lee S, Choi J (2008) Wearable augmented reality system using gaze interaction. In: Proceedings of the 2008 7th IEEE/ACM international symposium on mixed and augmented reality, vol 00. IEEE Computer Society, pp 175–176</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pentland A (1998) Wearable Intelligence. Exploring Intelligence; Scientific American Presents" /><p class="c-article-references__text" id="ref-CR33">Pentland A (1998) Wearable Intelligence. Exploring Intelligence; Scientific American Presents</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Pentland, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Pentland A (2000) Looking at people: sensing for ubiquitous and wearable computing. IEEE Trans Pattern Anal Ma" /><p class="c-article-references__text" id="ref-CR34">Pentland A (2000) Looking at people: sensing for ubiquitous and wearable computing. IEEE Trans Pattern Anal Mach Intell 22(1):107–119. doi:<a href="https://doi.org/10.1109/34.824823">10.1109/34.824823</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.824823" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Looking%20at%20people%3A%20sensing%20for%20ubiquitous%20and%20wearable%20computing&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;doi=10.1109%2F34.824823&amp;volume=22&amp;issue=1&amp;pages=107-119&amp;publication_year=2000&amp;author=Pentland%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pfeiffer T, Latoschik ME, Wachsmuth I (2008) Evaluation of binocular eye trackers and algorithms for 3D gaze i" /><p class="c-article-references__text" id="ref-CR35">Pfeiffer T, Latoschik ME, Wachsmuth I (2008) Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments. J Virtual Real Broadcast 5(16). urn:nbn:de:0009-6-16605, ISSN 1860-2037</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Puolamäki K, Salojärvi J, Savia E, Simola J, Kaski S (2005) Combining eye movements and collaborative filterin" /><p class="c-article-references__text" id="ref-CR36">Puolamäki K, Salojärvi J, Savia E, Simola J, Kaski S (2005) Combining eye movements and collaborative filtering for proactive information retrieval. In: Proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. ACM, Salvador, Brazil, pp 146–153</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pylvänäinen T, Järvenpää T, Nummela V (2008) Gaze tracking for near to eye displays. In: Proceedings of the 18" /><p class="c-article-references__text" id="ref-CR37">Pylvänäinen T, Järvenpää T, Nummela V (2008) Gaze tracking for near to eye displays. In: Proceedings of the 18th international conference on artificial reality and telexistence (ICAT 2008), Yokohama, Japan. pp 5–11</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qvarfordt P, Zhai S (2005) Conversing with the user based on eye-gaze patterns. In: CHI ’05: proceedings of th" /><p class="c-article-references__text" id="ref-CR38">Qvarfordt P, Zhai S (2005) Conversing with the user based on eye-gaze patterns. In: CHI ’05: proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, NY, USA, pp 221–230, doi:<a href="https://doi.org/10.1145/1054972.1055004">10.1145/1054972.1055004</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rauhala M, Gunnarsson AS, Henrysson A (2006) A novel interface to sensor networks using handheld augmented rea" /><p class="c-article-references__text" id="ref-CR39">Rauhala M, Gunnarsson AS, Henrysson A (2006) A novel interface to sensor networks using handheld augmented reality. In: MobileHCI ’06: proceedings of the 8th conference on human–computer interaction with mobile devices and services. ACM, New York, NY, USA, pp 145–148, doi:<a href="https://doi.org/10.1145/1152215.1152245">10.1145/1152215.1152245</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CM. Rebman, MW. Aiken, CG. Cegielski, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Rebman CM Jr., Aiken MW, Cegielski CG (2003) Speech recognition in the human-computer interface. Inf Manage 40" /><p class="c-article-references__text" id="ref-CR40">Rebman CM Jr., Aiken MW, Cegielski CG (2003) Speech recognition in the human-computer interface. Inf Manage 40(6):509–519. doi:<a href="https://doi.org/10.1016/S0378-7206(02)00067-8">10.1016/S0378-7206(02)00067-8</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0378-7206%2802%2900067-8" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Speech%20recognition%20in%20the%20human-computer%20interface&amp;journal=Inf%20Manage&amp;doi=10.1016%2FS0378-7206%2802%2900067-8&amp;volume=40&amp;issue=6&amp;pages=509-519&amp;publication_year=2003&amp;author=Rebman%2CCM&amp;author=Aiken%2CMW&amp;author=Cegielski%2CCG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rekimoto J, Ayatsuka Y, Hayashi K (1998) Augment-able reality: situated communication through physical and dig" /><p class="c-article-references__text" id="ref-CR41">Rekimoto J, Ayatsuka Y, Hayashi K (1998) Augment-able reality: situated communication through physical and digital spaces. In: Proceedings of the 2nd IEEE international symposium on wearable computers. IEEE Computer Society, Washington, DC, USA, pp 68–75</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Singletary BA, Starner TE (2001) Symbiotic interfaces for wearable face recognition. In: Proceedings of HCI in" /><p class="c-article-references__text" id="ref-CR42">Singletary BA, Starner TE (2001) Symbiotic interfaces for wearable face recognition. In: Proceedings of HCI international 2001 workshop on wearable computing, New Orleans, LA. pp 813–817</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Starner, S. Mann, B. Rhodes, J. Levine, J. Healey, D. Kirsch, RW. Picard, A. Pentland, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Starner T, Mann S, Rhodes B, Levine J, Healey J, Kirsch D, Picard RW, Pentland A (1997) Augmented reality thro" /><p class="c-article-references__text" id="ref-CR43">Starner T, Mann S, Rhodes B, Levine J, Healey J, Kirsch D, Picard RW, Pentland A (1997) Augmented reality through wearable computing. Presence Teleoperators Virtual Environ 6(4):452–460</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20through%20wearable%20computing&amp;journal=Presence%20Teleoperators%20Virtual%20Environ&amp;volume=6&amp;issue=4&amp;pages=452-460&amp;publication_year=1997&amp;author=Starner%2CT&amp;author=Mann%2CS&amp;author=Rhodes%2CB&amp;author=Levine%2CJ&amp;author=Healey%2CJ&amp;author=Kirsch%2CD&amp;author=Picard%2CRW&amp;author=Pentland%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sun Y, Prendinger H, Shi Y, Chen F, Chung V, Ishizuka M (2008) The hinge between input and output: understandi" /><p class="c-article-references__text" id="ref-CR44">Sun Y, Prendinger H, Shi Y, Chen F, Chung V, Ishizuka M (2008) The hinge between input and output: understanding the multimodal input fusion results in an agent-based multimodal presentation system. In: Conference on human factors in computing systems (CHI ’08), Florence, Italy. pp 3483–3488</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tanriverdi V, Jacob R (2000) Interacting with eye movements in virtual environments. In: Proceedings of the SI" /><p class="c-article-references__text" id="ref-CR45">Tanriverdi V, Jacob R (2000) Interacting with eye movements in virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, p 272</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tomasi C, Kanade T (1991) Detection and tracking of point features. Tech. Rep. CMU-CS-91-132, Carnegie Mellon " /><p class="c-article-references__text" id="ref-CR46">Tomasi C, Kanade T (1991) Detection and tracking of point features. Tech. Rep. CMU-CS-91-132, Carnegie Mellon University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Turpin A, Scholer F (2006) User performance versus precision measures for simple search tasks. In: SIGIR ’06: " /><p class="c-article-references__text" id="ref-CR47">Turpin A, Scholer F (2006) User performance versus precision measures for simple search tasks. In: SIGIR ’06: proceedings of the international ACM SIGIR conference on research and development in information retrieval. ACM, New York, NY, pp 11–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vertegaal R (2002) Designing attentive interfaces. In: Proceedings of the 2002 symposium on eye tracking resea" /><p class="c-article-references__text" id="ref-CR48">Vertegaal R (2002) Designing attentive interfaces. In: Proceedings of the 2002 symposium on eye tracking research &amp; applications. ACM, p 30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Viola P, Jones M (2001) Rapid object detection using a boosted cascade of simple features. In: IEEE Computer S" /><p class="c-article-references__text" id="ref-CR49">Viola P, Jones M (2001) Rapid object detection using a boosted cascade of simple features. In: IEEE Computer Society conference on computer vision and pattern recognition (CVPR’01), Kauai, Hawaii, pp 511–518</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang H, Tan CC, Li Q (2008) Snoogle: a search engine for the physical world. In: Proceedings of the 27th confe" /><p class="c-article-references__text" id="ref-CR50">Wang H, Tan CC, Li Q (2008) Snoogle: a search engine for the physical world. In: Proceedings of the 27th conference on computer communications (IEEE INFOCOM), pp 1382–1390</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Ward, DJC. MacKay, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Ward DJ, MacKay DJC (2002) Fast hands-free writing by gaze direction. Nature 418(6900):838" /><p class="c-article-references__text" id="ref-CR51">Ward DJ, MacKay DJC (2002) Fast hands-free writing by gaze direction. Nature 418(6900):838</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F418838a" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20hands-free%20writing%20by%20gaze%20direction&amp;journal=Nature&amp;volume=418&amp;issue=6900&amp;publication_year=2002&amp;author=Ward%2CDJ&amp;author=MacKay%2CDJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yamagishi J, Usabaev B, King S, Watts O, Dines J, Tian J, Hu R, Guan Y, Oura K, Tokuda K, Karhila R, Kurimo M " /><p class="c-article-references__text" id="ref-CR52">Yamagishi J, Usabaev B, King S, Watts O, Dines J, Tian J, Hu R, Guan Y, Oura K, Tokuda K, Karhila R, Kurimo M (2009) Thousands of voices for HMM-based speech synthesis. In: Proceedings of the 10th annual conference of the international speech communication association, INTERSPEECH 2009, ISCA, Brighton, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yap KK, Srinivasan V, Motani M (2005) MAX: human-centric search of the physical world. In: Proceedings of the " /><p class="c-article-references__text" id="ref-CR53">Yap KK, Srinivasan V, Motani M (2005) MAX: human-centric search of the physical world. In: Proceedings of the 3rd international conference on embedded networked sensor systems. ACM, pp 166–179</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0183-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>Antti Ajanki, Melih Kandemir, Samuel Kaski, Markus Koskela, Mikko Kurimo, Jorma Laaksonen, Kai Puolamäki, and Teemu Ruokolainen belong to Adaptive Informatics Research Centre at Aalto University, Antti Ajanki, Melih Kandemir, Samuel Kaski, and Kai Puolamäki to Helsinki Institute for Information Technology HIIT, and Kai Puolamäki to the Finnish Centre of Excellence in Algorithmic Data Analysis. This work has been funded by Aalto MIDE programme (project UI-ART) and in part by Finnish Funding Agency for Technology and Innovation (TEKES) under the project DIEM/MMR and by the PASCAL2 Network of Excellence, ICT 216886.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Information and Computer Science, Aalto University, Espoo, Finland</p><p class="c-article-author-affiliation__authors-list">Antti Ajanki, Melih Kandemir, Markus Koskela, Mikko Kurimo, Jorma Laaksonen &amp; Teemu Ruokolainen</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Media Technology, Aalto University, Espoo, Finland</p><p class="c-article-author-affiliation__authors-list">Hannes Gamper, Kai Puolamäki &amp; Timo Tossavainen</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">The Human Interface Technology Laboratory New Zealand (HIT Lab NZ), University of Canterbury, Christchurch, New Zealand</p><p class="c-article-author-affiliation__authors-list">Mark Billinghurst</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Nokia Research Center, Tampere, Finland</p><p class="c-article-author-affiliation__authors-list">Toni Järvenpää</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Aalto University and University of Helsinki, Helsinki Institute for Information Technology HIIT, Helsinki, Finland</p><p class="c-article-author-affiliation__authors-list">Samuel Kaski</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Antti-Ajanki"><span class="c-article-authors-search__title u-h3 js-search-name">Antti Ajanki</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Antti+Ajanki&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Antti+Ajanki" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Antti+Ajanki%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mark-Billinghurst"><span class="c-article-authors-search__title u-h3 js-search-name">Mark Billinghurst</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mark+Billinghurst&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mark+Billinghurst" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mark+Billinghurst%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hannes-Gamper"><span class="c-article-authors-search__title u-h3 js-search-name">Hannes Gamper</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hannes+Gamper&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hannes+Gamper" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hannes+Gamper%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Toni-J_rvenp__"><span class="c-article-authors-search__title u-h3 js-search-name">Toni Järvenpää</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Toni+J%C3%A4rvenp%C3%A4%C3%A4&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Toni+J%C3%A4rvenp%C3%A4%C3%A4" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Toni+J%C3%A4rvenp%C3%A4%C3%A4%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Melih-Kandemir"><span class="c-article-authors-search__title u-h3 js-search-name">Melih Kandemir</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Melih+Kandemir&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Melih+Kandemir" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Melih+Kandemir%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Samuel-Kaski"><span class="c-article-authors-search__title u-h3 js-search-name">Samuel Kaski</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Samuel+Kaski&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samuel+Kaski" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samuel+Kaski%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Markus-Koskela"><span class="c-article-authors-search__title u-h3 js-search-name">Markus Koskela</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Markus+Koskela&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Markus+Koskela" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Markus+Koskela%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mikko-Kurimo"><span class="c-article-authors-search__title u-h3 js-search-name">Mikko Kurimo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mikko+Kurimo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mikko+Kurimo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mikko+Kurimo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jorma-Laaksonen"><span class="c-article-authors-search__title u-h3 js-search-name">Jorma Laaksonen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jorma+Laaksonen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jorma+Laaksonen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jorma+Laaksonen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kai-Puolam_ki"><span class="c-article-authors-search__title u-h3 js-search-name">Kai Puolamäki</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kai+Puolam%C3%A4ki&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kai+Puolam%C3%A4ki" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kai+Puolam%C3%A4ki%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Teemu-Ruokolainen"><span class="c-article-authors-search__title u-h3 js-search-name">Teemu Ruokolainen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Teemu+Ruokolainen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Teemu+Ruokolainen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Teemu+Ruokolainen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Timo-Tossavainen"><span class="c-article-authors-search__title u-h3 js-search-name">Timo Tossavainen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Timo+Tossavainen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Timo+Tossavainen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Timo+Tossavainen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0183-5/email/correspondent/c1/new">Antti Ajanki</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20augmented%20reality%20interface%20to%20contextual%20information&amp;author=Antti%20Ajanki%20et%20al&amp;contentID=10.1007%2Fs10055-010-0183-5&amp;publication=1359-4338&amp;publicationDate=2010-12-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ajanki, A., Billinghurst, M., Gamper, H. <i>et al.</i> An augmented reality interface to contextual information.
                    <i>Virtual Reality</i> <b>15, </b>161–173 (2011). https://doi.org/10.1007/s10055-010-0183-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0183-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-27">27 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-15">15 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-12-16">16 December 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0183-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0183-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gaze tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Information retrieval</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Machine learning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Pattern recognition</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0183-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=183;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

