<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Supporting visually impaired children with software agents in a multim"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Supporting visually impaired children with software agents in a multimodal learning environment"/>

    <meta name="dc.source" content="Virtual Reality 2006 9:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-01-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work presented in this paper was to develop a multimodal software architecture and applications to support learning of visually impaired children. The software architecture is based on software agents, and has specific support for visual, auditory and haptic interaction. It has been used successfully with different groups of 7-8&#160;year-old and 12&#160;year-old visually impaired children. In this paper we discuss the enabling software technology and interaction techniques aimed to realize our goal and our experiences in the actual use of the system."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-01-11"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="108"/>

    <meta name="prism.endingPage" content="117"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0011-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0011-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0011-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0011-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Supporting visually impaired children with software agents in a multimodal learning environment"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2006/03"/>

    <meta name="citation_online_date" content="2006/01/11"/>

    <meta name="citation_firstpage" content="108"/>

    <meta name="citation_lastpage" content="117"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0011-5"/>

    <meta name="DOI" content="10.1007/s10055-005-0011-5"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0011-5"/>

    <meta name="description" content="Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inapprop"/>

    <meta name="dc.creator" content="Rami Saarinen"/>

    <meta name="dc.creator" content="Janne J&#228;rvi"/>

    <meta name="dc.creator" content="Roope Raisamo"/>

    <meta name="dc.creator" content="Eva Tuominen"/>

    <meta name="dc.creator" content="Marjatta Kangassalo"/>

    <meta name="dc.creator" content="Kari Peltola"/>

    <meta name="dc.creator" content="Jouni Salo"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Patom&#228;ki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI&#8217;04): ACM Press, pp 281-288"/>

    <meta name="citation_reference" content="SensAble Technologies Inc. http://www.sensable.com"/>

    <meta name="citation_reference" content="Reachin Technologies AB. http://www.reachin.se"/>

    <meta name="citation_reference" content="3Dconnexion. http://www.3dconnexion.com/"/>

    <meta name="citation_reference" content="Kangassalo M (1997) The Formation of Children&#8217;s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188"/>

    <meta name="citation_reference" content="Kangassalo M (1991/1999) PICCO -kuvallinen tietokonesimulaatio [PICCO, Pictorial Computer Simulation]. CD-ROM 951-98035-0-5 "/>

    <meta name="citation_reference" content="citation_title=The pictorial computer-based simulation in natural sciences for children&#8217;s use; citation_inbook_title=Information modelling and knowledge bases 3rd Foundations, theory and applications; citation_publication_date=1992; citation_pages=511-524; citation_id=CR7; citation_author=M Kangassalo; citation_publisher=IOS Press"/>

    <meta name="citation_reference" content="Jansson G, Billberger K (1999) The PHANToM used without visual guidance. In: The first phantom users research symposium (PURS 99). http://mbi.dkfz-heidelberg.de/purs99"/>

    <meta name="citation_reference" content="Magnusson C, Rassmus-Gr&#246;hn K, Sj&#246;str&#246;m C, Danielsson H (2002) Navigation and recognition in complex haptic virtual environments&#8212;reports from an extensive study with blind users. In: Proceedings of Eurohaptics 2002"/>

    <meta name="citation_reference" content="Sj&#246;str&#246;m C (2001) Designing haptic computer interfaces for blind people. In: proceedings of sixth international symposium on signal processing and its applications (ISSPA 2001). IEEE, pp68-71"/>

    <meta name="citation_reference" content="Sj&#246;str&#246;m C (2002) Non-visual haptic interaction design: guidelines and applications. Doctoral dissertation, Certec, Lund Institute of Technology"/>

    <meta name="citation_reference" content="Computer graphics access for Blind people through a haptic virtual environment. http://www.grab-eu.com"/>

    <meta name="citation_reference" content="Wood J, Magennis M, Arias E, Gutierrez T, Graupp H, Bergamasco M (2003) The design and evaluation of a computer game for the blind in the GRAB haptic audio virtual environment. In: Proceedings of Eurohaptics 2003"/>

    <meta name="citation_reference" content="Moran D-B, Cheyer AJ, Julia LE, Martin DL, Park S (1997) Multimodal user interfaces in the open agent architecture. In: Proceedings of the 2nd international conference on intelligent user interfaces. ACM Press, pp61-68"/>

    <meta name="citation_reference" content="Cheyer A, Julia L (1999) InfoWiz: An animated voice interactive information system. In: Third international conference on autonomous agents (Agents&#8217;99), communicative agents workshop"/>

    <meta name="citation_reference" content="Moore R, Dowding J, Bratt H, Gawron J M, Gorfu Y, Cheyer A (1997) CommandTalk: A spoken-language interface for battlefield simulations. In: Proceedings of the fifth conference on applied natural language processing. morgan kaufmann publishers inc. San Francisco, CA, USA, pp1-7"/>

    <meta name="citation_reference" content="Cheyer A, Julia L (1998) Multimodal Maps: An agent-based approach. multimodal human-computer communication. lecture notes in artificial intelligence 1374. Springer, pp111-121"/>

    <meta name="citation_reference" content="Coutaz J (1987) PAC, an object oriented model for dialog design. in: proceedings of interact &#8216;87. North-Holland, pp431-436"/>

    <meta name="citation_reference" content="Nigay L, Coutaz J (1995) A generic platform for addressing the multimodal challenge. In: Proceedings of ACM CHI&#8217;95. ACM press, pp98-105"/>

    <meta name="citation_reference" content="Hietala P, Niemirepo T (1995) A framework for building agent-based learning environments. In: Proceedings of AI-ED 95: Artificial intelligence in education. AACE, p 578"/>

    <meta name="citation_reference" content="Foundation for Intelligent Physical Agents. http://www.fipa.org/"/>

    <meta name="citation_reference" content="CLIPS: A Tool for Building Expert Systems. http://www.ghg.net/clips/CLIPS.html"/>

    <meta name="citation_reference" content="SQLite. http://www.sqlite.org/"/>

    <meta name="citation_reference" content="citation_title=Modelling a natural phenomenon for a pictorial computer-based simulation; citation_inbook_title=Information modeling and knowledge bases IX; citation_publication_date=1998; citation_pages=239-254; citation_id=CR24; citation_author=M Kangassalo; citation_publisher=IOS press"/>

    <meta name="citation_reference" content="citation_title=PICCO as a Cognitive Tool; citation_inbook_title=Information modelling and knowledge bases VII; citation_publication_date=1996; citation_pages=344-357; citation_id=CR25; citation_author=M Kangassalo; citation_publisher=IOS press"/>

    <meta name="citation_reference" content="citation_journal_title=Eur Early Childhood Educ Res J; citation_title=Progressive inquiry learning for children&#8211;Experiences, Possibilities, Limitations; citation_author=K Lonka, K Hakkarainen, M Sintonen; citation_volume=8; citation_publication_date=2000; citation_pages=7-23; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_title=Proactive agents that support children&#8217;s exploratory learning; citation_inbook_title=Information modelling and knowledge bases XVI; citation_publication_date=2005; citation_pages=123-133; citation_id=CR27; citation_author=M Kangassalo; citation_author=R Raisamo; citation_author=P Hietala; citation_author=J J&#228;rvi; citation_author=K Peltola; citation_author=R Saarinen; citation_author=E Tuominen; citation_author=A Hippula; citation_publisher=IOS press"/>

    <meta name="citation_author" content="Rami Saarinen"/>

    <meta name="citation_author_institution" content="Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Janne J&#228;rvi"/>

    <meta name="citation_author_email" content="janne.jarvi@cs.uta.fi"/>

    <meta name="citation_author_institution" content="Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Roope Raisamo"/>

    <meta name="citation_author_institution" content="Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Eva Tuominen"/>

    <meta name="citation_author_institution" content="Department of Teacher Education, Early Childhood Education, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Marjatta Kangassalo"/>

    <meta name="citation_author_institution" content="Department of Teacher Education, Early Childhood Education, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Kari Peltola"/>

    <meta name="citation_author_institution" content="Department of Teacher Education, Early Childhood Education, University of Tampere, Tampere, Finland"/>

    <meta name="citation_author" content="Jouni Salo"/>

    <meta name="citation_author_institution" content="Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, Tampere, Finland"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0011-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0011-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Supporting visually impaired children with software agents in a multimodal learning environment"/>
        <meta property="og:description" content="Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work presented in this paper was to develop a multimodal software architecture and applications to support learning of visually impaired children. The software architecture is based on software agents, and has specific support for visual, auditory and haptic interaction. It has been used successfully with different groups of 7-8&amp;nbsp;year-old and 12&amp;nbsp;year-old visually impaired children. In this paper we discuss the enabling software technology and interaction techniques aimed to realize our goal and our experiences in the actual use of the system."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Supporting visually impaired children with software agents in a multimodal learning environment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0011-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Multimodal interaction, Software agent architecture, Visually impaired children, Learning environments, Haptics, Auditory feedback","kwrd":["Multimodal_interaction","Software_agent_architecture","Visually_impaired_children","Learning_environments","Haptics","Auditory_feedback"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0011-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0011-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=11;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0011-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Supporting visually impaired children with software agents in a multimodal learning environment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0011-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0011-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-01-11" itemprop="datePublished">11 January 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Supporting visually impaired children with software agents in a multimodal learning environment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rami-Saarinen" data-author-popup="auth-Rami-Saarinen">Rami Saarinen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, 33014, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Janne-J_rvi" data-author-popup="auth-Janne-J_rvi" data-corresp-id="c1">Janne Järvi<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, 33014, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Roope-Raisamo" data-author-popup="auth-Roope-Raisamo">Roope Raisamo</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, 33014, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Eva-Tuominen" data-author-popup="auth-Eva-Tuominen">Eva Tuominen</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Department of Teacher Education, Early Childhood Education, University of Tampere, 33014, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Marjatta-Kangassalo" data-author-popup="auth-Marjatta-Kangassalo">Marjatta Kangassalo</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Department of Teacher Education, Early Childhood Education, University of Tampere, 33014, Tampere, Finland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kari-Peltola" data-author-popup="auth-Kari-Peltola">Kari Peltola</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Department of Teacher Education, Early Childhood Education, University of Tampere, 33014, Tampere, Finland" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jouni-Salo" data-author-popup="auth-Jouni-Salo">Jouni Salo</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Tampere" /><meta itemprop="address" content="grid.5509.9, 0000000123146254, Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, 33014, Tampere, Finland" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">108</span>–<span itemprop="pageEnd">117</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">215 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">11 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0011-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Visually impaired children have a great disadvantage in the modern society since their ability to use modern computer technology is limited due to inappropriate user interfaces. The aim of the work presented in this paper was to develop a multimodal software architecture and applications to support learning of visually impaired children. The software architecture is based on software agents, and has specific support for visual, auditory and haptic interaction. It has been used successfully with different groups of 7-8 year-old and 12 year-old visually impaired children. In this paper we discuss the enabling software technology and interaction techniques aimed to realize our goal and our experiences in the actual use of the system.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In the modern society computers are used to teach many subjects in schools. This has created a new problem for visually impaired children, since they often cannot take advantage of the teaching materials created to be used with a graphical user interface. Even if they had up-to-date hardware at home, it is of no real use without appropriate software that supports non-visual use.</p><p>There has been some development in teaching materials for the blind and visually impaired, but these are not available widely or have limited use. An exception is audio books that have become popular, but they lack the interactive quality of teaching programs.</p><p>Children are a special group of computer users who require their own software and user interfaces that are consistent with their development level. It is challenging to design user interfaces for young children, but this challenge is much greater when the children are blind or visually impaired. Since it is necessary to support their learning and using of computers with other modalities, the complexity of user interface design and supporting architecture is greater than in interfaces for common computer users.</p><p>In studies by Patomäki et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e374">1</a>] games and learning environments were built for visually impaired children 3.5-7.5 years of age. It is evident that young children’s use of the PHANTOM haptic device [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="SensAble Technologies Inc. http://www.sensable.com" href="/article/10.1007/s10055-005-0011-5#ref-CR2" id="ref-link-section-d58352e377">2</a>] (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig1">1</a>) is greatly affected by their motor abilities and development level. When planning the present study we decided to direct our efforts in pre-school and elementary school children who should have more developed fine-motor skills and who are more capable of expressing themselves.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A child using the PHANTOM device</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>The goal of our research was to produce a proactive and multimodal agent-based learning environment that would support visually impaired children’s learning and cognitive development. The pedagogical approach of this system is based on exploratory learning. This means that a child can explore the phenomena independently, guided by his/her own interests and questions. No direct drills or tasks were included in the system. A single application in the system is named as a micro world.</p><p>We used a Reachin Display System [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Reachin Technologies AB. http://www.reachin.se" href="/article/10.1007/s10055-005-0011-5#ref-CR3" id="ref-link-section-d58352e407">3</a>] with a SensAble PHANTOM Desktop haptic device [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="SensAble Technologies Inc. http://www.sensable.com" href="/article/10.1007/s10055-005-0011-5#ref-CR2" id="ref-link-section-d58352e410">2</a>] and stereo audio through speakers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig1">1</a>). There was also a 2D projected view of the virtual environment in case the child had a residual sight and could make use of it. The buttons of the Magellan SpaceMouse [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="3Dconnexion. http://www.3dconnexion.com/" href="/article/10.1007/s10055-005-0011-5#ref-CR4" id="ref-link-section-d58352e416">4</a>] were used for input.</p><p>The term <i>software agent</i> is used in the literature for a variety of purposes. Also, in this paper it is used in two levels of abstraction. In the lower level, agent-based models describe a system as a set of interconnected agents. An agent in this sense is an autonomous software component which has a state and which communicates with other agents by using messages. In the higher level, our approach is to use software agents to support learning and navigation. These agents can be personified having individual voices, and they interact directly with the user.</p><p>The phenomena chosen for computer simulation were our solar system, the interrelations between the Earth and the Sun, the Earth, the atmosphere, and the interior layers of the Earth. The preliminary results of the user studies with two groups of visually impaired children support our design solutions and the usability of the interaction techniques developed in the system.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Exploratory learning and the chosen phenomena were previously used in the PICCO project [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188" href="/article/10.1007/s10055-005-0011-5#ref-CR5" id="ref-link-section-d58352e435">5</a>]. The software had 2D graphics and the focus group was 5–8 year-old children with normal eyesight. The experiences gained from the PICCO project were used as a starting point of the project presented in this paper.</p><p>The pictorial computer simulation PICCO concentrates in the variations of sunlight and heat of the Sun as experienced on the Earth related to the positions of the Earth and the Sun in space. In the simulation it is possible to explore the variations of sunlight and heat of the Sun and their effects on the Earth in a natural environment. It is also possible to examine the origin of these phenomena from the basis of the interconnections and positions of the Earth and the Sun in space. On the Earth level the simulation concentrates on phenomena, which are close to the everyday experiences of children, such as day and night, seasons, changes in the life of plants and birds etc. The simulation program has been implemented in such a way that the knowledge structure and theory of the phenomenon are based on events appearing together with the phenomenon in question, and these events are illustrated. The simulation tool has been developed for children’s spontaneous exploratory activity with the goal of supporting children’s conceptual learning while interacting with the environment. (See [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kangassalo M (1991/1999) PICCO -kuvallinen tietokonesimulaatio [PICCO, Pictorial Computer Simulation]. CD-ROM 951-98035-0-5 " href="/article/10.1007/s10055-005-0011-5#ref-CR6" id="ref-link-section-d58352e441">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kangassalo M (1992) The pictorial computer-based simulation in natural sciences for children’s use. In: Ohsuga S, Kangassalo H, Jaakkola H, Hori K, Yonezaki N (eds) Information modelling and knowledge bases 3rd Foundations, theory and applications. IOS Press, Amsterdam, pp 511-524" href="/article/10.1007/s10055-005-0011-5#ref-CR7" id="ref-link-section-d58352e444">7</a>]).</p><p>In the recent years there has been some research concerning the use of the PHANTOM device in developing software for visually impaired persons. For instance, Jansson and Billberger [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Jansson G, Billberger K (1999) The PHANToM used without visual guidance. In: The first phantom users research symposium (PURS 99). http://mbi.dkfz-heidelberg.de/purs99" href="/article/10.1007/s10055-005-0011-5#ref-CR8" id="ref-link-section-d58352e450">8</a>] reported that blind persons can identify 3D objects faster with hands than with a PHANTOM device. They argued that with practice the performance can improve somewhat. According to Magnusson et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Magnusson C, Rassmus-Gröhn K, Sjöström C, Danielsson H (2002) Navigation and recognition in complex haptic virtual environments—reports from an extensive study with blind users. In: Proceedings of Eurohaptics 2002" href="/article/10.1007/s10055-005-0011-5#ref-CR9" id="ref-link-section-d58352e453">9</a>], blind users can recognize quite complex objects, and they are also able to navigate in virtual environments as long as the environment is realistic. Sjöström [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sjöström C (2001) Designing haptic computer interfaces for blind people. In: proceedings of sixth international symposium on signal processing and its applications (ISSPA 2001). IEEE, pp68-71" href="/article/10.1007/s10055-005-0011-5#ref-CR10" id="ref-link-section-d58352e456">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Sjöström C (2002) Non-visual haptic interaction design: guidelines and applications. Doctoral dissertation, Certec, Lund Institute of Technology" href="/article/10.1007/s10055-005-0011-5#ref-CR11" id="ref-link-section-d58352e459">11</a>] has studied non-visual haptic interaction using the PHANTOM device. In his informal experiments with visually impaired users he came up with a list of design guidelines for one-point haptics, which include guidelines for navigation, finding objects and understanding objects. Patomäki et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e462">1</a>] suggest that for young children the objects in the virtual environment should be very simple. They used real mockup models of the environment to help the children get familiar with the simulation. This proved to be useful in their study. The results of Patomäki et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e466">1</a>] also supported Sjöström’s findings.</p><p>The GRAB project [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Computer graphics access for Blind people through a haptic virtual environment. http://www.grab-eu.com" href="/article/10.1007/s10055-005-0011-5#ref-CR12" id="ref-link-section-d58352e472">12</a>] has developed an architecture which enables visually impaired and blind people to explore three-dimensional virtual worlds using the senses of touch and hearing. The architecture is based on three tools: 3D force-feedback haptic interface used with two fingers; an audio interface for audio messages and verbal commands; and a haptic modeler for designing the objects in the environment. The GRAB system was successfully used in a computer game for the blind [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Wood J, Magennis M, Arias E, Gutierrez T, Graupp H, Bergamasco M (2003) The design and evaluation of a computer game for the blind in the GRAB haptic audio virtual environment. In: Proceedings of Eurohaptics 2003" href="/article/10.1007/s10055-005-0011-5#ref-CR13" id="ref-link-section-d58352e475">13</a>]. The GRAB system uses two-handed interaction, which makes identifying objects and orientation easier for the blind. In one-point interaction, especially when used with only one hand like with the PHANTOM device, 3D objects must be designed carefully. We also believe that the use of such a system should be supported with other senses. Software agents are a natural way of providing this support.</p><p>Agents allow autonomous execution of multiple tasks making the monitoring of user actions easier. For instance, the Open Agent Architecture [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Moran D-B, Cheyer AJ, Julia LE, Martin DL, Park S (1997) Multimodal user interfaces in the open agent architecture. In: Proceedings of the 2nd international conference on intelligent user interfaces. ACM Press, pp61-68" href="/article/10.1007/s10055-005-0011-5#ref-CR14" id="ref-link-section-d58352e482">14</a>] (OAA) provides a distributed agent architecture especially targeted for multimodal user interfaces. The main emphasis is on the speech recognition, gestures and pen input. The system is built around a central facilitator that handles and forwards tasks that the agents want to have completed. The system allows dividing the task into subtasks and parallel execution of each subtask. The facilitator also provides global data storage and a blackboard-like functionality for it. Several applications have been built using OAA. For example the InfoWiz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Cheyer A, Julia L (1999) InfoWiz: An animated voice interactive information system. In: Third international conference on autonomous agents (Agents’99), communicative agents workshop" href="/article/10.1007/s10055-005-0011-5#ref-CR15" id="ref-link-section-d58352e485">15</a>], CommandTalk [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Moore R, Dowding J, Bratt H, Gawron J M, Gorfu Y, Cheyer A (1997) CommandTalk: A spoken-language interface for battlefield simulations. In: Proceedings of the fifth conference on applied natural language processing. morgan kaufmann publishers inc. San Francisco, CA, USA, pp1-7" href="/article/10.1007/s10055-005-0011-5#ref-CR16" id="ref-link-section-d58352e488">16</a>] and Multimodal maps [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Cheyer A, Julia L (1998) Multimodal Maps: An agent-based approach. multimodal human-computer communication. lecture notes in artificial intelligence 1374. Springer, pp111-121" href="/article/10.1007/s10055-005-0011-5#ref-CR17" id="ref-link-section-d58352e491">17</a>] take advantage of the multimodal capabilities of OAA. Using a central dispatcher is very common in agent-based systems.</p><p>Coutaz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Coutaz J (1987) PAC, an object oriented model for dialog design. in: proceedings of interact ‘87. North-Holland, pp431-436" href="/article/10.1007/s10055-005-0011-5#ref-CR18" id="ref-link-section-d58352e497">18</a>] has suggested an agent based approach to dialogue control. Her PAC model (Presentation, Abstraction, Control) describes an interactive system as a hierarchical collection of PAC agents. The Presentation facet of a PAC agent handles input and output behavior and the Abstraction facet contains its functional core. The Control facet controls communication between other agents and also between agent’s facets. The PAC model and its descendant PAC Amodeus [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Nigay L, Coutaz J (1995) A generic platform for addressing the multimodal challenge. In: Proceedings of ACM CHI’95. ACM press, pp98-105" href="/article/10.1007/s10055-005-0011-5#ref-CR19" id="ref-link-section-d58352e500">19</a>] have been used in fusion of multimodal input modalities.</p><p>In the agent-based learning environment EduAgents [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Hietala P, Niemirepo T (1995) A framework for building agent-based learning environments. In: Proceedings of AI-ED 95: Artificial intelligence in education. AACE, p 578" href="/article/10.1007/s10055-005-0011-5#ref-CR20" id="ref-link-section-d58352e506">20</a>] Hietala and Niemirepo introduced teacher and companion agents that have their own personalities and abilities. The teachers have different ways of teaching and companions may try to help the user with the mathematical exercises. The companions were designed to be human-like and thus they also could make mistakes. A part of the learning process was to work with the companion agent in various exercises and, for example, study the solution offered by it.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Software architecture for multimodal applications</h2><div class="c-article-section__content" id="Sec3-content"><h3 class="c-article__sub-heading" id="Sec4">Structure of the agent architecture</h3><p>We used a distributed agent architecture consisting of a set of concurrently running agents. The basic agent architecture can be divided into three separate functional components (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig2">2</a>). The system can be seen as a realization of a typical message dispatcher architecture where the Message Channel is the central dispatcher and plays the most vital part of the system by providing a centralized way for passing messages between agents around the network. The Agent Containers (and thus agents) are connected to the agent system via the Message Channel. The third component is the actual application that is handled by the Controller. The functionality and structure of Message Channel, Agent Container and agents are based on FIPA agent specifications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Foundation for Intelligent Physical Agents. http://www.fipa.org/" href="/article/10.1007/s10055-005-0011-5#ref-CR21" id="ref-link-section-d58352e524">21</a>].
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An overview of the agent system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The Controller works as a bridge between the user and the rest of the agent system. It provides the means to navigate in the scene as well as between scenes and allows the agents to manipulate the 3D environment and to interact with the user. It also provides information about the state of the world for the rest of the agent system. There are two special agents situated in the Agent Container of the Controller. The mediator agent acts as the representative of the Controller in the agent system and forwards messages sent to it to the Controller for further processing. A simple coordinate agent sends the user’s coordinates to the logging agent once every second.</p><p>The system can initiate interaction with the user by playing sounds, by moving the user to another micro world, or by using tactile or force feedback to inform the user on various things. Currently, the notification is done by playing a sound and shaking the stylus. The user either accepts or declines to hear the new information. The information may be a question in which case the system waits for the user’s answer for a certain amount of time.</p><h3 class="c-article__sub-heading" id="Sec5">Classes of agents</h3><p>Agents have different roles in our system. For example, all the interactions between the system and the user happen through a pedagogic agent (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0011-5#Sec9">4.3</a>) so that if another agent wants to interact with the user, it will send a corresponding message to the pedagogic agent asking it to do so.</p><p>Three other vital agents are present in the current system. First, there is a database agent that collects and keeps up all information in the system. It contains facts that the agents can query, modify and observe. Any agent can inform the database agent that it is interested to get a notification when a fact changes, and the database agent will send a message to the agent whenever the change happens.</p><p>Second, a filter agent inspects the information going through the system trying to find meaningful information. Filter agents may update the findings through the database agent or send the new information to other agents. A filter agent is a special type of agent that can contain one or more filters. These filters are usually used to observe very specific events or data and to provide new information based on them. The filters can and should be chained to provide a chain of information refining the data starting with atomic facts and ending up with high, level information, assumptions and conclusions. Each micro world has a specific filter agent that monitors events specific to this micro world and updates the database accordingly.</p><p>Third, perhaps the most important agent is the rule engine agent that handles most of the interactive functionality. The agent has certain patterns that it tries to find in the user’s actions and it will react to those patterns. The rule engine agent is built on top of the CLIPS rule engine [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="CLIPS: A Tool for Building Expert Systems. http://www.ghg.net/clips/CLIPS.html" href="/article/10.1007/s10055-005-0011-5#ref-CR22" id="ref-link-section-d58352e567">22</a>] and the SQLite database library [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="SQLite. http://www.sqlite.org/" href="/article/10.1007/s10055-005-0011-5#ref-CR23" id="ref-link-section-d58352e570">23</a>]. A Prolog engine has also been integrated in the system. The agent is tightly connected to the database agent so that it is possible to efficiently pass information between these two components. That is, the rule engine agent observes all the facts in the database and updates the corresponding facts in its working memory, and vice versa. As a rule engine, the agent has certain prerequisites (time being one) that, once being fulfilled, will cause some event to happen: another new piece of information being created or perhaps a request for interaction with the user.</p><p>The following example (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig3">3</a>) illustrates the activity of the agent architecture and individual agents. In this scenario, the user is studying the Solar System and has been listening to the detailed information of Mercury and Venus. The user finds the Earth by following its orbit. The user presses the planet to hear more detailed information of the Earth. This will cause a sequence of events to happen: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>The Controller sends a message that the Earth was pressed to all logging agents.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>As step one happens, one of our filters was listening logging messages and decides that one fact in the shared database has to be updated. The filter sends an update message to all databases.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>The database agent updates the information and sends a message about the change to all the agents that currently observe the changed fact.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>As step three happens, our rule-engine agent listens to every single fact in the database and receives the update message. The agent updates its working memory and this causes two rules to be fired</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>Detailed information should be played.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">6.</span>
                      
                        <p>More information about rock planets should be played.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">7.</span>
                      
                        <p>These two interaction requests are sent to the pedagogical agent that forwards them to the Controller.</p>
                      
                    </li>
                  </ol>
                <p>All of the agents in this example use the default way of communication through the Agent Container and Message Channel. However, this can easily become a bottleneck, so agents that send constantly a lot of data should negotiate a custom connection to another agent using the conventional means and then use the custom connection to send the data to the receiving parties. The architecture allows agents to open new TCP/IP and UDP network connections and to create new threads.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A chain of filters. An example of handling an event in the agent architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">A learning environment for visually impaired children</h2><div class="c-article-section__content" id="Sec6-content"><p>We have constructed a simulation application that makes use of the software architecture presented above. The simulation is specifically aimed for visually impaired children. The children can study natural phenomena, which are related to the Earth, the Sun, and the Solar System. The aim was to produce agents that support the concept learning process of visually impaired and normally seeing children. In this chapter, we concentrate on presenting the applications and the navigation interface.</p><h3 class="c-article__sub-heading" id="Sec7"> Selected natural phenomena</h3><p>When selecting the natural phenomena for the simulation application it was essential that the phenomenon was important and significant in everyday life. The simulated phenomena have to awaken sufficient interest in the children and to efficiently utilize possibilities offered by the multimode interface technology. The phenomena chosen are the ones that can in no other way be easily and illustratively presented, such as phenomena linked with space and elementary astronomy. As an important selection criteria for the chosen natural phenomenon, it can be stated that a selected natural phenomenon with its conformities to law, forms a clear, well-organized knowledge structure and theory, and these aspects lay a strong and well-defined foundation for the modeling of the phenomena for the simulation application. The selected phenomena include multilevel interrelationships and central concepts such as space and time. The whole phenomenon is rather complex and abstract, but the phenomena are clearly integrated with each other and they form a coherent theory. (See also [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188" href="/article/10.1007/s10055-005-0011-5#ref-CR5" id="ref-link-section-d58352e689">5</a>])</p><h3 class="c-article__sub-heading" id="Sec8">Cognitive requirements for modeling and designing</h3><p>The cognitive requirements for modeling and designing the natural phenomena for computer-based learning environment are based on theories and concepts of cognitive psychology, cognitive science, socio-cognitive approach and science learning. The main aim is that a constructed learning environment could support children in forming integrated abstract conceptual structures and models of the selected natural phenomena and support them in continuous knowledge construction process concerning the phenomena in question. Thus, cognitive requirements have to be taken into account when selecting the natural phenomena, writing the manuscript, modeling the phenomena, and simulating the phenomena onto the computer, as well as displaying the simulation on the screen and using the computer simulation. (See also [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188" href="/article/10.1007/s10055-005-0011-5#ref-CR5" id="ref-link-section-d58352e700">5</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kangassalo M (1998) Modelling a natural phenomenon for a pictorial computer-based simulation. In: Kangassalo H, Charrel PJ, Jaakkola H (eds) Information modeling and knowledge bases IX. IOS press, Amsterdam, pp 239-254" href="/article/10.1007/s10055-005-0011-5#ref-CR24" id="ref-link-section-d58352e703">24</a>])</p><p>The phenomena have to be modeled for the simulation application according to the theory and existing knowledge of the phenomena. This means, for example, that information and knowledge on the screen and in the agents’ descriptions, explanations and guidance have been designed and implemented according to the present scientific knowledge. This is important because of children’s knowledge construction process and the formation of information and conceptual structures. These are significant because integrated and organized information, as well as knowledge structures in human memory at a general level of the phenomenon in question, is important to effective and demanding thinking, continuous knowledge construction and the theory formation. In this application these requirements have been taken into account. (See e.g. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188" href="/article/10.1007/s10055-005-0011-5#ref-CR5" id="ref-link-section-d58352e709">5</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Kangassalo M (1996) PICCO as a Cognitive Tool. In: Tanaka Y, Kangassalo H, Jaakkola H, Yamamoto A (eds) Information modelling and knowledge bases VII. IOS press, Amsterdam, pp 344-357" href="/article/10.1007/s10055-005-0011-5#ref-CR25" id="ref-link-section-d58352e712">25</a>])</p><p>It is important that the use of the application is based on the user’s own activity. Children can proceed according to their own interests and ideas. In the application, there are no paths or rules on how to explore and go forward. Children can use as much time as they like each time. All this provides the children with possibilities to explore the phenomenon any time as long as they want and in the order they wish. When the program is under the user’s control, it is possible for the user to concentrate on the phenomenon in question. A child’s own activity, attention and interest, supports the development and construction of conceptual structures of the phenomenon within children. The more complicated the phenomenon is, the more important is a child’s own activity and interest in analyzing and organizing information and its storing into the memory. (See e.g. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188" href="/article/10.1007/s10055-005-0011-5#ref-CR5" id="ref-link-section-d58352e718">5</a>])</p><h3 class="c-article__sub-heading" id="Sec9">Proactive pedagogical agents</h3><p>The computer simulation provides the children with an exploratory learning environment where they can explore the selected phenomena according to their own interests and questions. Children’s own questions are considered as a starting point for explorations. A child’s learning is viewed as an active process guided by his/her own questions and previous knowledge [see e.g. 26]. For achieving progress and deeper understanding children need guidance and support for their exploration. In our system proactive pedagogical agents have been used to scaffold each child’s inquiries in the simulation by making questions and encouraging child’s own questioning and hypothesis formation as an aim to guide the child’s exploration process towards scientific inquiry. Agents’ operations in this system are based mainly on auditory and haptic feedback, since the system is developed especially for visually impaired children.</p><p>Proactive pedagogic agents support children’s explorations by encouraging a child’s own questioning, directing a child’s attention to objects and their relationships in phenomena, making questions and suggestions, guiding from familiar everyday phenomena, and progress gradually to more complicated topics and the causes and explanations of the phenomena. The agents scaffold each child with respects to his/her capabilities and exploration paths. The agents don’t make any decisions for the child or force him or her in any particular exploration path. At any moment a child can choose either to listen to what the agents ask or suggest or to ignore them by pressing the yes or no button. As a child’s explorations proceed, the agents support may decrease step by step. The agents support is based on a child’s explorations and user profiles. Very important in the agents action is the right timing and the form of the support and questions. The construction of the rules of proactive pedagogic agents has been one of subjects of designing and testing.</p><p>The agents in each micro world have different imaginary characters and different names and voices. The entire learning environment has been constructed so that the narration and play are essential parts in children’s explorations. These elements form an important pedagogical support system for children’s exploration, science learning and thinking. This is because children’s thinking takes place in the form of continuing events, and fairy tales and stories support in recognizing and keeping in mind wholeness’s. In addition the meaning of different imaginary characters is to help children in analyzing and recognizing each individual application (so called micro world) and this again helps children in navigation and the formation of interrelationships of different phenomena. (See also [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Kangassalo M, Raisamo R, Hietala P, Järvi J, Peltola K, Saarinen R, Tuominen E, Hippula A (2005) Proactive agents that support children’s exploratory learning. In: Kiyoki Y, Wangler B, Jaakkola H, Kangassalo H (eds) Information modelling and knowledge bases XVI. IOS press, Amsterdam, pp 123-133" href="/article/10.1007/s10055-005-0011-5#ref-CR27" id="ref-link-section-d58352e733">27</a>])</p><h3 class="c-article__sub-heading" id="Sec10">Micro worlds</h3><p>The simulation consists of six micro worlds. The micro worlds are the Earth, the Solar System, the Earth Orbit, the Atmosphere, the Earth Internal Layers, and the Study Room (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>). The user starts from the central station (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, center). From there he/she can open doors to other micro worlds. A virtual door can be opened, by pushing it with the PHANTOM stylus. The system then selects the corresponding micro world and guides the stylus to a suitable starting point.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The navigation structure and the six different applications developed on top of the architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>When the user is navigating from one micro world to another, he/she must travel through the Central station. This lessens the likelihood of getting lost in the environment because the user is always only one step away from the Central station.</p><p>Every micro world has its own representative with a different voice (such as Captain Planet, Andy Astronaut and the Earth Giant). When the user enters in a micro world, the representative gives an introduction and tells what the user can do in it. The main task of the agents in the system is to follow the user’s steps and paths of exploration and to provide adequate support for their exploration with questions and additional information through the representative.</p><p>In the <i>Solar System</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, top) the children can explore the Sun, the different planets and the circular system of the planets. The orbits of the planets are implemented as grooves on a black plane that represents the void of the space. The plane helps the child to find the planets by restricting the depth of the scene to the level of the planets so that the PHANTOM stylus can’t fall under the planets. Whenever the user enters an orbit the system tells which planet is in question. The planets are stationary, and implemented with a light magnetic pulling force in them. When the user finds a planet, the system tells its name and gives a brief description of it. If the user pushes the planet with the stylus the system tells some more information about it.</p><p>In the <i>Earth</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, top-right) the children can explore the surface of the globe. The spherical Earth can be felt three-dimensionally with the stylus. The surface of the globe is implemented with a bump map that gives it a noticeable texture that feels different in oceans, plain land and mountains. The Earth has a gravity field that can be felt with the stylus as a slight pull towards the Earth. The gravity pull helps the child to locate planet Earth in empty space. After the child has found the Earth the gravity helps to explore the globe and prevents the stylus from falling into the empty space around the globe. When exploring the surface of the Earth the child can hear the names of the continents spoken. There is also ambient background noise that contains people’s voices and different vehicle and animal sounds. The sounds of the sea can be heard when exploring the oceans. The user can rotate the Earth by moving the stylus in the far right side of the world. When doing so, the Earth starts to spin slowly and the user can hear the sound of a clock ticking. As a visual feedback it is possible to see a view of the spherical Earth and the virtual representation of the PHANTOM stylus.</p><p>In the <i>Earth Orbit</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, top-left) the children explore the Earth’s revolution around the Sun. They learn the relative position of the Earth to the Sun during different seasons. When the user follows the orbit the stylus is shown as a small planet Earth and when the user is outside of the Earth’s orbit the stylus is shown as a small space rocket. The system tells the season and plays sounds that are characteristic to that season (e.g. rain for autumn). The system also gives some general information such as the distance of the Earth from the Sun.</p><p>The <i>Study Room</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, bottom-right) contains a room with doors along its walls. When the user presses a door with the stylus the system presents him/her with a question about the contents of the micro worlds. These questions have simple yes/no answers and the user answers by pressing a yes or no button in the Space Mouse. The room has grooves on the floor and the users can find doors by following these aids.</p><p>In the <i>Earth Internal Layers</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, bottom-left) users are able to explore the internal layers of the planet Earth. The layers are represented as a cross section of northern half of the Earth. Layers can be freely explored with the PHANTOM stylus. The topmost layer is the hardest and ‘rockiest’ of all layers. When a user is moving towards the bottom the ‘feeling’ gets smoother and smoother. When reaching the Earth’s core the haptic feedback is simulating the feel of Earth’s liquid center. As visual feedback the user can see the cross section and the layers and a virtual representation of the PHANTOM stylus.</p><p>In the <i>Atmosphere</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, bottom) children can explore the layers of the atmosphere of the Earth. The different layers are presented as different ambient sounds. The lowest layer contains human sounds, birds and humming of trees. The next layers contain airplane sounds and different kinds of windy air current sounds. When a user approaches the top border of the application the sounds will get more silent and will disappear as the user goes out of the atmosphere to the space. Haptic feeling is very subtle and light but there’s still some damping when moving the stylus so that the child gets a concrete ‘feel of the air’. Visual feedback is presented as a simple background picture that shows how the atmosphere is fading into black space.</p><h3 class="c-article__sub-heading" id="Sec11">Navigation support</h3><p>There can be three types of transitions between the micro worlds. First, a pie-shaped navigation menu provides means to move back one level. Second, a micro world may also have a set of objects that can be used as push buttons that trigger a direct transition to another micro world. This kind of transition is called a ‘route’. Each object must have a pushable haptic surface that the ReachinAPI [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Reachin Technologies AB. http://www.reachin.se" href="/article/10.1007/s10055-005-0011-5#ref-CR3" id="ref-link-section-d58352e836">3</a>] provides. Finally, the agents may ask the pedagogic agent to ask the Controller to switch to some specific micro world. This enables a flexible application structure that could, for example, only implement navigation through agents.</p><p>We ended up using a door metaphor similar to the one by Patomäki et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e842">1</a>], and a single center point, the central station, from which the user would only be one step away (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig5">5</a>). Furthermore, we decided that the user should always travel through the central point when traveling from a micro world to another. That way getting lost between the micro worlds would be very unlikely. This was accomplished by disabling the navigation menu in every micro world and defining appropriate routes between the micro worlds. The user can get back to the center point by pushing a Magellan SpaceMouse button.When the user moves from one micro world to another the stylus is moved to the central position and held there until the new micro world is loaded and displayed. This is done to give the user a possibility to start exploring from the same point every time he/she enters the micro world thus allowing memorization of the scene and to strengthen the sense of location. The other reason to guide the stylus to a certain location when moving from scene to scene is that there may be some accidental force spikes if the stylus is located within the same space as some solid object in the new micro world. Moving the stylus away from the potential interference area makes the use of the application pleasant and smooth. In the early stages of the project we used a separate magnet that appeared at the same coordinates as the stylus and started to move towards the certain location dragging the stylus along. The early user studies proved this technique to be flawed, as the probability for the user to lose the guiding magnet was relatively high. The magnet was soon replaced with a global force vector that provides much smoother and more reliable way to move the stylus to a given location.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The central station</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Other haptic elements are also used to make the navigation in the micro worlds easier. The shape of the central station (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig5">5</a>, center) is hexagonal and the “doors” to other micro worlds are in the corners of the shape as they are easier to find that way. The Study Room and Solar System have grooves that the user can follow to locate something interesting; in the Solar System the grooves are actually the orbits of planets. Some of the interesting objects are magnetic to give the user a hint of their existence and to guide the user to find the objects. In some cases, mainly in the Earth micro world (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>, top-right), we use spring forces to restrict the user’s movement and to guide him or her to the interesting parts of the micro world. The spring in the Earth micro world also represents the idea of gravity.</p><p>For every micro world we also had a corresponding real plastic model. When touching it by fingers the child can get a general picture of the world he/she is exploring. Each micro world also has a distinct ambient sound or music to support the sense of location.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Experiences with the children</h2><div class="c-article-section__content" id="Sec12-content"><p>From the beginning of this study we have tried to incorporate children in the planning and testing process. The development and testing of the learning system started in January of 2003 and it has continued throughout the project. At first we tested the manuscript we had written for the agents’ operations. The testing included interviews of ten 5–9 year-old children. The interviews were conducted mainly at a day care center. In the interviews the children were asked about their interests and questions with regard to the selected phenomena. We also read some short narrations of the agents’ operations for them, and after that the children were asked how they liked the manuscript, how the narration could be improved and whether there were any difficult utterances used in agents’ expressions. This testing helped us improve the lines we had designed for the agents and at the same time it gave us insight into children’s interests concerning the selected natural phenomena.</p><p>The user tests have been carried out both in a usability laboratory and at the school for visually impaired children. The tests in the laboratory have been organized whenever some new designs have needed testing. Both visually impaired and sighted children have participated in these tests at different phases of the system development. Each test has been carefully planned, and to a large extent the testing procedure developed by Patomäki <i>et al.</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e894">1</a>] was used in the laboratory tests. This testing procedure is specifically adapted for visually impaired young children. All the tests were recorded on video. These small tests have helped us observe what kind of solutions support children’s independent explorations, and many of the solutions mentioned in a previous section (Navigation aids) were developed from the basis of these tests.</p><p>A larger test was conducted in spring 2004. The test was carried out at the school for visually impaired children. As we wanted to get as much feedback as possible from the usability and accessibility of our system, we wished to have a little older children than our actual target group to participate in this test. Thus, seven 12 year-old visually impaired children took part in this test during their one-week teaching period at the school. The testing procedure at the school differed from the laboratory tests because in this test the children’s task was to use the system and act as ‘child experts’ to give comments and feedback about the system and to evaluate how they thought that smaller children would be able to use the system. The tested micro worlds in this test were the Solar system, the Earth, and the Study room. Four of the children were totally blind and three children were partially sighted. For the children who could benefit from visual feedback, this was included through a 2D projected view of the virtual environment.</p><p>The children were encouraged to give feedback during the use of the simulation, and they were also shortly interviewed directly after the use. It surprised us how versatile and diverse feedback we received from the children. The children commented, for example, the usability of the system, and especially the micro world Earth was experienced as difficult. One child, for example, said that “The Earth was still quite too disorganized and confusing to explore”, and another one commented the Earth’s revolving: “I did not know how much the Earth revolved with a sound of “click”“. The children also gave us new ideas for the agents’ operations with regard to the selected natural phenomena. For example, in the micro world Solar System the children suggested: “Maybe more information about the Sun and its structure—a small child might think that the Sun is solid, although it indeed is not” and “You could say, for example, how cold it is on Pluto...if possible.” In regard to the micro world Earth, one child proposed that “When exploring the Earth, the Earth Giant could tell something about the people who live on the continents you are currently exploring”. The exploring of a micro world happens by using some imaginative vehicle: for example, Solar System is explored by using a space shuttle. One of the children commented also the Central Station where one chooses which micro world to explore: “It would be nice, if the vehicles would sometimes function and sometimes not. It would be more realistic, too. But they shouldn’t be out of order for too long, otherwise one could get bored.” All in all, the children took the evaluation of the system very seriously, and we received very valuable feedback from them. The children’s feedbacks as well as the video recordings of the situations have been used as we have developed the system further.</p><p>The next evaluation was a research experiment and it was targeted for the actual target group of our learning system, namely for 7-8 year-old visually impaired children. The research experiment was carried out in autumn 2004 at the school for visually impaired children. Two 7-8 year-old blind children participated in this test. Afterwards, another child from this same age group participated, and this test was carried out in the usability laboratory a few months later. The tested micro worlds were the Solar system, the Earth, and the Earth’s Orbit. Tangible plastic models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig6">6</a>) of the micro worlds were used in this research experiment for the first time. They proved to be a good solution, and especially familiarizing the children first with the plastic model of the Central Station seemed to help the children’s navigation in the system. Also pedagogic agents had more operations than in the previous tests. The children were very excited and interested in using the system and they reacted keenly on the questions and thoughts raised by the agents. Some of the children even commented and thought aloud the agents’ questions and explanations. Allowing the user to choose the information he or she wants to hear made the new information more interesting for the children. Stylus shaking seemed to be a very natural way to inform the blind user of the agents’ message, and children learned quickly how to receive a message. They also actively selected if they wanted to hear what the agent has to say, and some users were also eagerly waiting to hear more information.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0011-5/MediaObjects/10055_2005_11_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>A tangible model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0011-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>From the children’s comments it became evident that we had succeeded to model, for example, the Earth and its surface realistically enough so that the children were able to recognize the areas of sea and land from the basis of auditory and haptic feedback. In the Earth’s orbit micro world it was also easy to follow the orbit after it was first examined once. The sounds of different seasons were also recognized. Especially good results were obtained from the use of planet orbits in Solar system: the children could easily follow them and acquire information of the planets.</p><p>The test showed that 7-8 year-old blind children are able to use the system quite independently. The researcher helped the child in situations in which the hold of the stylus got too difficult or the device overheated. The researcher also encouraged the children’s explorations and made questions and suggestions and offered some explanations, but it was the child who decided how to proceed in the simulation and what was explored.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>The design and implementation of the system has been a huge learning experience for us. Working with the special target group of visually impaired children making use of specific hardware such as the PHANTOM device poses its own questions, possibilities and limitations.</p><p>Our system has rather simple unimodal input as we use only manual (haptic) input. The filter agents are used in a similar way to the PAC Amodeus [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Nigay L, Coutaz J (1995) A generic platform for addressing the multimodal challenge. In: Proceedings of ACM CHI’95. ACM press, pp98-105" href="/article/10.1007/s10055-005-0011-5#ref-CR19" id="ref-link-section-d58352e941">19</a>] to process information from low-level input such as touching a surface to high-level abstractions concerning exploratory learning.</p><p>As Patomäki et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288" href="/article/10.1007/s10055-005-0011-5#ref-CR1" id="ref-link-section-d58352e947">1</a>] suggested we kept the objects in the scene as simple as possible. In addition, we learned that the navigation inside the environment should be restricted or at least guided in some way. For example, we used a spring force in the Earth micro world (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0011-5#Fig4">4</a>) to guide the user to the area where there was something to study. Our user studies proved that older children can use PHANTOM and 3D applications adequately when the use is guided and supported by the system.</p><p>Sjöström [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Sjöström C (2001) Designing haptic computer interfaces for blind people. In: proceedings of sixth international symposium on signal processing and its applications (ISSPA 2001). IEEE, pp68-71" href="/article/10.1007/s10055-005-0011-5#ref-CR10" id="ref-link-section-d58352e956">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Sjöström C (2002) Non-visual haptic interaction design: guidelines and applications. Doctoral dissertation, Certec, Lund Institute of Technology" href="/article/10.1007/s10055-005-0011-5#ref-CR11" id="ref-link-section-d58352e959">11</a>] states that in virtual haptic environment for the blind there should be reference points, which provide navigation help and sense of location for the user. In our system this is realized in micro worlds by moving the stylus to the same location every time the user enters the micro world. In the whole simulation the Central Station acts as reference point when navigating from one micro world to another.</p><p>Stylus shaking and tapping is parameterized, but it would also require further studying to determine which tapping patterns can be distinguished as unique and which ones could be used to give hint of what kind of information the incoming message contains.</p><p>We could also create more specialized objects like spheres that react to the proximity of the stylus by sending the distance of the stylus from the center of the sphere. Such proximity objects could be used to monitor the user’s actions or they could also be used to create ‘interest points’ to the micro world. The system could monitor the user’s distance from the interest points and if the user would be far away from them all, the system could aid the user to find the areas of interest by using sound and force feedback.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusions</h2><div class="c-article-section__content" id="Sec14-content"><p>In this paper we presented an agent-based multimodal software architecture aimed, to support visually impaired children. Several teaching applications were implemented on top of the architecture. The initial user studies support the usefulness and applicability of the architecture, as well as our choice of technology used to build it. We were especially pleased to find out that the PHANTOM technology produced such haptic feedback that is applicable for visually impaired children of this age group. As the basic functionality of the system and micro worlds is now complete, the system will be refined and used in further pedagogical studies.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaire" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Patomäki S, Raisamo R, Salo J, Pasto V, Hippula A (2004) Experiences on haptic interfaces for visually impaired young children. In: Proceedings of sixth international conference on multimodal interfaces (ICMI’04): ACM Press, pp 281-288</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SensAble Technologies Inc. http://www.sensable.com" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">SensAble Technologies Inc. http://www.sensable.com</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reachin Technologies AB. http://www.reachin.se" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Reachin Technologies AB. http://www.reachin.se</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="3Dconnexion. http://www.3dconnexion.com/" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">3Dconnexion. http://www.3dconnexion.com/</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon U" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Kangassalo M (1997) The Formation of Children’s Conceptual Models Concerning a Particular Natural Phenomenon Using PICCO, a Pictorial Computer Simulation. Doctoral dissertation. Acta Universitatis Tamperensis 559. University of Tampere. Tampere, p 188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kangassalo M (1991/1999) PICCO -kuvallinen tietokonesimulaatio [PICCO, Pictorial Computer Simulation]. CD-ROM " /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Kangassalo M (1991/1999) PICCO -kuvallinen tietokonesimulaatio [PICCO, Pictorial Computer Simulation]. CD-ROM 951-98035-0-5 </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kangassalo, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Kangassalo M (1992) The pictorial computer-based simulation in natural sciences for children’s use. In: Ohsuga" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Kangassalo M (1992) The pictorial computer-based simulation in natural sciences for children’s use. In: Ohsuga S, Kangassalo H, Jaakkola H, Hori K, Yonezaki N (eds) Information modelling and knowledge bases 3rd Foundations, theory and applications. IOS Press, Amsterdam, pp 511-524</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20modelling%20and%20knowledge%20bases%203rd%20Foundations%2C%20theory%20and%20applications&amp;pages=511-524&amp;publication_year=1992&amp;author=Kangassalo%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jansson G, Billberger K (1999) The PHANToM used without visual guidance. In: The first phantom users research " /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Jansson G, Billberger K (1999) The PHANToM used without visual guidance. In: The first phantom users research symposium (PURS 99). http://mbi.dkfz-heidelberg.de/purs99</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Magnusson C, Rassmus-Gröhn K, Sjöström C, Danielsson H (2002) Navigation and recognition in complex haptic vir" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Magnusson C, Rassmus-Gröhn K, Sjöström C, Danielsson H (2002) Navigation and recognition in complex haptic virtual environments—reports from an extensive study with blind users. In: Proceedings of Eurohaptics 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sjöström C (2001) Designing haptic computer interfaces for blind people. In: proceedings of sixth internationa" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Sjöström C (2001) Designing haptic computer interfaces for blind people. In: proceedings of sixth international symposium on signal processing and its applications (ISSPA 2001). IEEE, pp68-71</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sjöström C (2002) Non-visual haptic interaction design: guidelines and applications. Doctoral dissertation, Ce" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Sjöström C (2002) Non-visual haptic interaction design: guidelines and applications. Doctoral dissertation, Certec, Lund Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Computer graphics access for Blind people through a haptic virtual environment. http://www.grab-eu.com" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Computer graphics access for Blind people through a haptic virtual environment. http://www.grab-eu.com</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wood J, Magennis M, Arias E, Gutierrez T, Graupp H, Bergamasco M (2003) The design and evaluation of a compute" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Wood J, Magennis M, Arias E, Gutierrez T, Graupp H, Bergamasco M (2003) The design and evaluation of a computer game for the blind in the GRAB haptic audio virtual environment. In: Proceedings of Eurohaptics 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moran D-B, Cheyer AJ, Julia LE, Martin DL, Park S (1997) Multimodal user interfaces in the open agent architec" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Moran D-B, Cheyer AJ, Julia LE, Martin DL, Park S (1997) Multimodal user interfaces in the open agent architecture. In: Proceedings of the 2nd international conference on intelligent user interfaces. ACM Press, pp61-68</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cheyer A, Julia L (1999) InfoWiz: An animated voice interactive information system. In: Third international co" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Cheyer A, Julia L (1999) InfoWiz: An animated voice interactive information system. In: Third international conference on autonomous agents (Agents’99), communicative agents workshop</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moore R, Dowding J, Bratt H, Gawron J M, Gorfu Y, Cheyer A (1997) CommandTalk: A spoken-language interface for" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Moore R, Dowding J, Bratt H, Gawron J M, Gorfu Y, Cheyer A (1997) CommandTalk: A spoken-language interface for battlefield simulations. In: Proceedings of the fifth conference on applied natural language processing. morgan kaufmann publishers inc. San Francisco, CA, USA, pp1-7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cheyer A, Julia L (1998) Multimodal Maps: An agent-based approach. multimodal human-computer communication. le" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Cheyer A, Julia L (1998) Multimodal Maps: An agent-based approach. multimodal human-computer communication. lecture notes in artificial intelligence 1374. Springer, pp111-121</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coutaz J (1987) PAC, an object oriented model for dialog design. in: proceedings of interact ‘87. North-Hollan" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Coutaz J (1987) PAC, an object oriented model for dialog design. in: proceedings of interact ‘87. North-Holland, pp431-436</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nigay L, Coutaz J (1995) A generic platform for addressing the multimodal challenge. In: Proceedings of ACM CH" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Nigay L, Coutaz J (1995) A generic platform for addressing the multimodal challenge. In: Proceedings of ACM CHI’95. ACM press, pp98-105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hietala P, Niemirepo T (1995) A framework for building agent-based learning environments. In: Proceedings of A" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Hietala P, Niemirepo T (1995) A framework for building agent-based learning environments. In: Proceedings of AI-ED 95: Artificial intelligence in education. AACE, p 578</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Foundation for Intelligent Physical Agents. http://www.fipa.org/" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Foundation for Intelligent Physical Agents. http://www.fipa.org/</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="CLIPS: A Tool for Building Expert Systems. http://www.ghg.net/clips/CLIPS.html" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">CLIPS: A Tool for Building Expert Systems. http://www.ghg.net/clips/CLIPS.html</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SQLite. http://www.sqlite.org/" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">SQLite. http://www.sqlite.org/</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kangassalo, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Kangassalo M (1998) Modelling a natural phenomenon for a pictorial computer-based simulation. In: Kangassalo H" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Kangassalo M (1998) Modelling a natural phenomenon for a pictorial computer-based simulation. In: Kangassalo H, Charrel PJ, Jaakkola H (eds) Information modeling and knowledge bases IX. IOS press, Amsterdam, pp 239-254</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20modeling%20and%20knowledge%20bases%20IX&amp;pages=239-254&amp;publication_year=1998&amp;author=Kangassalo%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kangassalo, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Kangassalo M (1996) PICCO as a Cognitive Tool. In: Tanaka Y, Kangassalo H, Jaakkola H, Yamamoto A (eds) Inform" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Kangassalo M (1996) PICCO as a Cognitive Tool. In: Tanaka Y, Kangassalo H, Jaakkola H, Yamamoto A (eds) Information modelling and knowledge bases VII. IOS press, Amsterdam, pp 344-357</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20modelling%20and%20knowledge%20bases%20VII&amp;pages=344-357&amp;publication_year=1996&amp;author=Kangassalo%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Lonka, K. Hakkarainen, M. Sintonen, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Lonka K, Hakkarainen K, Sintonen M (2000) Progressive inquiry learning for children–Experiences, Possibilities" /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Lonka K, Hakkarainen K, Sintonen M (2000) Progressive inquiry learning for children–Experiences, Possibilities, Limitations. Eur Early Childhood Educ Res J 8:7-23</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Progressive%20inquiry%20learning%20for%20children%E2%80%93Experiences%2C%20Possibilities%2C%20Limitations&amp;journal=Eur%20Early%20Childhood%20Educ%20Res%20J&amp;volume=8&amp;pages=7-23&amp;publication_year=2000&amp;author=Lonka%2CK&amp;author=Hakkarainen%2CK&amp;author=Sintonen%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Kangassalo, R. Raisamo, P. Hietala, J. Järvi, K. Peltola, R. Saarinen, E. Tuominen, A. Hippula, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Kangassalo M, Raisamo R, Hietala P, Järvi J, Peltola K, Saarinen R, Tuominen E, Hippula A (2005) Proactive age" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Kangassalo M, Raisamo R, Hietala P, Järvi J, Peltola K, Saarinen R, Tuominen E, Hippula A (2005) Proactive agents that support children’s exploratory learning. In: Kiyoki Y, Wangler B, Jaakkola H, Kangassalo H (eds) Information modelling and knowledge bases XVI. IOS press, Amsterdam, pp 123-133</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20modelling%20and%20knowledge%20bases%20XVI&amp;pages=123-133&amp;publication_year=2005&amp;author=Kangassalo%2CM&amp;author=Raisamo%2CR&amp;author=Hietala%2CP&amp;author=J%C3%A4rvi%2CJ&amp;author=Peltola%2CK&amp;author=Saarinen%2CR&amp;author=Tuominen%2CE&amp;author=Hippula%2CA">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0011-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was funded by Academy of Finland, Proactive Computing Research Program (grants 202179 and 202180).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Tampere Unit for Computer-Human Interaction (TAUCHI), Department of Computer Sciences, University of Tampere, 33014, Tampere, Finland</p><p class="c-article-author-affiliation__authors-list">Rami Saarinen, Janne Järvi, Roope Raisamo &amp; Jouni Salo</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Teacher Education, Early Childhood Education, University of Tampere, 33014, Tampere, Finland</p><p class="c-article-author-affiliation__authors-list">Eva Tuominen, Marjatta Kangassalo &amp; Kari Peltola</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Rami-Saarinen"><span class="c-article-authors-search__title u-h3 js-search-name">Rami Saarinen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Rami+Saarinen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rami+Saarinen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rami+Saarinen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Janne-J_rvi"><span class="c-article-authors-search__title u-h3 js-search-name">Janne Järvi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Janne+J%C3%A4rvi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Janne+J%C3%A4rvi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Janne+J%C3%A4rvi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Roope-Raisamo"><span class="c-article-authors-search__title u-h3 js-search-name">Roope Raisamo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Roope+Raisamo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Roope+Raisamo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Roope+Raisamo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Eva-Tuominen"><span class="c-article-authors-search__title u-h3 js-search-name">Eva Tuominen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Eva+Tuominen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Eva+Tuominen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Eva+Tuominen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Marjatta-Kangassalo"><span class="c-article-authors-search__title u-h3 js-search-name">Marjatta Kangassalo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Marjatta+Kangassalo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Marjatta+Kangassalo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Marjatta+Kangassalo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kari-Peltola"><span class="c-article-authors-search__title u-h3 js-search-name">Kari Peltola</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kari+Peltola&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kari+Peltola" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kari+Peltola%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jouni-Salo"><span class="c-article-authors-search__title u-h3 js-search-name">Jouni Salo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jouni+Salo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jouni+Salo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jouni+Salo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0011-5/email/correspondent/c1/new">Janne Järvi</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Supporting%20visually%20impaired%20children%20with%20software%20agents%20in%20a%20multimodal%20learning%20environment&amp;author=Rami%20Saarinen%20et%20al&amp;contentID=10.1007%2Fs10055-005-0011-5&amp;publication=1359-4338&amp;publicationDate=2006-01-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Saarinen, R., Järvi, J., Raisamo, R. <i>et al.</i> Supporting visually impaired children with software agents in a multimodal learning environment.
                    <i>Virtual Reality</i> <b>9, </b>108–117 (2006). https://doi.org/10.1007/s10055-005-0011-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0011-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-18">18 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-10-07">07 October 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-01-11">11 January 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03">March 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0011-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0011-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Software agent architecture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visually impaired children</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Learning environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Auditory feedback</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0011-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=11;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

