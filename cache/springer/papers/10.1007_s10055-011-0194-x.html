<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A novel approach in rehabilitation of hand-eye coordination and finger"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Stroke patients or victims who have been involved in serious accidents often suffer from impaired hand-eye coordination and muscle dexterity. Products, such as nine-hole pegboards, have been..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A novel approach in rehabilitation of hand-eye coordination and finger dexterity"/>

    <meta name="dc.source" content="Virtual Reality 2011 16:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-08-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Stroke patients or victims who have been involved in serious accidents often suffer from impaired hand-eye coordination and muscle dexterity. Products, such as nine-hole pegboards, have been designed to help rehabilitate various skills, e.g., perceptual accuracy and finger dexterity. Patients who do not have sufficient muscle strength would not be able to carry out such traditional exercises. This paper presents the research aims at providing a fresh and viable approach to physiotherapy for such patients while emulating the rehabilitation capabilities of traditional products. In this paper, a novel approach, AR-Rehab, for the rehabilitation of hand-eye coordination and finger dexterity has been developed incorporating Augmented Reality (AR) technology. In this application, the users can interact with virtual piano keys in a real-life scene by moving the real hands wearing data-gloves to detect the flexing of the fingers and markers to detect the position of the hands."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-08-05"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="161"/>

    <meta name="prism.endingPage" content="171"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0194-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0194-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0194-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0194-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A novel approach in rehabilitation of hand-eye coordination and finger dexterity"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2012/06"/>

    <meta name="citation_online_date" content="2011/08/05"/>

    <meta name="citation_firstpage" content="161"/>

    <meta name="citation_lastpage" content="171"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0194-x"/>

    <meta name="DOI" content="10.1007/s10055-011-0194-x"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0194-x"/>

    <meta name="description" content="Stroke patients or victims who have been involved in serious accidents often suffer from impaired hand-eye coordination and muscle dexterity. Products, suc"/>

    <meta name="dc.creator" content="Y. Shen"/>

    <meta name="dc.creator" content="P. W. Gu"/>

    <meta name="dc.creator" content="S. K. Ong"/>

    <meta name="dc.creator" content="A. Y. C. Nee"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Adamovich SV, Fluet GG, Mathai A, Qiu Q, Lewis J, Merians AS (2009) Design of a complex virtual reality simulation to train finger motion for persons with hemiparesis: a proof of concept study. J NeuroEng Rehabilitation 6(28). doi:
                    10.1186/1743-0003-6-28
                    
                  
                "/>

    <meta name="citation_reference" content="Aguiar R, Piano VR (2007) Available from: 
                    http://www.youtube.com/watch?v=wdDxXdMOKX0
                    
                  . Accessed: 28th Aug 2009"/>

    <meta name="citation_reference" content="ARToolKit (2010) [online] Human Interface Technology Laboratory, University of Washington. Available from: 
                    http://www.hitl.washington.edu/research/shared_space/download/
                    
                  . Accessed 12 June 2010"/>

    <meta name="citation_reference" content="Barakonyi I, Schmalstieg D (2005) Augmented reality agents in the development pipeline of computer entertainment. In: Proceedings of the 4th International conference on entertainment computer, Sanda, Japan, 2005 September 19&#8211;21, pp 345&#8211;356"/>

    <meta name="citation_reference" content="citation_journal_title=StudHealth Technol Inform; citation_title=Virtual rehabilitation after stroke; citation_author=J Broeren, A Bjorkdahl, L Claesson, D Goude, A Lundgren-Nilsson, H Samuelsson, C Blomstrand, KS Sunnerhagen, M Rydmark; citation_volume=136; citation_publication_date=2008; citation_pages=77-82; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Rehabilitation Eng; citation_title=Virtual reality-based orthopedic telerehabilitation; citation_author=G Burdea, V Popescu, V Hentz, K Colbert; citation_volume=8; citation_issue=3; citation_publication_date=2000; citation_pages=430-432; citation_id=CR6"/>

    <meta name="citation_reference" content="Burke JW, McNeill MDJ, Charles DK, Morrow PJ, Crosbie JH, McDonough SM (2009) Serious games for upper limb rehabilitation following stroke. 2009 Conference in games and virtual worlds for serious applications, Coventry, UK, March 23&#8211;24, pp 103&#8211;110"/>

    <meta name="citation_reference" content="Chen Y, Huang H, Xu W, Wallis RI, Sundaram H, Rikakis T, Ingalls T, Olson L, He J (2006) The design of a real-time, multimodal biofeedback system for stroke patient rehabilitation. In: Proceeding of the 14th annual ACM international conference on Multimedia, ACM Press, New York, Santa Barbara, CA, USA, 2006 October 23&#8211;27, pp 763&#8211;772"/>

    <meta name="citation_reference" content="Choi KS, Chow CM, Lo KH (2010) A rehabilitation method with visual and haptic guidance for children with upper extremity disability. Lecture Notes Comput Sci 6180/2010, 2010, 77&#8211;84"/>

    <meta name="citation_reference" content="citation_journal_title=J Neurophysiol; citation_title=Spatial transformations for eye&#8211;hand coordination; citation_author=JD Crawford, WP Medendorp, JJ Marotta; citation_volume=92; citation_issue=1; citation_publication_date=2004; citation_pages=10-19; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Disability &amp; Rehabilitation; citation_title=Virtual reality in stroke rehabilitation: still more virtual than real; citation_author=JH Crosbie, S Lennon, JR Basford, SM McDonough; citation_volume=29; citation_issue=14; citation_publication_date=2007; citation_pages=1139-1146; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol &amp; Behav; citation_title=Virtual environments for motor rehabilitation: review; citation_author=MK Holden; citation_volume=8; citation_issue=3; citation_publication_date=2005; citation_pages=187-211; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Rehabilitation Res; citation_title=The geriatric hand: correlation of hand-muscle function and activity restriction in elderly; citation_author=NA Incel, M Sezgin, I As, OB Cimen, G Sahin; citation_volume=32; citation_issue=3; citation_publication_date=2009; citation_pages=213-218; citation_id=CR13"/>

    <meta name="citation_reference" content="Integrated Bio-medics &amp; Technologies website (2009) Available from: 
                    http://www.ibmtindia.com/occup.asp
                    
                  . Accessed 15 March, 2009"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE transactions on neural systems and rehabilitation engineering; citation_title=Virtual reality-enhanced stroke rehabilitation; citation_author=D Jack, R Boian, AS Merians, M Tremaine, GC Burdea, SV Adamovich, M Recce, H Poizner; citation_volume=9; citation_issue=3; citation_publication_date=2001; citation_pages=308-318; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Syst Rehabilitation Eng; citation_title=Virtual reality-enhanced stroke rehabilitation; citation_author=D Jack, R Boian, AS Merians, M Tremaine, GC Burdea, SV Adamovich, M Recce, H Poizner; citation_volume=9; citation_issue=3; citation_publication_date=2001; citation_pages=308-318; citation_id=CR16"/>

    <meta name="citation_reference" content="Li S, Frisoli A, Avizzano CA, Ruffaldi E, Lugo-Villeda LI, Bergamasco M (2009) Bimanual haptic&#8212;desktop platform for upper-limb post-stroke rehabilitation: practical trials. In: Proceedings of the 2009 IEEE International conference on robotics and biomimetics, December 19&#8211;23, 2009, Guilin, China, pp 480&#8211;485"/>

    <meta name="citation_reference" content="Luo X, Kline T, Fischer HC, Stubblefield KA, Kenyon RV, Kamper DG (2005) Integration of augmented reality and assistive devices for post-stroke hand opening rehabilitation. In: Proceedings of the 2005 IEEE, Engineering in medicine and biology 27th annual conference, Shanghai, China, 2005 September 1&#8211;4, pp 6855&#8211;6858"/>

    <meta name="citation_reference" content="citation_journal_title=M&#233;decine Sci (Paris); citation_title=New Perspectives of locomotor rehabilitation after stroke; citation_author=F Malouin, CL Richards, B McFadyen, J Doyon; citation_volume=19; citation_issue=10; citation_publication_date=2003; citation_pages=994-998; citation_id=CR19"/>

    <meta name="citation_reference" content="Pareto L, Broeren J, Goude D, Rydmark M (2008) Virtual reality, haptics and post-stroke rehabilitation in practical therapy. In: Proceedings of 7th International conference series on disability, virtual reality and associated technologies (ICDVRAT) with ArtAbilitation, Maia, Portugal, 2008 September 8&#8211;10, pp 245&#8211;252"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Visual interpretation of hand gestures for human-computer interaction: a review; citation_author=VI Pavlovic, R Sharma, TS Huang; citation_volume=19; citation_issue=7; citation_publication_date=1997; citation_pages=677-695; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Inf Technol Biomed; citation_title=A virtual-reality-based telerehabilitation system with force feedback; citation_author=VG Popescu, CG Burdea, M Bouzit, VR Hentz; citation_volume=4; citation_issue=1; citation_publication_date=2000; citation_pages=45-51; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Transa Inf Technol Biomed; citation_title=Virtual environments in neuroscience; citation_author=G Riva; citation_volume=2; citation_issue=4; citation_publication_date=1998; citation_pages=275-281; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Endourol Soc; citation_title=Can video games be used to predict or improve laparoscopic skills?; citation_author=BH Rosenberg, D Landsittel, TD Averch; citation_volume=19; citation_issue=3; citation_publication_date=2005; citation_pages=372-376; citation_id=CR24"/>

    <meta name="citation_reference" content="Sucar LE, Leder RS, Reinkensmeyer D, Hern&#225;ndez J, Azc&#225;rate G, Caste&#241;eda N, Saucedo P (2008) Gesture therapy&#8212;a low-cost vision-based system for rehabilitation after stroke. HEALTHINF 2008, Funchal, Madeira, Portugal, 2008 January 28&#8211;31, pp 107&#8211;111"/>

    <meta name="citation_reference" content="Sveistrup H (2004) Motor rehabilitation using virtual reality. J NeuroEng Rehabilitation 1(10). doi:
                    10.1186/1743-0003-1-10
                    
                  
                "/>

    <meta name="citation_author" content="Y. Shen"/>

    <meta name="citation_author_email" content="mpesy@nus.edu.sg"/>

    <meta name="citation_author_institution" content="Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, Singapore, Singapore"/>

    <meta name="citation_author" content="P. W. Gu"/>

    <meta name="citation_author_email" content="peiwei@nus.edu.sg"/>

    <meta name="citation_author_institution" content="Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, Singapore, Singapore"/>

    <meta name="citation_author" content="S. K. Ong"/>

    <meta name="citation_author_email" content="mpeongsk@nus.edu.sg"/>

    <meta name="citation_author_institution" content="Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, Singapore, Singapore"/>

    <meta name="citation_author" content="A. Y. C. Nee"/>

    <meta name="citation_author_email" content="mpeneeyc@nus.edu.sg"/>

    <meta name="citation_author_institution" content="Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, Singapore, Singapore"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0194-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0194-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A novel approach in rehabilitation of hand-eye coordination and finger dexterity"/>
        <meta property="og:description" content="Stroke patients or victims who have been involved in serious accidents often suffer from impaired hand-eye coordination and muscle dexterity. Products, such as nine-hole pegboards, have been designed to help rehabilitate various skills, e.g., perceptual accuracy and finger dexterity. Patients who do not have sufficient muscle strength would not be able to carry out such traditional exercises. This paper presents the research aims at providing a fresh and viable approach to physiotherapy for such patients while emulating the rehabilitation capabilities of traditional products. In this paper, a novel approach, AR-Rehab, for the rehabilitation of hand-eye coordination and finger dexterity has been developed incorporating Augmented Reality (AR) technology. In this application, the users can interact with virtual piano keys in a real-life scene by moving the real hands wearing data-gloves to detect the flexing of the fingers and markers to detect the position of the hands."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A novel approach in rehabilitation of hand-eye coordination and finger dexterity | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0194-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Rehabilitation, Hand-eye coordination, Finger dexterity","kwrd":["Augmented_reality","Rehabilitation","Hand-eye_coordination","Finger_dexterity"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0194-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0194-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=194;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0194-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A novel approach in rehabilitation of hand-eye coordination and finger dexterity
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0194-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0194-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-08-05" itemprop="datePublished">05 August 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A novel approach in rehabilitation of hand-eye coordination and finger dexterity</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Y_-Shen" data-author-popup="auth-Y_-Shen">Y. Shen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National University of Singapore" /><meta itemprop="address" content="grid.4280.e, 0000000121806431, Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, 9 Engineering Drive 1, Singapore, 117576, Singapore" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-P__W_-Gu" data-author-popup="auth-P__W_-Gu">P. W. Gu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National University of Singapore" /><meta itemprop="address" content="grid.4280.e, 0000000121806431, Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, 9 Engineering Drive 1, Singapore, 117576, Singapore" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-S__K_-Ong" data-author-popup="auth-S__K_-Ong" data-corresp-id="c1">S. K. Ong<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National University of Singapore" /><meta itemprop="address" content="grid.4280.e, 0000000121806431, Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, 9 Engineering Drive 1, Singapore, 117576, Singapore" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-A__Y__C_-Nee" data-author-popup="auth-A__Y__C_-Nee">A. Y. C. Nee</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National University of Singapore" /><meta itemprop="address" content="grid.4280.e, 0000000121806431, Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, 9 Engineering Drive 1, Singapore, 117576, Singapore" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">161</span>–<span itemprop="pageEnd">171</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">770 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0194-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Stroke patients or victims who have been involved in serious accidents often suffer from impaired hand-eye coordination and muscle dexterity. Products, such as nine-hole pegboards, have been designed to help rehabilitate various skills, e.g., perceptual accuracy and finger dexterity. Patients who do not have sufficient muscle strength would not be able to carry out such traditional exercises. This paper presents the research aims at providing a fresh and viable approach to physiotherapy for such patients while emulating the rehabilitation capabilities of traditional products. In this paper, a novel approach, AR-Rehab, for the rehabilitation of hand-eye coordination and finger dexterity has been developed incorporating Augmented Reality (AR) technology. In this application, the users can interact with virtual piano keys in a real-life scene by moving the real hands wearing data-gloves to detect the flexing of the fingers and markers to detect the position of the hands.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Patients afflicted with stroke or people who have been involved in serious accidents often have to deal with serious neurological impairments. These illnesses and injuries can impair various abilities, such as speech, sense of balance, muscle dexterity, and also hand-eye coordination. These impairments would reduce the quality and independence of life of these patients (Incel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Incel NA, Sezgin M, As I, Cimen OB, Sahin G (2009) The geriatric hand: correlation of hand-muscle function and activity restriction in elderly. Int J Rehabilitation Res 32(3):213–218" href="/article/10.1007/s10055-011-0194-x#ref-CR13" id="ref-link-section-d45253e338">2009</a>). Without hand-eye coordination, which is the “control of eye movement and the processing of visual input to guide bodily movement”, a simple action, such as picking up a book or eating from a spoon would not be possible (Crawford et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Crawford JD, Medendorp WP, Marotta JJ (2004) Spatial transformations for eye–hand coordination. J Neurophysiol 92(1):10–19" href="/article/10.1007/s10055-011-0194-x#ref-CR10" id="ref-link-section-d45253e341">2004</a>).</p><p>Various rehabilitative exercises have been developed to help patients improve their hand-eye coordination. One common example would be catching and throwing balls that are directed at the patients from various directions. This exercise aims to stimulate a number of skills pertaining to hand-eye coordination, such as sight and speed visualization, reaction time, and perceptual accuracy (Popescu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Popescu VG, Burdea CG, Bouzit M, Hentz VR (2000) A virtual-reality-based telerehabilitation system with force feedback. IEEE Trans Inf Technol Biomed&#xA; 4(1):45–51" href="/article/10.1007/s10055-011-0194-x#ref-CR22" id="ref-link-section-d45253e347">2000</a>). However, more seriously afflicted patients may not have sufficient strength and/or muscle dexterity to carry out such exercises. Intensive training has been proven to be an effective method for the recovery of motor functions (Malouin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Malouin F, Richards CL, McFadyen B, Doyon J (2003) New Perspectives of locomotor rehabilitation after stroke. Médecine Sci (Paris), 19(10), 994–998" href="/article/10.1007/s10055-011-0194-x#ref-CR19" id="ref-link-section-d45253e350">2003</a>). However, many conventional rehabilitation systems have been reported to be boring and humdrum by the patients (Burke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Burke JW, McNeill MDJ, Charles DK, Morrow PJ, Crosbie JH, McDonough SM (2009) Serious games for upper limb rehabilitation following stroke. 2009 Conference in games and virtual worlds for serious applications, Coventry, UK, March 23–24, pp 103–110" href="/article/10.1007/s10055-011-0194-x#ref-CR7" id="ref-link-section-d45253e353">2009</a>), and this would prevent the patients from undergoing the practices needed to effect neural and functional changes. Thus, novel and entertaining rehabilitation systems providing physically less demanding exercises and focusing more on skills, such as perceptual accuracy and reaction times, are desirable (Integrated Bio-medics and Technologies website <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Integrated Bio-medics &amp; Technologies website (2009) Available from: &#xA;                    http://www.ibmtindia.com/occup.asp&#xA;                    &#xA;                  . Accessed 15 March, 2009" href="/article/10.1007/s10055-011-0194-x#ref-CR14" id="ref-link-section-d45253e356">2009</a>). The rehabilitation systems should be able to provide intensive training with salient feedbacks to encourage the patients to take on more practices.</p><p>Virtual Reality (VR) technology has emerged as a potential method in the field of rehabilitation with advantages, such as a controllable environment, salient feedbacks, the sense of presence, and in vivo simulation (Jack et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M and Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE transactions on neural systems and rehabilitation engineering 2001. 9(3):308–318" href="/article/10.1007/s10055-011-0194-x#ref-CR15" id="ref-link-section-d45253e362">2001</a>; Sveistrup <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sveistrup H (2004) Motor rehabilitation using virtual reality. J NeuroEng Rehabilitation 1(10). doi:&#xA;                    10.1186/1743-0003-1-10&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0194-x#ref-CR26" id="ref-link-section-d45253e365">2004</a>). VR applications can provide a fresh and viable approach for rehabilitation purposes with these characteristics. Holden (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol &amp; Behav 8(3):187–211" href="/article/10.1007/s10055-011-0194-x#ref-CR12" id="ref-link-section-d45253e368">2005</a>) has presented a comprehensive review of the applications of VR in motor rehabilitation. In this review, several existing studies comparing the real and virtual practices are introduced. He concluded that VR training is superior to motor learning in a real environment as the former is fully controllable, with salient feedbacks, entertaining treatments and digital records in the VR rehabilitation systems. Although it has been proven that the movements recovered in VR environment can be transferred to the real world (Holden <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol &amp; Behav 8(3):187–211" href="/article/10.1007/s10055-011-0194-x#ref-CR12" id="ref-link-section-d45253e371">2005</a>), the degree of similarity between virtual and real tasks would affect the effectiveness of the transfer (Riva <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Riva G (1998) Virtual environments in neuroscience. IEEE Transa Inf Technol Biomed 2(4):275–281" href="/article/10.1007/s10055-011-0194-x#ref-CR23" id="ref-link-section-d45253e374">1998</a>).</p><p>In most of these VR rehabilitation systems for rehabilitation of hand-eye coordination and finger dexterity, the patients interact with the virtual objects using intermediary devices, such as a mouse or a joystick. The haptic PHAMToM device has also been used to provide recovering interventions with the haptic feedbacks (Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Choi KS, Chow CM, Lo KH (2010) A rehabilitation method with visual and haptic guidance for children with upper extremity disability. Lecture Notes Comput Sci 6180/2010, 2010, 77–84" href="/article/10.1007/s10055-011-0194-x#ref-CR9" id="ref-link-section-d45253e380">2010</a>). When immersed in a virtual environment, the patients cannot see their hands and have to sense the hand placements relative to the virtual environment (Pareto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pareto L, Broeren J, Goude D, Rydmark M (2008) Virtual reality, haptics and post-stroke rehabilitation in practical therapy. In: Proceedings of 7th International conference series on disability, virtual reality and associated technologies (ICDVRAT) with ArtAbilitation, Maia, Portugal, 2008 September 8–10, pp 245–252" href="/article/10.1007/s10055-011-0194-x#ref-CR20" id="ref-link-section-d45253e383">2008</a>). The movements of the hands and the interactions with the virtual objects are in separate independent coordinate references. The patients have to map the movements of the hands to the movements of the cursors themselves, and this would affect the effectiveness of these rehabilitation systems. To provide intuitive interaction to the patient, efforts are taken to design algorithms mapping the movements of the real hands to the movements of the virtual objects (Choi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Choi KS, Chow CM, Lo KH (2010) A rehabilitation method with visual and haptic guidance for children with upper extremity disability. Lecture Notes Comput Sci 6180/2010, 2010, 77–84" href="/article/10.1007/s10055-011-0194-x#ref-CR9" id="ref-link-section-d45253e386">2010</a>) and special platforms were developed (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Li S, Frisoli A, Avizzano CA, Ruffaldi E, Lugo-Villeda LI, Bergamasco M (2009) Bimanual haptic—desktop platform for upper-limb post-stroke rehabilitation: practical trials. In: Proceedings of the 2009 IEEE International conference on robotics and biomimetics, December 19–23, 2009, Guilin, China, pp 480–485" href="/article/10.1007/s10055-011-0194-x#ref-CR17" id="ref-link-section-d45253e389">2009</a>). A Bimanual Haptic Desktop System integrating the haptic functionalities and Video Display Terminal (VDT) has been designed to provide training to recover hand movements (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Li S, Frisoli A, Avizzano CA, Ruffaldi E, Lugo-Villeda LI, Bergamasco M (2009) Bimanual haptic—desktop platform for upper-limb post-stroke rehabilitation: practical trials. In: Proceedings of the 2009 IEEE International conference on robotics and biomimetics, December 19–23, 2009, Guilin, China, pp 480–485" href="/article/10.1007/s10055-011-0194-x#ref-CR17" id="ref-link-section-d45253e392">2009</a>). With the display designed in the work-plane of a desk, the movements of the hands placed above the display will control the virtual objects directly. While performing the exercises, the patient can observe the hands and the virtual objects simultaneously. With this platform, the disconnection between the hand and virtual objects can be minimized.</p><p>Augmented Reality (AR) technology is capable of addressing these challenges by superimposing virtual objects onto the physical world, in which the users can interact with virtual and real objects using their hands directly. Compared with VR systems, AR-based rehabilitation systems could provide a better realism feeling to the patients during the exercising process, while retaining the advantages such as a fully controllable environment, the salient feedback, entertaining practices and digital records. Thus, they would provide more cognitive and psychological stimuli to improve the recovering progresses and encourage more recovering exercising. In the AR environment, the virtual objects and the hands of the patients are in the same physical space. Therefore, the natural interaction in everyday life can be simulated. In AR-based rehabilitation systems, the patients can visualize the real environment. This characteristic is especially helpful for the training of hand-eye coordination as the real interaction scenarios can be simulated and the real environment can be used as a reference to facilitate the coordination of the hands and eyes.</p><p>In recent years, there has been increasing interest in applying computer gaming in rehabilitation as rehabilitation systems incorporating computer games could provide a more engaging context and better motivation to the patients during practices (Burke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Burke JW, McNeill MDJ, Charles DK, Morrow PJ, Crosbie JH, McDonough SM (2009) Serious games for upper limb rehabilitation following stroke. 2009 Conference in games and virtual worlds for serious applications, Coventry, UK, March 23–24, pp 103–110" href="/article/10.1007/s10055-011-0194-x#ref-CR7" id="ref-link-section-d45253e401">2009</a>). Numerous global studies have concluded that playing serious games and computer games can have a positive effect on one’s hand-eye coordination and dexterity. Some researches have found that serious games and computer games can facilitate the training of the surgeons to improve their finger dexterity and accuracy (Rosenberg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rosenberg BH, Landsittel D, Averch TD (2005) Can video games be used to predict or improve laparoscopic skills? J Endourol Soc 19(3):372–376" href="/article/10.1007/s10055-011-0194-x#ref-CR24" id="ref-link-section-d45253e404">2005</a>).</p><p>This paper presents a rehabilitation system, AR-Rehab for hand-eye coordination and finger dexterity based on the AR technology and computer gaming concept. This system integrates task-dependent physical therapy and cognitive stimuli within an interactive, multi-modal environment, where users can play computer games in this AR environment. A virtual piano game is designed to provide the training. Using a 5DT data-glove, the hand movements of a user can be detected and mapped onto a virtual hand in the AR-based rehabilitation environment. Using the markers that are attached on the data-glove, the position of the hand and the spatial relationship between the hand and the virtual piano keys can be obtained. Thus, the user can depress the virtual piano keys using the real hand in the real environment. In addition, the system can evaluate and score the performance of the patients. Multi-modal feedbacks are provided according to the performance to facilitate and encourage the patients during the rehabilitation.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Research background</h2><div class="c-article-section__content" id="Sec2-content"><p>Hand-eye coordination and finger dexterity are typically treated with intensive, hand-on physical and occupational therapies. Traditional hand movement rehabilitation, such as the peg board, is monotonous and boring. VR and AR are emerging and promising technologies for hand movement rehabilitation.</p><p>With a specially simulated environment, VR could provide a fully controllable environment to the patients with salient feedback. In the system designed for the recovery of hand movements (Burdea et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Burdea G, Popescu V, Hentz V, Colbert K (2000) Virtual reality-based orthopedic telerehabilitation. IEEE Trans Rehabilitation Eng 8(3):430–432" href="/article/10.1007/s10055-011-0194-x#ref-CR6" id="ref-link-section-d45253e419">2000</a>), several rehabilitation exercises are designed with the display of a virtual hand and virtual objects within a virtual exercise room. A sensing glove called “Rutgers Master” is used to measure finger movements to control the virtual hand and produce resistive forces at the fingertips to provide haptic feedbacks. The virtual room is designed with a tiled floor to give the patient a better 3-D perspective. In the peg board games, the shadow of the peg is rendered to facilitate depth perception. Crosbie et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Crosbie JH, Lennon S, Basford JR, McDonough SM (2007) Virtual reality in stroke rehabilitation: still more virtual than real. Disability &amp; Rehabilitation 29(14):1139–1146" href="/article/10.1007/s10055-011-0194-x#ref-CR11" id="ref-link-section-d45253e422">2007</a>) have assessed the utility of VR in stroke rehabilitation, and they conclude that VR is a promising technology for stroke rehabilitation although a definitive assessment of the value is not available yet. In the VR rehabilitation system designed by Jack et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE Trans Neural Syst Rehabilitation Eng 9(3):308–318" href="/article/10.1007/s10055-011-0194-x#ref-CR16" id="ref-link-section-d45253e425">2001</a>) to recover hand functions of stroke patients, there are two input devices, i.e., a cyber-glove and a Rutgers master II-ND force feedback glove. With these interaction tools, the patients can control several simple games, which are designed according to the training targets to motivate the users to perform more exercises. In the VR-based biofeedback system developed by Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Chen Y, Huang H, Xu W, Wallis RI, Sundaram H, Rikakis T, Ingalls T, Olson L, He J (2006) The design of a real-time, multimodal biofeedback system for stroke patient rehabilitation. In: Proceeding of the 14th annual ACM international conference on Multimedia, ACM Press, New York, Santa Barbara, CA, USA, 2006 October 23–27, pp 763–772" href="/article/10.1007/s10055-011-0194-x#ref-CR8" id="ref-link-section-d45253e428">2006</a>), an immersive, visual, and auditory environment is provided for patients to practice functional therapeutic reaching tasks, while different types of feedbacks are provided to indicate the performance of the patients. Broeren et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Broeren J, Bjorkdahl A, Claesson L, Goude D, Lundgren-Nilsson A, Samuelsson H, Blomstrand C, Sunnerhagen KS, Rydmark M (2008) Virtual rehabilitation after stroke. StudHealth Technol Inform 136:77–82" href="/article/10.1007/s10055-011-0194-x#ref-CR5" id="ref-link-section-d45253e431">2008</a>) have assessed the use of VR on the recovery of motor functions. In this system, a VR activity station is used to provide a virtual space, in which the user can interact with 3D objects with a handheld stylus. Telemedicine based on Skype<sup>™</sup> with a camera was used as a communication tool between the therapist and the user. It has been observed that a VR-based rehabilitation system can increase the user’s motor and cognitive skills. In the PianoVR application (Aguiar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Aguiar R, Piano VR (2007) Available from: &#xA;                    http://www.youtube.com/watch?v=wdDxXdMOKX0&#xA;                    &#xA;                  . Accessed: 28th Aug 2009" href="/article/10.1007/s10055-011-0194-x#ref-CR2" id="ref-link-section-d45253e437">2007</a>), virtual hands and keyboard are rendered in a virtual environment. Controlling the virtual hands using a data-glove, the user is able to interact with the virtual keyboard. The position and orientation of the hand is tracked using a magnetic tracker receiver which is attached to the data-glove. Following the various visual cues indicating the correct notes to play, the user is able to learn to play a piece of music. However, as the user is moving his/her hands in a real environment but experiencing the interaction between the virtual hands and the piano in a virtual environment, there is a disconnection in the interaction between the physical and virtual elements. In other words, the sensation of playing the keyboard may be diminished. Adamovich et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Adamovich SV, Fluet GG, Mathai A, Qiu Q, Lewis J, Merians AS (2009) Design of a complex virtual reality simulation to train finger motion for persons with hemiparesis: a proof of concept study. J NeuroEng Rehabilitation 6(28). doi:&#xA;                    10.1186/1743-0003-6-28&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0194-x#ref-CR1" id="ref-link-section-d45253e440">2009</a>) designed and tested a virtual environment system to train the hand and arm of persons with hemi paresis. The system employs a simulated piano that presents visual, auditory, and tactile feedback comparable to an actual piano. The piano trainer includes songs and scales that can be performed with one or both hands. The equipments used for interaction in this system are a CyberGrasp haptic device and a CyberGlove. The haptic device can be used to provide the haptic feeling, and the data-glove is used to detect the hand movements. The system has been proven to be safe and feasible for hand function training for persons with hemi paresis.</p><p>From the literature review, it can be observed that in these VR-based rehabilitation systems, specific tasks with multi-modal feedbacks (e.g., visual, tactile, audio) are designed to engage the patients effectively. However, the disconnection between the real hand and the virtual objects still exists. This disconnection would affect the effectiveness of the rehabilitation systems in the treatment of the hand-eye coordination and finger dexterity.</p><p>Several AR-based hand motion rehabilitation systems have been reported in which the user can view the real world, and real elements can be synthesized into the exercising environment for the therapeutic exercises. Luo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Luo X, Kline T, Fischer HC, Stubblefield KA, Kenyon RV, Kamper DG (2005) Integration of augmented reality and assistive devices for post-stroke hand opening rehabilitation. In: Proceedings of the 2005 IEEE, Engineering in medicine and biology 27th annual conference, Shanghai, China, 2005 September 1–4, pp 6855–6858" href="/article/10.1007/s10055-011-0194-x#ref-CR18" id="ref-link-section-d45253e448">2005</a>) have developed a rehabilitation system integrating AR and assistive devices to recover the hand opening functions of stroke survivors. Mechanical devices, i.e., a body-powered orthosis and a pneumatic-powered device are implemented in this system to support the repetitive practice. Dynamic feedbacks are provided in the form of force records and waveform on a computer screen. The devices are relatively low cost and small in size to be used in clinics and at home. Sucar et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Sucar LE, Leder RS, Reinkensmeyer D, Hernández J, Azcárate G, Casteñeda N, Saucedo P (2008) Gesture therapy—a low-cost vision-based system for rehabilitation after stroke. HEALTHINF 2008, Funchal, Madeira, Portugal, 2008 January 28–31, pp 107–111" href="/article/10.1007/s10055-011-0194-x#ref-CR25" id="ref-link-section-d45253e451">2008</a>) have proposed a gesture therapeutic system using the AR technology to provide intensive movement training at low cost. Two inexpensive cameras are used to track the hand movement in the 3D space. Visual feedback of the patient’s performance is provided, and the practice progress is illustrated using a simple statistical chart. This system is designed to improve the moving range of the upper limb. The AR Piano Tutor is an AR application involving virtual objects augmented onto a real MIDI keyboard in a video captured scene which represents the user’s viewpoint (Barakonyi and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Barakonyi I, Schmalstieg D (2005) Augmented reality agents in the development pipeline of computer entertainment. In: Proceedings of the 4th International conference on entertainment computer, Sanda, Japan, 2005 September 19–21, pp 345–356" href="/article/10.1007/s10055-011-0194-x#ref-CR4" id="ref-link-section-d45253e454">2005</a>). The keyboard is connected to a computer via a MIDI interface which captures the order and timing of the piano key strokes. In such a way, the program can then check how well the user is progressing and provide instant visual feedback using graphics superimposed over the keyboard. Since the user is playing directly on a real keyboard, this application can offer the same features as the system of PianoVR (Aguiar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Aguiar R, Piano VR (2007) Available from: &#xA;                    http://www.youtube.com/watch?v=wdDxXdMOKX0&#xA;                    &#xA;                  . Accessed: 28th Aug 2009" href="/article/10.1007/s10055-011-0194-x#ref-CR2" id="ref-link-section-d45253e457">2007</a>) without the “disconnection” feeling. However, the disadvantage is that a real keyboard would have to be used and thus the portability of the application is limited. In addition, with regard to the rehabilitation purposes of AR Piano Tutor, a patient with limited finger strength and control may not be able to depress the physical keys.</p><p>This paper presents a novel approach in the rehabilitation of hand-eye coordination and finger dexterity in patients through the use of AR by addressing two main problems, namely, the disconnection between the real hand and the virtual contents in the virtual environment in these VR-based rehabilitation systems, and the insufficient strength in the muscle of the seriously afflicted patients to carry out the conventional recovering exercises. An AR-based application, AR-Rehab has been developed that allows the users to interact with a virtual keyboard as though it were a real object in their viewpoint. By getting the users to play various notes, the system helps the users to train up vision-motor skills, such as perceptual accuracy and finger dexterity.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Methodologies</h2><div class="c-article-section__content" id="Sec3-content"><h3 class="c-article__sub-heading" id="Sec4">Concept</h3><p>The approach taken in this research is to create a rehabilitation environment that provides a seamless experience where the patients can interact with the virtual objects as though they were real and right in front of them. In the system presented in this paper, a web camera representing the user’s viewpoint captures a video of the real scene within which a virtual hand and keyboard will be superimposed. With a marker attached to a data-glove worn on the user’s hand, its position is tracked in real time using ARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="ARToolKit (2010) [online] Human Interface Technology Laboratory, University of Washington. Available from: &#xA;                    http://www.hitl.washington.edu/research/shared_space/download/&#xA;                    &#xA;                  . Accessed 12 June 2010" href="/article/10.1007/s10055-011-0194-x#ref-CR3" id="ref-link-section-d45253e475">2010</a>). Thus, the virtual hand can be overlaid directly over the user’s hand. Using a 5DT data-glove, the finger flexure information of the user can be obtained and translated to the virtual hand. Using collision detection methods, touching between the virtual hand and keyboard can be detected, giving the users the feeling that they are able to interact with the keyboard with their real hand. Visual cues are used to guide the users on the keys to depress. A scoring system is also designed to award points for correctly depressed notes. This encourages the patient to perform well and to keep track of his progress.</p><h3 class="c-article__sub-heading" id="Sec5">Overall framework</h3><p>The overall system architecture is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig1">1</a>. The whole scene is captured using a web cam on a head mounted display (HMD). The user can view the whole screen through the HMD. By mimicking the user’s viewpoint and allowing the user to interact with objects in view, the disconnection between the interaction of physical and virtual elements would be minimized and the need for a real keyboard is also eliminated.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Overall system architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>There are six main components in the system framework, namely,</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <i>Visuals component</i>
                      </p>
                    </li>
                  </ul>
                <p>The user has to be able to look at the virtual hand and the virtual keyboard as though they are real physical objects. The main objectives for this component are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Creation of a virtual hand and a keyboard, which are rendered onto the scene using OpenGL.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Real-time tracking of the camera position in relation to the markers and rendering of the virtual objects according to the viewpoint of the camera.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">•</span>
                      
                        <p>
                          <i>Audio component</i>
                        </p>
                      
                    </li>
                  </ol>
                <p>In order to simulate the sounds of a real keyboard, sound samples of various notes on a keyboard are needed. When the virtual keys are being depressed, the corresponding notes are to be played. This is achieved using the <i>PlaySound</i> function from Windows Multimedia API. The main tasks of this component are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Record sound samples of various keyboard notes and convert them into WAV files, which can be played using the <i>PlaySound</i> function.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Associate the WAV files with the corresponding virtual keys.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">•</span>
                      
                        <p>
                          <i>Component for the control of virtual hand using data</i>-<i>glove</i>
                        </p>
                      
                    </li>
                  </ol>
                <p>The user must be able to move the virtual hand around the scene and also flex the virtual fingers as though it is his/her own hand. The main objectives of this component are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Reading the user’s finger flexure information using the 5DT data-glove and using this information to control the joint angles of the virtual hand.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Tracking the marker attached on the data-glove so that the position of the user’s hand can be tracked in real time.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">•</span>
                      
                        <p>
                          <i>Component for interaction between virtual hand and virtual keyboard</i>
                        </p>
                      
                    </li>
                  </ol>
                <p>In order for the user to depress the virtual keys using the virtual hand, collisions between the virtual objects have to be determined. In addition, the playing of the corresponding keyboard sounds and the movement of the keys need to be considered in order to simulate the scenario of playing a real keyboard. The main tasks are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Real-time detection of the spatial relationships between the markers using ARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="ARToolKit (2010) [online] Human Interface Technology Laboratory, University of Washington. Available from: &#xA;                    http://www.hitl.washington.edu/research/shared_space/download/&#xA;                    &#xA;                  . Accessed 12 June 2010" href="/article/10.1007/s10055-011-0194-x#ref-CR3" id="ref-link-section-d45253e666">2010</a>).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Real-time check of the collisions using the above-mentioned data by transforming the coordinates and analyzing the spatial relationship.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">III.</span>
                      
                        <p>Relevant actions to be taken when collisions are detected.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">•</span>
                      
                        <p>
                          <i>User guide component</i>
                        </p>
                      
                    </li>
                  </ol>
                <p>For the purpose of guiding the user to the notes to be depressed, visual cues are drawn and augmented onto the scene at suitable positions. The positions of these visual cues have to be changed according to the notes being depressed. The main objectives are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Creation of a guiding arrow that will be rendered onto the scene using OpenGL.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Development of a preset pattern that will display the guiding arrows at various points according to the note to be played next and whether the correct note has been played. In order to achieve this, the process is linked to the collision response process.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">•</span>
                      
                        <p>
                          <i>Scoring component</i>
                        </p>
                      
                    </li>
                  </ol>
                <p>The algorithm in this component must be able to determine the notes that are being depressed correctly and award points or deduct points based this. The algorithm will also comment on how well the user has performed according to the number of correct notes achieved. The main tasks identified are as follows:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">I.</span>
                      
                        <p>Development of an algorithm that adds points when the correct notes are depressed and deduct points with incorrect notes. This is done within the collision response process and linked with the preset pattern process.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">II.</span>
                      
                        <p>Displaying the information of the user’s score and the corresponding comments onto a suitable position in the scene.</p>
                      
                    </li>
                  </ol>
                <h3 class="c-article__sub-heading" id="Sec6">Hand movement detection</h3><p>The hardware implemented in this system to detect the hand movements and interact with the virtual objects is the 5DT 5-W data-glove, which is used to allow the user to control the fingers of the virtual hand. This is a wireless glove that is capable of measuring finger flexure and the orientation (pitch and roll) of a user’s hand. The data-glove, consisting of seven sensors in which five sensors (one for each finger), makes use of proprietary fiber optic-based technology to detect flexure information. The other two sensors are used to detect the tilt angles of the palm. The sensors attached to the fingers are capable of providing a flexure resolution of up to 256 positions for each finger.</p><p>As part of the manufacturing process, the data-glove has been calibrated to obtain the largest possible dynamic range, which is the difference between the maximum and minimum output values. By default, the glove outputs raw sensor values, which range from 0 to 4,095 whereby 0 corresponds to a fully un-flexed state and 4,095 corresponds to a fully flexed state.</p><p>However, due to differences in hand size and finger flexing angle, the dynamic range for each individual may differ. For example, when a user keeps his/her hand flat, the output value could be 40. As the default fully un-flexed value is 0, the data-glove driver program would interpret this as a slight flexing of the finger. Likewise, when the user’s hands are fully flexed, the output value could be 4,070 instead of 4,095. In this case, the wrong signals are being sent and the dynamic range is also reduced.</p><p>To address this issue, the data-glove driver has an auto-calibration process that provides sensor outputs in a linearly calibrated fashion. During each update of the sensor values, the raw output value read is compared with the current maximum and minimum raw values. If the current boundary values are exceeded, they are overwritten by the new values. Hence, the maximum and minimum values are being pushed “outwards” continuously and the dynamic range is increased accordingly. Thus, by flexing one’s fingers repeatedly before use, the glove is automatically calibrated to suit the particular user. A scaled output value can then be obtained using (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0194-x#Equ1">1</a>),</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\text{out}} = \frac{{{\text{raw}}_{\text{val}} - {\text{raw}}_{\min } }}{{{\text{raw}}_{\max } - {\text{raw}}_{\min } }}\;{\text{Max}} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where Max is the scaled maximum range determined by the user.</p><p>Through this calibration process, the output value is scaled dynamically against the current dynamic range in every update. Through setting the value of Max, the user can utilize a scale that ranges from 0 to the stipulated value of Max. In this way, the obtained values can then be conveniently manipulated to interpret the finger flexure angles. The data output from the data-glove is always between 0 and 1.</p><p>After the calibration process, mapping the output data from the data-glove to the rotating ranges of the first joints of the fingers, the real-time joint rotating angles of the first joints θ<sub>1</sub> can be estimated. When a hand is fully flexed, the angles of the joints starting from the knuckle joint are approximated to be 60, 120, and 30 degrees, respectively. By multiplying the output data from the data-glove with the estimated maximum bending angles, the flexing angles of the joints can be approximated. The next step computes the movements of the other joints as the sensors are attached to the first joints and the rotating angles of the other joints are unavailable. To map the joint angle of the first joint to the other joints, (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0194-x#Equ2">2</a>), which is used as the dynamic constraints in the hand gesture recognition system (Pavlovic et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: a review. IEEE Trans Pattern Anal Mach Intell 19(7):677–695" href="/article/10.1007/s10055-011-0194-x#ref-CR21" id="ref-link-section-d45253e814">1997</a>), is implemented. In (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0194-x#Equ2">2</a>), θ<sub>1</sub> is the joint angle of the first joint, and θ<sub>2</sub> is the joint angle of the second joint. If the finger is not the thumb, θ<sub>3</sub> is the joint angle of the third joint.</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {{\uptheta}}_{2} &amp;= 2{{\uptheta}}_{1} \hfill \\ {{\uptheta}}_{3} &amp;= 4/3{{\uptheta}}_{1} \hfill \\ \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                <p>After the calibration and mapping processes, the flexing status of the hand can be detected and mapped to the virtual hand, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Range of virtual hand flexure states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec7">Collision detection</h3><p>To detect the touching between the tips of the virtual fingers and the virtual keys, a simple mechanism for collision detection has been developed by monitoring the distance between suitable points on the two virtual objects. This mechanism can meet the needs of this application. In addition, it is more efficient computationally than the existing exhaustive collision detection mechanisms that have been reported.</p><p>The virtual hand and the virtual keyboard are rendered on two different markers. The spatial relationship between the two markers and the two <i>fixed</i> points on each virtual object can be computed using the ARToolKit library. The transformations between the coordinate systems of the markers and the <i>fixed</i> points are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig3">3</a>. Due to the finger flexure movements, the positions of the points on the virtual fingers are translated in the coordinate system that is built on the marker attached to the data-glove, i.e., the marker to render the virtual hand. Hence, mathematical methods are used to determine the relative y and z positions between the virtual fingers and the individual keys of the virtual keyboard (the finger flexure movement is in the <i>y</i>- and <i>z</i>-directions in this application after the coordinate systems of the markers are defined).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Coordinate systems transformations</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Due to the nature of the application, only collisions between the tips of the virtual fingers and the tops of the keys need to be detected. In order to achieve this, a suitable test point for each finger tip is selected for collision testing.</p><p>In order to test for collisions at the bottom-most point of each finger tip, a sphere of a suitable size is selected for each finger tip. The selected test point is then the bottom-most point of the test sphere as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig4">4</a>. When the finger flexes to a lower position, the test point will shift to the new bottom-most point. In other words, the test point will always be the bottom-most point of the finger.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Resolving the position of the test point</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Based on the bending angles of each joint as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig4">4</a>, changes in the position of the test point in the <i>y</i>- and <i>z</i>-axis can be calculated with the following equations:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ Y_{t} = L_{a} \cos {{\uptheta}}_{a} + L_{b} \cos {{\uptheta}}_{b} + L_{c} \cos {{\uptheta}}_{c} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                  <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ Z_{t} = L_{a} \sin {{\uptheta}}_{a} + L_{b} \sin {{\uptheta}}_{b} + L_{c} \sin \theta_{c} + \, R_{\text{testsphere}} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                <p>In order to determine whether the test point has penetrated the boundaries of the virtual keys, the distance between the test point and the boundaries of the keyboard needs to be computed. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig5">5</a> illustrates the calculation of the test distances between the bottom edge and top surface boundary of a key. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig5">5</a>, <i>K</i>
                  <sub>
                    <i>y</i>
                  </sub> is the distance between the first bending point (center of knuckle bone) of the index finger and a point on the bottom edge of the keyboard, which is computed directly from the transformation matrices between the two markers in the system. The test distance, <i>D</i>
                  <sub>
                    <i>y</i>
                  </sub>, can be found easily as follows:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ D_{y} = K_{y} - Y_{t} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>Similarly, the test distance for the z-axis can be found as follows:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ D_{z} = K_{z} - Z_{t} $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>As the test point does not move relative to the virtual hand in the <i>x</i>-direction, <i>D</i>
                  <sub>
                    <i>x</i>
                  </sub> is simply:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ D_{x} = K_{x} $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Resolving the test distance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Similarly, the test distances with all other boundaries can be found. When the condition for all three test distances is met, i.e., <i>Dx</i>, <i>Dy</i>, and <i>Dz</i> are all less than zero, collision between the fingertip and the key has occurred.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Implementation</h2><div class="c-article-section__content" id="Sec8-content"><p>This system has been implemented on a P4 3 GHz PC equipped with 1 GB RAM, an ordinary web camera and a display device. Two wireless 5DT data-gloves (right- and left-handed versions) are used as input devices. This proposed system is developed as a stand-alone home rehabilitation system for hand-eye coordination and finger dexterity.</p><h3 class="c-article__sub-heading" id="Sec9">Rendering and interaction</h3><p>The virtual hand is modeled using a combination of OpenGL drawing functions and 3DS Max modeling. In order to allow for flexure of the finger joints, the fingers are modeled separately from the palm using cylinders and spheres, which are drawn directly using OpenGL functions. To allow for relatively more complex geometry and surface features, the palm of the hand is first modeled using 3DS Max and then converted into the ASCII STL format. The keys of the virtual keyboard are also premodeled using 3DS Max and rendered using the OpenGL. In order to allow independent movement of each key, the keys are modeled and rendered individually. The virtual keyboard consists of one octave of keys. Using the drawing functions of triangles in OpenGL, it is easy to render objects in the STL files in AR environment as virtual objects.</p><p>To avoid occlusion of the <i>Hiro</i> marker during interaction, the keyboard model is rendered at a distance away from the marker, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig6">6</a>. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig6">6</a>, the bounding boxes of the markers indicate that the program has detected the two markers. The virtual keyboard is rendered on the right of the <i>Hiro</i> marker, and the virtual hand can move freely along the virtual keyboard without occluding the marker.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Virtual hand with keyboard</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Attaching the kanji marker onto the data-glove, the virtual hand will be overlaid on the user’s hand and move with movements of the user’s hand. Using the data-glove and the movement detection methods introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0194-x#Sec6">3.3</a>, the flexure angles of the user’s fingers can be obtained. Using the rotation functions in OpenGL, the rendering of the virtual hand will be updated to ensure that the virtual hand would flex with the hand movements of the user.</p><p>After the flexure information of the hand has been obtained, the touching status between the virtual fingertips and the virtual piano keyboards can be obtained through the collision detection methods introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0194-x#Sec7">3.4</a>.</p><p>With a marker tracked using the tracking methods in ARToolkit, the virtual piano keyboard can be rendered in a local coordinate system (LCS), which is defined relative to the world coordinate system (WCS). The origin of WCS is at the center of the marker. A virtual hand rendered on the Kanji marker can be used to interact with the virtual piano. The transformation between the coordinate systems is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig3">3</a>. Equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0194-x#Equ8">8</a>) defines the camera model based on the perspective projection. Based on (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0194-x#Equ8">8</a>), information of the two tracked markers, and some predefined parameters, the position of the test point on the fingertips of the virtual hand in the WCS can be obtained. In addition, the transformation between the WCS and LCS is known.</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \rho m = A\left[ {R\left| t \right.} \right]M $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where <span class="mathjax-tex">\( m = \left( {u,v} \right)^{T} ,\quad M = \left( {X,Y,X} \right)^{T} ,\;\rho \)</span> is an arbitrary factor.</p><p>When the spatial relationships between the virtual objects have been obtained, the respective test distances between each finger and the various piano keys can be calculated. The test distances are monitored for any interference of the collision test boundaries.</p><p>When there is no collision detected between a finger and a particular key, the key model is at its default height. If the collision criteria for a key are met, the sound of that key is played and the key model is translated downwards as if it is being pressed. A variable has been defined in the program to ensure that the sound does not play repeatedly while the key is held down. When the virtual finger is lifted, the collision criteria cannot be met and the key is returned to its default height. The sound of the key is prepared by recording one octave of notes from an electronic keyboard and converting them into the WAV format. These WAV files are then associated with their respective keys and played back when the respective keys are depressed.</p><h3 class="c-article__sub-heading" id="Sec10">User guide/scoring system</h3><p>In this system, a guiding arrow is displayed to indicate the current note to be played to the user. A 3D polygon arrow is modeled using 3DS Max, converted to STL format and rendered onto the scene using OpenGL.</p><p>The guiding arrow is designed to be moved to different positions on the virtual keyboard according to certain patterned stages. At the first stage, which is the default stage, the arrow is moved to point at the position of the first note to be played. When a key is depressed, the system then proceeds to check whether it corresponds to the current stage. If the correct note is depressed, the pattern is then advanced to the next stage and the guiding arrow is moved to the next key to be played. However, if the incorrect note is depressed, the pattern remains in its current stage and the guiding arrow remains on the current key. This process repeats until all the correct notes have been played. In order to indicate whether the user is pressing the correct or wrong keys, the color of the piano key will be changed accordingly. Correctly depressed keys will turn green while incorrect ones will turn red.</p><p>A scoring system which deals with awarding points is also designed in the system to evaluate the performance of the users. When it has been detected that the correct note has been depressed, the system adds one point to the current score. One point will be deducted when an incorrect note has been pressed. The information of the score awarded is also displayed on the interface to indicate the real-time performance of the users, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0194-x/MediaObjects/10055_2011_194_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Case study</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0194-x/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>After all the notes in a pattern are completed, the scoring system will display a comment on the scene based on the score achieved by the user. The comments for various points achieved in an 8-note pattern are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0194-x#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Commentary based on points</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-011-0194-x/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Case study</h2><div class="c-article-section__content" id="Sec11-content"><p>In this section, more details of this system are illustrated using a simple case study, which is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>. In this case study, the system instructed the user to play a simple 8-note pattern.</p><p>In this case study, the user put on the data-glove with the <i>Kanji</i> marker attached to it and the <i>Hiro</i> marker was placed on the table. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>a, the virtual objects were rendered in the scene and the guiding arrow displayed at the first note to be played. The starting score was zero. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>b, the user moved his hand toward the first note and flexed his/her finger to depress the key. The key turned green, indicating that the correct note had been depressed and the guiding arrow moved to the next note to be played. The user was awarded one point for playing the correct note. When the user played the wrong note, the key turned red and the guiding arrow remained at the current note to be depressed, indicating that the wrong note had been depressed, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>c. One point was deducted and the score was zero. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>d, the user tried again and played the correct note this time. The key turned green and the guiding arrow moved to the next note. The score was increased to one again. When all eight notes had been played, the guiding arrow disappeared and the AR-Rehab system displayed a comment on the performance of the user. In this case, the user had achieved six points and “Well done!” was displayed, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0194-x#Fig7">7</a>e.</p><p>In order to depress the correct keys, the user needs to be able to judge the positions of the keys that are to be depressed and the relative position of his/her hand to the keys. This addresses the problem of perceptual accuracy, which is one of the skills pertaining to hand-eye coordination. After moving his/her hand to the correct position of a key, the user only needs to flex his finger to virtually touch and depress the key while keeping his/her hand steady at the correct position. This addresses the depth perception and finger dexterity of the user.</p><p>Visual and audio feedbacks are provided to the users during the exercising process to encourage the users. The changed colors of the keys would provide instant feedback to the users on whether the correct note has been depressed. By being awarded points for correctly depressed keys, the user is encouraged to continue trying to do well. Through the deduction of points for incorrectly depressed notes, the user will be inclined to avoid making mistakes. Lastly, the comments given to the user at the completion of a pattern would indicate the user’s performance, which would encourage the user and give an indication of the user’s progress toward recovery.</p><p>By depositing a variety of patterns which vary in complexity and number of notes in the library of the system, users with different degrees of impairment can be catered and progressive training can be provided.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Conclusions</h2><div class="c-article-section__content" id="Sec12-content"><p>In this paper, an AR application based on data-glove has been proposed and developed for rehabilitation of hand-eye coordination and finger dexterity. In this system, the user can interact with a virtual keyboard using a virtual hand that has been augmented onto his/her hand. The movements of the virtual hand are controlled by the movements of the user’s real hand that are detected using the 5DT data-glove. Guiding the user via visual cues to play predefined patterns and scoring the user based on performance, the system can serve as a rehabilitation tool that focuses on skills pertaining to hand-eye coordination and finger dexterity. This application is especially useful for patients who are suffering from severe neurological impairment and are unable to perform traditional rehabilitative exercises, such as ball catching. It is also a novel and viable alternative to current rehabilitation equipment, such as pin slotting devices that aim at training similar skills.</p><p>Compared with VR rehabilitation systems providing similar functions, this application can minimize the disconnection between the real hand and virtual objects as they are in the same physical world and the user can visualize them simultaneously. Transforming the coordinates of the hand and the virtual piano into the same world coordinate system established by the <i>Hiro</i> marker, the user can have the experience of playing the virtual piano using his/her real hand. In the developed system, the movements of the virtual hand and real hand are consistent. Therefore, the user has to extend his/her real hands to the position of virtual piano to press the keys, which is useful for the training of hand-eye coordination. The interaction provided by this application is more realistic and intuitive.</p><p>In this system, in order to address the problems of reaction time and higher-level finger dexterity, a more advanced scoring system that takes into account rhythm and timing will be considered in future works. As the data-glove used in this project has only one sensor per finger, the flexure angles of the various joints within a finger have to be approximated in relation to each other. In order to produce more realistic hand motion and greater flexibility in hand gestures, a data-glove with more sensors will be used in the future. When depressing the keys, the user does not receive a physical sensation of playing the keyboard. To further enhance the user experience, inclusion of tactile feedback where vibrations are induced when the keys are “touched” will be considered.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Adamovich SV, Fluet GG, Mathai A, Qiu Q, Lewis J, Merians AS (2009) Design of a complex virtual reality simula" /><p class="c-article-references__text" id="ref-CR1">Adamovich SV, Fluet GG, Mathai A, Qiu Q, Lewis J, Merians AS (2009) Design of a complex virtual reality simulation to train finger motion for persons with hemiparesis: a proof of concept study. J NeuroEng Rehabilitation 6(28). doi:<a href="https://doi.org/10.1186/1743-0003-6-28">10.1186/1743-0003-6-28</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aguiar R, Piano VR (2007) Available from: http://www.youtube.com/watch?v=wdDxXdMOKX0. Accessed: 28th Aug 2009" /><p class="c-article-references__text" id="ref-CR2">Aguiar R, Piano VR (2007) Available from: <a href="http://www.youtube.com/watch?v=wdDxXdMOKX0">http://www.youtube.com/watch?v=wdDxXdMOKX0</a>. Accessed: 28th Aug 2009</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ARToolKit (2010) [online] Human Interface Technology Laboratory, University of Washington. Available from: htt" /><p class="c-article-references__text" id="ref-CR3">ARToolKit (2010) [online] Human Interface Technology Laboratory, University of Washington. Available from: <a href="http://www.hitl.washington.edu/research/shared_space/download/">http://www.hitl.washington.edu/research/shared_space/download/</a>. Accessed 12 June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Barakonyi I, Schmalstieg D (2005) Augmented reality agents in the development pipeline of computer entertainme" /><p class="c-article-references__text" id="ref-CR4">Barakonyi I, Schmalstieg D (2005) Augmented reality agents in the development pipeline of computer entertainment. In: Proceedings of the 4th International conference on entertainment computer, Sanda, Japan, 2005 September 19–21, pp 345–356</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Broeren, A. Bjorkdahl, L. Claesson, D. Goude, A. Lundgren-Nilsson, H. Samuelsson, C. Blomstrand, KS. Sunnerhagen, M. Rydmark, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Broeren J, Bjorkdahl A, Claesson L, Goude D, Lundgren-Nilsson A, Samuelsson H, Blomstrand C, Sunnerhagen KS, R" /><p class="c-article-references__text" id="ref-CR5">Broeren J, Bjorkdahl A, Claesson L, Goude D, Lundgren-Nilsson A, Samuelsson H, Blomstrand C, Sunnerhagen KS, Rydmark M (2008) Virtual rehabilitation after stroke. StudHealth Technol Inform 136:77–82</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20rehabilitation%20after%20stroke&amp;journal=StudHealth%20Technol%20Inform&amp;volume=136&amp;pages=77-82&amp;publication_year=2008&amp;author=Broeren%2CJ&amp;author=Bjorkdahl%2CA&amp;author=Claesson%2CL&amp;author=Goude%2CD&amp;author=Lundgren-Nilsson%2CA&amp;author=Samuelsson%2CH&amp;author=Blomstrand%2CC&amp;author=Sunnerhagen%2CKS&amp;author=Rydmark%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Burdea, V. Popescu, V. Hentz, K. Colbert, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Burdea G, Popescu V, Hentz V, Colbert K (2000) Virtual reality-based orthopedic telerehabilitation. IEEE Trans" /><p class="c-article-references__text" id="ref-CR6">Burdea G, Popescu V, Hentz V, Colbert K (2000) Virtual reality-based orthopedic telerehabilitation. IEEE Trans Rehabilitation Eng 8(3):430–432</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality-based%20orthopedic%20telerehabilitation&amp;journal=IEEE%20Trans%20Rehabilitation%20Eng&amp;volume=8&amp;issue=3&amp;pages=430-432&amp;publication_year=2000&amp;author=Burdea%2CG&amp;author=Popescu%2CV&amp;author=Hentz%2CV&amp;author=Colbert%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burke JW, McNeill MDJ, Charles DK, Morrow PJ, Crosbie JH, McDonough SM (2009) Serious games for upper limb reh" /><p class="c-article-references__text" id="ref-CR7">Burke JW, McNeill MDJ, Charles DK, Morrow PJ, Crosbie JH, McDonough SM (2009) Serious games for upper limb rehabilitation following stroke. 2009 Conference in games and virtual worlds for serious applications, Coventry, UK, March 23–24, pp 103–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen Y, Huang H, Xu W, Wallis RI, Sundaram H, Rikakis T, Ingalls T, Olson L, He J (2006) The design of a real-" /><p class="c-article-references__text" id="ref-CR8">Chen Y, Huang H, Xu W, Wallis RI, Sundaram H, Rikakis T, Ingalls T, Olson L, He J (2006) The design of a real-time, multimodal biofeedback system for stroke patient rehabilitation. In: Proceeding of the 14th annual ACM international conference on Multimedia, ACM Press, New York, Santa Barbara, CA, USA, 2006 October 23–27, pp 763–772</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Choi KS, Chow CM, Lo KH (2010) A rehabilitation method with visual and haptic guidance for children with upper" /><p class="c-article-references__text" id="ref-CR9">Choi KS, Chow CM, Lo KH (2010) A rehabilitation method with visual and haptic guidance for children with upper extremity disability. Lecture Notes Comput Sci 6180/2010, 2010, 77–84</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JD. Crawford, WP. Medendorp, JJ. Marotta, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Crawford JD, Medendorp WP, Marotta JJ (2004) Spatial transformations for eye–hand coordination. J Neurophysiol" /><p class="c-article-references__text" id="ref-CR10">Crawford JD, Medendorp WP, Marotta JJ (2004) Spatial transformations for eye–hand coordination. J Neurophysiol 92(1):10–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20transformations%20for%20eye%E2%80%93hand%20coordination&amp;journal=J%20Neurophysiol&amp;volume=92&amp;issue=1&amp;pages=10-19&amp;publication_year=2004&amp;author=Crawford%2CJD&amp;author=Medendorp%2CWP&amp;author=Marotta%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JH. Crosbie, S. Lennon, JR. Basford, SM. McDonough, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Crosbie JH, Lennon S, Basford JR, McDonough SM (2007) Virtual reality in stroke rehabilitation: still more vir" /><p class="c-article-references__text" id="ref-CR11">Crosbie JH, Lennon S, Basford JR, McDonough SM (2007) Virtual reality in stroke rehabilitation: still more virtual than real. Disability &amp; Rehabilitation 29(14):1139–1146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20in%20stroke%20rehabilitation%3A%20still%20more%20virtual%20than%20real&amp;journal=Disability%20%26%20Rehabilitation&amp;volume=29&amp;issue=14&amp;pages=1139-1146&amp;publication_year=2007&amp;author=Crosbie%2CJH&amp;author=Lennon%2CS&amp;author=Basford%2CJR&amp;author=McDonough%2CSM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MK. Holden, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol &amp; Behav 8(3):187–211" /><p class="c-article-references__text" id="ref-CR12">Holden MK (2005) Virtual environments for motor rehabilitation: review. Cyberpsychol &amp; Behav 8(3):187–211</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environments%20for%20motor%20rehabilitation%3A%20review&amp;journal=Cyberpsychol%20%26%20Behav&amp;volume=8&amp;issue=3&amp;pages=187-211&amp;publication_year=2005&amp;author=Holden%2CMK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="NA. Incel, M. Sezgin, I. As, OB. Cimen, G. Sahin, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Incel NA, Sezgin M, As I, Cimen OB, Sahin G (2009) The geriatric hand: correlation of hand-muscle function and" /><p class="c-article-references__text" id="ref-CR13">Incel NA, Sezgin M, As I, Cimen OB, Sahin G (2009) The geriatric hand: correlation of hand-muscle function and activity restriction in elderly. Int J Rehabilitation Res 32(3):213–218</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20geriatric%20hand%3A%20correlation%20of%20hand-muscle%20function%20and%20activity%20restriction%20in%20elderly&amp;journal=Int%20J%20Rehabilitation%20Res&amp;volume=32&amp;issue=3&amp;pages=213-218&amp;publication_year=2009&amp;author=Incel%2CNA&amp;author=Sezgin%2CM&amp;author=As%2CI&amp;author=Cimen%2COB&amp;author=Sahin%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Integrated Bio-medics &amp; Technologies website (2009) Available from: http://www.ibmtindia.com/occup.asp. Access" /><p class="c-article-references__text" id="ref-CR14">Integrated Bio-medics &amp; Technologies website (2009) Available from: <a href="http://www.ibmtindia.com/occup.asp">http://www.ibmtindia.com/occup.asp</a>. Accessed 15 March, 2009</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Jack, R. Boian, AS. Merians, M. Tremaine, GC. Burdea, SV. Adamovich, M. Recce, H. Poizner, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M and Poizner H (2001) Virtual reality" /><p class="c-article-references__text" id="ref-CR15">Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M and Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE transactions on neural systems and rehabilitation engineering 2001. 9(3):308–318</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality-enhanced%20stroke%20rehabilitation&amp;journal=IEEE%20transactions%20on%20neural%20systems%20and%20rehabilitation%20engineering&amp;volume=9&amp;issue=3&amp;pages=308-318&amp;publication_year=2001&amp;author=Jack%2CD&amp;author=Boian%2CR&amp;author=Merians%2CAS&amp;author=Tremaine%2CM&amp;author=Burdea%2CGC&amp;author=Adamovich%2CSV&amp;author=Recce%2CM&amp;author=Poizner%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Jack, R. Boian, AS. Merians, M. Tremaine, GC. Burdea, SV. Adamovich, M. Recce, H. Poizner, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-en" /><p class="c-article-references__text" id="ref-CR16">Jack D, Boian R, Merians AS, Tremaine M, Burdea GC, Adamovich SV, Recce M, Poizner H (2001) Virtual reality-enhanced stroke rehabilitation. IEEE Trans Neural Syst Rehabilitation Eng 9(3):308–318</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality-enhanced%20stroke%20rehabilitation&amp;journal=IEEE%20Trans%20Neural%20Syst%20Rehabilitation%20Eng&amp;volume=9&amp;issue=3&amp;pages=308-318&amp;publication_year=2001&amp;author=Jack%2CD&amp;author=Boian%2CR&amp;author=Merians%2CAS&amp;author=Tremaine%2CM&amp;author=Burdea%2CGC&amp;author=Adamovich%2CSV&amp;author=Recce%2CM&amp;author=Poizner%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li S, Frisoli A, Avizzano CA, Ruffaldi E, Lugo-Villeda LI, Bergamasco M (2009) Bimanual haptic—desktop platfor" /><p class="c-article-references__text" id="ref-CR17">Li S, Frisoli A, Avizzano CA, Ruffaldi E, Lugo-Villeda LI, Bergamasco M (2009) Bimanual haptic—desktop platform for upper-limb post-stroke rehabilitation: practical trials. In: Proceedings of the 2009 IEEE International conference on robotics and biomimetics, December 19–23, 2009, Guilin, China, pp 480–485</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Luo X, Kline T, Fischer HC, Stubblefield KA, Kenyon RV, Kamper DG (2005) Integration of augmented reality and " /><p class="c-article-references__text" id="ref-CR18">Luo X, Kline T, Fischer HC, Stubblefield KA, Kenyon RV, Kamper DG (2005) Integration of augmented reality and assistive devices for post-stroke hand opening rehabilitation. In: Proceedings of the 2005 IEEE, Engineering in medicine and biology 27th annual conference, Shanghai, China, 2005 September 1–4, pp 6855–6858</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Malouin, CL. Richards, B. McFadyen, J. Doyon, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Malouin F, Richards CL, McFadyen B, Doyon J (2003) New Perspectives of locomotor rehabilitation after stroke. " /><p class="c-article-references__text" id="ref-CR19">Malouin F, Richards CL, McFadyen B, Doyon J (2003) New Perspectives of locomotor rehabilitation after stroke. Médecine Sci (Paris), 19(10), 994–998</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=New%20Perspectives%20of%20locomotor%20rehabilitation%20after%20stroke&amp;journal=M%C3%A9decine%20Sci%20%28Paris%29&amp;volume=19&amp;issue=10&amp;pages=994-998&amp;publication_year=2003&amp;author=Malouin%2CF&amp;author=Richards%2CCL&amp;author=McFadyen%2CB&amp;author=Doyon%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pareto L, Broeren J, Goude D, Rydmark M (2008) Virtual reality, haptics and post-stroke rehabilitation in prac" /><p class="c-article-references__text" id="ref-CR20">Pareto L, Broeren J, Goude D, Rydmark M (2008) Virtual reality, haptics and post-stroke rehabilitation in practical therapy. In: Proceedings of 7th International conference series on disability, virtual reality and associated technologies (ICDVRAT) with ArtAbilitation, Maia, Portugal, 2008 September 8–10, pp 245–252</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VI. Pavlovic, R. Sharma, TS. Huang, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: " /><p class="c-article-references__text" id="ref-CR21">Pavlovic VI, Sharma R, Huang TS (1997) Visual interpretation of hand gestures for human-computer interaction: a review. IEEE Trans Pattern Anal Mach Intell 19(7):677–695</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20interpretation%20of%20hand%20gestures%20for%20human-computer%20interaction%3A%20a%20review&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=19&amp;issue=7&amp;pages=677-695&amp;publication_year=1997&amp;author=Pavlovic%2CVI&amp;author=Sharma%2CR&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VG. Popescu, CG. Burdea, M. Bouzit, VR. Hentz, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Popescu VG, Burdea CG, Bouzit M, Hentz VR (2000) A virtual-reality-based telerehabilitation system with force " /><p class="c-article-references__text" id="ref-CR22">Popescu VG, Burdea CG, Bouzit M, Hentz VR (2000) A virtual-reality-based telerehabilitation system with force feedback. IEEE Trans Inf Technol Biomed
 4(1):45–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20virtual-reality-based%20telerehabilitation%20system%20with%20force%20feedback&amp;journal=IEEE%20Trans%20Inf%20Technol%20Biomed&amp;volume=4&amp;issue=1&amp;pages=45-51&amp;publication_year=2000&amp;author=Popescu%2CVG&amp;author=Burdea%2CCG&amp;author=Bouzit%2CM&amp;author=Hentz%2CVR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Riva, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Riva G (1998) Virtual environments in neuroscience. IEEE Transa Inf Technol Biomed 2(4):275–281" /><p class="c-article-references__text" id="ref-CR23">Riva G (1998) Virtual environments in neuroscience. IEEE Transa Inf Technol Biomed 2(4):275–281</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environments%20in%20neuroscience&amp;journal=IEEE%20Transa%20Inf%20Technol%20Biomed&amp;volume=2&amp;issue=4&amp;pages=275-281&amp;publication_year=1998&amp;author=Riva%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BH. Rosenberg, D. Landsittel, TD. Averch, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Rosenberg BH, Landsittel D, Averch TD (2005) Can video games be used to predict or improve laparoscopic skills" /><p class="c-article-references__text" id="ref-CR24">Rosenberg BH, Landsittel D, Averch TD (2005) Can video games be used to predict or improve laparoscopic skills? J Endourol Soc 19(3):372–376</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20video%20games%20be%20used%20to%20predict%20or%20improve%20laparoscopic%20skills%3F&amp;journal=J%20Endourol%20Soc&amp;volume=19&amp;issue=3&amp;pages=372-376&amp;publication_year=2005&amp;author=Rosenberg%2CBH&amp;author=Landsittel%2CD&amp;author=Averch%2CTD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sucar LE, Leder RS, Reinkensmeyer D, Hernández J, Azcárate G, Casteñeda N, Saucedo P (2008) Gesture therapy—a " /><p class="c-article-references__text" id="ref-CR25">Sucar LE, Leder RS, Reinkensmeyer D, Hernández J, Azcárate G, Casteñeda N, Saucedo P (2008) Gesture therapy—a low-cost vision-based system for rehabilitation after stroke. HEALTHINF 2008, Funchal, Madeira, Portugal, 2008 January 28–31, pp 107–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sveistrup H (2004) Motor rehabilitation using virtual reality. J NeuroEng Rehabilitation 1(10). doi:10.1186/17" /><p class="c-article-references__text" id="ref-CR26">Sveistrup H (2004) Motor rehabilitation using virtual reality. J NeuroEng Rehabilitation 1(10). doi:<a href="https://doi.org/10.1186/1743-0003-1-10">10.1186/1743-0003-1-10</a>
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0194-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Mechanical Engineering Department, Faculty of Engineering, National University of Singapore, 9 Engineering Drive 1, Singapore, 117576, Singapore</p><p class="c-article-author-affiliation__authors-list">Y. Shen, P. W. Gu, S. K. Ong &amp; A. Y. C. Nee</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Y_-Shen"><span class="c-article-authors-search__title u-h3 js-search-name">Y. Shen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Y.+Shen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Y.+Shen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Y.+Shen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-P__W_-Gu"><span class="c-article-authors-search__title u-h3 js-search-name">P. W. Gu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;P. W.+Gu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=P. W.+Gu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22P. W.+Gu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-S__K_-Ong"><span class="c-article-authors-search__title u-h3 js-search-name">S. K. Ong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;S. K.+Ong&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=S. K.+Ong" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22S. K.+Ong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-A__Y__C_-Nee"><span class="c-article-authors-search__title u-h3 js-search-name">A. Y. C. Nee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;A. Y. C.+Nee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=A. Y. C.+Nee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22A. Y. C.+Nee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0194-x/email/correspondent/c1/new">S. K. Ong</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20novel%20approach%20in%20rehabilitation%20of%20hand-eye%20coordination%20and%20finger%20dexterity&amp;author=Y.%20Shen%20et%20al&amp;contentID=10.1007%2Fs10055-011-0194-x&amp;publication=1359-4338&amp;publicationDate=2011-08-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Shen, Y., Gu, P.W., Ong, S.K. <i>et al.</i> A novel approach in rehabilitation of hand-eye coordination and finger dexterity.
                    <i>Virtual Reality</i> <b>16, </b>161–171 (2012). https://doi.org/10.1007/s10055-011-0194-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0194-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-06-15">15 June 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-07-22">22 July 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-08-05">05 August 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-06">June 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0194-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0194-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Rehabilitation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand-eye coordination</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Finger dexterity</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0194-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=194;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

