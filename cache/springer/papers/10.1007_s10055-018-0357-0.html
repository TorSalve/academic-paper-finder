<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Exploiting sensing devices availability in AR/VR deployments to foster"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the assessment of the user&#8217;s..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Exploiting sensing devices availability in AR/VR deployments to foster engagement"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-07-14"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the assessment of the user&#8217;s performance are based on her/his (re)actions which are continuously traced/sensed. In this paper, we propose the exploitation of the sensors available in the AR/VR systems to enhance the current AR/VR experiences, taking into account the users&#8217; affect state that changes in real time. Adapting the difficulty level of the experience to the users&#8217; affect state fosters their engagement which is a crucial issue in educational environments and prevents boredom and anxiety. The users&#8217; cues are processed enabling dynamic user profiling. The detection of the affect state based on different sensing inputs, since diverse sensing devices exist in different AR/VR systems, is investigated, and techniques that have been undergone validation using state-of-the-art sensors are presented."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-07-14"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="399"/>

    <meta name="prism.endingPage" content="410"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0357-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0357-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0357-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0357-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Exploiting sensing devices availability in AR/VR deployments to foster engagement"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2019/12"/>

    <meta name="citation_online_date" content="2018/07/14"/>

    <meta name="citation_firstpage" content="399"/>

    <meta name="citation_lastpage" content="410"/>

    <meta name="citation_article_type" content="S.I. : VR in Education"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0357-0"/>

    <meta name="DOI" content="10.1007/s10055-018-0357-0"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0357-0"/>

    <meta name="description" content="Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the"/>

    <meta name="dc.creator" content="Nicholas Vretos"/>

    <meta name="dc.creator" content="Petros Daras"/>

    <meta name="dc.creator" content="Stylianos Asteriadis"/>

    <meta name="dc.creator" content="Enrique Hortal"/>

    <meta name="dc.creator" content="Esam Ghaleb"/>

    <meta name="dc.creator" content="Evaggelos Spyrou"/>

    <meta name="dc.creator" content="Helen C. Leligou"/>

    <meta name="dc.creator" content="Panagiotis Karkazis"/>

    <meta name="dc.creator" content="Panagiotis Trakadas"/>

    <meta name="dc.creator" content="Kostantinos Assimakopoulos"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell Rev; citation_title=Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011; citation_author=CN Anagnostopoulos, T Iliou, I Giannoukos; citation_volume=43; citation_issue=2; citation_publication_date=2015; citation_pages=155-177; citation_doi=10.1007/s10462-012-9368-5; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Int. J. Hum.-Comput. Stud.; citation_title=Better to be frustrated than bored: the incidence, persistence, and impact of learners&#8217; cognitive-affective states during interactions with three different computer-based learning environments; citation_author=RSJ Baker, SK D&#8217;Mello, MT Rodrigo, AC Graesser; citation_volume=68; citation_publication_date=2010; citation_pages=223-241; citation_doi=10.1016/j.ijhcs.2009.12.003; citation_id=CR2"/>

    <meta name="citation_reference" content="Baltru&#353;aitis T, Robinson P, Morency LP (2016) Openface: an open source facial behavior analysis toolkit. 2016 IEEE Winter conference on IEEE applications of computer vision (WACV)"/>

    <meta name="citation_reference" content="Burkhardt F et al (2005) A database of german emotional speech. In: Proceedings of interspeech, Lissabon"/>

    <meta name="citation_reference" content="citation_journal_title=J Real-Time Image Proc; citation_title=Real-time human action recognition based on depth motion maps; citation_author=C Chen, K Liu, N Kehtarnavaz; citation_volume=12; citation_issue=1; citation_publication_date=2016; citation_pages=155-163; citation_doi=10.1007/s11554-013-0370-1; citation_id=CR5"/>

    <meta name="citation_reference" content="Costantini G et al (2014) Emovo corpus: an Italian emotional speech database. In: Chair NCC, Choukri K, Declerck T, Loftsson H, Maegaard B, Mariani J, Moreno A, Odijk J, Piperidis S (eds) Proceedings of the ninth international conference on language resources and evaluation (LREC&#8217;14). European Language Resources Association (ELRA), Reykjavik, Iceland"/>

    <meta name="citation_reference" content="Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gestures. In: Proceedings of the 2012 ACM conference on ubiquitous computing"/>

    <meta name="citation_reference" content="citation_title=Flow and the psychology of discovery and invention; citation_publication_date=1996; citation_id=CR8; citation_author=M Csikszentmihalyi; citation_publisher=Harper Collins"/>

    <meta name="citation_reference" content="citation_title=Flow: the psychology of optimal experience; citation_publication_date=2008; citation_id=CR9; citation_author=M Cs&#237;kszentmih&#225;lyi; citation_publisher=Harper Perennial"/>

    <meta name="citation_reference" content="DigiCapital (2017) Augmented/Virtual Reality Report Q4. 
                    https://www.digi-capital.com/news/2017/01/after-mixed-year-mobile-ar-to-drive-108-billion-vrar-market-by-2021/#.WdyhmTBx3IV
                    
                  . Accessed 10 Oct 2017"/>

    <meta name="citation_reference" content="Du Y, Wang W, Wang L (2015) Hierarchical recurrent neural network for skeleton based action recognition. In: Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR)"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn; citation_title=Survey on speech emotion recognition: features, classification schemes, and databases; citation_author=M Ayadi, MS Kamel, F Karray; citation_volume=44; citation_issue=3; citation_publication_date=2011; citation_pages=572-587; citation_doi=10.1016/j.patcog.2010.09.020; citation_id=CR12"/>

    <meta name="citation_reference" content="Ghaleb E, Popa M, Hortal E, Asteriadis S (2017). Multimodal fusion based on information gain for emotion recognition in the wild, intelligent systems conference 2017, 7&#8211;8 Sept 2017, London, UK"/>

    <meta name="citation_reference" content="Hansen DW, Ji Q (2010). In the eye of the beholder: a survey of models for eyes. IEEE Trans Pattern Anal Mach Intell"/>

    <meta name="citation_reference" content="Haq S, Jackson P (2009) Speaker-dependent audio-visual emotion recognition. In: Proceedings of the international conference on auditory-visual speech processing (AVSP&#8217;08), Norwich, UK"/>

    <meta name="citation_reference" content="Kanade T, Cohn JF, Lucey P, Saragih J, Ambadar Z, Matthews I (2010) The extended Cohn&#8211;Kanade Dataset (CK+): a complete expression dataset for action unit and emotion-specified expression, San Francisco, USA"/>

    <meta name="citation_reference" content="Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on consumer electronics (ICCE). IEEE"/>

    <meta name="citation_reference" content="Leifman G et al (2016) Learning gaze transitions from depth to improve video saliency estimation. arXiv preprint 
                    arXiv:1603.03669
                    
                  
                        "/>

    <meta name="citation_reference" content="Li W, Zhang Z, Liu Z (2010) Action recognition based on a bag of 3d points. In: Proceedings of IEEE international conference in computer vision and pattern recognition workshop (CVPRW)"/>

    <meta name="citation_reference" content="Mora KAF, Monay F, Odobez J-M (2014) Eyediap: a database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In: Proceedings of the symposium on eye tracking research and applications. ACM"/>

    <meta name="citation_reference" content="Nottingham Trent University (ed) (2017) Adaptation and Personalization principles based on MaTHiSiS findings. Deliverable for the MaTHiSiS project. 
                    http://www.mathisis-project.eu/en/content/d61-adaptation-and-personalization-principles-based-mathisis-findings
                    
                  . Accessed 3/4/2018"/>

    <meta name="citation_reference" content="citation_journal_title=Journal of Machine Learning Research; citation_title=Scikit-learn: machine learning in Python; citation_author=F Pedregosa; citation_volume=12; citation_publication_date=2011; citation_pages=2825-2830; citation_id=CR22"/>

    <meta name="citation_reference" content="Rikert TD, Jones MJ (1998) Gaze estimation using morphable models. In: Proceedings of third IEEE international conference on automatic face and gesture recognition. IEEE"/>

    <meta name="citation_reference" content="Vemulapalli R, Arrate F, Chellappa R (2014) Human action recognition by representing 3d skeletons as points in a lie group. In: Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR)"/>

    <meta name="citation_reference" content="Wang L, Zhang J, Zhou L, Tang C, Li W (2015) Beyond covariance: feature representation with nonlinear kernel matrices. In: Proceedings of the IEEE international conference on computer vision (ICCV)"/>

    <meta name="citation_reference" content="Xiong X, De la Torre F (2013) Supervised descent method and its applications to face alignment. In: 2013 IEEE conference on computer vision and pattern recognition, Portland, pp 532&#8211;539"/>

    <meta name="citation_reference" content="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511&#8211;4520"/>

    <meta name="citation_reference" content="Zhang X, Sugano Y, Fritz M, Bulling A (2017) MPIIGaze: real-world dataset and deep appearance-based gaze estimation. IEEE Trans Pattern Anal Mach Intell"/>

    <meta name="citation_author" content="Nicholas Vretos"/>

    <meta name="citation_author_institution" content="Centre of Research and Technology Hellas, Thermi, Thessaloniki, Greece"/>

    <meta name="citation_author" content="Petros Daras"/>

    <meta name="citation_author_institution" content="Centre of Research and Technology Hellas, Thermi, Thessaloniki, Greece"/>

    <meta name="citation_author" content="Stylianos Asteriadis"/>

    <meta name="citation_author_institution" content="University of Maastricht, Maastricht, The Netherlands"/>

    <meta name="citation_author" content="Enrique Hortal"/>

    <meta name="citation_author_institution" content="University of Maastricht, Maastricht, The Netherlands"/>

    <meta name="citation_author" content="Esam Ghaleb"/>

    <meta name="citation_author_institution" content="University of Maastricht, Maastricht, The Netherlands"/>

    <meta name="citation_author" content="Evaggelos Spyrou"/>

    <meta name="citation_author_institution" content="National Centre for Scientific Research &#8220;Demokritos&#8221;, Agia Paraskevi, Athens, Greece"/>

    <meta name="citation_author" content="Helen C. Leligou"/>

    <meta name="citation_author_institution" content="Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece"/>

    <meta name="citation_author" content="Panagiotis Karkazis"/>

    <meta name="citation_author_institution" content="Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece"/>

    <meta name="citation_author" content="Panagiotis Trakadas"/>

    <meta name="citation_author_email" content="pkarkazis@isc.tuc.gr"/>

    <meta name="citation_author_institution" content="Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece"/>

    <meta name="citation_author" content="Kostantinos Assimakopoulos"/>

    <meta name="citation_author_institution" content="Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0357-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0357-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Exploiting sensing devices availability in AR/VR deployments to foster engagement"/>
        <meta property="og:description" content="Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the assessment of the user’s performance are based on her/his (re)actions which are continuously traced/sensed. In this paper, we propose the exploitation of the sensors available in the AR/VR systems to enhance the current AR/VR experiences, taking into account the users’ affect state that changes in real time. Adapting the difficulty level of the experience to the users’ affect state fosters their engagement which is a crucial issue in educational environments and prevents boredom and anxiety. The users’ cues are processed enabling dynamic user profiling. The detection of the affect state based on different sensing inputs, since diverse sensing devices exist in different AR/VR systems, is investigated, and techniques that have been undergone validation using state-of-the-art sensors are presented."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Exploiting sensing devices availability in AR/VR deployments to foster engagement | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0357-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Affect state detection, Engagement, Interpretation of interaction, Multimodal affect state detection","kwrd":["Affect_state_detection","Engagement","Interpretation_of_interaction","Multimodal_affect_state_detection"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0357-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0357-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=357;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0357-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Exploiting sensing devices availability in AR/VR deployments to foster engagement
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0357-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0357-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I. : VR in Education</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-07-14" itemprop="datePublished">14 July 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Exploiting sensing devices availability in AR/VR deployments to foster engagement</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nicholas-Vretos" data-author-popup="auth-Nicholas-Vretos">Nicholas Vretos</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre of Research and Technology Hellas" /><meta itemprop="address" content="grid.423747.1, 0000 0001 2216 5285, Centre of Research and Technology Hellas, Thermi, Thessaloniki, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Petros-Daras" data-author-popup="auth-Petros-Daras">Petros Daras</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Centre of Research and Technology Hellas" /><meta itemprop="address" content="grid.423747.1, 0000 0001 2216 5285, Centre of Research and Technology Hellas, Thermi, Thessaloniki, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stylianos-Asteriadis" data-author-popup="auth-Stylianos-Asteriadis">Stylianos Asteriadis</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Maastricht" /><meta itemprop="address" content="grid.5012.6, 0000 0001 0481 6099, University of Maastricht, Maastricht, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Enrique-Hortal" data-author-popup="auth-Enrique-Hortal">Enrique Hortal</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Maastricht" /><meta itemprop="address" content="grid.5012.6, 0000 0001 0481 6099, University of Maastricht, Maastricht, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Esam-Ghaleb" data-author-popup="auth-Esam-Ghaleb">Esam Ghaleb</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Maastricht" /><meta itemprop="address" content="grid.5012.6, 0000 0001 0481 6099, University of Maastricht, Maastricht, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Evaggelos-Spyrou" data-author-popup="auth-Evaggelos-Spyrou">Evaggelos Spyrou</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National Centre for Scientific Research “Demokritos”" /><meta itemprop="address" content="grid.6083.d, 0000 0004 0635 6999, National Centre for Scientific Research “Demokritos”, Agia Paraskevi, Athens, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Helen_C_-Leligou" data-author-popup="auth-Helen_C_-Leligou">Helen C. Leligou</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technological Educational Institute of Sterea Ellada" /><meta itemprop="address" content="grid.466175.7, Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Panagiotis-Karkazis" data-author-popup="auth-Panagiotis-Karkazis">Panagiotis Karkazis</a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-4971-826X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-4971-826X</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technological Educational Institute of Sterea Ellada" /><meta itemprop="address" content="grid.466175.7, Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Panagiotis-Trakadas" data-author-popup="auth-Panagiotis-Trakadas" data-corresp-id="c1">Panagiotis Trakadas<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technological Educational Institute of Sterea Ellada" /><meta itemprop="address" content="grid.466175.7, Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kostantinos-Assimakopoulos" data-author-popup="auth-Kostantinos-Assimakopoulos">Kostantinos Assimakopoulos</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Technological Educational Institute of Sterea Ellada" /><meta itemprop="address" content="grid.466175.7, Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">399</span>–<span itemprop="pageEnd">410</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">646 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0357-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the assessment of the user’s performance are based on her/his (re)actions which are continuously traced/sensed. In this paper, we propose the exploitation of the sensors available in the AR/VR systems to enhance the current AR/VR experiences, taking into account the users’ affect state that changes in real time. Adapting the difficulty level of the experience to the users’ affect state fosters their engagement which is a crucial issue in educational environments and prevents boredom and anxiety. The users’ cues are processed enabling dynamic user profiling. The detection of the affect state based on different sensing inputs, since diverse sensing devices exist in different AR/VR systems, is investigated, and techniques that have been undergone validation using state-of-the-art sensors are presented.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Even though the learning goals, the available resources (time, equipment, teachers and teaching facilities) and the contexts may change, learning needs appear throughout our lifetime irrespective of age, gender, nationality, culture and learning intricacies. Education can be distinguished in formal, non-formal and informal, depending on the setting; it may target a homogeneous or heterogeneous learners’ group, and it may target large or small learners’ groups and diverse settings (e.g., home, school, enterprise premises). The common goal of any educational procedure is to make each and every learner acquire the desired skill or piece of knowledge in the <i>minimum time</i>, with the <i>maximum pleasure/convenience</i>. To achieve this goal, researchers agree that <i>engagement</i> is of key importance.</p><p>Although augmented reality (AR) or virtual reality (VR) technologies have been available for several years, it is only recently that they have developed and matured to a level that enables rapid penetration in the consumer space and educational environments. Enhancing and extending the learning experience is at the heart of what VR can offer students, and it is possibly one of the most powerful of all technologies that could help change how we learn forever. Virtual and augmented reality (VR/AR) aspires to contribute in fostering learners’ engagement almost in any educational context, realizing Albert Einstein’s words “the only source of knowledge is experience.” Moreover, it has penetrated the educational sector as a promising tool, helping teachers face one of the biggest timeless issues which is engagement; AR, with its ability to combine both digital and physical worlds, while VR, with its ability to completely immerse users in new environments, bring new dimensions to teaching and learning. Virtual and augmented reality in education can lead to increased knowledge retention exploiting the unique multisensory experience. This technology fits the needs of user groups of all ages, of almost all cultures as it shifts learning from a text-based process to an <i>experience</i>-based process and of diverse learning needs. The next question that may arise is the range of learning materials. Although not unlimited, available materials support different subject areas from improving creative writing to understanding science and math topics through enhanced visualization and immersion with current systems supporting a certain level of flexibility in learning material creation.</p><p>Although VR may enhance the learning experience, the pedagogists insist that achieving engagement directly depends on personal characteristics including not only competence levels but also, and more importantly, <i>affect state</i>. According to well-established theories (Csíkszentmihályi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Csíkszentmihályi M (2008) Flow: the psychology of optimal experience. Harper Perennial, New York" href="/article/10.1007/s10055-018-0357-0#ref-CR9" id="ref-link-section-d66749e556">2008</a>), to maximize results, the learner has to remain in the so-called flow state (i.e., feeling neither frustrated nor bored), which depends on the level of challenge/difficulty and on their skills. Flow has been defined by Csíkszentmihályi (Csíkszentmihályi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Csíkszentmihályi M (2008) Flow: the psychology of optimal experience. Harper Perennial, New York" href="/article/10.1007/s10055-018-0357-0#ref-CR9" id="ref-link-section-d66749e559">2008</a>) as “an optimal psychological state that people experience when engaged in an activity that is both appropriately challenging to one’s skill level, often resulting in immersion and concentrated focus on a task; this can result in deep learning and high levels of personal and work satisfaction.” When the challenge does not match the user’s skills, she/he feels either bored (i.e., when dealing with a too easy task for her/his skills) or anxious (i.e., when experiencing a too difficult task). <i>To keep the learners in the flow state, their affect state has to be constantly and in real time detected, i.e., real-time dynamic profiling has to be implemented. Once they come close to boredom, the challenge has to be intensified, while when they come close to anxiety, the challenge has to be slightly relaxed.</i></p><p>In this paper, we propose the exploitation of sensing devices which are available in AR/VR installations to detect the affect state of the user and to tailor the experience not only to her/his personal preferences (which is the case today) but most importantly to her/his affect state which changes dynamically. To prove that this approach is realistic, we investigate how this can happen through a variety of sensor types and we describe practical implementation of techniques well-grounded to algorithms available in the literature. The implemented techniques have been evaluated using <i>publicly available datasets</i> as well as into <i>real</i>-<i>life deployments</i>. Such an integrated approach is expected to boost engagement with AR/VR application and thus, learning efficiency and user satisfaction.</p><p>The rest of the paper is organized as follows: In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec2">2</a>, we present the modifications required for the realization of the proposed approach in a typical AR/VR system architecture. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec3">3</a>, we thoroughly investigate how affect detection can be achieved through different sensors/modalities and include relevant results. Then, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec9">4</a>, we discuss the benefits, challenges and prospects of the proposed approach. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec10">5</a> concludes the paper.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">AR/VR system architecture</h2><div class="c-article-section__content" id="Sec2-content"><p>Adopting a rather simplistic approach, any VR/AR system can be considered to consist of three parts (as also illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig1">1</a>): the AR/VR learning material logic (e.g., a game), which is the heart of the system, a component (or components) that deliver/create the “environment” to the user, which is in essence the output of the system to the user, e.g., the speakers and the HoloLens display, and the device(s) that sense the user’s activity (inertia, microphone, etc.) which are fed to the heart of the system to trigger a change (the next step) of the experience. The AR/VR logic component can be organized in different ways and may impose diverse computing and storage requirements. Turning our attention to the devices interfacing the user, hardware components for augmented reality typically include sensors and input devices. Modern mobile augmented reality systems use one or more of the following tracking technologies: digital cameras and/or other optical sensors, microphones, accelerometers, GPS, gyroscopes, solid state compasses, RFID and wireless sensors. These technologies offer varying levels of accuracy and precision.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="191" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Legacy high-level AR/VR system architecture enhanced with affect detection components</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Effective, hyper-personalized learning relies on the adaptation of parameters in the learning experience, based upon each learner’s affective state. That is, the physical behavior of a learner, expressed by a number of cues in their facial, bodily and vocal expressions, their gaze locality and their physical manipulation of the devices with which they interact, can shed light to their uptake of the process. Once the sensors are already available in such a system and feed the operation of the learning material logic, we propose to use their readings for an additional purpose: to detect the affect state, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig1">1</a>. The component named “affect detection” receives the readings of the available sensors and estimates the affect state of the learner. Once the affect state is not boredom or anxiety, the logic of the learning material evolves as would evolve without the “affect detection” component implemented (i.e., today). If the affect state is shown to tend to boredom, this is signaled to the logic component and the challenge level is increased. In the case of anxiety detection, the challenge is relaxed, so as to keep the learner in the flow state.</p><p>An educational platform that embraces the principle of learning experience adaptation to the learners affect state has been developed under the H2020 MaTHiSiS project (<a href="http://mathisis-project.eu/">http://mathisis-project.eu/</a>). MaTHiSiS is an educational platform providing every type of learner, in every type of setting, on the device they have at their disposal, with a bespoke, individualized learning experience that is adapted to their personal requirements. With regard to sensory-based affect recognition, MaTHiSiS uses five Sensorial Component modalities: facial expression analysis, gaze detection, skeleton motion analysis, audio analysis and mobile-based inertia sensors analysis. The technical architecture of the platform is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig2">2</a> and consists mainly of: (a) the MaTHiSiS back-end which implements all the MaTHiSiS algorithms for personalizing the learning experience and for supporting learning in a diversity of environments, (b) the platform agents which are the devices the learner interacts with and can be a laptop, tablet, smartphone, interactive whiteboard, robots, HoloLens or other and (c) the MaTHiSiS front-end gateway which is the user interface through which the teachers, the IT personnel responsible in a school and the platform administrator interact with the platform. Teachers use it to create learning content (e.g., games or exercises) and to define at high level the learning experience suitable for their students (e.g., the subject and the target learning outcomes), IT personnel use it to configure the platform, i.e., define the platform agents (devices) each class will use so that the appropriate learning material reaches the appropriate people, and the administrator uses it to manage the users and their roles.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="513" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>MaTHiSiS platform architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In each device used by the learners, a different (per device type) “platform agent layer” (PA-layer) instance is implemented comprising of the “Experiencing Service Platform Agent” (ES PA, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig2">2</a>), the “Experiencing Service Sensorial Component” (ES SC in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig2">2</a>) and the “Sensorial Component” (SC in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig2">2</a>). The latter captures the readings from the sensors available in each device and either processes them to derive the user’s affect state and delivers it to the ES SC or delivers to the ES SC the raw captured data (which happens in the case of the absence of sufficient processing resources as is the case of the mobile devices). The ES SC forwards the received information to the “Experience Service server” using web socket technology, along with other information that it receives from the ES PA, e.g., regarding the score achieved by the learner. At the back-end, all the information gathered through the platform agents is received by the Affect Information Repository Library (AIR Lib in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig2">2</a>) component which is responsible for delivering the learner’s affect state to the “experience engine.” In case, raw data from sensor are received by AIR Lib, a dedicated sub-component uses them to derive the affect state. This information is then used by the experience engine to define the next learning material and level the user will interact with to ensure the learner is kept at the engagement state. This is then passed to the Platform Agent (through the ES server). The <i>challenge</i> then moves to the identification of <i>affect detection techniques</i> that lead to results of adequate accuracy with commercially available state-of-the-art sensors.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Affect state detection based on diverse sensing devices</h2><div class="c-article-section__content" id="Sec3-content"><p>The sensorial component on the Platform Agents and the AIR Lib component in the back-end are the basis of the recognition of the learners’ affect states. Their goal is to gather (physical) behavioral cues of the learner and apply machine learning techniques in order to interpret them into comprehensive affective cues that tell the story of the learner’s uptake of the learning objective(s). Its role is to provide for the learner’s affective state. In the MaTHiSiS system outlined above, a variety of algorithms for affect detection has been implemented and tested per modality, as it will be described in the forthcoming sections. All adopted algorithms utilize machine learning techniques. Thus, appropriate training of the algorithms needs to take place prior to the normal operation of the system. At the following, we shall refer to students without disabilities as “mainstream” students.</p><h3 class="c-article__sub-heading" id="Sec4">Facial expression analysis</h3><p>The facial expressions are often considered as the strongest indicative communication tool of human emotions. They may expose people’s feelings and mood state, from simple spontaneous emotions like happiness and disgust to time-dependent affective expressions states like anxiety, boredom and engagement during a current task and/or a situation. This allows the person’s interaction counterpart to understand their affective state and adjust its behavior according to the person’s underlying feelings. Facial images are one of the data cues that will be captured through the Sensorial Component by means of different types of cameras across devices.</p><p>For the extraction of facial expressions, a graph-based method (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on consumer electronics (ICCE). IEEE" href="/article/10.1007/s10055-018-0357-0#ref-CR17" id="ref-link-section-d66749e696">2013</a>) has been adopted. More specifically, the face is represented as a graph, which is formed by points extracted from specific areas. The variation of muscle movements on the face during the expression of different emotions leads to different positions of points on the image and may generate different graphs. The input of the algorithm is an image. Then, facial landmarks are detected using the Supervised Descent Method (Xiong and De la Torre <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Xiong X, De la Torre F (2013) Supervised descent method and its applications to face alignment. In: 2013 IEEE conference on computer vision and pattern recognition, Portland, pp 532–539" href="/article/10.1007/s10055-018-0357-0#ref-CR26" id="ref-link-section-d66749e699">2013</a>). For instance, such landmarks may be the nose, the eyes, the brows, the mouth, etc. These points are tracked, so that the movement of the facial muscles is followed over time. Assuming that all landmarks are connected, they may be considered as a graph. We then make the hypothesis that the density of the graph differs in each facial expression. More specifically, we use spectral graph analysis, through which a feature vector is extracted. This vector depicts areas of density in the graph by using the graph’s Laplacian matrix and solving the eigen decomposition problem for the eigenvectors corresponding to the first and second greatest eigenvalues which capture information regarding different density areas of the initial graph. Such areas in the specific problem are those of eyes, mouth and nose.</p><p>More specifically, the Laplacian matrix <i>L</i> of a graph <i>G</i> is defined as <i>L </i>=<i> D − A</i>, with <i>D</i> denoting the degree matrix and <i>A</i> the adjacency matrix of <i>G</i>. <i>A</i>(<i>i, j</i>) is computed as: <span class="mathjax-tex">\(A\left( {i,j} \right) = 1 - e^{{\frac{{\left( { - \left| {x_{i} - x_{j} } \right|} \right)}}{d}}}\)</span>, where |•| denotes the Euclidean distance, <i>x</i><sub><i>i</i></sub><i>, x</i><sub><i>j</i></sub> any two given landmark points and <i>d</i> a constant depicting the variance of the overall distance between the facial landmarks. In order to normalize between different image scales and sizes (i.e., for recognition “in the wild”), the symmetric Laplacian matrix is adopted as it is considered to be a more robust option: <i>L</i><sup>sym</sup>= <i>D</i><sup>−1/2</sup><i>LD</i><sup>−1/2</sup>. Then, its eigen decomposition follows: <i>L</i><sup>sym</sup><i>v</i><sub><i>i</i></sub>= <i>λ</i><sub><i>i</i></sub><i>v</i><sub><i>i</i></sub>.</p><p>For the classification, support vector machines (SVM) are used. The initial evaluation of the algorithm is performed using images from the well-known public available Cohn–Kanade (CK) database (Kanade et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kanade T, Cohn JF, Lucey P, Saragih J, Ambadar Z, Matthews I (2010) The extended Cohn–Kanade Dataset (CK+): a complete expression dataset for action unit and emotion-specified expression, San Francisco, USA" href="/article/10.1007/s10055-018-0357-0#ref-CR16" id="ref-link-section-d66749e865">2010</a>), leading to very satisfying results. Although this dataset involves expressions of the six basic Ekmanian emotions, namely Anger, Disgust, Fear, Happiness, Sadness and Surprise, a correlation of the aforementioned emotions with affective states was retrieved in Russell’s Core Affect Framework (Baker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Baker RSJ, D’Mello SK, Rodrigo MT, Graesser AC (2010) Better to be frustrated than bored: the incidence, persistence, and impact of learners’ cognitive-affective states during interactions with three different computer-based learning environments. Int. J. Hum.-Comput. Stud. 68:223–241" href="/article/10.1007/s10055-018-0357-0#ref-CR2" id="ref-link-section-d66749e868">2010</a>). A direct mapping of the spontaneous emotions to affect states conveys this correlation. Using this mapping, Sadness corresponds to Boredom, Happiness to Engagement and Surprise, Anger, Fear to Frustration. The performance of this algorithm using Cohn–Kanade dataset to predict affective stated reached a classification score that rounds up close to 100% accuracy. Results per emotion are depicted in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0357-0#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Experimental results of facial analysis</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec5">Gaze estimation</h3><p>Gaze estimation refers to an emerging computer vision research topic that is defined as the process of determining the eye’s point of regard, usually with respect to a specific plane such as a computer screen but also in the more general case of the eye’s orientation or a person’s “look at” direction. Recent gaze estimation methods aspire to estimate a person’s gaze accurately, invariant of the head pose, the lighting conditions and the eye’s appearance, using low-cost commodity hardware and simple setups. Gaze-based methods can be classified into three general categories: (a) shape (Feature)-based ones, (b) Appearance-based and (c) hybrid ones, combining elements from the two previously mentioned ones. In another recent work, deep networks are employed to estimate a user’s gaze direction (see Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511–4520" href="/article/10.1007/s10055-018-0357-0#ref-CR27" id="ref-link-section-d66749e999">2015</a>). In this work, head poses and eye images are the input to a convolutional neural network (CNN). The output is the gaze direction in the camera coordinate system. Data collection was performed by the RGB cameras (webcams) of laptops, while the evaluation was based on the public MPIIGaze dataset (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Zhang X, Sugano Y, Fritz M, Bulling A (2017) MPIIGaze: real-world dataset and deep appearance-based gaze estimation. IEEE Trans Pattern Anal Mach Intell" href="/article/10.1007/s10055-018-0357-0#ref-CR28" id="ref-link-section-d66749e1002">2017</a>) containing images by 15 users. Data were recorded by users applying the data collection software on their laptops in situ without any special guidelines given about the head pose, the time of execution or the illumination conditions. The MPIIGaze dataset consists of 213,659 images with large variation in both illumination and head pose.</p><p>We have experimented with a regression CNN-based gaze estimation algorithm; the concept of this algorithm is similar to the MPIIGaze method (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511–4520" href="/article/10.1007/s10055-018-0357-0#ref-CR27" id="ref-link-section-d66749e1008">2015</a>). Initially, face frontalization was a relatively costly operation processing-wise compared to the other steps of the pipeline (face detection, landmark detection and head pose estimation). Therefore, a perspective warping step was opted for, instead of the face frontalization; this was faster processing-wise and also achieved higher accuracy after training. The processing pipeline is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig3">3</a>. For face detection, Li et al.’s SURF cascade method (Leifman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Leifman G et al (2016) Learning gaze transitions from depth to improve video saliency estimation. arXiv preprint &#xA;                    arXiv:1603.03669&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-018-0357-0#ref-CR18" id="ref-link-section-d66749e1014">2016</a>) is employed, while for the landmark detection, a cascade of boosted regression forests (Rikert and Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Rikert TD, Jones MJ (1998) Gaze estimation using morphable models. In: Proceedings of third IEEE international conference on automatic face and gesture recognition. IEEE" href="/article/10.1007/s10055-018-0357-0#ref-CR23" id="ref-link-section-d66749e1017">1998</a>) regresses the positions of the facial landmarks in around 12 ms. For head pose estimation, by utilizing the 2D detected landmarks, correspondences with pre-annotated 3D positions may be now established. These are annotated on a generic 3D mean facial shape head model and are used to estimate the user’s head pose by fitting the 3D model data to the 2D image correspondences via nonlinear optimization. The result is the head’s pose (rotation and translation) with respect to the coordinate system defined by the camera. For data normalization, the generic 3D mean facial shape is rotated and translated according to the head pose extracted previously. Then, the 3D eye position of the left and right eye is estimated, and a vector from each eye looking at the target is predicted. Moreover, similar to Zhang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511–4520" href="/article/10.1007/s10055-018-0357-0#ref-CR27" id="ref-link-section-d66749e1020">2015</a>), the normalization is done by scaling and rotating the camera-captured image so that the eye image is centered at the midpoint of the eye corners from a fixed distance <i>d</i> and so that the horizontal axes of the head coordinate system and the camera’s coordinate system are aligned. The head pose is then parameterized as a 2D angle, and the eye images are contrast-enhanced. Regarding gaze estimation, the vector of each eye along with the respective image are fed into a pre-trained deep CNN which then regresses a feature vector representing the user’s gaze direction. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="367" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Overall gaze estimation pipeline</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>After training the network and achieving similar accuracy to that of Nottingham Trent University (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Nottingham Trent University (ed) (2017) Adaptation and Personalization principles based on MaTHiSiS findings. Deliverable for the MaTHiSiS project. &#xA;                    http://www.mathisis-project.eu/en/content/d61-adaptation-and-personalization-principles-based-mathisis-findings&#xA;                    &#xA;                  . Accessed 3/4/2018" href="/article/10.1007/s10055-018-0357-0#ref-CR21" id="ref-link-section-d66749e1047">2017</a>) by using a subset (15%, randomly chosen) of the dataset as a test set, it was also tested on another publicly available dataset, EYEDIAP (Mora et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Mora KAF, Monay F, Odobez J-M (2014) Eyediap: a database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In: Proceedings of the symposium on eye tracking research and applications. ACM" href="/article/10.1007/s10055-018-0357-0#ref-CR20" id="ref-link-section-d66749e1050">2014</a>). The EYEDIAP dataset contains low-resolution (VGA) video capturing 94 video sequences of 16 participants looking at three different targets (discrete and continuous markers displayed on a screen, and floating physical targets) under both static and free head motion, while it includes two different illumination conditions on some participant recordings. Moreover, along with the database, the authors include a framework of calculating performance measures, with respect to each visual target (discrete, continuous or floating).</p><p>The gaze estimation network was tested on multiple sequences from EYEDIAP and achieved a mean angular error on the VGA sequences of 10.46° with high head mobility, and 9.55° with no head mobility. On the contrary, on the challenging non-frontal viewpoint and the HD camera the mean angular error achieved was 18.22° with high mobility and 17.43° with none. In comparison, the method in Zhang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511–4520" href="/article/10.1007/s10055-018-0357-0#ref-CR27" id="ref-link-section-d66749e1056">2015</a>) that outperforms other state-of-the-art methods achieves an accuracy of 13.9° on the MPII dataset and 10.5° on the EYEDIAP dataset. The results for mobile head pose are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0357-0#Fig4">4</a>, where in the left-hand side figure comparison with the results presented in Nottingham Trent University (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Nottingham Trent University (ed) (2017) Adaptation and Personalization principles based on MaTHiSiS findings. Deliverable for the MaTHiSiS project. &#xA;                    http://www.mathisis-project.eu/en/content/d61-adaptation-and-personalization-principles-based-mathisis-findings&#xA;                    &#xA;                  . Accessed 3/4/2018" href="/article/10.1007/s10055-018-0357-0#ref-CR21" id="ref-link-section-d66749e1062">2017</a>) for the MPIIGaze dataset is included in green.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0357-0/MediaObjects/10055_2018_357_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="170" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Results for mobile head pose</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0357-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Apart from the CNN regression-based approach, a Conditional Local Neural Fields (CLNF) approach OpenFace (Baltrušaitis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Baltrušaitis T, Robinson P, Morency LP (2016) Openface: an open source facial behavior analysis toolkit. 2016 IEEE Winter conference on IEEE applications of computer vision (WACV)" href="/article/10.1007/s10055-018-0357-0#ref-CR3" id="ref-link-section-d66749e1086">2016</a>) was also investigated. After testing with the same datasets, the OpenFace achieved a mean angular error on VGA sequences of 11.17° with mobile head pose activity, therefore underperforming on the particular conditions over the previously described methodology for the particular setting (VGA images). On the HD sequences, the mean angular error achieved was 15° with mobile head pose activity, therefore outperforming the previous method in this setting (HD images). The lower performance in VGA images can be attributed to their low resolution and thus, the lower quality of the eye images that prevent the accurate localization of the eye landmarks. Therefore, it can be deducted that the initially implemented method is more appropriate for VGA images (e.g., a standard non-HD web camera or a NAO robot’s camera) while the OpenFace method would be better served in HD resolutions (e.g., Kinect).</p><h3 class="c-article__sub-heading" id="Sec6">Inertia sensors</h3><p>Recently there have been many studies that support the potential usage of recognizing users’ emotional states through various inertia sensors such as accelerometer and gyroscope (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on consumer electronics (ICCE). IEEE" href="/article/10.1007/s10055-018-0357-0#ref-CR17" id="ref-link-section-d66749e1097">2013</a>). Inspired by the research performed by Coutrix (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gestures. In: Proceedings of the 2012 ACM conference on ubiquitous computing" href="/article/10.1007/s10055-018-0357-0#ref-CR7" id="ref-link-section-d66749e1100">2012</a>), an affect recognition system which exploits the expression through 3D gesture using aforementioned sensors can be implemented. As demonstrated in Coutrix (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gestures. In: Proceedings of the 2012 ACM conference on ubiquitous computing" href="/article/10.1007/s10055-018-0357-0#ref-CR7" id="ref-link-section-d66749e1103">2012</a>), 3D descriptors contribute to emotion expression while interacting and using mobile devices. In this work, a high number of significant correlations were detected in 3D motion descriptors of gestures and the arousal dimension. This study can be expanded, in order to infer affective states which are commonly experienced during the learning process, focusing our effort in the recognition of engagement, boredom and frustration. This 3D and continuous space can be accurately mapped to affective states from the Theory of Flow. The features extracted are analyzed in order to detect common patterns which can allow the system to infer the affect state of the user. Ideally, these features will help in the identification of erratic movements or unexpected behaviors such as the lack of motion or interactions with the devices. This information could denote frustration or boredom, respectively.</p><p>Due to the absence of public datasets, we decided to gather data to investigate the relationship between users’ movements while using smartphones and their affective states. The data were recorded on an android phone using a K6DS3TR gyroscope and accelerometer by STMicroelectronics. For this purpose, a mobile game application was developed. During the game play, sensorial data of accelerometer and gyroscope were recorded. The two sensors use the right-handed coordinate system, where <i>x</i>-axis increases as user moves toward the screen, y-axis increases as the user moves to the top of the screen and the <i>z</i>-axis is perpendicular to the screen. These <i>x</i>, <i>y</i> and <i>z</i> coordinates are recorded with sampling rate of 50 Hz. Data were gathered in sessions for multiple subjects in an adjusted environment such as a classroom or at home. Diversity of the subjects was considered such that they have different profiles in terms of age, and gender. In total, there were 24 subjects, 18 were males and 6 were females. Their ages ranged between 18 and 53 with a mean of 25.6 and 7.6 standard deviation.</p><p>Subjects were asked to use the developed mobile application in their usual way when they are bored, engaged or frustrated to gather sensorial data aligned with the Theory of Flow. Prior to the game play and data collection, they were given instructions to sit down on a chair, to avoid putting their elbows or arms on flat surface, and to use the device either with one or two hands. Each subject played the game eight times, and the affective state was recorded/sampled every 20 s, for each affective state. The resulted total number of sessions (samples) per subject was 24. Consequently, the final number of samples was 576. It is important to note that each sample was labeled by the intended affective state which the subject simulated while using the mobile application.</p><p>Following the step of data collection, feature extraction and analysis are applied on the raw gesture logs from the 3D sensorial outputs of accelerometers, and gyroscopes. Features were computed on the 3D vector in both time and frequency domains. In the time domain, features were computed for the acceleration and jerk, in the <i>x</i>, <i>y</i> and <i>z</i> directions separately, and on the 3D vector length of the three directions. Acceleration and Jerk are the third and fourth derivative of displacement with respect to time. For each orientation and 3D vector length, the maximum, minimum, mean, median, variance and amplitude of these descriptors have been calculated, for both absolute and signed values. These features were computed on both the raw acceleration and high-pass filtered acceleration values. In the frequency domain, a 1D Fourier transform was applied to perform the spectrum analysis. Then, the following set of features was computed: the gap (<i>G</i>) that maximizes the difference between the most and least important frequencies in the spectrum of acceleration signal, the number (<i>N</i>) of important frequencies, as well as the most important (<i>M</i>) frequency. This set of features was obtained for the <i>x</i>, <i>y</i> and <i>z</i> projections of raw acceleration signal and for the high-pass and low-pass filtered signals of acceleration.</p><p>Data collection, modeling and feature extraction methods were tested to study how they are correlated to users’ movements and annotations. Prior to the supervised classification, each sample’s features were standardized to have zero mean and unit variance. For the classification task, support vector machines (SVM) implementation provided by scikit-learn (Pedregosa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Pedregosa F et al (2011) Scikit-learn: machine learning in Python. Journal of Machine Learning Research 12:2825–2830" href="/article/10.1007/s10055-018-0357-0#ref-CR22" id="ref-link-section-d66749e1162">2011</a>) was used. The evaluation is validated using threefold cross-validation. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0357-0#Tab2">2</a> provides the confusion matrix of the three affective states. In this matrix, each cell <i>i</i>, <i>j</i> reflects the percentage of cases where sentiment <i>i</i> was detected by the algorithm while sentiment <i>j</i> was experienced by the learner as marked by the teacher. For example, when boredom is detected by the algorithm, 64% of cases the sentiment was actually “boredom” while 31.8% was engagement (i.e., the algorithm confused the actual sentiment with boredom) and 4.2% of times the algorithm confused frustration with boredom. The results show high classification rates of the three states which indicate the correlation between the 3D features while using of a mobile phone and users’ affective states. In addition, the Cohen’s kappa value and the average precision are 50.4 and 68%, respectively. It also supports the usability of our approach to study this correspondence. Furthermore, the high detection of frustration is due to the fact that subjects were more expressive when it comes to events such as losing game or facing higher level of challenge. When it comes to boredom or engagement, expressivity through device’s movement was reduced substantially. Most subjects paid less attention to the mobile device when they were bored, and when they were engaged, they tended to hold upright the device with both hands and more concentration.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Confusion matrix in terms of classification rate for the three affective states for the inertia sensors</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec7">Skeleton motion analysis</h3><p>Human action recognition became a necessity for applications in surveillance, human–robot interaction, robot perception, etc. A variety of methods has been introduced, each one applying a different feature extraction and classification methodology. In the latter decades, handmade features have dominated in the field of action recognition. Those features were carefully selected to enable the interpretation of the performing action. However, more recently, based on the deep learning approaches, automatically generated features were introduced (Du et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Du Y, Wang W, Wang L (2015) Hierarchical recurrent neural network for skeleton based action recognition. In: Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR)" href="/article/10.1007/s10055-018-0357-0#ref-CR11" id="ref-link-section-d66749e1309">2015</a>). Through a deep learning framework and a large corpus of data samples, deep learning methods have provided us with very promising results, overcoming the handmade methodologies. After a thorough review of the available literature, we proposed a novel method called Speed Relation Preserving Slow Feature Analysis (srpSFA) in Kim et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on consumer electronics (ICCE). IEEE" href="/article/10.1007/s10055-018-0357-0#ref-CR17" id="ref-link-section-d66749e1312">2013</a>).</p><p>The <i>Speed Relation Preserving Slow Feature Analysis</i> (srpSFA) approach works as follows: We define the loss function we want to minimize, and we continue with the appropriate constraints that need to be imposed. Similar to the standard SFA, in our approach the optimal parameters matrix <span class="mathjax-tex">\(\varvec{W} \in R^{I \times J}\)</span> needs to be computed, through which the new representations are <span class="mathjax-tex">\(\varvec{y}_{\varvec{n}}^{{\left( \varvec{t} \right)}} = \varvec{W}^{\varvec{T}} \phi_{n}^{\left( t \right)}\)</span>. In order to fulfill the preservation of speed in the feature space, we want to minimize the objective function (loss function):</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\mathop \sum \limits_{ij} {\mathbb{E}}_{t} [\left( {\dot{\varvec{y}}_{i}^{\left( t \right)} - \dot{\varvec{y}}_{j}^{\left( t \right)} } \right)^{2} \dot{\varGamma }_{ij }^{\left( t \right)}$$</span></div></div><p>under standard SFA-oriented constrains, i.e., zero mean, unit variance and uncorrelated features for the new mapped node representations. The weight factor <span class="mathjax-tex">\(\dot{\varGamma }_{ij}^{\left( t \right)}\)</span> penalizes the distance between the new representations <span class="mathjax-tex">\(\varvec{y}_{i}^{\left( t \right)}\)</span> and <span class="mathjax-tex">\(\varvec{y}_{j}^{\left( t \right)}\)</span>. The greater the weight factor, the greater the penalty the new mappings to be “close.” A great value of <span class="mathjax-tex">\(\dot{\varGamma }_{ij}^{\left( t \right)}\)</span> denotes that the speed vectors <span class="mathjax-tex">\(\dot{\phi }_{i}^{\left( t \right)}\)</span> and <span class="mathjax-tex">\(\dot{\phi }_{j}^{\left( t \right)}\)</span> of the skeleton nodes <span class="mathjax-tex">\(\phi_{i}\)</span> and <span class="mathjax-tex">\(\phi_{j}\)</span> in the input space are “close.” Thus, minimizing the loss function above, it is ensured that for two speed vectors <span class="mathjax-tex">\(\dot{\phi }_{i}^{\left( t \right)}\)</span> and <span class="mathjax-tex">\(\dot{\phi }_{j}^{\left( t \right)}\)</span> that are “close” in the input space, their corresponding mappings <span class="mathjax-tex">\(\varvec{y}_{\varvec{i}}^{{\left( \varvec{t} \right)}}\)</span> and <span class="mathjax-tex">\(\varvec{y}_{\varvec{j}}^{{\left( \varvec{t} \right)}}\)</span> will also be “close” in the feature space. Provided the new skeleton node representation of the <span class="mathjax-tex">\(n\)</span>-th node <span class="mathjax-tex">\(\varvec{y}_{n}^{\left( t \right)} = \varvec{W}^{T} \phi_{n}^{\left( t \right)}\)</span>, the matrix notation of the objective function along with the constrains is given by:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\mathop {\hbox{min} }\limits_{\varvec{W}} {\text{trace}}\left( {\varvec{W}^{T} {\dot{\mathbf{\varPhi }}}{\text{diag}}\left( {\dot{\varvec{L}}} \right){\dot{\mathbf{\varPhi }}}^{\varvec{T}} {\mathbf{W}} } \right)$$</span></div></div><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\varvec{W}{\mathbf{\varPhi \varPhi }}^{\varvec{T}} \varvec{W}^{\varvec{T}} = \varvec{I}$$</span></div></div><p>To evaluate the algorithm, we used the MSR-Action3D dataset (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li W, Zhang Z, Liu Z (2010) Action recognition based on a bag of 3d points. In: Proceedings of IEEE international conference in computer vision and pattern recognition workshop (CVPRW)" href="/article/10.1007/s10055-018-0357-0#ref-CR19" id="ref-link-section-d66749e2256">2010</a>): A Kinect-like depth sensor was used to obtain the recorded skeletons. It consists of 20 different recorded actions performed by 10 different subjects/actors. In addition, each subject repeated each recorded action two or three times. Namely, the actions are <i>high arm wave</i>, <i>horizontal arm wave</i>, <i>hammer</i>, <i>hand catch</i>, <i>forward punch</i>, <i>high throw</i>, <i>draw x</i>, draw <i>tick</i>, <i>draw circle</i>, <i>hand clap</i>, <i>two hand wave</i>, <i>side boxing</i>, <i>bend</i>, <i>forward kick, side kick</i>, <i>jogging</i>, <i>tennis swing</i>, <i>tennis serve</i>, <i>golf swing</i>, <i>pick up \ throw</i>. For each skeleton, the 3D joint locations through time were provided. In addition, the connections of the nodes that define the recorded skeleton were also given. Each recording was done in 15 fps. Finally, because of the similarity of the actions, this dataset is considered difficult. In the validation, the method described in Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li W, Zhang Z, Liu Z (2010) Action recognition based on a bag of 3d points. In: Proceedings of IEEE international conference in computer vision and pattern recognition workshop (CVPRW)" href="/article/10.1007/s10055-018-0357-0#ref-CR19" id="ref-link-section-d66749e2319">2010</a>) was adopted. The whole dataset was split into three subsets namely, the AS1, AS2 and AS3. The data samples that correspond to the subjects with odd identification were used for training, i.e., 1, 3, 5, 7, 9, while the ones that correspond to the subjects with even identification, i.e., 2, 4, 6, 8, 10, were used for testing.</p><p>The results presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0357-0#Tab3">3</a> show that the srpSFA method provides the best performance for difficult (according to the literature) datasets, such as the MSR-Action 3D publicly available dataset, compared to the state-of-the-art reported accuracies. These emotive actions are mapped to the three affective states (engagement, boredom, frustration), and therefore, the accuracy of detected actions in turn yield accurate results in affect recognition. The accuracy exceeds 90% for all tested datasets.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Experimental results in MSR-Action 3D</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">Speech</h3><p>It is well-known that human communication in everyday life is mainly carried out by vocalized speech. Speech is used to transfer meaning between individuals. However, apart from meaning, speech also carries emotions that may be related to the speaker’s affect state. Even though such emotions may be more easily recognized through visual channels, as discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec4">3.1</a>, in many practical applications speech may be the single available modality for the recognition of emotions. An example closely related to the goals of the presented system is human–computer interaction through voice–user interfaces. Among various speech-related applications such as automatic speech recognition (ASR), and speaker identification, emotion recognition from speech appears to be the most challenging one. Of course, the task of emotion recognition from speech is a significantly difficult one. Even human experts (e.g., psychologists) often fail to recognize emotions without visual information from the subject. As expected, the task is more difficult for non-experts (e.g., in the context of our work, tutors and/or pedagogists).</p><p>Information carried by a typical speech signal may be divided into two distinct types (Anagnostopoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Anagnostopoulos CN, Iliou T, Giannoukos I (2015) Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011. Artif Intell Rev 43(2):155–177" href="/article/10.1007/s10055-018-0357-0#ref-CR1" id="ref-link-section-d66749e2591">2015</a>): (a) the explicit (or linguistic) information, comprising of articulated patterns produced by the speaker; and (b) the implicit (or paralinguistic information, concerning the variation in pronunciation of the aforementioned linguistic patterns). Linguistic information of speech may be qualitatively described, while paralinguistic may be quantitatively measured. To this goal, several spectral features and also features such as the peak and the intensity may be used. A speech segment may be then classified to one or several emotions by using one or a fusion of both types of information. Many emotion recognition approaches from speech are assisted by ASR. The main disadvantage of these is that they are not able to provide language-independent models. Another crucial disadvantage is that there exists a plethora of different sentences, speakers, speaking styles and rates (El Ayadi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="El Ayadi M, Kamel MS, Karray F (2011) Survey on speech emotion recognition: features, classification schemes, and databases. Pattern Recogn 44(3):572–587" href="/article/10.1007/s10055-018-0357-0#ref-CR12" id="ref-link-section-d66749e2594">2011</a>). Thus, the majority of approaches that aim to be language independent tend to rely on paralinguistic speech information. Nevertheless, even in this case, such information may be significantly diverse, depending on cultural particularities. Additionally, a speaker’s potential chronic emotional state may suppress the expressiveness of several emotions. Still, relying solely on paralinguistic information is probably the most appealing approach, when dealing with speakers’ emotion recognition.</p><p>Within our system, initially each audio signal is transformed to a sequence of feature vectors. Features are extracted on a short-term basis and from 20-ms windows, and afterward, the final feature vectors are formed by concatenating the mean and variance values of the features over a mid-term window of 1 s. The short-term process can be conducted either using overlapping (frame step is shorter than the frame length) or non-overlapping (frame step is equal to the frame length) framing. The extracted features are summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0357-0#Tab4">4</a>. The aforementioned concatenation of the mean and the variance values results to a feature vector of dimension equal to 68.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Extracted audio features</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For classification of feature vectors to emotions, the well-known and widely used support vector machines (SVMs) are used. SVMs are well-known supervised learning models, and during recent years, they have been extensively used in both classification and regression problems.</p><p>All features have been implemented in Python, while for classification, the scikit-learn library (Coutrix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gestures. In: Proceedings of the 2012 ACM conference on ubiquitous computing" href="/article/10.1007/s10055-018-0357-0#ref-CR7" id="ref-link-section-d66749e2854">2012</a>) has been used.</p><p>For training and evaluation of this technique, we have used three public and well-known datasets: (a) EMOVO (Costantini <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Costantini G et al (2014) Emovo corpus: an Italian emotional speech database. In: Chair NCC, Choukri K, Declerck T, Loftsson H, Maegaard B, Mariani J, Moreno A, Odijk J, Piperidis S (eds) Proceedings of the ninth international conference on language resources and evaluation (LREC’14). European Language Resources Association (ELRA), Reykjavik, Iceland" href="/article/10.1007/s10055-018-0357-0#ref-CR6" id="ref-link-section-d66749e2860">2014</a>), containing speech in Italian and for disgust, fear, anger, joy, surprise and sadness; (b) SAVEE (Haq and Jackson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Haq S, Jackson P (2009) Speaker-dependent audio-visual emotion recognition. In: Proceedings of the international conference on auditory-visual speech processing (AVSP’08), Norwich, UK" href="/article/10.1007/s10055-018-0357-0#ref-CR15" id="ref-link-section-d66749e2863">2009</a>), containing speech in English for the same emotions; and (c) EMO-DB (Burkhardt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Burkhardt F et al (2005) A database of german emotional speech. In: Proceedings of interspeech, Lissabon" href="/article/10.1007/s10055-018-0357-0#ref-CR4" id="ref-link-section-d66749e2866">2005</a>) containing speech in German for anger, boredom, disgust, fear, happiness, sadness and neutral. Note that all emotions have been performed by actors in all three datasets. Since in MaTHiSiS it is planned that emotions/states recognized are going to be based on the Theory of Flow (Csikszentmihalyi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Csikszentmihalyi M (1996) Flow and the psychology of discovery and invention. Harper Collins, New York" href="/article/10.1007/s10055-018-0357-0#ref-CR8" id="ref-link-section-d66749e2869">1996</a>), i.e., boredom, engagement and anxiety, five of the common emotion classes, namely Happiness, Sadness, Anger, Fear and Neutral, were selected from the aforementioned datasets. These were the emotions deemed to be closest to the ones that we would need to extract within MaTHiSiS. A major difficulty resulting from the choice of datasets is the differences between languages, since besides the linguistic differences, there is also big variability in the way each emotion is expressed. For each classification method, six different experiments were carried out where a single dataset was used for training and the remaining two for testing. We calculated the <i>F</i><sub>1</sub> score which is the harmonic average of the well-known precision (<i>P</i>—the number of correct positive results divided by the number of predicted positive results) and recall (<i>R</i>—the number of positive results divided by all samples that should be predicted as positive) measures. More specifically, it is calculated as <span class="mathjax-tex">\(F_{1} = 2 \cdot P \cdot R/1\left( {P + R} \right)\)</span>. Results are depicted in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0357-0#Tab5">5</a> and are indicative of the capabilities of the adopted approach, emphasizing the fact that in many cases training and testing data derive from different (i.e., cross-language) datasets.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Experimental results for speech-based affect recognition</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Benefits, challenges and prospects</h2><div class="c-article-section__content" id="Sec9-content"><p>In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec3">3</a>, we described practical implementations of affect detection schemes using commonly available sensors and provided real-life results. An important <i>technical challenge</i> faced during the integration of the presented techniques in MaTHiSiS system which uses diverse devices (mobile phones, tablets, robots, laptops) was the fact that the diverse devices (which have sensors installed on them) come with different computing capabilities. This implies that a robot (with a laptop attached to it, as for example, the turtle-bot or even the NAO robot) has completely different processing capabilities from a smartphone and these processing capabilities are mandatory since our aim is affect detection in real time. In the smartphone case, the device may not have adequate processing resources to execute sophisticated affect detection algorithms based on the inertia sensor readings. In such cases, the device carrying the sensors cannot reach a conclusion regarding the affect state but, instead, it sends the sensor readings to the cloud (through a web socket interface) where ample processing power exists. We consider that in any AR/VR solution, the affect detection algorithm can/will be implemented in the same processing resources where the AR/VR application main logic is executed. This relaxes the requirement for heavy processing in the sensing devices themselves.</p><p>Furthermore, we have explored the option to combine the results reached from readings from different modalities to improve affect detection accuracy. In Ghaleb et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ghaleb E, Popa M, Hortal E, Asteriadis S (2017). Multimodal fusion based on information gain for emotion recognition in the wild, intelligent systems conference 2017, 7–8 Sept 2017, London, UK" href="/article/10.1007/s10055-018-0357-0#ref-CR13" id="ref-link-section-d66749e3090">2017</a>), multimodal emotion recognition is attempted leveraging the properties of each modality using different fusion schemes. The method proposed in Ghaleb et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ghaleb E, Popa M, Hortal E, Asteriadis S (2017). Multimodal fusion based on information gain for emotion recognition in the wild, intelligent systems conference 2017, 7–8 Sept 2017, London, UK" href="/article/10.1007/s10055-018-0357-0#ref-CR13" id="ref-link-section-d66749e3093">2017</a>) outperforms other approaches existing in the literature for challenging datasets and emotion recognition in the wild. Given that in any AR/VR system usually multiple sensing modalities exist, further exploring the combination of techniques may enable either higher accuracy or balancing the complexity of the algorithm (and relevant processing resources) implemented per modality.</p><p>AR/VR applications can <i>benefit</i> from affect detection since the evolution of the AR/VR experience does not longer depend only on the physical reaction of the user but also of the emotional reaction. The interactions between the user and the system are no longer used solely for evaluating the physical reaction of the user (and thus his performance) but also for evaluating how he feels. This important information enables not only personalization (tailoring to static user preferences/intricacies) but also adaptation of the experience (educational or other) to the real-time conditions of the user. This adaptation adds significant value to AR/VR applications which nowadays are rapidly penetrating the educational sector, as witnessed by the large investments from governments of different countries across the globe [such as the USA, France and China among others (DigiCapital <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="DigiCapital (2017) Augmented/Virtual Reality Report Q4. &#xA;                    https://www.digi-capital.com/news/2017/01/after-mixed-year-mobile-ar-to-drive-108-billion-vrar-market-by-2021/#.WdyhmTBx3IV&#xA;                    &#xA;                  . Accessed 10 Oct 2017" href="/article/10.1007/s10055-018-0357-0#ref-CR10" id="ref-link-section-d66749e3102">2017</a>)] through the education and technology ministries. This market is huge with mobile AR market alone anticipated to reach $108 billion by 2021 (DigiCapital <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="DigiCapital (2017) Augmented/Virtual Reality Report Q4. &#xA;                    https://www.digi-capital.com/news/2017/01/after-mixed-year-mobile-ar-to-drive-108-billion-vrar-market-by-2021/#.WdyhmTBx3IV&#xA;                    &#xA;                  . Accessed 10 Oct 2017" href="/article/10.1007/s10055-018-0357-0#ref-CR10" id="ref-link-section-d66749e3105">2017</a>). Apart from education, the aforementioned affect state approach can be exploited in other application sectors where AR and VR penetrate. These indicatively include gaming, healthcare, travel and real estate. The feeling/affect of the user can be taken into account and change the actual parameters (e.g. levels) of the game. This way the user can be kept to the desired emotional state which can be engagement or excitement or other depending on the nature of the game. An adolescent enjoys driving to the edge, while an older person enjoys staying in less stressing situations. In healthcare, AR and VR solutions augmented with affect detection capabilities can extensively be used to curate people suffering from dementia and mild cognitive disabilities. In this sector, prompting the user to act is mandatory and tailoring the pace of the experience to his preferences is safeguarding proper operation. For example, if the user is prompted to play cards, he should neither experience anxiety (being stressed to act/select a card), neither bored (being allowed to endlessly wait his action). Affect detection can ensure he is kept in what his carers feel “ideal” affect states. Similar opportunities exist in the travel and real estate sectors, where the user has to be presented with environments that lead to pleasant experience.</p><p>In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0357-0#Sec3">3</a>, we proved that commonly available sensors can deliver accurate affect information. This fact creates new <i>business prospects</i> for software/solution developers. The standardization of the way the information of affect state is communicated among devices could have an important impact on the business potential of the relevant solutions. It would allow for the development of affect detection modules (either hardware with appropriate software or software that can be run in already available hardware) which can be flexibly integrated with applications (not only learning) that can exploit affect information. This can further boost AR/VR systems uptake. For example, if there is a web-based learning application that uses affect state information, when a student uses his tablet to exercise at home, the application can receive affect state as detected by the inertia sensors of the tablet, while in a training center, the installation may include a camera or a Kinect device. In the education industry, xAPI (Hansen and Ji <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hansen DW, Ji Q (2010). In the eye of the beholder: a survey of models for eyes. IEEE Trans Pattern Anal Mach Intell" href="/article/10.1007/s10055-018-0357-0#ref-CR14" id="ref-link-section-d66749e3117">2010</a>) is a format widely adopted to communicate “statements” and could be used to communicate information like “user A is bored.” This information can trigger the modification of the difficulty level of the learning material the user interacts with (or any other application-dependent aspect). In case of standardization, a new wave of sophisticated affect detection software modules tailored to different devices or to diverse user groups (e.g., autistic children or elder people with dementia) to offer increased affect detection accuracy can emerge.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Conclusions</h2><div class="c-article-section__content" id="Sec10-content"><p>Augmented and virtual reality systems and applications invade educational environment rapidly capitalizing on their inherent support of enhanced experience. Although AR/VR technologies contribute to the learner engagement, this can be further boosted if the affect state of the learner is taken into account to tailor the experience to the learner’s temporal state and to keep her/him in the flow state which has been shown to be the optimal state for learning processes. We proposed to take advantage of the availability of sensors in such systems to detect in real time the affect state and guide/shape the provided experience. We presented a set of practical implementations of techniques that rely on widely available sensors and achieve affect state detection with adequate accuracy for learning environments. Finally, the prospect of the penetration of such techniques in the market is discussed, and innovation opportunities were identified. In our future work, we will use the presented prototype system to evaluate the accuracy of affect detection and the benefits brought in real-life operations in multiple EU countries and diverse user groups including among others children suffering profound multiple learning disabilities, children in mainstream schools and adults in vocational training environments in different countries.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CN. Anagnostopoulos, T. Iliou, I. Giannoukos, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Anagnostopoulos CN, Iliou T, Giannoukos I (2015) Features and classifiers for emotion recognition from speech:" /><p class="c-article-references__text" id="ref-CR1">Anagnostopoulos CN, Iliou T, Giannoukos I (2015) Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011. Artif Intell Rev 43(2):155–177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10462-012-9368-5" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Features%20and%20classifiers%20for%20emotion%20recognition%20from%20speech%3A%20a%20survey%20from%202000%20to%202011&amp;journal=Artif%20Intell%20Rev&amp;volume=43&amp;issue=2&amp;pages=155-177&amp;publication_year=2015&amp;author=Anagnostopoulos%2CCN&amp;author=Iliou%2CT&amp;author=Giannoukos%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RSJ. Baker, SK. D’Mello, MT. Rodrigo, AC. Graesser, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Baker RSJ, D’Mello SK, Rodrigo MT, Graesser AC (2010) Better to be frustrated than bored: the incidence, persi" /><p class="c-article-references__text" id="ref-CR2">Baker RSJ, D’Mello SK, Rodrigo MT, Graesser AC (2010) Better to be frustrated than bored: the incidence, persistence, and impact of learners’ cognitive-affective states during interactions with three different computer-based learning environments. Int. J. Hum.-Comput. Stud. 68:223–241</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2009.12.003" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Better%20to%20be%20frustrated%20than%20bored%3A%20the%20incidence%2C%20persistence%2C%20and%20impact%20of%20learners%E2%80%99%20cognitive-affective%20states%20during%20interactions%20with%20three%20different%20computer-based%20learning%20environments&amp;journal=Int.%20J.%20Hum.-Comput.%20Stud.&amp;volume=68&amp;pages=223-241&amp;publication_year=2010&amp;author=Baker%2CRSJ&amp;author=D%E2%80%99Mello%2CSK&amp;author=Rodrigo%2CMT&amp;author=Graesser%2CAC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baltrušaitis T, Robinson P, Morency LP (2016) Openface: an open source facial behavior analysis toolkit. 2016 " /><p class="c-article-references__text" id="ref-CR3">Baltrušaitis T, Robinson P, Morency LP (2016) Openface: an open source facial behavior analysis toolkit. 2016 IEEE Winter conference on IEEE applications of computer vision (WACV)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burkhardt F et al (2005) A database of german emotional speech. In: Proceedings of interspeech, Lissabon" /><p class="c-article-references__text" id="ref-CR4">Burkhardt F et al (2005) A database of german emotional speech. In: Proceedings of interspeech, Lissabon</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Chen, K. Liu, N. Kehtarnavaz, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Chen C, Liu K, Kehtarnavaz N (2016) Real-time human action recognition based on depth motion maps. J Real-Time" /><p class="c-article-references__text" id="ref-CR5">Chen C, Liu K, Kehtarnavaz N (2016) Real-time human action recognition based on depth motion maps. J Real-Time Image Proc 12(1):155–163</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11554-013-0370-1" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20human%20action%20recognition%20based%20on%20depth%20motion%20maps&amp;journal=J%20Real-Time%20Image%20Proc&amp;volume=12&amp;issue=1&amp;pages=155-163&amp;publication_year=2016&amp;author=Chen%2CC&amp;author=Liu%2CK&amp;author=Kehtarnavaz%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Costantini G et al (2014) Emovo corpus: an Italian emotional speech database. In: Chair NCC, Choukri K, Decler" /><p class="c-article-references__text" id="ref-CR6">Costantini G et al (2014) Emovo corpus: an Italian emotional speech database. In: Chair NCC, Choukri K, Declerck T, Loftsson H, Maegaard B, Mariani J, Moreno A, Odijk J, Piperidis S (eds) Proceedings of the ninth international conference on language resources and evaluation (LREC’14). European Language Resources Association (ELRA), Reykjavik, Iceland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gesture" /><p class="c-article-references__text" id="ref-CR7">Coutrix C et al (2012) Identifying emotions expressed by mobile users through 2D surface and 3D motion gestures. In: Proceedings of the 2012 ACM conference on ubiquitous computing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Csikszentmihalyi, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Csikszentmihalyi M (1996) Flow and the psychology of discovery and invention. Harper Collins, New York" /><p class="c-article-references__text" id="ref-CR8">Csikszentmihalyi M (1996) Flow and the psychology of discovery and invention. Harper Collins, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Flow%20and%20the%20psychology%20of%20discovery%20and%20invention&amp;publication_year=1996&amp;author=Csikszentmihalyi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Csíkszentmihályi, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Csíkszentmihályi M (2008) Flow: the psychology of optimal experience. Harper Perennial, New York" /><p class="c-article-references__text" id="ref-CR9">Csíkszentmihályi M (2008) Flow: the psychology of optimal experience. Harper Perennial, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Flow%3A%20the%20psychology%20of%20optimal%20experience&amp;publication_year=2008&amp;author=Cs%C3%ADkszentmih%C3%A1lyi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DigiCapital (2017) Augmented/Virtual Reality Report Q4. https://www.digi-capital.com/news/2017/01/after-mixed-" /><p class="c-article-references__text" id="ref-CR10">DigiCapital (2017) Augmented/Virtual Reality Report Q4. <a href="https://www.digi-capital.com/news/2017/01/after-mixed-year-mobile-ar-to-drive-108-billion-vrar-market-by-2021/%23.WdyhmTBx3IV">https://www.digi-capital.com/news/2017/01/after-mixed-year-mobile-ar-to-drive-108-billion-vrar-market-by-2021/#.WdyhmTBx3IV</a>. Accessed 10 Oct 2017</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Du Y, Wang W, Wang L (2015) Hierarchical recurrent neural network for skeleton based action recognition. In: P" /><p class="c-article-references__text" id="ref-CR11">Du Y, Wang W, Wang L (2015) Hierarchical recurrent neural network for skeleton based action recognition. In: Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ayadi, MS. Kamel, F. Karray, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="El Ayadi M, Kamel MS, Karray F (2011) Survey on speech emotion recognition: features, classification schemes, " /><p class="c-article-references__text" id="ref-CR12">El Ayadi M, Kamel MS, Karray F (2011) Survey on speech emotion recognition: features, classification schemes, and databases. Pattern Recogn 44(3):572–587</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patcog.2010.09.020" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%20speech%20emotion%20recognition%3A%20features%2C%20classification%20schemes%2C%20and%20databases&amp;journal=Pattern%20Recogn&amp;volume=44&amp;issue=3&amp;pages=572-587&amp;publication_year=2011&amp;author=Ayadi%2CM&amp;author=Kamel%2CMS&amp;author=Karray%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ghaleb E, Popa M, Hortal E, Asteriadis S (2017). Multimodal fusion based on information gain for emotion recog" /><p class="c-article-references__text" id="ref-CR13">Ghaleb E, Popa M, Hortal E, Asteriadis S (2017). Multimodal fusion based on information gain for emotion recognition in the wild, intelligent systems conference 2017, 7–8 Sept 2017, London, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hansen DW, Ji Q (2010). In the eye of the beholder: a survey of models for eyes. IEEE Trans Pattern Anal Mach " /><p class="c-article-references__text" id="ref-CR14">Hansen DW, Ji Q (2010). In the eye of the beholder: a survey of models for eyes. IEEE Trans Pattern Anal Mach Intell</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haq S, Jackson P (2009) Speaker-dependent audio-visual emotion recognition. In: Proceedings of the internation" /><p class="c-article-references__text" id="ref-CR15">Haq S, Jackson P (2009) Speaker-dependent audio-visual emotion recognition. In: Proceedings of the international conference on auditory-visual speech processing (AVSP’08), Norwich, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kanade T, Cohn JF, Lucey P, Saragih J, Ambadar Z, Matthews I (2010) The extended Cohn–Kanade Dataset (CK+): a " /><p class="c-article-references__text" id="ref-CR16">Kanade T, Cohn JF, Lucey P, Saragih J, Ambadar Z, Matthews I (2010) The extended Cohn–Kanade Dataset (CK+): a complete expression dataset for action unit and emotion-specified expression, San Francisco, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on" /><p class="c-article-references__text" id="ref-CR17">Kim M et al (2013) A touch based affective user interface for smartphone. In: IEEE international conference on consumer electronics (ICCE). IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leifman G et al (2016) Learning gaze transitions from depth to improve video saliency estimation. arXiv prepri" /><p class="c-article-references__text" id="ref-CR18">Leifman G et al (2016) Learning gaze transitions from depth to improve video saliency estimation. arXiv preprint <a href="http://arxiv.org/abs/1603.03669">arXiv:1603.03669</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li W, Zhang Z, Liu Z (2010) Action recognition based on a bag of 3d points. In: Proceedings of IEEE internatio" /><p class="c-article-references__text" id="ref-CR19">Li W, Zhang Z, Liu Z (2010) Action recognition based on a bag of 3d points. In: Proceedings of IEEE international conference in computer vision and pattern recognition workshop (CVPRW)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mora KAF, Monay F, Odobez J-M (2014) Eyediap: a database for the development and evaluation of gaze estimation" /><p class="c-article-references__text" id="ref-CR20">Mora KAF, Monay F, Odobez J-M (2014) Eyediap: a database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. In: Proceedings of the symposium on eye tracking research and applications. ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nottingham Trent University (ed) (2017) Adaptation and Personalization principles based on MaTHiSiS findings. " /><p class="c-article-references__text" id="ref-CR21">Nottingham Trent University (ed) (2017) Adaptation and Personalization principles based on MaTHiSiS findings. Deliverable for the MaTHiSiS project. <a href="http://www.mathisis-project.eu/en/content/d61-adaptation-and-personalization-principles-based-mathisis-findings">http://www.mathisis-project.eu/en/content/d61-adaptation-and-personalization-principles-based-mathisis-findings</a>. Accessed 3/4/2018</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Pedregosa, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Pedregosa F et al (2011) Scikit-learn: machine learning in Python. Journal of Machine Learning Research 12:282" /><p class="c-article-references__text" id="ref-CR22">Pedregosa F et al (2011) Scikit-learn: machine learning in Python. Journal of Machine Learning Research 12:2825–2830</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2854348" aria-label="View reference 22 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1280.68189" aria-label="View reference 22 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scikit-learn%3A%20machine%20learning%20in%20Python&amp;journal=Journal%20of%20Machine%20Learning%20Research&amp;volume=12&amp;pages=2825-2830&amp;publication_year=2011&amp;author=Pedregosa%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rikert TD, Jones MJ (1998) Gaze estimation using morphable models. In: Proceedings of third IEEE international" /><p class="c-article-references__text" id="ref-CR23">Rikert TD, Jones MJ (1998) Gaze estimation using morphable models. In: Proceedings of third IEEE international conference on automatic face and gesture recognition. IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vemulapalli R, Arrate F, Chellappa R (2014) Human action recognition by representing 3d skeletons as points in" /><p class="c-article-references__text" id="ref-CR24">Vemulapalli R, Arrate F, Chellappa R (2014) Human action recognition by representing 3d skeletons as points in a lie group. In: Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang L, Zhang J, Zhou L, Tang C, Li W (2015) Beyond covariance: feature representation with nonlinear kernel m" /><p class="c-article-references__text" id="ref-CR25">Wang L, Zhang J, Zhou L, Tang C, Li W (2015) Beyond covariance: feature representation with nonlinear kernel matrices. In: Proceedings of the IEEE international conference on computer vision (ICCV)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xiong X, De la Torre F (2013) Supervised descent method and its applications to face alignment. In: 2013 IEEE " /><p class="c-article-references__text" id="ref-CR26">Xiong X, De la Torre F (2013) Supervised descent method and its applications to face alignment. In: 2013 IEEE conference on computer vision and pattern recognition, Portland, pp 532–539</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR27">Zhang X, Sugano Y, Fritz M, Bulling A (2015) Appearance-based gaze estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4511–4520</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang X, Sugano Y, Fritz M, Bulling A (2017) MPIIGaze: real-world dataset and deep appearance-based gaze estim" /><p class="c-article-references__text" id="ref-CR28">Zhang X, Sugano Y, Fritz M, Bulling A (2017) MPIIGaze: real-world dataset and deep appearance-based gaze estimation. IEEE Trans Pattern Anal Mach Intell</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0357-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The work presented in this document has been partially funded through H2020-MaTHiSiS Project. This project has received funding from the European Union’s Horizon 2020 Programme (H2020-ICT-2015) under Grant Agreement No. 687772.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Centre of Research and Technology Hellas, Thermi, Thessaloniki, Greece</p><p class="c-article-author-affiliation__authors-list">Nicholas Vretos &amp; Petros Daras</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">University of Maastricht, Maastricht, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Stylianos Asteriadis, Enrique Hortal &amp; Esam Ghaleb</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">National Centre for Scientific Research “Demokritos”, Agia Paraskevi, Athens, Greece</p><p class="c-article-author-affiliation__authors-list">Evaggelos Spyrou</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Technological Educational Institute of Sterea Ellada, Psahna, Halkida, Greece</p><p class="c-article-author-affiliation__authors-list">Helen C. Leligou, Panagiotis Karkazis, Panagiotis Trakadas &amp; Kostantinos Assimakopoulos</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Nicholas-Vretos"><span class="c-article-authors-search__title u-h3 js-search-name">Nicholas Vretos</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Nicholas+Vretos&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nicholas+Vretos" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nicholas+Vretos%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Petros-Daras"><span class="c-article-authors-search__title u-h3 js-search-name">Petros Daras</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Petros+Daras&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Petros+Daras" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Petros+Daras%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Stylianos-Asteriadis"><span class="c-article-authors-search__title u-h3 js-search-name">Stylianos Asteriadis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Stylianos+Asteriadis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stylianos+Asteriadis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stylianos+Asteriadis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Enrique-Hortal"><span class="c-article-authors-search__title u-h3 js-search-name">Enrique Hortal</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Enrique+Hortal&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Enrique+Hortal" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Enrique+Hortal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Esam-Ghaleb"><span class="c-article-authors-search__title u-h3 js-search-name">Esam Ghaleb</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Esam+Ghaleb&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Esam+Ghaleb" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Esam+Ghaleb%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Evaggelos-Spyrou"><span class="c-article-authors-search__title u-h3 js-search-name">Evaggelos Spyrou</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Evaggelos+Spyrou&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Evaggelos+Spyrou" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Evaggelos+Spyrou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Helen_C_-Leligou"><span class="c-article-authors-search__title u-h3 js-search-name">Helen C. Leligou</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Helen C.+Leligou&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Helen C.+Leligou" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Helen C.+Leligou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Panagiotis-Karkazis"><span class="c-article-authors-search__title u-h3 js-search-name">Panagiotis Karkazis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Panagiotis+Karkazis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Panagiotis+Karkazis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Panagiotis+Karkazis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Panagiotis-Trakadas"><span class="c-article-authors-search__title u-h3 js-search-name">Panagiotis Trakadas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Panagiotis+Trakadas&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Panagiotis+Trakadas" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Panagiotis+Trakadas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kostantinos-Assimakopoulos"><span class="c-article-authors-search__title u-h3 js-search-name">Kostantinos Assimakopoulos</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kostantinos+Assimakopoulos&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kostantinos+Assimakopoulos" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kostantinos+Assimakopoulos%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0357-0/email/correspondent/c1/new">Panagiotis Trakadas</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Exploiting%20sensing%20devices%20availability%20in%20AR%2FVR%20deployments%20to%20foster%20engagement&amp;author=Nicholas%20Vretos%20et%20al&amp;contentID=10.1007%2Fs10055-018-0357-0&amp;publication=1359-4338&amp;publicationDate=2018-07-14&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0357-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0357-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Vretos, N., Daras, P., Asteriadis, S. <i>et al.</i> Exploiting sensing devices availability in AR/VR deployments to foster engagement.
                    <i>Virtual Reality</i> <b>23, </b>399–410 (2019). https://doi.org/10.1007/s10055-018-0357-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0357-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-10-12">12 October 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-07-06">06 July 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-07-14">14 July 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12">December 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0357-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0357-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Affect state detection</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Engagement</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Interpretation of interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal affect state detection</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0357-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=357;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

