<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Comparing methods for numerical input in immersive virtual environment"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward task for windows-based environments, this is..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Comparing methods for numerical input in immersive virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-05-02"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward task for windows-based environments, this is not the case for immersive virtual environments. In this paper we design, implement, and evaluate different methods based on gestures or hand-held devices such as wireless numeric keyboards and gamepads. The assessment showed that hand-held devices can be easier to use and faster for data input. A numeric keyboard can offer fast input for positive, small numbers, while the gamepad-based method can offer uniform performance. Although the gesture-based method did not perform as well, there is still scope for its use when a designer needs a method that leaves both user&#8217;s hands free during interaction."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-05-02"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="63"/>

    <meta name="prism.endingPage" content="77"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0312-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0312-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0312-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0312-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Comparing methods for numerical input in immersive virtual environments"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2018/03"/>

    <meta name="citation_online_date" content="2017/05/02"/>

    <meta name="citation_firstpage" content="63"/>

    <meta name="citation_lastpage" content="77"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0312-5"/>

    <meta name="DOI" content="10.1007/s10055-017-0312-5"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0312-5"/>

    <meta name="description" content="Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward tas"/>

    <meta name="dc.creator" content="Georgios Lepouras"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ardito C, Buono P, Costabile MF, Lanzilotti R, Simeone AL (2009) Comparing low cost input devices for interacting with 3D virtual environments. In: 2nd conference on human system interactions, 2009. HSI&#8217;09. IEEE, pp. 292&#8211;297"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Design and evaluation of finger-count interaction: Combining multitouch gestures and menus; citation_author=G Bailly, J M&#252;ller, E Lecolinet; citation_volume=70; citation_issue=10; citation_publication_date=2012; citation_pages=673-689; citation_doi=10.1016/j.ijhcs.2012.05.006; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Multimed Comput Commun Appl (TOMM); citation_title=Investigating on-screen gamepad designs for smartphone-controlled video games; citation_author=M Baldauf, P Fr&#246;hlich, F Adegeye, S Suette; citation_volume=12; citation_issue=1s; citation_publication_date=2015; citation_pages=22; citation_id=CR3"/>

    <meta name="citation_reference" content="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154&#8211;2158"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Novel uses of Pinch Gloves&#8482; for virtual environment interaction techniques; citation_author=DA Bowman, CA Wingrave, JM Campbell, VQ Ly, CJ Rhoton; citation_volume=6; citation_issue=3; citation_publication_date=2002; citation_pages=122-129; citation_doi=10.1007/s100550200013; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces: theory and practice; citation_publication_date=2004; citation_id=CR6; citation_author=DA Bowman; citation_author=E Kruijff; citation_author=JJ LaViola; citation_author=I Poupyrev; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="Braffort A (1996) A gesture recognition architecture for sign language. In: Proceedings of the second annual ACM conference on assistive technologies, ACM, pp. 102&#8211;109"/>

    <meta name="citation_reference" content="citation_title=Beyond the gamepad: HCI and game controller design and evaluation; citation_inbook_title=Game user experience evaluation; citation_publication_date=2015; citation_pages=263-285; citation_id=CR8; citation_author=M Brown; citation_author=A Kehoe; citation_author=J Kirakowski; citation_author=I Pitt; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The keystroke-level model for user performance time with interactive systems; citation_author=SK Card, TP Moran, A Newell; citation_volume=23; citation_issue=7; citation_publication_date=1980; citation_pages=396-410; citation_doi=10.1145/358886.358895; citation_id=CR9"/>

    <meta name="citation_reference" content="Cardoso A, Lamounier E, Lima G, Oliveira L, Mattioli L, Junior G, Silva A, Nogueira K, do Prado P, Newton J (2013) VRCEMIG: a virtual reality system for real time control of electric substations. In: 2013 IEEE virtual reality (VR), IEEE, pp. 165&#8211;166"/>

    <meta name="citation_reference" content="Clarkson E, Clawson J, Lyons K, Starner T (2005) An empirical study of typing rates on mini-QWERTY keyboards. In: CHI&#8217;05 extended abstracts on human factors in computing systems. ACM, New York, pp. 1288&#8211;1291"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Untethered gesture acquisition and recognition for virtual world manipulation; citation_author=D Demirdjian, T Ko, T Darrell; citation_volume=8; citation_issue=4; citation_publication_date=2005; citation_pages=222-230; citation_doi=10.1007/s10055-005-0155-3; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Neural Netw; citation_title=Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls; citation_author=SS Fels, GE Hinton; citation_volume=9; citation_issue=1; citation_publication_date=1998; citation_pages=205-212; citation_doi=10.1109/72.655042; citation_id=CR13"/>

    <meta name="citation_reference" content="Foong OM, Low TJ, Wibowo S (2008) Hand gesture recognition: sign to voice system (S2V). In: Proceedings of world academy of science: engineering and technology, p 44"/>

    <meta name="citation_reference" content="Go K, Konishi H, Matsuura Y (2008) Itone: a Japanese text input method for a dual joystick game controller. In: CHI&#8217;08 extended abstracts on human factors in computing systems. ACM, New York, pp 3141&#8211;3146"/>

    <meta name="citation_reference" content="Gonz&#225;lez G, Molina JP, Garc&#237;a AS, Mart&#237;nez D, Gonz&#225;lez P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human&#8211;computer interaction. Springer, London, pp. 109&#8211;118"/>

    <meta name="citation_reference" content="Hesselmann T, Heuten W, Boll S (2011) Tap2Count: numerical input for interactive tabletops. In: Proceedings of the ACM international conference on interactive tabletops and surfaces. ACM, pp. 256&#8211;257"/>

    <meta name="citation_reference" content="Hoste L, Dumas B, Signer B (2012) SpeeG: a multimodal speech-and gesture-based text input solution. In: Proceedings of the international working conference on advanced visual interfaces. ACM, pp. 156&#8211;163"/>

    <meta name="citation_reference" content="Isokoski P, Raisamo R (2004) Quikwriting as a multi-device text entry method. In: Proceedings of the third Nordic conference on human&#8211;computer interaction. ACM, pp. 105&#8211;108"/>

    <meta name="citation_reference" content="Kim S, Kim GJ (2004) Using keyboards with head mounted displays. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp. 336&#8211;343"/>

    <meta name="citation_reference" content="K&#246;ltringer T, Isokoski P, Grechenig T (2007) TwoStick: writing with a game controller. In: Proceedings of graphics interface 2007. ACM, pp. 103&#8211;110"/>

    <meta name="citation_reference" content="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, Camps Bay, Cape Town, South Africa, November 05&#8211;07, 2001. AFRIGRAPH &#8216;01. ACM, New York, NY, pp 95&#8211;100"/>

    <meta name="citation_reference" content="LaViola J (2000) MSVT: A virtual reality-based multimodal scientific visualization tool. In: Proceedings of the third IASTED international conference on computer graphics and imaging, pp. 1&#8211;7"/>

    <meta name="citation_reference" content="LaViola J, Zeleznik R (1999) Flex and pinch: a case study of whole hand input design for virtual environment interaction. In: Proceedings of the second IASTED international conference on computer graphics and imaging, pp. 221&#8211;225"/>

    <meta name="citation_reference" content="Lee S, Hong SH, Jeon JW (2003) Designing a universal keyboard using chording gloves. In: ACM SIGCAPH computers and the physically handicapped, no 73&#8211;74. ACM, pp 142&#8211;147"/>

    <meta name="citation_reference" content="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human&#8211;computer interfaces and measurements systems, 2009. VECIMS&#8217;09, IEEE, pp. 240&#8211;245"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comput Interact; citation_title=Text entry for mobile computing: models and methods, theory and practice; citation_author=IS MacKenzie, RW Soukoreff; citation_volume=17; citation_issue=2&#8211;3; citation_publication_date=2002; citation_pages=147-198; citation_doi=10.1207/S15327051HCI172&amp;3_2; citation_id=CR27"/>

    <meta name="citation_reference" content="Muller J, Krapichler C, Nguyen LS, Englmeier KH, Lang M (1998) Speech interaction in virtual reality. In: Proceedings of the 1998 IEEE international conference on acoustics, speech and signal processing, 1998, vol 6. IEEE, pp 3757&#8211;3760"/>

    <meta name="citation_reference" content="Natapov D, Castellucci SJ, MacKenzie IS (2009) ISO 9241-9 evaluation of video game controllers. In: Proceedings of graphics interface 2009. Canadian Information Processing Society, pp. 223&#8211;230"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Design and evaluation of freehand menu selection interfaces using tilt and pinch gestures; citation_author=T Ni, DA Bowman, C North, RP McMahan; citation_volume=69; citation_issue=9; citation_publication_date=2011; citation_pages=551-562; citation_doi=10.1016/j.ijhcs.2011.05.001; citation_id=CR30"/>

    <meta name="citation_reference" content="Norman DA (2013) The design of everyday things, revised and expanded edition. Basic Books, New York, p 368. ISBN-13:978-0465050659"/>

    <meta name="citation_reference" content="Oshita M, Ishikawa H (2012) Gamepad vs. touchscreen: a comparison of action selection interfaces in computer games. In: Proceedings of the workshop at SIGGRAPH Asia. ACM, pp 27&#8211;31"/>

    <meta name="citation_reference" content="Poupyrev I, Tomokazu N, Weghorst S (1998) Virtual notepad: handwriting in immersive VR. In: Virtual reality annual international symposium, 1998. Proceedings, IEEE 1998. IEEE, pp 126&#8211;132"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybernetics Part C Appl Rev; citation_title=The chording glove: a glove-based text input device; citation_author=R Rosenberg, M Slater; citation_volume=29; citation_issue=2; citation_publication_date=1999; citation_pages=186-191; citation_doi=10.1109/5326.760563; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=Bimanual text entry using game controllers: relying on users&#8217; spatial familiarity with QWERTY; citation_author=FE Sandnes, A Aubert; citation_volume=19; citation_issue=2; citation_publication_date=2007; citation_pages=140-150; citation_doi=10.1016/j.intcom.2006.08.003; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Soft Comput Eng (IJSCE); citation_title=Design and implementation of interactive visualisation configuration using interaction paradigms in virtual reality environment; citation_author=T Skripcak, P Tanuska, N Schmeisser; citation_volume=1; citation_issue=5; citation_publication_date=2011; citation_pages=57-65; citation_id=CR36"/>

    <meta name="citation_reference" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001). VRPN: a device-independent, network-transparent VR peripheral system. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, pp 55&#8211;61"/>

    <meta name="citation_reference" content="Terajima K, Komuro T, Ishikawa M (2009) Fast finger tracking system for in-air typing interface. In: CHI&#8217;09 extended abstracts on human factors in computing systems. ACM, pp 3739&#8211;3744"/>

    <meta name="citation_reference" content="citation_journal_title=Aust J Sci Res Ser A; citation_title=Experimental designs balanced for the estimation of residual effects of treatments; citation_author=EJ Williams; citation_volume=2; citation_publication_date=1949; citation_pages=149-168; citation_id=CR39"/>

    <meta name="citation_reference" content="Wilson AD, Agrawala M (2006) Text entry using a dual joystick game controller. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 475&#8211;478"/>

    <meta name="citation_author" content="Georgios Lepouras"/>

    <meta name="citation_author_email" content="G.Lepouras@uop.gr"/>

    <meta name="citation_author_institution" content="Department of Informatics and Telecommunications, University of the Peloponnese, Tripolis, Greece"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0312-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0312-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Comparing methods for numerical input in immersive virtual environments"/>
        <meta property="og:description" content="Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward task for windows-based environments, this is not the case for immersive virtual environments. In this paper we design, implement, and evaluate different methods based on gestures or hand-held devices such as wireless numeric keyboards and gamepads. The assessment showed that hand-held devices can be easier to use and faster for data input. A numeric keyboard can offer fast input for positive, small numbers, while the gamepad-based method can offer uniform performance. Although the gesture-based method did not perform as well, there is still scope for its use when a designer needs a method that leaves both user’s hands free during interaction."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Comparing methods for numerical input in immersive virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0312-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Numerical input, Immersive virtual environments","kwrd":["Virtual_reality","Numerical_input","Immersive_virtual_environments"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0312-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0312-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=312;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0312-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Comparing methods for numerical input in immersive virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0312-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0312-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-05-02" itemprop="datePublished">02 May 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Comparing methods for numerical input in immersive virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Georgios-Lepouras" data-author-popup="auth-Georgios-Lepouras" data-corresp-id="c1">Georgios Lepouras<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0001-6094-3308"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-6094-3308</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of the Peloponnese" /><meta itemprop="address" content="0000 0001 0731 9119, grid.36738.39, Department of Informatics and Telecommunications, University of the Peloponnese, Tripolis, Arcadia, Greece" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">63</span>–<span itemprop="pageEnd">77</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">490 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0312-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward task for windows-based environments, this is not the case for immersive virtual environments. In this paper we design, implement, and evaluate different methods based on gestures or hand-held devices such as wireless numeric keyboards and gamepads. The assessment showed that hand-held devices can be easier to use and faster for data input. A numeric keyboard can offer fast input for positive, small numbers, while the gamepad-based method can offer uniform performance. Although the gesture-based method did not perform as well, there is still scope for its use when a designer needs a method that leaves both user’s hands free during interaction.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Data input is a common task in current desktop computing environments. In such environments, alphanumeric input is carried out easily with the familiar desktop keyboard. However, in virtual reality environments, this is not necessarily the case. Currently, a new wave of HMD devices has arrived such as Oculus Rift, HTC Vive, LG 360 VR, Samsung Gear VR, and Sony VR headset. These devices allow development of fully immersed virtual reality environments. In particular for these environments, where a keyboard-based solution cannot be easily employed, other solutions for character input have to be devised. It could be argued that since in virtual reality applications most interaction is usually limited to navigation and object manipulation, no need exists for methods supporting data input.</p><p>However, Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison-Wesley, Boston" href="/article/10.1007/s10055-017-0312-5#ref-CR6" id="ref-link-section-d11386e285">2004</a>) provide a number of possible scenarios of use that demonstrate the potential for alphanumeric input, such as the need for entering design annotations, file names, labeling objects, precise object manipulation, parameter setting, and communication between users and markup. Of these scenarios, some require only numeric input.</p><p>Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. So far, in the literature most input methods proposed cater for both alphabetic and numeric characters. This, on the one hand, has the advantage of being more generic and broadly applicable, but on the other hand a more focused input technique can be better suited for cases where only numeric input is needed.</p><p>Furthermore, a numeric input technique may have more applications than simple parameter setting. For example, it can be exploited in selecting from numbered menu lists or even entering alphabetic characters through a keyboard with a layout of a dial pad on a phone.</p><p>To this end, we describe methods employing either gestures or hand-held devices that support input of numbers in immersive virtual reality environments. We also test these methods in two phases: one with a small user sample to detect positive and negative sides of the methods and one with a larger sample to evaluate the revised versions of the methods.</p><p>The rest of the paper is structured as follows: The next section outlines existing approaches, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec7">3</a> presents the proposed methods, while Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec14">4</a> describes the first phase of user testing and the improvements made to the methods. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec15">5</a> outlines the user evaluation, and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec16">6</a> presents in detail the evaluation’s results. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec33">7</a> offers a discussion of the results, and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec34">8</a> concludes the paper and depicts future research.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Existing approaches</h2><div class="c-article-section__content" id="Sec2-content"><p> In immersive virtual reality environments, users are often standing up. Therefore, when designing an input technique for such an environment, typical desktop input devices such as the mouse and the full-size keyboard cannot be readily employed. To this end, one may design a new technique especially for immersive virtual reality environments or exploit a technique that does not use typical desktop input devices. Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison-Wesley, Boston" href="/article/10.1007/s10055-017-0312-5#ref-CR6" id="ref-link-section-d11386e328">2004</a>) identify four categories for input interaction techniques: pen, keyboard, gesture, and speech-based techniques. We could combine and extend the first two categories in a new category named device-based, to include all techniques that are based on a specific device and they could not have been implemented the same way with another device. In contrast, a gesture-based technique could employ data gloves for its implementation or another device able to recognize gestures such as Kinect. Additionally, the speech-based techniques category is extended to multimodal techniques, as in most cases speech is complemented with another input modality, most typically gestures. We categorize existing research in these three categories and include a fourth category of research that compares interaction techniques.</p><h3 class="c-article__sub-heading" id="Sec3">Device-based techniques</h3><p>Poupyrev et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Poupyrev I, Tomokazu N, Weghorst S (1998) Virtual notepad: handwriting in immersive VR. In: Virtual reality annual international symposium, 1998. Proceedings, IEEE 1998. IEEE, pp 126–132" href="/article/10.1007/s10055-017-0312-5#ref-CR33" id="ref-link-section-d11386e338">1998</a>) presented a collection of interface tools that can be used in a virtual environment and allow the user to take notes, annotate documents, and input text using a pen with a spatially tracked, pressure-sensitive graphics tablet, a pen, and handwriting recognition software.</p><p>Rosenberg and Slater (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rosenberg R, Slater M (1999) The chording glove: a glove-based text input device. IEEE Trans Syst Man Cybernetics Part C Appl Rev 29(2):186–191" href="/article/10.1007/s10055-017-0312-5#ref-CR34" id="ref-link-section-d11386e344">1999</a>) introduced a text input device called the chording glove, which implements a chord type of keyboard with a glove, designed as a text input device for wearable computers and virtual environments. Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Lee S, Hong SH, Jeon JW (2003) Designing a universal keyboard using chording gloves. In: ACM SIGCAPH computers and the physically handicapped, no 73–74. ACM, pp 142–147" href="/article/10.1007/s10055-017-0312-5#ref-CR25" id="ref-link-section-d11386e347">2003</a>) used a chording glove to implement a technique, which allowed both text and Braille input and was tested for input of Korean and Braille characters as well as numbers.</p><p>Kim and Kim (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kim S, Kim GJ (2004) Using keyboards with head mounted displays. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp. 336–343" href="/article/10.1007/s10055-017-0312-5#ref-CR20" id="ref-link-section-d11386e353">2004</a>) proposed the use a real keyboard as a tangible interface to the virtual keyboard and empirically compared to that of the real through an experiment. The results showed that users can obtain near-real-world performance out of such a system. In their research they employed a full-size QWERTY keyboard that cannot be easily employed when a user is standing.</p><p>The use of simple, lightweight devices usually employed for gaming interfaces has also been explored by some researchers. Isokoski and Raisamo (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Isokoski P, Raisamo R (2004) Quikwriting as a multi-device text entry method. In: Proceedings of the third Nordic conference on human–computer interaction. ACM, pp. 105–108" href="/article/10.1007/s10055-017-0312-5#ref-CR19" id="ref-link-section-d11386e359">2004</a>) present Quikwriting method for joystick-based text entry to show that with adequate practice users can reach speeds comparable to handwriting in the stylus mode. Wilson and Agrawala (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Wilson AD, Agrawala M (2006) Text entry using a dual joystick game controller. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 475–478" href="/article/10.1007/s10055-017-0312-5#ref-CR40" id="ref-link-section-d11386e362">2006</a>) employed the two joysticks of a gamepad controller to implement a text entry technique and showed that the technique can increase entry speed significantly compared to single-stick selection keyboard technique, while it is readily learnable. Sandnes and Aubert (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sandnes FE, Aubert A (2007) Bimanual text entry using game controllers: relying on users’ spatial familiarity with QWERTY. Interact Comput 19(2):140–150" href="/article/10.1007/s10055-017-0312-5#ref-CR35" id="ref-link-section-d11386e365">2007</a>) on the other hand used a two-handed game controller with two analog joysticks where characters are organized into a QWERTY layout with the joystick resting position conceptually located where the index fingers are in touch position. The technique is promising as text can potentially be entered with limited visual feedback, and initial experimental evaluations show that text can be entered at a mean rate of 6.75 words per minute with less than 1 h of practice. Költringer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Költringer T, Isokoski P, Grechenig T (2007) TwoStick: writing with a game controller. In: Proceedings of graphics interface 2007. ACM, pp. 103–110" href="/article/10.1007/s10055-017-0312-5#ref-CR21" id="ref-link-section-d11386e368">2007</a>) presented the design and evaluation of a game controller text entry method and compared user performance with the proposed technique to a selection keyboard. Results showed the need for a learning period at the end of which the technique achieved a rate of 14.9 versus 12.9 wpm for the selection keyboard. Go et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Go K, Konishi H, Matsuura Y (2008) Itone: a Japanese text input method for a dual joystick game controller. In: CHI’08 extended abstracts on human factors in computing systems. ACM, New York, pp 3141–3146" href="/article/10.1007/s10055-017-0312-5#ref-CR15" id="ref-link-section-d11386e371">2008</a>) employed a dual-joystick game controller to input Japanese text with preliminary results showing potential for easy learning and high performance.</p><p>Cardoso et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Cardoso A, Lamounier E, Lima G, Oliveira L, Mattioli L, Junior G, Silva A, Nogueira K, do Prado P, Newton J (2013) VRCEMIG: a virtual reality system for real time control of electric substations. In: 2013 IEEE virtual reality (VR), IEEE, pp. 165–166" href="/article/10.1007/s10055-017-0312-5#ref-CR10" id="ref-link-section-d11386e378">2013</a>) presented a virtual environment based on an integrated hardware and software platform that employs different devices such as joystick, gamepad, and VR glasses to navigate and operate an electric substation for training purposes.</p><h3 class="c-article__sub-heading" id="Sec4">Gesture-based techniques</h3><p>Another approach would be to employ gesture-based techniques to implement numeric input techniques for immersive virtual reality environments. Capturing of user gestures can be achieved using data gloves or pinch gloves. For example, Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154–2158" href="/article/10.1007/s10055-017-0312-5#ref-CR4" id="ref-link-section-d11386e389">2002a</a>) presented three interaction techniques designed for virtual environments and employing pinch gloves including a text input and a menu item selection technique.</p><p>Sign language recognition systems and techniques can potentially be modified to work for immersive virtual reality systems. Fels and Hinton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Fels SS, Hinton GE (1998) Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Trans Neural Netw 9(1):205–212" href="/article/10.1007/s10055-017-0312-5#ref-CR13" id="ref-link-section-d11386e395">1998</a>) described a system employing a data glove, tracker, keyboard, and a foot pedal that translates gestures to speech. While the proposed system could be used to input numbers, the system was not originally designed to cope with the intricacies of an immersive virtual environment. Similarly, Braffort (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Braffort A (1996) A gesture recognition architecture for sign language. In: Proceedings of the second annual ACM conference on assistive technologies, ACM, pp. 102–109" href="/article/10.1007/s10055-017-0312-5#ref-CR7" id="ref-link-section-d11386e398">1996</a>) presented an architecture for sign language gesture recognition, while Foong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Foong OM, Low TJ, Wibowo S (2008) Hand gesture recognition: sign to voice system (S2V). In: Proceedings of world academy of science: engineering and technology, p 44" href="/article/10.1007/s10055-017-0312-5#ref-CR14" id="ref-link-section-d11386e401">2008</a>) described a system that recognizes sign language gestures and converts them to voice. Terajima et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Terajima K, Komuro T, Ishikawa M (2009) Fast finger tracking system for in-air typing interface. In: CHI’09 extended abstracts on human factors in computing systems. ACM, pp 3739–3744" href="/article/10.1007/s10055-017-0312-5#ref-CR38" id="ref-link-section-d11386e404">2009</a>) developed a system which performs 3D motion tracking of human’s hand and fingers from images of a single high-frame-rate camera and that recognizes her typing motion in the air and achieved fast real-time recognition of typing motion.</p><p>Ni et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ni T, Bowman DA, North C, McMahan RP (2011) Design and evaluation of freehand menu selection interfaces using tilt and pinch gestures. Int J Hum Comput Stud 69(9):551–562" href="/article/10.1007/s10055-017-0312-5#ref-CR30" id="ref-link-section-d11386e410">2011</a>) introduced rapMenu technique, which allows menu selection by controlling wrist tilt and employing multiple pinch gestures. Skripcak et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Skripcak T, Tanuska P, Schmeisser N (2011) Design and implementation of interactive visualisation configuration using interaction paradigms in virtual reality environment. Int J Soft Comput Eng (IJSCE) 1(5):57–65" href="/article/10.1007/s10055-017-0312-5#ref-CR36" id="ref-link-section-d11386e413">2011</a>) illustrated a technique applied in interactive visualizations for virtual reality environments employing gestures to increase and decrease numeric input values, while in Hesselmann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Hesselmann T, Heuten W, Boll S (2011) Tap2Count: numerical input for interactive tabletops. In: Proceedings of the ACM international conference on interactive tabletops and surfaces. ACM, pp. 256–257" href="/article/10.1007/s10055-017-0312-5#ref-CR17" id="ref-link-section-d11386e416">2011</a>) the authors presented a technique which allows a user to enter numbers on interactive multitouch tabletops using the 10 fingers of both hands. Bailly et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Bailly G, Müller J, Lecolinet E (2012) Design and evaluation of finger-count interaction: Combining multitouch gestures and menus. Int J Hum Comput Stud 70(10):673–689" href="/article/10.1007/s10055-017-0312-5#ref-CR2" id="ref-link-section-d11386e419">2012</a>) on the other hand presented Finger-Count gestures for multitouch displays, a coherent set of multifinger and two-handed gestures and present a variation of Finger-Count technique for in-the-air gestures.</p><h3 class="c-article__sub-heading" id="Sec5">Multimodal techniques</h3><p>Another prominent approach explored by researchers is the use of multimodal interaction techniques, mainly techniques that exploit voice and another modality. Latoschik (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, Camps Bay, Cape Town, South Africa, November 05–07, 2001. AFRIGRAPH ‘01. ACM, New York, NY, pp 95–100" href="/article/10.1007/s10055-017-0312-5#ref-CR22" id="ref-link-section-d11386e430">2001</a>) presented a gesture detection and analysis framework for modeling multimodal interactions, designed specifically for virtual reality applications. Muller et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Muller J, Krapichler C, Nguyen LS, Englmeier KH, Lang M (1998) Speech interaction in virtual reality. In: Proceedings of the 1998 IEEE international conference on acoustics, speech and signal processing, 1998, vol 6. IEEE, pp 3757–3760" href="/article/10.1007/s10055-017-0312-5#ref-CR28" id="ref-link-section-d11386e433">1998</a>) developed a system that enables a physician to navigate and interact with a patient’s 3D scans in a virtual environment with multimodal interaction capabilities.</p><p>LaViola (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="LaViola J (2000) MSVT: A virtual reality-based multimodal scientific visualization tool. In: Proceedings of the third IASTED international conference on computer graphics and imaging, pp. 1–7" href="/article/10.1007/s10055-017-0312-5#ref-CR23" id="ref-link-section-d11386e439">2000</a>) described a prototype application for visualizing fluid flow around a dataset, which uses a multimodal interface that combines whole-hand and voice input to allow users to visualize and interact with the dataset in a natural manner. Demirdjian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipulation. Virtual Real 8(4):222–230" href="/article/10.1007/s10055-017-0312-5#ref-CR12" id="ref-link-section-d11386e442">2005</a>) presented a system that incorporates user body-pose estimation, gesture recognition, and speech recognition for interaction in virtual reality environments. Hoste et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hoste L, Dumas B, Signer B (2012) SpeeG: a multimodal speech-and gesture-based text input solution. In: Proceedings of the international working conference on advanced visual interfaces. ACM, pp. 156–163" href="/article/10.1007/s10055-017-0312-5#ref-CR18" id="ref-link-section-d11386e445">2012</a>) developed a multimodal speech and body gesture-based text input system targeting media centers, set-top boxes and game consoles. The evaluation of the prototype has revealed that after a minimal learning phase one can achieve low error rates for a text input speed of about six words per minute.</p><h3 class="c-article__sub-heading" id="Sec6">Technique comparisons</h3><p>Comparative evaluations of input techniques have revealed some strong points and weaknesses. Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154–2158" href="/article/10.1007/s10055-017-0312-5#ref-CR4" id="ref-link-section-d11386e457">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Bowman DA, Wingrave CA, Campbell JM, Ly VQ, Rhoton CJ (2002b) Novel uses of Pinch Gloves™ for virtual environment interaction techniques. Virtual Real 6(3):122–129" href="/article/10.1007/s10055-017-0312-5#ref-CR5" id="ref-link-section-d11386e460">b</a>) compared four input techniques: a pinch keyboard, a pen and tablet keyboard, a chord keyboard, and a speech-based approach. The results showed that while the speech-based technique was the fastest it also produced more errors than the others. The pen and tablet keyboard produced the least number of errors. However, it also produced high levels of arm strain. The pinch keyboard was characterized by users as a natural technique, but its performance was not at the level of the other two techniques. Overall the experiment showed that none of the techniques tested was clearly the best for text input.</p><p>MacKenzie and Soukoreff (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hum Comput Interact 17(2–3):147–198" href="/article/10.1007/s10055-017-0312-5#ref-CR27" id="ref-link-section-d11386e466">2002</a>) presented a survey of mobile text entry techniques, both in research papers and in commercial products. González et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118" href="/article/10.1007/s10055-017-0312-5#ref-CR16" id="ref-link-section-d11386e469">2009</a>) described six different text input techniques and their evaluation. They compare mobile phone keyboard, chord keyboard, pinch keyboard, pen-based QWERTY keyboard, pen-based disk keyboard, and handwritten character recognition. From these techniques, the mobile phone keyboard offered high typing speed and low typing errors, while on the other hand the handwritten character recognition did not perform as good as expected. It has to be noted that for the experiments which employed a mobile phone keyboard, the authors represented user’s hands with yellow cubes and embossed labels were stuck onto each physical key surface to help users feel and recognize the keyboard keys.</p><p>In Lepouras (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245" href="/article/10.1007/s10055-017-0312-5#ref-CR26" id="ref-link-section-d11386e475">2009</a>) two numeric input techniques were compared: a gesture and a pinch glove based. The first employed whole-hand gloves gestures and the second pinch gloves. While the first technique was proper only for small numbers and for limited use, the second technique performed well in all cases with input rate comparable to input techniques such as pinch keyboard and chord keyboard (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154–2158" href="/article/10.1007/s10055-017-0312-5#ref-CR4" id="ref-link-section-d11386e478">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Bowman DA, Wingrave CA, Campbell JM, Ly VQ, Rhoton CJ (2002b) Novel uses of Pinch Gloves™ for virtual environment interaction techniques. Virtual Real 6(3):122–129" href="/article/10.1007/s10055-017-0312-5#ref-CR5" id="ref-link-section-d11386e481">b</a>). One possible enhancement of the pinch glove technique would be to include capabilities for reading bending data as also suggested in (LaViola and Zeleznik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola J, Zeleznik R (1999) Flex and pinch: a case study of whole hand input design for virtual environment interaction. In: Proceedings of the second IASTED international conference on computer graphics and imaging, pp. 221–225" href="/article/10.1007/s10055-017-0312-5#ref-CR24" id="ref-link-section-d11386e484">1999</a>). This would permit the use of simple touch gestures without the need for recognizing continuous gestures and at the same time would provide accurate visualization of the physical interface to users.</p><p>Natapov et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Natapov D, Castellucci SJ, MacKenzie IS (2009) ISO 9241-9 evaluation of video game controllers. In: Proceedings of graphics interface 2009. Canadian Information Processing Society, pp. 223–230" href="/article/10.1007/s10055-017-0312-5#ref-CR29" id="ref-link-section-d11386e490">2009</a>) compared video game controllers for point-select tasks and found that the standard mouse performed significantly better than Nintendo Wii and Nintendo Classic Controller. However, the standard mouse cannot be used in situations where the user is standing. Oshita and Ishikawa (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Oshita M, Ishikawa H (2012) Gamepad vs. touchscreen: a comparison of action selection interfaces in computer games. In: Proceedings of the workshop at SIGGRAPH Asia. ACM, pp 27–31" href="/article/10.1007/s10055-017-0312-5#ref-CR32" id="ref-link-section-d11386e493">2012</a>) compared gamepad and touch screen interfaces for action selection tasks and show that the touch screen interface achieved better than or similar results to the gamepad interface. Ardito et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ardito C, Buono P, Costabile MF, Lanzilotti R, Simeone AL (2009) Comparing low cost input devices for interacting with 3D virtual environments. In: 2nd conference on human system interactions, 2009. HSI’09. IEEE, pp. 292–297" href="/article/10.1007/s10055-017-0312-5#ref-CR1" id="ref-link-section-d11386e496">2009</a>) tested low-cost input devices for 3D environments to conclude that the gamepad seems to be the most appropriate device to use in settings where the use of mouse and keyboard is unpractical.</p><p>Brown et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Brown M, Kehoe A, Kirakowski J, Pitt I (2015) Beyond the gamepad: HCI and game controller design and evaluation. In: Bernhaupt R (ed) Game user experience evaluation. Springer International Publishing, Berlin, pp 263–285" href="/article/10.1007/s10055-017-0312-5#ref-CR8" id="ref-link-section-d11386e503">2015</a>) presented the findings of a case study comparing and evaluating three different game control techniques: gamepad, keyboard, and force feedback steering wheel. They notice that “although the gamepad comes out above the steering wheel and the keyboard on usability performance measures, the keyboard has the advantage that it is regarded as very easy to learn.”</p><p>Lately a number of researchers focused on the use of virtual devices. For example, (Baldauf et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Baldauf M, Fröhlich P, Adegeye F, Suette S (2015) Investigating on-screen gamepad designs for smartphone-controlled video games. ACM Trans Multimed Comput Commun Appl (TOMM) 12(1s):22" href="/article/10.1007/s10055-017-0312-5#ref-CR3" id="ref-link-section-d11386e509">2015</a>) conducted a comparative laboratory study to investigate four smartphone gamepads inspired by traditional game controllers and mobile game controls (directional buttons, directional pad, floating joystick, tilt control). Their research showed the potential of using mobile devices as controlling devices for casual or spontaneous gaming; however, the user has to look on the device to control the interaction.</p><p>Summarizing the findings of the previously presented work, one may conclude the following. Speech-based systems can offer fast input, without the need for the user to hold or wear a special device, but they may suffer in noisy environments, or in rooms with problematic acoustics. Additionally, speaking to a computer may feel odd to some users (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154–2158" href="/article/10.1007/s10055-017-0312-5#ref-CR4" id="ref-link-section-d11386e515">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Bowman DA, Wingrave CA, Campbell JM, Ly VQ, Rhoton CJ (2002b) Novel uses of Pinch Gloves™ for virtual environment interaction techniques. Virtual Real 6(3):122–129" href="/article/10.1007/s10055-017-0312-5#ref-CR5" id="ref-link-section-d11386e518">b</a>). Device-dependent techniques on the other hand force the user to hold a special device. Research so far reveals that keyboard-based technique offer fast input in comparison with gesture-based techniques. However, this may not be applicable to the extent noted before. New mobile phones offer virtual keyboards instead of the old keyboard type, and this may affect the user’s performance. Game device studies so far have shown mixed results. Depending on the technique, game controller-based techniques can be easy to learn and offer fast input rate, comparable but probably not better than the typical keyboard. When a numeric keyboard cannot be readily employed, one may opt to use a gamepad device, which may prove easy to use and lightweight to carry. Finally, if input rate is not of the essence, gesture-based techniques may offer a viable—and if designed properly—an easy to learn alternative. Nevertheless, most gesture-based techniques can be difficult to implement and cumbersome for users for extended use. A notable exemption may be the use of pinch gloves, which require only touch of the fingertips.</p><p>Based on these findings, we have designed, implemented, evaluated, and redesigned input techniques focused on numeric input. The techniques proposed address the problem of character input for a character set of limited size. To this end, the solutions can also be applied to other small-alphabet domains such as numbered menu items or simple system control commands. We chose a numeric keyboard-based technique as a number of researchers have already implemented keyboard-based techniques, albeit with different types of keyboards, and can be used as a reference to other techniques. From the category of gesture-based techniques, we choose to extent a pinch glove technique with accurate visualization of the physical interface, as the combined simplicity of the technique with the improved visualization may offer faster input rates, which most gesture techniques lack. Finally, we elect to experiment with a game device. As noted earlier, game devices have so far offered mixed results. We feel that a device such as a wireless gamepad can be used to implement an input technique that is easy to learn and offers comparable input rates to the keyboard. A wireless gamepad is light weight and easy to carry, while its shape offers direct means for manipulating its controls (i.e., affordances as defined in Norman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Norman DA (2013) The design of everyday things, revised and expanded edition. Basic Books, New York, p 368. ISBN-13:978-0465050659" href="/article/10.1007/s10055-017-0312-5#ref-CR31" id="ref-link-section-d11386e524">2013</a>) p. 11 “An affordance is a relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used.”) without a need for visualization of the physical interface.</p><p>Based on the findings in (LaViola and Zeleznik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola J, Zeleznik R (1999) Flex and pinch: a case study of whole hand input design for virtual environment interaction. In: Proceedings of the second IASTED international conference on computer graphics and imaging, pp. 221–225" href="/article/10.1007/s10055-017-0312-5#ref-CR24" id="ref-link-section-d11386e530">1999</a>) and (Lepouras <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245" href="/article/10.1007/s10055-017-0312-5#ref-CR26" id="ref-link-section-d11386e533">2009</a>), one may hypothesize that:</p>
                  <h3 class="c-article__sub-heading">
                    <b>H</b>
                    <sub>
                      <b>1</b>
                    </sub>
                  </h3>
                  <p>A more accurate visual feedback will improve input rate for pinch glove technique</p>
                <p>Based on the review on numeric keyboard-based techniques and on gamepads, we can make the following hypotheses:</p>
                  <h3 class="c-article__sub-heading">
                    <b>H</b>
                    <sub>
                      <b>2</b>
                    </sub>
                  </h3>
                  <p>A numeric keyboard would offer the fastest input rate</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H</b>
                    <sub>
                      <b>3</b>
                    </sub>
                  </h3>
                  <p>Gamepad would be easier to use</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H</b>
                    <sub>
                      <b>31</b>
                    </sub>
                  </h3>
                  <p>Users will prefer the gamepad</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H</b>
                    <sub>
                      <b>32</b>
                    </sub>
                  </h3>
                  <p>Gamepad technique will be easier to learn</p>
                </div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Proposed methods</h2><div class="c-article-section__content" id="Sec7-content"><h3 class="c-article__sub-heading" id="Sec8">Pinch: data gloves input method</h3><p>Since pinch gloves report only contact between fingers (single touch), they cannot be employed to recognize gestures. To this end, we extended the technique described in (Lepouras <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245" href="/article/10.1007/s10055-017-0312-5#ref-CR26" id="ref-link-section-d11386e643">2009</a>) by synthesizing a pair of 5DT gloves with a pair of pinch gloves. 5DT gloves provide bending data to cater for real-time visualization of fingers flexing in the virtual environment. The user wears the 5DT gloves first and then the pinch gloves on top. A pair of trackers offers the necessary data for monitoring hand movement, while numeric input is implemented as previously using touch between fingers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Setup of pinch, 5DT gloves with Polhemus trackers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Using the index of the dominant hand, a user touches the fingers of the non-dominant hand starting from the thumb to denote one to pinky to denote five, and with the middle finger of the dominant hand (if the number is greater than five) to count from six to zero (i.e., 6, 7, 8, 9, and 0).</p><p>To minimize the set of gestures, minus sign and decimal point can be denoted by the same gesture, by touching the two pinky fingers. If this is the first digit, then it denotes a minus sign otherwise it denotes the decimal point. The backspace gesture was assigned to both thumbs touching since it is a gesture that is not easy to perform accidentally. Finally, start and completion of the input procedure is denoted by thumb and index touch, a gesture associated with ‘Okay’ sign in many cultures. This gesture also signifies the dominant hand, alleviating the need for additional initialization gestures. The next table summarizes the list of gestures (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0312-5#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 List of gestures</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0312-5/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Usage scenario</h4><p>A simple scenario illustrating the technique is the following. Assuming the user wants to enter number −1.07. The user would start by touching the thumb and the index finger of the dominant hand to initiate the input procedure and denote the dominant hand. The system would respond by displaying “ready” and the user would touch both pinkies to denote minus sign, then touch with the index finger of the dominant hand touch the thumb of the other hand to denote 1. The user would then touch both pinkies to denote the decimal point, touch the middle finger of the dominant hand with the pinky of the non-dominant to denote 0, and then touch the middle of the dominant hand to the index finger of the other hand to denote 7. Finally the user would touch the index and the thumb of the dominant hand to denote the termination of the input procedure.</p><h3 class="c-article__sub-heading" id="Sec10">Numeric keyboard input method</h3><p>For the numeric keyboard input technique, a small wireless numeric keyboard was employed. A 3D model of the keyboard was introduced in the virtual environment to match the real-world object. To offer the user visual feedback, the user wore a 5DT glove on the dominant hand. Both keyboard and glove were tracked in real time using a Polhemus Patriot tracker to capture the hand’s and the keyboard’s position and visualize them in the virtual environment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>User’s view of numeric keyboard and virtual hand</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Usage scenario</h4><p>A simple scenario illustrating the technique is the following. Assuming the user wants to enter number −3.84. The user would start by touching any button on the numeric keyboard to initiate the input procedure. The system would respond by displaying “ready” and the user would then key-in one digit at a time. Finally, the user would hit Enter to denote the termination of the input procedure (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Actual numeric keyboard</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Prior to running the experimental evaluation, we performed a keystroke level model analysis (Card et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Card SK, Moran TP, Newell A (1980) The keystroke-level model for user performance time with interactive systems. Commun ACM 23(7):396–410" href="/article/10.1007/s10055-017-0312-5#ref-CR9" id="ref-link-section-d11386e848">1980</a>) of the method. Since the user has simply to press the corresponding key to input a digit, the analysis predicts that the user will need time to think once in the beginning of the action and one key-press to enter it and Enter to finish. Assuming an average typist and a five digit number, total time to input the number would equal time to think (mental preparation, <i>T</i>
                        <sub>M</sub>) and time to press the five digit plus the Enter key (<i>T</i>
                        <sub>K</sub>): <i>T</i>
                        <sub>Total</sub> = <i>T</i>
                        <sub>M</sub> + 5 <i>T</i>
                        <sub>K</sub> + <i>T</i>
                        <sub>K</sub> = <i>T</i>
                        <sub>K</sub> + 6 <i>T</i>
                        <sub>K</sub>. Substituting with the values found in (Card et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Card SK, Moran TP, Newell A (1980) The keystroke-level model for user performance time with interactive systems. Commun ACM 23(7):396–410" href="/article/10.1007/s10055-017-0312-5#ref-CR9" id="ref-link-section-d11386e894">1980</a>) <i>T</i>
                        <sub>Total</sub> = 1.35 + 6 * 0.28 = 3.03 s per number or 0.5 s per character (corresponding to 120 characters per minute), while the worst case (for a novice user) is 1.35 + 8.40 = 8.55 s per number or 1.425 per character or 42.1 characters per minute.</p><h3 class="c-article__sub-heading" id="Sec12">Gamepad input method</h3><p>For the gamepad input technique, a small wireless gamepad was employed. No 3D model of the device was introduced in the virtual environment as it was deemed that the ergonomic design of the device guides the user without the need of a visual representation. To offer the user visual feedback, a simple numeric display was showing the digit currently selected as well as the two previous and the next two with smaller font in a circular list (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig4">4</a>). The correspondence between gamepad keys and actions is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>User’s view for gamepad input method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Assignment of gamepad keys. <i>a</i> Display previous character, <i>b</i> display next character, <i>c</i> select character, <i>d</i> delete last character, <i>e</i> start/finish input procedure, <i>f</i> up and <i>g</i> down keys used in redesigned version of the technique, <i>h</i> used in second version of redesigned technique instead of keys <i>a</i>, <i>b</i>, <i>f</i> and <i>g</i>
                                </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Usage scenario</h4><p>A simple scenario illustrating the method is the following, supposing the user wants to enter number 7.83. The user would start by pressing any button on the gamepad to initiate the input procedure. The system would respond by displaying the number display, and the user would then press button (a) or button (b) to move down or up the list to find the right digit. Finally the user would hit button (e) to denote the termination of the input procedure.</p><p>Prior to running the experimental evaluation, we performed a keystroke level model analysis (Card et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Card SK, Moran TP, Newell A (1980) The keystroke-level model for user performance time with interactive systems. Commun ACM 23(7):396–410" href="/article/10.1007/s10055-017-0312-5#ref-CR9" id="ref-link-section-d11386e1002">1980</a>) of the method. Since the user has to select one out of twelve characters (10 number digits, minus sign and decimal sign), the analysis predicts that the user will need time to think and a maximum of six key-presses back or forth to find the digit and one key-press to enter it. Assuming an average typist and assuming the time needed for pressing the gamepad keys correspond to that of the keyboard’s keys <i>T</i>
                        <sub>Total</sub> = <i>T</i>
                        <sub>M</sub> + 6 <i>T</i>
                        <sub>K</sub> + <i>T</i>
                        <sub>K</sub> = <i>T</i>
                        <sub>K</sub> + 7 <i>T</i>
                        <sub>K</sub> = 1.35 + 7 * 0.28 = 3.31 s per character (corresponding to 18.13 characters per minute), while the worst case (for a novice user) is 1.35 + 8.40 = 9.75 s per character.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Initial user testing</h2><div class="c-article-section__content" id="Sec14-content"><p>In order to identify strengths and weaknesses of all methods, a controlled experiment took place. Four male and two female users participated in this initial user testing. All the users had no prior experience with the gloves, three users had used a gamepad before to play games, and all users had used numeric keyboards before, albeit as part of a 102-key keyboard and not as a separate wireless device. A program implementing each technique was developed using Vizard Toolkit. The setup included a PC with a pair of 5DT gloves and a pair of pinch gloves, eMagin 3DVISOR goggles, Polhemus Patriot system. To connect to pinch gloves, VRPN (Taylor et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001). VRPN: a device-independent, network-transparent VR peripheral system. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, pp 55–61" href="/article/10.1007/s10055-017-0312-5#ref-CR37" id="ref-link-section-d11386e1047">2001</a>) was used. The program displayed 12 numbers, one at a time, which users had to key-in.</p><p>The program recorded the time required for inputting each number, the number of corrections made as well as whether the user managed to input it correctly. The set of numbers included positive and negative integers and real numbers and was created to contain numbers with digits uniformly distributed in the range of [1, 2, 3, 4, 5] and [6, 7, 8, 9, 0], with 3 to 9 characters length and mean length of four characters. The first three numbers were considered to be the user-training set, and the results from keying-in the rest nine numbers were used for statistical purposes. The experiments were performed over three days. Users inputted all numbers first with pinch—5DT gloves configuration, then with the numeric keyboard, and finally with the gamepad. Prior to the experiment, each user was briefed in the way each technique worked. After the experiment, users were interviewed to record their comments. Since the goggles do not fully occlude, the user’s view subjects were instructed not to look down to their hands, as it would affect their performance. From our observations during the experiments, we noticed they adhered to the request.</p><p>From this initial evaluation, some preliminary observations can be drawn. With the first technique (pinch glove) a mean value of 24.17 chars/min (S.D. was 7.85) was achieved, while the second (numeric keyboard) achieved a mean value of 42.57 chars/min (S.D. was 19.90) and the third (gamepad) a mean value of 25 chars/min (S.D. was 11.51).</p><p>The use of 5DT gloves for the first technique did not seem to improve input rate to that observed in (Lepouras <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245" href="/article/10.1007/s10055-017-0312-5#ref-CR26" id="ref-link-section-d11386e1059">2009</a>) (25 chars/min). Furthermore, the user comments remained similar. In addition to the pinch glove size comments made previously, there were comments on the setup complexity (having to wear two sets of gloves, too many cables from the two sets and the trackers). Two users actually felt that the visual representation accuracy was somewhat misleading as the gloves moves and virtual hand models did not fully coincide. This was due to both the accuracy of the 5DT glove which (after extended use) could report a finger as semi-bent when it was fully bent and the accuracy of the 3D model. The latter is not usually a problem, but this technique relied on tracking the real hands and representing them in the virtual environment with 3D models. Any difference between the real and virtual worlds became apparent when the user tried to touch the virtual hands and understood that the real hands were not as big or as small. The end result was that the actual hands could touch when the virtual did not and vice versa. Depending on the user, the difference could be negligible or significant, especially, for users with very small or very big hands. From the user responses, it seems that the new setup initially increased expectations, but actually disappointed them as they felt they could not rely on the visual feedback.</p><p>User comments for the second technique were more positive. While some of the comments regarding the hand 3D model remained, users were overall happier with the setup. Overall the input rate was better than the other two techniques, however, lower than the rate observed in (González et al.) (mean input rate greater than 60 cpm). The third technique received the least number of negative comments. The 25 characters per minute are better than the rate calculated with KLM method (~18 cpm for an average typist), but still far less than the keyboard technique. All users agreed that is was the most enjoyable of the three techniques, with two of the users commenting it felt like playing a game.</p><p>Based on the results of this initial evaluation, we cannot either accept or refute H<sub>1</sub>. Τhe addition of accurate visual feedback to the gesture technique did not improve results observed in (Lepouras <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245" href="/article/10.1007/s10055-017-0312-5#ref-CR26" id="ref-link-section-d11386e1071">2009</a>). However, based on user comments in regard to the complexity of the setup, we decided to not fully evaluate this technique. For the device-based techniques, we decided to make improvements based on the results of this initial evaluation and run the experimental evaluation for these two techniques with a larger set of subjects. One observation was that the input rate observed for the second technique was much lower than the one observed in (González et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118" href="/article/10.1007/s10055-017-0312-5#ref-CR16" id="ref-link-section-d11386e1074">2009</a>). Taking into consideration the differences in the setups as well as user comments, we assumed that hand gesture tracking could have been the crucial factor. To this end, we altered the second technique to resemble the setup described in (González et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118" href="/article/10.1007/s10055-017-0312-5#ref-CR16" id="ref-link-section-d11386e1077">2009</a>), which did not offer visual feedback for the user’s hand. In the new version the numeric keyboard and the user’s hand are not tracked anymore, and we do not offer a visual representation (i.e., 3D model) of the user’s hand. We do, however, keep the 3D model of the keyboard. To further support identification of the keyboard keys, embossed labels were stuck onto “1”, “3”, “7” and “9” keys. In contrast to the setup described in (González et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118" href="/article/10.1007/s10055-017-0312-5#ref-CR16" id="ref-link-section-d11386e1080">2009</a>), the keys layout followed that of the numeric keyboard and not that of the mobile phone keyboard.</p><p>The third technique was also altered to minimize key-presses required to select a number and offer an improved visual feedback. A virtual keypad was introduced and the user was able to move left–right (keys <i>a</i> and <i>b</i> of gamepad) and up–down (keys <i>f</i> and <i>g</i> of gamepad). Movement left–right and up–down was circular, i.e., if the user pressed left key three times, she would end up at the same digit. A second version of this technique was also implemented. This version employed the right stick of the gamepad for moving up–down and left–right at the virtual keypad, instead of keys <i>a</i>, <i>b</i>, f and <i>g</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>User’s view for redesigned gamepad input method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Revised methods evaluation</h2><div class="c-article-section__content" id="Sec15-content"><p>For this evaluation, a total of 22 users participated: 19 males and 3 females (different from the first set of subjects). Most of the subjects (18 out of 22) were undergraduate students. In this setup, users had to input 14 numbers in total. The numbers varied in length from two characters to ten with six numbers containing more than four characters, ten of the numbers were positive and four negative, eight were integers and six of the numbers had decimal parts. This setup created three number categories: short versus long number (short up to 4 characters, long with five or more characters), positive versus negative, integers versus numbers with decimal part.</p><p>To test whether there was a learning curve, the first four numbers, one from each category, were considered to be a training phase, while users were getting accustomed to the techniques. During the training phase both time and number of corrections (as counted by backspace presses) were recorded to offer an assessment for the importance of learning in user’s performance. Standing up, each user tried all techniques (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig7">7</a>). The techniques order was randomized according to Williams Design (Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1949" title="Williams EJ (1949) Experimental designs balanced for the estimation of residual effects of treatments. Aust J Sci Res Ser A 2:149–168" href="/article/10.1007/s10055-017-0312-5#ref-CR39" id="ref-link-section-d11386e1143">1949</a>). To minimize learning effects, the sequence of techniques per user was chosen at random. Apart from these changes, the experiment setup remained the same as in the initial evaluation. Data recorded included input time, corrections per number (one error was attributed to one press of the backspace key), and successful input. At the end of its experimental run, a structured interview took place, to capture the subject’s view. A translated to English version of the questionnaire is available at hci-vr.dit.uop.gr/VIRE_questionnaire_v1_en.pdf (last accessed June 2016). Once all subjects performed the experiment, a statistical analysis was carried out on the collected data to reveal possible trends.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>User trying the redesigned gamepad input method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            </div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Statistical analysis</h2><div class="c-article-section__content" id="Sec16-content"><h3 class="c-article__sub-heading" id="Sec17">Input rate per method</h3><p>We first checked digit input rate per technique to see whether there was statistical difference. Overall the input rate was 1.57 s per character (38.15 chars per minute, S.D. was 17.70) for numeric keyboard technique, 2.06 s per character (28.99 chars per minute, S.D. was 11.80) for button-based gamepad technique, and 1.80 s per character (33.34 chars per minute, S.D. was 6.81) for stick-based gamepad technique as presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig8">8</a>. To compare between methods Bartlett statistic was used to test whether there was statistically significant difference between standard deviations (SD) of each method, as ANOVA assumes that data are sampled from populations with identical SDs. The result was statistically significant (<i>p</i> = 0.0052), so we performed analysis using a nonparametric test. We employed Friedman test, which returned a <i>p</i> value of 0.0036 (<i>x</i>
                    <sup>2</sup>(2) = 11.273), considered significant, indicating variation between columns was considerably greater than expected by chance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Mean input time (s) per digit and std. error during and after learning for numerical keyboard method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To identify which columns produced statistically significant results we performed Dunn’s multiple comparisons test. The test revealed that the statistically significant difference was between numerical keyboard method and gamepad first method (<i>p</i> &lt; 0.001). There was no statistically significant difference between the two gamepad methods, although users seemed to perform better with the second method.</p><h3 class="c-article__sub-heading" id="Sec18">Learning curve</h3><p>Next, we checked whether there was statistically significant difference between the input rate of the first four numbers (considered to be a learning set) and the rest of the numbers.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Numerical keyboard</h4><p>We performed paired <i>t</i> test to test for statistical significance between the input rate during and after the first set of numbers. We also checked whether pairing was effective (one-tailed <i>p</i> value &lt;0.0001 suggesting it was) and used Kolmogorov–Smirnov method to test for normality (test passed, Kolmogorov–Smirnov distance (KS) was 0.10). Overall, the values during and after training did not differ significantly (mean difference was 0.1416, the 95% confidence interval of the difference was −0.05370 to 0.3370).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Gamepad first method</h4><p>We performed paired <i>t</i> test to test for statistical significance between the input rate during and after the first set of numbers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig9">9</a>). We also checked whether pairing was effective (one-tailed <i>p</i> value was 0.0011 suggesting it was) and used Kolmogorov–Smirnov method to test for normality. Since the first group of data did not pass KS test, we employed a nonparametric test (Wilcoxon matched pairs test). There was a statistically significant difference during and after the first set of numbers (two-tailed <i>p</i> value was 0.0053). Nonparametric Spearman correlation coefficient was 0.5919, signifying the pairing was effective.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Mean input time (s) per digit and std. error during and after learning for gamepad first and gamepad second methods</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Gamepad second method</h4><p>Again we performed paired <i>t</i> test to test for statistical significance between the input rate during and after the first set of numbers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig9">9</a>). As previously, we checked whether pairing was effective (correlation coefficient was 0.4902, one-tailed <i>p</i> value was 0.0103 suggesting it was) and used Kolmogorov–Smirnov method to test for normality (test passed, Kolmogorov–Smirnov distance (KS) was 0.13, the <i>p</i> value was &gt;0.10, the data passed the normality test with <i>p</i> &gt; 0.05). There was a statistically significant difference during and after the first set of numbers (two-tailed <i>p</i> value was 0.0003, <i>t</i> = 4.336).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Between methods during and after first set of numbers</h4><p>We also compared input rate between methods during and after first set of numbers and found there was statistically significant difference in both cases (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig10">10</a>), employing Friedman test. In the first case <i>p</i> value was 0.0018 (<i>x</i>
                        <sup>2</sup>(2) = 12.636) and in the second 0.0122 (<i>x</i>
                        <sup>2</sup>(2) = 8.818).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Comparison of mean input time (s) per digit and std. error for each input method, during and after first set of numbers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We then performed Dunn’s multiple comparisons test to detect which columns produced statistically significant results. During the first set of numbers, there was statistical difference in input rate between numerical keyboard and both of the gamepad methods (<i>p</i> &lt; 0.01 keyboard vs. first technique and <i>p</i> &lt; 0.05 keyboard vs. second technique), but there was no statistical difference between the two gamepad methods. After the first set of numbers, there was statistical difference in input rate between numerical keyboard and the first of the gamepad methods (<i>p</i> &lt; 0.05), but this time no statistical difference between numerical keyboard and the second of the gamepad methods and no statistical difference between the two gamepad methods.</p><h3 class="c-article__sub-heading" id="Sec23">Input rate per number type</h3><p>We then tested to see if there were differences in input rate between different types of numbers (positive, negative, numbers with decimal, numbers with more than four characters).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Numerical keyboard</h4><p>We first checked input rates for different types of numbers for the wireless numeric keyboard as there seemed to be considerable variations per number type (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig11">11</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Mean input time (s) per digit and std. error for numerical keyboard method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We performed repeated measures analysis of variances and employed Friedman test, as Column C (decimal) failed normality test. The <i>p</i> value was &lt;0.0001 (Friedman statistic was 78.727), indicating variation among column medians was significantly greater than expected by chance. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig11">11</a> illustrates the pairs that produced statistically significant results.</p><p>The main result one should note is that in all cases there was a statistically significant difference between input rate of negative numbers and all other types of numbers, which will be further elaborated on in the Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec33">7</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Gamepad first method</h4><p>We checked input rates for different types of numbers for gamepad first method as there were variations per number type (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Mean input time (s) per digit and std. error for gamepad first method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We performed repeated measures analysis of variances and employed Friedman test, as Column C (decimal) failed normality test. The <i>p</i> value was &lt;0.0001 (Friedman statistic was 40.675), indicating variation among column medians was significantly greater than expected by chance. What shall be noted is that in most cases there was not a statistically significant difference apart from negative numbers.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Gamepad second method</h4><p>We checked input rates for different types of numbers for gamepad second method as there were some considerable variations per number type (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig13">13</a>)</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0312-5/MediaObjects/10055_2017_312_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Mean input time (s) per digit and std. error for gamepad second method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0312-5/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Since all columns passed normality test (normality test <i>p</i> value &gt;0.10), we employed standard parametric methods. The <i>p</i> value was &lt;0.0001, indicating variation among column medians was significantly greater than expected by chance. We used Tukey–Kramer multiple comparisons tests to identify the pairs that produced statistically significant results (illustrated at Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0312-5#Fig13">13</a>). The intermediate calculations produced <i>F</i> = 8.184, and for testing whether the matching effective <i>F</i> was 12.449 and <i>p</i> value is &lt;0.0001, signifying matching was effective. We also tested with Bonferroni multiple comparisons test, which is too conservative with a large number of comparisons. The test identified the same pairs of columns.</p><p>Overall results were better than the first gamepad method, but again with no statistically significant difference between most different types, except for the case of numbers with decimal.</p><h3 class="c-article__sub-heading" id="Sec27">Number of corrections</h3><p>Finally, we analyzed number of corrections as indicated by backspace presses (one key-press corresponding to one correction). There existed considerable differences both between methods and between subjects (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0312-5#Tab2">2</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Number of corrections for each method</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0312-5/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We performed Friedman test to determine whether the differences in the number of corrections between methods is statistically significant. The <i>p</i> value was 0.0020 (<i>x</i>
                    <sup>2</sup>(2) = 12.463), considered significant, indicating that variation among column medians was significantly greater than expected by chance. Friedman statistic was 12.463. To identify which columns produced statistically significant results, we performed Dunn’s multiple comparisons test. While the median for errors for the gamepad second method is far less than the other two methods, there was statistical significance only between the numeric keyboard method and the second gamepad technique (<i>p</i> &lt; 0.01).</p><p>We also checked to see whether training had an effect on the number of errors, but no statistical significance was found in the number of corrections during and after training for any method. No statistical significance was detected for the number of corrections or number of wrong input (user entering wrong number) per number type (after analyzing the input rate results we were expecting some statistically significant results at least in the correction rate for negative numbers when the user was employing the numeric keyboard technique).</p><h3 class="c-article__sub-heading" id="Sec28">User response</h3><p>The analysis of the user questionnaire responses showed no statistically significant difference in the rating of the techniques. However, some of the comments were worth noting.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec29">Keyboard</h4><p>Most negative comments were on the numeric keyboard’s layout, especially on the position of minus and backspace keys. Users felt that the position of minus key was difficult to find as it was further away from the core of numeric keys. The same was also true for backspace key, whenever was needed. On the other hand, only three users thought that a keyboard layout similar to mobile phone’s would have been better. When asked most users responded that their touch screen phones they were used to, had virtual keyboards not similar to the old mobile phone’s layout. On the positive side almost all users thought it was the fastest method for entering numbers.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec30">Gamepad first method</h4><p>Comments for the gamepad first method were more on the negative side. Users with previous experience from games found the choice of keys for input odd. Depending on the game subjects were used to play, they would have prepared different keys for entering digits or completing the number input procedure. All left-handed and even some right-handed subjects would have preferred the left keys for selecting a digit on the screen. On the positive side, almost all subjects noted that the device offered an easy, rather fast alternative to numeric keyboard that did not require visualization of the physical interface to know where keys were located.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec31">Gamepad second method</h4><p>Comments for the gamepad second method were rather on the positive side. The choice of the gamepad stick seemed to alleviate most of the problems and consequent negative comments of the users. The only comment made by about half of the users was that they would have preferred the stick press down for entering selected digits instead of a separate key (which was actually tried during initial design but rejected due to the stick’s sensitivity). Overall, users found that this method was very easy to learn, almost as fast as the numeric keyboard and the most usable of the three.</p><h3 class="c-article__sub-heading" id="Sec32">Hypotheses assessment</h3><p>In regard to the hypotheses the statistical analysis shows that H<sub>2</sub> can be partly accepted. The numeric keyboard proved faster than both gamepad methods during training, but in total it was faster only in comparison with the first gamepad method. H<sub>3</sub> cannot be readily accepted. While users performed the least number of corrections when using the second gamepad-based method, the method required some (minimal) training (so H<sub>32</sub> is rejected). User comments were rather positive for the second gamepad method, but again there was no statistically significant difference to the rating of the other methods, therefore H<sub>31</sub> cannot be accepted.</p></div></div></section><section aria-labelledby="Sec33"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33">Discussion</h2><div class="c-article-section__content" id="Sec33-content"><p>From these experiments, a number of conclusions can be drawn on the design and implementation of numeric input techniques for immersive virtual environments. In the first part of this experiment, we combined pinch and data gloves along with position tracking in an effort to gain the advantages of them all: the simplicity of touch gestures with the realism of finger and hand movement. However, this combination was not problem free. The real and the virtual environment did not always fully coincide leading to contradictory sensory information for users. Additionally, prolonged use led to user fatigue, making this method suitable mostly for performing simple, small gestures to input small numbers. The method would be advisable for a designer if the user needs to have both hands free during interaction.</p><p>The use of digital gloves in combination with the wireless numeric keyboard offered real-time capturing of the user’s hand movement and improved visualization but also made the hardware setup more complex. Furthermore, wearing a glove the user lost a part of her touch sense and was depending only on the visual interface to discern keys. Overall, the initial version with hand tracking produced comparable results with the second version. For the small sample of users, the initial version gave a mean 42.57 cpm (S.D. was 19.90), while the new version 38.14 cpm (S.D. was 17.70) mean rate for all type of numbers or 45.80 cpm (S.D. was 20.36) for positive numbers. These findings were also comparable to the ones reported by González et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118" href="/article/10.1007/s10055-017-0312-5#ref-CR16" id="ref-link-section-d11386e1712">2009</a>). The authors report a speed variance between 32.75 and 107.39 depending on the user’s experience with mobile phone keyboards. In our case familiarity with mobile phone keyboards may play a role, but it seems less important. All of the subjects had new mobile phones with touch screens and virtual keyboards, dissimilar to the old-style mobile phone keyboards, so we could not therefore try to correlate input rate with mobile phone keyboard experience. We can speculate that if we had chosen a mobile phone style of keyboard, subjects might have performed better but we cannot be sure. Our speculation is based on the mobile phone keyboard’s affordability rather than the users’ experience with them, which diminishes as new types of phones with virtual keyboards prevail. The classic mobile phone keyboard misses some extra keys (enter, backspace, minus, decimal) present in the numeric keyboard, which alters significantly the layout and the input scenario. To this end, one may be able to type fast on a mobile phone keyboard, Clarkson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Clarkson E, Clawson J, Lyons K, Starner T (2005) An empirical study of typing rates on mini-QWERTY keyboards. In: CHI’05 extended abstracts on human factors in computing systems. ACM, New York, pp. 1288–1291" href="/article/10.1007/s10055-017-0312-5#ref-CR11" id="ref-link-section-d11386e1715">2005</a>) report an input rate of over 31 wpm for a novice user. However, from the users’ responses only 3 out of 22 thought they would perform better with a mobile phone keyboard. The location of the “minus” key on the numeric keyboard seems to affect input rate significantly. The experiment showed that for a scenario where the user enters positive numbers, employing the basic set of numeric keyboard keys (0–9, Enter and “.”) of the numeric keyboard will offer the best input rate, especially if the user enters small (as to the number of digits) numbers.</p><p>If the scenario includes negative or large numbers, the second gamepad method will offer a uniform input rate across different types of numbers. In contrast to the numeric keyboard method, it requires some -even minimal- learning period, but especially after inputting the first few numbers input rate is very close to numeric keyboard’s. We feel that the method could produce even better results if the selection of keys is customized to fit user’s experience. Unfortunately, user responses showed that there is no standard set of keys. Depending on the user’s familiarity with specific games or game consoles the preferred set of keys would change. One has to note the difference in performance and in user comments between the two variations of the gamepad methods. To this end, a designer should offer different pre-assigned sets of keys to cater for different user preferences or offer a customization function for users. A designer should also take into consideration that users thought of the second gamepad method as the most usable and fun to use.</p><p>Overall, this study allows us to offer design guidelines for immersive virtual reality systems, when a need for developing a number input technique arises. Assuming the user wears an HMD and cannot readily employ a desktop device, the designer can opt for one of the following.</p><p>For fast input rate, if the user needs both hands free during interaction, one can select a speech interface. If the environment is noisy, the next solution would be to use a portable, numeric keyboard, which offers fast input with no need for training. If training time is not of the essence or the users’ preferences are known, the designer may offer a gamepad-based input technique, which can offer comparable input rates. Finally, if input rate is not first priority, as it would be when the user needs to enter only a few digits at a time, the designer can employ a gesture-based technique. To this end, a pinch glove can offer fast—but not as fast as the other techniques—input rate, with minimal learning curve. The following table summarizes these design guidelines (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0312-5#Tab3">3</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Summary of design guidelines</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0312-5/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec34"><div class="c-article-section" id="Sec34-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec34">Conclusions and future plans</h2><div class="c-article-section__content" id="Sec34-content"><p>This paper presents the design, implementation, evaluation, and redesign of input techniques for character sets of limited size with as special focus on numeric input. To some extent we confirmed previous findings with mobile phone keyboards, but we feel that as users get accustomed to new virtual keyboards one may have to lower expectations when it comes to input rate. Evaluation also showed that gestures, although used in everyday human to human communication, may not be the first choice for human–computer interaction when it comes to numeric input. In contrast, a gamepad device can be the basis for a portable, easy to learn input technique. Based on the evaluation results, the paper offers a set of guidelines for the selection of technique and discusses issues a designer should take into consideration. For example, one may assume that a keyboard offers fast input for all type of numbers (positive, negative, decimal, short or long). Evaluation showed this is not the case. A portable numeric keyboard seems to work better with positive short number. On the other hand, a gamepad device and the corresponding input method can offer uniform performance across different number types. These findings are examined in detail at the Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-017-0312-5#Sec33">7</a>.</p><p>User response for the gamepad-based method shows that familiar devices could be the hardware of choice for implementing input methods for virtual environments. This assumes that designers will pay extra effort in identifying user group’s background, defining their requirements and catering for their customization needs. We are planning to experiment with a Realsense device (<a href="http://click.intel.com/intelrrealsensetm-developer-kit-featuring-sr300.html">http://click.intel.com/intelrrealsensetm-developer-kit-featuring-sr300.html</a>, last accessed November 2016) to implement gesture-based techniques similar to those implemented with the data gloves. The use of such a device would save users of the burden of wearing gloves and trackers. We are also following a second plan aiming to improve the gamepad method exploiting user comments on the keys layout and additionally altering the layout of the virtual numeric keyboard to that of a pie menu. In this layout the pointer will be in the center of the circle and digits/actions arranged on the circle’s periphery. We are hoping that this change will increase input rate, as it will require only two key-presses to select any digit on the screen.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ardito C, Buono P, Costabile MF, Lanzilotti R, Simeone AL (2009) Comparing low cost input devices for interact" /><p class="c-article-references__text" id="ref-CR1">Ardito C, Buono P, Costabile MF, Lanzilotti R, Simeone AL (2009) Comparing low cost input devices for interacting with 3D virtual environments. In: 2nd conference on human system interactions, 2009. HSI’09. IEEE, pp. 292–297</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Bailly, J. Müller, E. Lecolinet, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Bailly G, Müller J, Lecolinet E (2012) Design and evaluation of finger-count interaction: Combining multitouch" /><p class="c-article-references__text" id="ref-CR2">Bailly G, Müller J, Lecolinet E (2012) Design and evaluation of finger-count interaction: Combining multitouch gestures and menus. Int J Hum Comput Stud 70(10):673–689</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2012.05.006" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20and%20evaluation%20of%20finger-count%20interaction%3A%20Combining%20multitouch%20gestures%20and%20menus&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=70&amp;issue=10&amp;pages=673-689&amp;publication_year=2012&amp;author=Bailly%2CG&amp;author=M%C3%BCller%2CJ&amp;author=Lecolinet%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Baldauf, P. Fröhlich, F. Adegeye, S. Suette, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Baldauf M, Fröhlich P, Adegeye F, Suette S (2015) Investigating on-screen gamepad designs for smartphone-contr" /><p class="c-article-references__text" id="ref-CR3">Baldauf M, Fröhlich P, Adegeye F, Suette S (2015) Investigating on-screen gamepad designs for smartphone-controlled video games. ACM Trans Multimed Comput Commun Appl (TOMM) 12(1s):22</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Investigating%20on-screen%20gamepad%20designs%20for%20smartphone-controlled%20video%20games&amp;journal=ACM%20Trans%20Multimed%20Comput%20Commun%20Appl%20%28TOMM%29&amp;volume=12&amp;issue=1s&amp;publication_year=2015&amp;author=Baldauf%2CM&amp;author=Fr%C3%B6hlich%2CP&amp;author=Adegeye%2CF&amp;author=Suette%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical " /><p class="c-article-references__text" id="ref-CR4">Bowman DA, Rhoton CJ, Pinho MS (2002a) Text input techniques for immersive virtual environments: an empirical comparison. In: Proceedings of the human factors and ergonomics society annual meeting, vol 46, no 26. SAGE Publications, pp. 2154–2158</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, CA. Wingrave, JM. Campbell, VQ. Ly, CJ. Rhoton, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Bowman DA, Wingrave CA, Campbell JM, Ly VQ, Rhoton CJ (2002b) Novel uses of Pinch Gloves™ for virtual environm" /><p class="c-article-references__text" id="ref-CR5">Bowman DA, Wingrave CA, Campbell JM, Ly VQ, Rhoton CJ (2002b) Novel uses of Pinch Gloves™ for virtual environment interaction techniques. Virtual Real 6(3):122–129</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs100550200013" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Novel%20uses%20of%20Pinch%20Gloves%E2%84%A2%20for%20virtual%20environment%20interaction%20techniques&amp;journal=Virtual%20Real&amp;volume=6&amp;issue=3&amp;pages=122-129&amp;publication_year=2002&amp;author=Bowman%2CDA&amp;author=Wingrave%2CCA&amp;author=Campbell%2CJM&amp;author=Ly%2CVQ&amp;author=Rhoton%2CCJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Bowman, E. Kruijff, JJ. LaViola, I. Poupyrev, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison-Wesley" /><p class="c-article-references__text" id="ref-CR6">Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2004) 3D user interfaces: theory and practice. Addison-Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20theory%20and%20practice&amp;publication_year=2004&amp;author=Bowman%2CDA&amp;author=Kruijff%2CE&amp;author=LaViola%2CJJ&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Braffort A (1996) A gesture recognition architecture for sign language. In: Proceedings of the second annual A" /><p class="c-article-references__text" id="ref-CR7">Braffort A (1996) A gesture recognition architecture for sign language. In: Proceedings of the second annual ACM conference on assistive technologies, ACM, pp. 102–109</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Brown, A. Kehoe, J. Kirakowski, I. Pitt, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Brown M, Kehoe A, Kirakowski J, Pitt I (2015) Beyond the gamepad: HCI and game controller design and evaluatio" /><p class="c-article-references__text" id="ref-CR8">Brown M, Kehoe A, Kirakowski J, Pitt I (2015) Beyond the gamepad: HCI and game controller design and evaluation. In: Bernhaupt R (ed) Game user experience evaluation. Springer International Publishing, Berlin, pp 263–285</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Game%20user%20experience%20evaluation&amp;pages=263-285&amp;publication_year=2015&amp;author=Brown%2CM&amp;author=Kehoe%2CA&amp;author=Kirakowski%2CJ&amp;author=Pitt%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SK. Card, TP. Moran, A. Newell, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Card SK, Moran TP, Newell A (1980) The keystroke-level model for user performance time with interactive system" /><p class="c-article-references__text" id="ref-CR9">Card SK, Moran TP, Newell A (1980) The keystroke-level model for user performance time with interactive systems. Commun ACM 23(7):396–410</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F358886.358895" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20keystroke-level%20model%20for%20user%20performance%20time%20with%20interactive%20systems&amp;journal=Commun%20ACM&amp;volume=23&amp;issue=7&amp;pages=396-410&amp;publication_year=1980&amp;author=Card%2CSK&amp;author=Moran%2CTP&amp;author=Newell%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cardoso A, Lamounier E, Lima G, Oliveira L, Mattioli L, Junior G, Silva A, Nogueira K, do Prado P, Newton J (2" /><p class="c-article-references__text" id="ref-CR10">Cardoso A, Lamounier E, Lima G, Oliveira L, Mattioli L, Junior G, Silva A, Nogueira K, do Prado P, Newton J (2013) VRCEMIG: a virtual reality system for real time control of electric substations. In: 2013 IEEE virtual reality (VR), IEEE, pp. 165–166</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Clarkson E, Clawson J, Lyons K, Starner T (2005) An empirical study of typing rates on mini-QWERTY keyboards. " /><p class="c-article-references__text" id="ref-CR11">Clarkson E, Clawson J, Lyons K, Starner T (2005) An empirical study of typing rates on mini-QWERTY keyboards. In: CHI’05 extended abstracts on human factors in computing systems. ACM, New York, pp. 1288–1291</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Demirdjian, T. Ko, T. Darrell, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipula" /><p class="c-article-references__text" id="ref-CR12">Demirdjian D, Ko T, Darrell T (2005) Untethered gesture acquisition and recognition for virtual world manipulation. Virtual Real 8(4):222–230</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-005-0155-3" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Untethered%20gesture%20acquisition%20and%20recognition%20for%20virtual%20world%20manipulation&amp;journal=Virtual%20Real&amp;volume=8&amp;issue=4&amp;pages=222-230&amp;publication_year=2005&amp;author=Demirdjian%2CD&amp;author=Ko%2CT&amp;author=Darrell%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Fels, GE. Hinton, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Fels SS, Hinton GE (1998) Glove-TalkII-a neural-network interface which maps gestures to parallel formant spee" /><p class="c-article-references__text" id="ref-CR13">Fels SS, Hinton GE (1998) Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls. IEEE Trans Neural Netw 9(1):205–212</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F72.655042" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Glove-TalkII-a%20neural-network%20interface%20which%20maps%20gestures%20to%20parallel%20formant%20speech%20synthesizer%20controls&amp;journal=IEEE%20Trans%20Neural%20Netw&amp;volume=9&amp;issue=1&amp;pages=205-212&amp;publication_year=1998&amp;author=Fels%2CSS&amp;author=Hinton%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Foong OM, Low TJ, Wibowo S (2008) Hand gesture recognition: sign to voice system (S2V). In: Proceedings of wor" /><p class="c-article-references__text" id="ref-CR14">Foong OM, Low TJ, Wibowo S (2008) Hand gesture recognition: sign to voice system (S2V). In: Proceedings of world academy of science: engineering and technology, p 44</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Go K, Konishi H, Matsuura Y (2008) Itone: a Japanese text input method for a dual joystick game controller. In" /><p class="c-article-references__text" id="ref-CR15">Go K, Konishi H, Matsuura Y (2008) Itone: a Japanese text input method for a dual joystick game controller. In: CHI’08 extended abstracts on human factors in computing systems. ACM, New York, pp 3141–3146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersi" /><p class="c-article-references__text" id="ref-CR16">González G, Molina JP, García AS, Martínez D, González P (2009) Evaluation of text input techniques in immersive virtual environments. In: New trends on human–computer interaction. Springer, London, pp. 109–118</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hesselmann T, Heuten W, Boll S (2011) Tap2Count: numerical input for interactive tabletops. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR17">Hesselmann T, Heuten W, Boll S (2011) Tap2Count: numerical input for interactive tabletops. In: Proceedings of the ACM international conference on interactive tabletops and surfaces. ACM, pp. 256–257</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hoste L, Dumas B, Signer B (2012) SpeeG: a multimodal speech-and gesture-based text input solution. In: Procee" /><p class="c-article-references__text" id="ref-CR18">Hoste L, Dumas B, Signer B (2012) SpeeG: a multimodal speech-and gesture-based text input solution. In: Proceedings of the international working conference on advanced visual interfaces. ACM, pp. 156–163</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Isokoski P, Raisamo R (2004) Quikwriting as a multi-device text entry method. In: Proceedings of the third Nor" /><p class="c-article-references__text" id="ref-CR19">Isokoski P, Raisamo R (2004) Quikwriting as a multi-device text entry method. In: Proceedings of the third Nordic conference on human–computer interaction. ACM, pp. 105–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim S, Kim GJ (2004) Using keyboards with head mounted displays. In: Proceedings of the 2004 ACM SIGGRAPH inte" /><p class="c-article-references__text" id="ref-CR20">Kim S, Kim GJ (2004) Using keyboards with head mounted displays. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp. 336–343</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Költringer T, Isokoski P, Grechenig T (2007) TwoStick: writing with a game controller. In: Proceedings of grap" /><p class="c-article-references__text" id="ref-CR21">Költringer T, Isokoski P, Grechenig T (2007) TwoStick: writing with a game controller. In: Proceedings of graphics interface 2007. ACM, pp. 103–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedi" /><p class="c-article-references__text" id="ref-CR22">Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, Camps Bay, Cape Town, South Africa, November 05–07, 2001. AFRIGRAPH ‘01. ACM, New York, NY, pp 95–100</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LaViola J (2000) MSVT: A virtual reality-based multimodal scientific visualization tool. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR23">LaViola J (2000) MSVT: A virtual reality-based multimodal scientific visualization tool. In: Proceedings of the third IASTED international conference on computer graphics and imaging, pp. 1–7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LaViola J, Zeleznik R (1999) Flex and pinch: a case study of whole hand input design for virtual environment i" /><p class="c-article-references__text" id="ref-CR24">LaViola J, Zeleznik R (1999) Flex and pinch: a case study of whole hand input design for virtual environment interaction. In: Proceedings of the second IASTED international conference on computer graphics and imaging, pp. 221–225</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee S, Hong SH, Jeon JW (2003) Designing a universal keyboard using chording gloves. In: ACM SIGCAPH computers" /><p class="c-article-references__text" id="ref-CR25">Lee S, Hong SH, Jeon JW (2003) Designing a universal keyboard using chording gloves. In: ACM SIGCAPH computers and the physically handicapped, no 73–74. ACM, pp 142–147</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international confer" /><p class="c-article-references__text" id="ref-CR26">Lepouras G (2009) Numerical input techniques for immersive virtual environments. In: IEEE international conference on virtual environments, human–computer interfaces and measurements systems, 2009. VECIMS’09, IEEE, pp. 240–245</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IS. MacKenzie, RW. Soukoreff, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hu" /><p class="c-article-references__text" id="ref-CR27">MacKenzie IS, Soukoreff RW (2002) Text entry for mobile computing: models and methods, theory and practice. Hum Comput Interact 17(2–3):147–198</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15327051HCI172%263_2" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Text%20entry%20for%20mobile%20computing%3A%20models%20and%20methods%2C%20theory%20and%20practice&amp;journal=Hum%20Comput%20Interact&amp;volume=17&amp;issue=2%E2%80%933&amp;pages=147-198&amp;publication_year=2002&amp;author=MacKenzie%2CIS&amp;author=Soukoreff%2CRW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Muller J, Krapichler C, Nguyen LS, Englmeier KH, Lang M (1998) Speech interaction in virtual reality. In: Proc" /><p class="c-article-references__text" id="ref-CR28">Muller J, Krapichler C, Nguyen LS, Englmeier KH, Lang M (1998) Speech interaction in virtual reality. In: Proceedings of the 1998 IEEE international conference on acoustics, speech and signal processing, 1998, vol 6. IEEE, pp 3757–3760</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Natapov D, Castellucci SJ, MacKenzie IS (2009) ISO 9241-9 evaluation of video game controllers. In: Proceeding" /><p class="c-article-references__text" id="ref-CR29">Natapov D, Castellucci SJ, MacKenzie IS (2009) ISO 9241-9 evaluation of video game controllers. In: Proceedings of graphics interface 2009. Canadian Information Processing Society, pp. 223–230</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Ni, DA. Bowman, C. North, RP. McMahan, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ni T, Bowman DA, North C, McMahan RP (2011) Design and evaluation of freehand menu selection interfaces using " /><p class="c-article-references__text" id="ref-CR30">Ni T, Bowman DA, North C, McMahan RP (2011) Design and evaluation of freehand menu selection interfaces using tilt and pinch gestures. Int J Hum Comput Stud 69(9):551–562</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2011.05.001" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20and%20evaluation%20of%20freehand%20menu%20selection%20interfaces%20using%20tilt%20and%20pinch%20gestures&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=69&amp;issue=9&amp;pages=551-562&amp;publication_year=2011&amp;author=Ni%2CT&amp;author=Bowman%2CDA&amp;author=North%2CC&amp;author=McMahan%2CRP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Norman DA (2013) The design of everyday things, revised and expanded edition. Basic Books, New York, p 368. IS" /><p class="c-article-references__text" id="ref-CR31">Norman DA (2013) The design of everyday things, revised and expanded edition. Basic Books, New York, p 368. ISBN-13:978-0465050659</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oshita M, Ishikawa H (2012) Gamepad vs. touchscreen: a comparison of action selection interfaces in computer g" /><p class="c-article-references__text" id="ref-CR32">Oshita M, Ishikawa H (2012) Gamepad vs. touchscreen: a comparison of action selection interfaces in computer games. In: Proceedings of the workshop at SIGGRAPH Asia. ACM, pp 27–31</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Tomokazu N, Weghorst S (1998) Virtual notepad: handwriting in immersive VR. In: Virtual reality an" /><p class="c-article-references__text" id="ref-CR33">Poupyrev I, Tomokazu N, Weghorst S (1998) Virtual notepad: handwriting in immersive VR. In: Virtual reality annual international symposium, 1998. Proceedings, IEEE 1998. IEEE, pp 126–132</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Rosenberg, M. Slater, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Rosenberg R, Slater M (1999) The chording glove: a glove-based text input device. IEEE Trans Syst Man Cybernet" /><p class="c-article-references__text" id="ref-CR34">Rosenberg R, Slater M (1999) The chording glove: a glove-based text input device. IEEE Trans Syst Man Cybernetics Part C Appl Rev 29(2):186–191</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5326.760563" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20chording%20glove%3A%20a%20glove-based%20text%20input%20device&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybernetics%20Part%20C%20Appl%20Rev&amp;volume=29&amp;issue=2&amp;pages=186-191&amp;publication_year=1999&amp;author=Rosenberg%2CR&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FE. Sandnes, A. Aubert, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Sandnes FE, Aubert A (2007) Bimanual text entry using game controllers: relying on users’ spatial familiarity " /><p class="c-article-references__text" id="ref-CR35">Sandnes FE, Aubert A (2007) Bimanual text entry using game controllers: relying on users’ spatial familiarity with QWERTY. Interact Comput 19(2):140–150</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.intcom.2006.08.003" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bimanual%20text%20entry%20using%20game%20controllers%3A%20relying%20on%20users%E2%80%99%20spatial%20familiarity%20with%20QWERTY&amp;journal=Interact%20Comput&amp;volume=19&amp;issue=2&amp;pages=140-150&amp;publication_year=2007&amp;author=Sandnes%2CFE&amp;author=Aubert%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Skripcak, P. Tanuska, N. Schmeisser, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Skripcak T, Tanuska P, Schmeisser N (2011) Design and implementation of interactive visualisation configuratio" /><p class="c-article-references__text" id="ref-CR36">Skripcak T, Tanuska P, Schmeisser N (2011) Design and implementation of interactive visualisation configuration using interaction paradigms in virtual reality environment. Int J Soft Comput Eng (IJSCE) 1(5):57–65</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20and%20implementation%20of%20interactive%20visualisation%20configuration%20using%20interaction%20paradigms%20in%20virtual%20reality%20environment&amp;journal=Int%20J%20Soft%20Comput%20Eng%20%28IJSCE%29&amp;volume=1&amp;issue=5&amp;pages=57-65&amp;publication_year=2011&amp;author=Skripcak%2CT&amp;author=Tanuska%2CP&amp;author=Schmeisser%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001). VRPN: a device-independent, network-t" /><p class="c-article-references__text" id="ref-CR37">Taylor II RM, Hudson TC, Seeger A, Weber H, Juliano J, Helser AT (2001). VRPN: a device-independent, network-transparent VR peripheral system. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, pp 55–61</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Terajima K, Komuro T, Ishikawa M (2009) Fast finger tracking system for in-air typing interface. In: CHI’09 ex" /><p class="c-article-references__text" id="ref-CR38">Terajima K, Komuro T, Ishikawa M (2009) Fast finger tracking system for in-air typing interface. In: CHI’09 extended abstracts on human factors in computing systems. ACM, pp 3739–3744</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EJ. Williams, " /><meta itemprop="datePublished" content="1949" /><meta itemprop="headline" content="Williams EJ (1949) Experimental designs balanced for the estimation of residual effects of treatments. Aust J " /><p class="c-article-references__text" id="ref-CR39">Williams EJ (1949) Experimental designs balanced for the estimation of residual effects of treatments. Aust J Sci Res Ser A 2:149–168</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=33508" aria-label="View reference 39 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Experimental%20designs%20balanced%20for%20the%20estimation%20of%20residual%20effects%20of%20treatments&amp;journal=Aust%20J%20Sci%20Res%20Ser%20A&amp;volume=2&amp;pages=149-168&amp;publication_year=1949&amp;author=Williams%2CEJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wilson AD, Agrawala M (2006) Text entry using a dual joystick game controller. In: Proceedings of the SIGCHI c" /><p class="c-article-references__text" id="ref-CR40">Wilson AD, Agrawala M (2006) Text entry using a dual joystick game controller. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, pp 475–478</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0312-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Informatics and Telecommunications, University of the Peloponnese, Tripolis, Arcadia, Greece</p><p class="c-article-author-affiliation__authors-list">Georgios Lepouras</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Georgios-Lepouras"><span class="c-article-authors-search__title u-h3 js-search-name">Georgios Lepouras</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Georgios+Lepouras&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Georgios+Lepouras" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Georgios+Lepouras%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0312-5/email/correspondent/c1/new">Georgios Lepouras</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Comparing%20methods%20for%20numerical%20input%20in%20immersive%20virtual%20environments&amp;author=Georgios%20Lepouras&amp;contentID=10.1007%2Fs10055-017-0312-5&amp;publication=1359-4338&amp;publicationDate=2017-05-02&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0312-5" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0312-5" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lepouras, G. Comparing methods for numerical input in immersive virtual environments.
                    <i>Virtual Reality</i> <b>22, </b>63–77 (2018). https://doi.org/10.1007/s10055-017-0312-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0312-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11-13">13 November 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-04-24">24 April 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-05-02">02 May 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03">March 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0312-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0312-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Numerical input</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive virtual environments</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0312-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=312;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

