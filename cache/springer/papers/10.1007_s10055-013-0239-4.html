<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Why, when and how to use augmented reality agents (AuRAs)"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use augmented reality agents, or AuRAs, as their principle interaction and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Why, when and how to use augmented reality agents (AuRAs)"/>

    <meta name="dc.source" content="Virtual Reality 2013 18:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-12-01"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use augmented reality agents, or AuRAs, as their principle interaction and development paradigm. This paper aims to address this new and distinct field of AuRAs by asking three questions: why should AuRAs be researched, when are they a useful paradigm, and how can they be developed? The first question explores the motivation behind applying AuRAs to AR. Specifically, it investigates whether AuRAs are purely an interaction paradigm, or whether they can also serve as a development paradigm, by outlining in which circumstances it is appropriate for a project to use AuRAs and where their addition would only add unnecessary complexity. A navigational experiment, performed in simulated AR, explores the second question of when AuRAs can be a useful concept in AR applications. Results from this experiment suggest that an embodied virtual character allows for faster navigation along a shorter route than directional arrows or marking the target with an AR &#8220;bubble&#8221;. An exploration of the limitations of the simulated AR environment illuminates how faithfully the experiment recreated the environmental challenges that AuRAs can help to address. Finally, the question of how to develop such applications is addressed through the introduction of the agent factory augmented reality toolkit that allows the rapid prototyping of such applications. Results from a usability study on the toolkit are also presented."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-12-01"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="139"/>

    <meta name="prism.endingPage" content="159"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0239-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0239-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0239-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0239-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Why, when and how to use augmented reality agents (AuRAs)"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2014/06"/>

    <meta name="citation_online_date" content="2013/12/01"/>

    <meta name="citation_firstpage" content="139"/>

    <meta name="citation_lastpage" content="159"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0239-4"/>

    <meta name="DOI" content="10.1007/s10055-013-0239-4"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0239-4"/>

    <meta name="description" content="Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use augmented reality agents, or Au"/>

    <meta name="dc.creator" content="Abraham G. Campbell"/>

    <meta name="dc.creator" content="John W. Stafford"/>

    <meta name="dc.creator" content="Thomas Holz"/>

    <meta name="dc.creator" content="G. M. P. O&#8217;Hare"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_title=Hyperland; citation_publication_date=1990; citation_id=CR1; citation_author=D Adams; citation_publisher=BBC"/>

    <meta name="citation_reference" content="Ahrndt S, F&#228;hndrich J, L&#252;tzenberger M, Rieger A, Albayrak S (2012) An agent-based augmented reality demonstrator in the domestic energy domain. In: PAAMS &#8217;12, pp 225&#8211;228"/>

    <meta name="citation_reference" content="citation_journal_title=Softw Pract Exp; citation_title=Developing multi-agent systems with a FIPA-compliant agent framework; citation_author=F Bellifemine, A Poggi, G Rimassa; citation_volume=31; citation_issue=2; citation_publication_date=2001; citation_pages=103-128; citation_id=CR3"/>

    <meta name="citation_reference" content="Black D, Clemmensen N, Skov M (2009) Supporting the supermarket shopping experience through a context-aware shopping trolley. In: Proceedings of the 21st annual conference of the Australian computer&#8211;human interaction special interest group: design: open 24/7. ACM, pp 33&#8211;40"/>

    <meta name="citation_reference" content="citation_title=Intentions, plans, and practical reason; citation_publication_date=1987; citation_id=CR5; citation_author=ME Bratman; citation_publisher=Harvard University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_title=Intelligence without representation; citation_author=RA Brooks; citation_volume=47; citation_issue=1&#8211;3; citation_publication_date=1991; citation_pages=139-159; citation_doi=10.1016/0004-3702(91)90053-M; citation_id=CR6"/>

    <meta name="citation_reference" content="Campbell A, Collier R, Dragone M, G&#246;rg&#252; L, Holz T, O&#8217;GradyM, O&#8217;Hare GMP, Sassu A, Stafford J (2012) Facilitating ubiquitous interaction using intelligent agents. In: Human&#8211;computer interaction: the agency perspective, pp 303&#8211;326"/>

    <meta name="citation_reference" content="Charles F, Cavazza M, Mead S, Martin O, Nandi A, Marichal X (2004) Compelling experiences in mixed reality interactive storytelling. In: Proceedings of the 2004 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, Singapore, pp 32&#8211;40"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The CAVE: audio visual experience automatic virtual environment; citation_author=C Cruz-Neira, DJ Sandin, TA DeFanti, RV Kenyon, JC Hart; citation_volume=35; citation_issue=6; citation_publication_date=1992; citation_pages=64-72; citation_doi=10.1145/129888.129892; citation_id=CR9"/>

    <meta name="citation_reference" content="Dautenhahn K (1999) Embodiment and interaction in socially intelligent life-like agents. In: Computation for metaphors, analogy, and agents, pp 102&#8211;141"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=The impact of animated interface agents: a review of empirical research; citation_author=D Dehn, S Van Mulken; citation_volume=52; citation_issue=1; citation_publication_date=2000; citation_pages=1-22; citation_doi=10.1006/ijhc.1999.0325; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The LAIR: lightweight affordable immersion room; citation_author=B Denby, A Campbell, H Carr, G O&#8217;Hare; citation_volume=18; citation_issue=5; citation_publication_date=2009; citation_pages=409-411; citation_id=CR12"/>

    <meta name="citation_reference" content="Dragone M, Holz T, O&#8217;Hare GMP (2007) Using mixed reality agents as social interfaces for robots. In: Proceedings of the 16th IEEE international symposium on robot and human interactive communication (RO-MAN 2007). IEEE, pp 1161&#8211;1166"/>

    <meta name="citation_reference" content="citation_journal_title=Stereosc Disp Virtual Real Syst III; citation_title=Perceptual issues in augmented reality; citation_author=D Drascic, P Milgram; citation_volume=2653; citation_issue=1; citation_publication_date=1996; citation_pages=123-134; citation_doi=10.1117/12.237425; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Kybernetes; citation_title=Future reasoning machines: mind and body; citation_author=B Duffy, G O&#8217;Hare, J Bradley, A Martin, B Sch&#246;n; citation_volume=34; citation_issue=9/10; citation_publication_date=2005; citation_pages=1404-1420; citation_doi=10.1108/03684920510614731; citation_id=CR15"/>

    <meta name="citation_reference" content="Ferber J (1999) Multi-agent systems: an introduction to distributed artificial intelligence, vol 33. Addison-Wesley, Reading"/>

    <meta name="citation_reference" content="Foner LN (1993) What&#8217;s an agent anyway? A sociological case study. FTP report. MIT Media Lab, Cambridge, MA"/>

    <meta name="citation_reference" content="Geiger C, Reimann C, Sticklein J, Paelke V (2002) JARToolKit: a java binding for ARToolKit. In: The first IEEE international augmented reality toolkit workshop (ART 02). IEEE"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Math Stat; citation_title=An extension of box&#8217;s results on the use of the f distribution in multivariate analysis; citation_author=S Geisser, SW Greenhouse; citation_volume=29; citation_issue=3; citation_publication_date=1958; citation_pages=885-891; citation_doi=10.1214/aoms/1177706545; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Intell Serv Robot; citation_title=Robot-assisted shopping for the blind: issues in spatial cognition and product selection; citation_author=C Gharpure, V Kulyukin; citation_volume=1; citation_issue=3; citation_publication_date=2008; citation_pages=237-251; citation_doi=10.1007/s11370-008-0020-9; citation_id=CR20"/>

    <meta name="citation_reference" content="Google (2009) Layar: augmented reality browser. 
                    http://www.google.com/mobile/goggles/
                    
                  , accessed Aug 2010"/>

    <meta name="citation_reference" content="G&#246;rg&#252; L, Campbell A, McCusker K, Dragone M, O&#8217;Grady M, O&#8217;Connor N, O&#8217;Hare G (2010) FreeGaming: mobile, collaborative, adaptive and augmented ExerGaming. In: Proceedings of the 8th international conference on advances in mobile computing and multimedia. ACM, pp 173&#8211;179"/>

    <meta name="citation_reference" content="citation_title=Artificial intelligence: the very idea; citation_publication_date=1989; citation_id=CR23; citation_author=J Haugeland; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Intell Syst; citation_title=Is AI going mainstream at last? A look inside Microsoft research; citation_author=SR Hedberg; citation_volume=13; citation_issue=2; citation_publication_date=1998; citation_pages=21-25; citation_doi=10.1109/5254.671087; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=MiRA&#8212;mixed reality agents; citation_author=T Holz, AG Campbell, GMP O&#8217;Hare, JW Stafford, A Martin, M Dragone; citation_volume=69; citation_publication_date=2011; citation_pages=251-268; citation_doi=10.1016/j.ijhcs.2010.10.001; citation_id=CR25"/>

    <meta name="citation_reference" content="Horvitz E, Breese J, Heckerman D, Hovel D, Rommelse K (1998) The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users. In: Proceedings of the 14th conference on uncertainty in artificial intelligence, pp 256&#8211;265"/>

    <meta name="citation_reference" content="ISO 9241-11 (1998) Ergonomic requirements for office work with visual display terminals (VDTs)&#8212;part 11: guidance on usability. International Organization for Standardization, Geneva, Switzerland"/>

    <meta name="citation_reference" content="Iso M (2007) Denno coil. NHK Educational, Geneva, Switzerland"/>

    <meta name="citation_reference" content="Ju W, Nickell S, Eng K, Nass C (2005) Influence of colearner agent behavior on learner performance and attitudes. In: Extended abstracts on human factors in computing systems (CHI &#8217;05). ACM, New York, pp 1509&#8211;1512"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Mobile Comput; citation_title=Radio sleep mode optimization in wireless sensor networks; citation_author=R Jurdak, A Ruzzelli, G O&#8217;Hare; citation_volume=9; citation_issue=7; citation_publication_date=2010; citation_pages=955-968; citation_doi=10.1109/TMC.2010.35; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Softw Eng Appl; citation_title=Code quality evaluation methodology using the ISO/IEC 9126 standard; citation_author=Y Kanellopoulos, P Antonellis, D Antoniou, C Makris, E Theodoridis, C Tjortjis, N Tsirakis; citation_volume=1; citation_issue=3; citation_publication_date=2010; citation_pages=17-36; citation_id=CR31"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality"/>

    <meta name="citation_reference" content="citation_journal_title=Technol Disabil; citation_title=Navig: guidance system for the visually impaired using virtual augmented reality; citation_author=B Katz, F Dramas, G Parseihian, O Gutierrez, S Kammoun, A Brilhault, L Brunet, M Gallay, B Oriola, M Auvray; citation_volume=24; citation_issue=2; citation_publication_date=2012; citation_pages=163-178; citation_id=CR33"/>

    <meta name="citation_reference" content="citation_journal_title=Br J Educ Technol; citation_title=SUMI: the software usability measurement inventory; citation_author=J Kirakowski, M Corbett; citation_volume=24; citation_issue=3; citation_publication_date=1993; citation_pages=210-212; citation_doi=10.1111/j.1467-8535.1993.tb00076.x; citation_id=CR34"/>

    <meta name="citation_reference" content="Koay K, Syrdal D, Walters M, Dautenhahn K (2009) A user study on visualization of agent migration between two companion robots. In: proceedings of 13th International Conference on Human-Computer Interaction (HCII 2009), San Diego, CA, USA"/>

    <meta name="citation_reference" content="Kretschmer U, Coors V, Spierling U, Grasbon D, Schneider K, Rojas I, Malaka R (2001) Meeting the spirit of history. In: Proceedings of the conference on virtual reality, archeology and cultural heritage (VAST &#8217;01), pp 141&#8211;152"/>

    <meta name="citation_reference" content="Kruijff E, Swan JE, Feiner S (2010) Perceptual issues in augmented reality revisited. In: Proceedings of 9th IEEE International Symposium on mixed and augmented reality (ISMAR 2010). IEEE Computer Society, Seoul, Korea, pp 3&#8211;12"/>

    <meta name="citation_reference" content="Lancelle M, Settgast V, Fellner D (2009) Definitely affordable virtual environment. In: Proceedings of IEEE virtual reality. IEEE, p 1"/>

    <meta name="citation_reference" content="Lee C, Bonebrake S, Hollerer T, Bowman DA (2009) A replication study testing the validity of AR simulation in VR for controlled experiments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2009). IEEE Computer Society, pp 203&#8211;204"/>

    <meta name="citation_reference" content="Lee C, Bonebrake S, H&#246;llerer T, Bowman DA (2010) The role of latency in the validity of AR simulation. In: IEEE virtual reality conference (VR 2010), pp 11&#8211;18"/>

    <meta name="citation_reference" content="Lee Y, Choi J, Kim S, Lee S, Jang S (2011) Social augmented reality for sensor visualization in ubiquitous virtual reality. In: Shumaker R (ed) Virtual and mixed reality: new trends, vol 6773. Lecture Notes in Computer Science. Springer, Berlin, pp 69&#8211;75"/>

    <meta name="citation_reference" content="Lieberman H (1997) Autonomous interface agents. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI &#8217;97). ACM, New York, pp 67&#8211;74"/>

    <meta name="citation_reference" content="citation_journal_title=J Micromech Microeng; citation_title=A single-pixel wireless contact lens display; citation_author=AR Lingley, M Ali, Y Liao, R Mirjalili, M Klonner, M Sopanen, S Suihkonen, T Shen, BP Otis, H Lipsanen; citation_volume=21; citation_issue=12; citation_publication_date=2011; citation_pages=125014; citation_doi=10.1088/0960-1317/21/12/125014; citation_id=CR43"/>

    <meta name="citation_reference" content="Littman M, Cassandra A, Pack Kaelbling L (1995) Learning policies for partially observable environments: scaling up. In: International conference on machine learning workshop&#8212;then conference, Citeseer, pp 362&#8211;370"/>

    <meta name="citation_reference" content="Maes P, Darrell T, Blumberg B, Pentland A (1995) The ALIVE system: full-body interaction with autonomous agents. In: Proceedings of the computer animation"/>

    <meta name="citation_reference" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst (special issue on networked reality), E77-D(12), 1321&#8211;1329"/>

    <meta name="citation_reference" content="Nagao K (1998) Agent augmented reality: agents integrate the real world with cyberspace. In: Ishida T (ed) Community computing: collaboration over global information networks. Wiley, New York"/>

    <meta name="citation_reference" content="Obaid M, Niewiadomski R, Pelachaud C (2011) Perception of spatial relations and of coexistence with virtual agents. In: Intelligent virtual agents. Springer, Berlin, pp 363&#8211;369"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Commun; citation_title=Gulliver&#8217;s genie: a multi-agent system for ubiquitous and intelligent content delivery; citation_author=G O&#8217;Hare, M O&#8217;Grady; citation_volume=26; citation_issue=11; citation_publication_date=2003; citation_pages=1177-1187; citation_doi=10.1016/S0140-3664(02)00252-9; citation_id=CR49"/>

    <meta name="citation_reference" content="O&#8217;Hare GMP, Collier R, Conlon J, Abbas S (1998) Agent factory: an environment for constructing and visualising agent communities. In: Proceedings of the ninth Irish conference on artificial intelligence and cognitive science (AICS 98), pp 249&#8211;261, Dublin, Ireland"/>

    <meta name="citation_reference" content="O&#8217;Hare GM, Campbell AG, Stafford JW, Aiken R (2005a) NeXuS: behavioural realism in mixed reality scenarios through virtual sensing. In: Proceedings of the eighteenth international conference on computer animation and social agents (CASA 2005), Hong Kong"/>

    <meta name="citation_reference" content="O&#8217;Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481&#8211;486"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Web Grid Serv; citation_title=Embedded agents: a paradigm for mobile services; citation_author=G O&#8217;Hare, M O&#8217;Grady, C Muldoon, J Bradley; citation_volume=2; citation_issue=4; citation_publication_date=2006; citation_pages=379-405; citation_doi=10.1504/IJWGS.2006.011711; citation_id=CR54"/>

    <meta name="citation_reference" content="citation_title=Market, supermarket and hypermarket design; citation_publication_date=1990; citation_id=CR55; citation_author=MM Pegler; citation_publisher=Visual Reference"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Pervasive Comput; citation_title=Automatic configuration of pervasive sensor networks for augmented reality; citation_author=D Pustka, M Huber, C Waechter, F Echtler, P Keitler, J Newman, D Schmalstieg, G Klinker; citation_volume=10; citation_issue=3; citation_publication_date=2011; citation_pages=68-79; citation_doi=10.1109/MPRV.2010.50; citation_id=CR56"/>

    <meta name="citation_reference" content="Ragan E, Wilkes C, Bowman DA, Hollerer T (2009) Simulation of augmented reality systems in purely virtual environments. In: Proceedings of the 2009 IEEE virtual reality conference (VR &#8217;09). IEEE Computer Society, Washington, DC, USA, pp 287&#8211;288"/>

    <meta name="citation_reference" content="Rao AS, Georgeff MP (1995) BDI agents: from theory to practice. In: Proceedings of the first international conference on multi-agent systems (ICMAS 95), pp 312&#8211;319"/>

    <meta name="citation_reference" content="Robert D, Breazeal C (2012) Blended reality characters. In: Proceedings of the seventh annual ACM/IEEE international conference on human&#8211;robot interaction. ACM, pp 359&#8211;366"/>

    <meta name="citation_reference" content="citation_title=Artificial intelligence: a modern approach; citation_publication_date=2003; citation_id=CR60; citation_author=SJ Russell; citation_author=P Norvig; citation_publisher=Prentice Hall"/>

    <meta name="citation_reference" content="citation_journal_title=J Micromech Microeng; citation_title=Self-assembled crystalline semiconductor optoelectronics on glass and plastic; citation_author=E Saeedi, S Kim, BA Parviz; citation_volume=18; citation_issue=7; citation_publication_date=2008; citation_pages=075019; citation_id=CR61"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Presence equation: an investigation into cognitive factors underlying presence; citation_author=C Sas, GMP O&#8217;Hare; citation_volume=12; citation_issue=5; citation_publication_date=2003; citation_pages=523-537; citation_doi=10.1162/105474603322761315; citation_id=CR62"/>

    <meta name="citation_reference" content="Schmalstieg D (2005) Augmented Reality techniques in games. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2005), pp 176&#8211;177"/>

    <meta name="citation_reference" content="Shneiderman B, Plaisant C (2010) Designing the user interface: strategies for effective human&#8211;computer interaction, 5th edn. Addison Wesley, Reading"/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_title=Agent-oriented programming; citation_author=Y Shoham; citation_volume=60; citation_issue=1; citation_publication_date=1993; citation_pages=51-92; citation_doi=10.1016/0004-3702(93)90034-9; citation_id=CR65"/>

    <meta name="citation_reference" content="Slater M, Steed A (2002) Meeting people virtually: experiments in shared virtual environments. The social life of avatars: presence and interaction in shared virtual environments, pp 146&#8211;171"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Depth of presence in virtual environments; citation_author=M Slater, M Usoh, A Steed; citation_volume=3; citation_publication_date=1994; citation_pages=130-144; citation_id=CR67"/>

    <meta name="citation_reference" content="Sutherland IE (1965) The ultimate display. In: Proceedings of the IFIP congress, vol 2. International Federation for Information Processing, Arlington, VA"/>

    <meta name="citation_reference" content="Syrdal D, Koay K, Walters M, Dautenhahn K (2009) The boy-robot should bark!-children&#8217;s impressions of agent migration into diverse embodiments. In: Proceedings of the new frontiers in human&#8211;robot interaction, a symposium at the AISB2009 convention"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Fact; citation_title=Large displays enhance optical flow cues and narrow the gender gap in 3-D virtual navigation; citation_author=DS Tan, MP Czerwinski, GG Robertson; citation_volume=48; citation_issue=2; citation_publication_date=2006; citation_pages=318-333; citation_doi=10.1518/001872006777724381; citation_id=CR70"/>

    <meta name="citation_reference" content="Thrun S (2005) Probabilistic robotics, vol 1. MIT press, Cambridge"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Using presence questionnaires in reality; citation_author=M Usoh, E Catena, S Arman, M Slater; citation_volume=9; citation_issue=5; citation_publication_date=2000; citation_pages=497-503; citation_doi=10.1162/105474600566989; citation_id=CR72"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Real-time detection and tracking for augmented reality on mobile phones; citation_author=D Wagner, G Reitmayr, A Mulloni, T Drummond, D Schmalstieg; citation_volume=16; citation_issue=3; citation_publication_date=2010; citation_pages=355-368; citation_doi=10.1109/TVCG.2009.99; citation_id=CR73"/>

    <meta name="citation_reference" content="Weiser M (1992) Does ubiquitous computing need interface agents? In: MIT media lab symposium on user interface agents"/>

    <meta name="citation_reference" content="citation_journal_title=Knowl Eng Rev; citation_title=Intelligent agents: theory and practice; citation_author=M Wooldridge, NR Jennings; citation_volume=10; citation_issue=2; citation_publication_date=1994; citation_pages=115-152; citation_doi=10.1017/S0269888900008122; citation_id=CR75"/>

    <meta name="citation_reference" content="citation_journal_title=J Organ End User Comput (JOEUC); citation_title=Design of the promopad: an automated augmented-reality shopping assistant; citation_author=W Zhu, C Owen; citation_volume=20; citation_issue=3; citation_publication_date=2008; citation_pages=41-56; citation_doi=10.4018/joeuc.2008070103; citation_id=CR76"/>

    <meta name="citation_reference" content="Ziemke T (2003) What&#8217;s that thing called embodiment. In: Proceedings of the 25th annual meeting of the cognitive science society. Lawrence Erlbaum, Mahwah, pp 1305&#8211;1310"/>

    <meta name="citation_author" content="Abraham G. Campbell"/>

    <meta name="citation_author_email" content="abey.campbell@gmail.com"/>

    <meta name="citation_author_institution" content="CLARITY: Centre for Sensor Web Technology, University College Dublin, Dublin 4, Ireland"/>

    <meta name="citation_author" content="John W. Stafford"/>

    <meta name="citation_author_email" content="jwstafford@gmail.com"/>

    <meta name="citation_author_institution" content="School of Computer Science and Informatics, University College Dublin, Dublin 4, Ireland"/>

    <meta name="citation_author" content="Thomas Holz"/>

    <meta name="citation_author_email" content="thomas.holz@ucd.ie"/>

    <meta name="citation_author_institution" content="CLARITY: Centre for Sensor Web Technology, University College Dublin, Dublin 4, Ireland"/>

    <meta name="citation_author" content="G. M. P. O&#8217;Hare"/>

    <meta name="citation_author_email" content="gregory.ohare@ucd.ie"/>

    <meta name="citation_author_institution" content="CLARITY: Centre for Sensor Web Technology, University College Dublin, Dublin 4, Ireland"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0239-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0239-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Why, when and how to use augmented reality agents (AuRAs)"/>
        <meta property="og:description" content="Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use augmented reality agents, or AuRAs, as their principle interaction and development paradigm. This paper aims to address this new and distinct field of AuRAs by asking three questions: why should AuRAs be researched, when are they a useful paradigm, and how can they be developed? The first question explores the motivation behind applying AuRAs to AR. Specifically, it investigates whether AuRAs are purely an interaction paradigm, or whether they can also serve as a development paradigm, by outlining in which circumstances it is appropriate for a project to use AuRAs and where their addition would only add unnecessary complexity. A navigational experiment, performed in simulated AR, explores the second question of when AuRAs can be a useful concept in AR applications. Results from this experiment suggest that an embodied virtual character allows for faster navigation along a shorter route than directional arrows or marking the target with an AR “bubble”. An exploration of the limitations of the simulated AR environment illuminates how faithfully the experiment recreated the environmental challenges that AuRAs can help to address. Finally, the question of how to develop such applications is addressed through the introduction of the agent factory augmented reality toolkit that allows the rapid prototyping of such applications. Results from a usability study on the toolkit are also presented."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Why, when and how to use augmented reality agents (AuRAs) | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0239-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Multi-agent systems, Virtual reality, AR simulation, Interaction techniques","kwrd":["Augmented_reality","Multi-agent_systems","Virtual_reality","AR_simulation","Interaction_techniques"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0239-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0239-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=239;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0239-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Why, when and how to use augmented reality agents (AuRAs)
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0239-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0239-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-12-01" itemprop="datePublished">01 December 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Why, when and how to use augmented reality agents (AuRAs)</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Abraham_G_-Campbell" data-author-popup="auth-Abraham_G_-Campbell" data-corresp-id="c1">Abraham G. Campbell<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University College Dublin" /><meta itemprop="address" content="grid.7886.1, 0000000107682743, CLARITY: Centre for Sensor Web Technology, University College Dublin, Belfield, Dublin 4, Ireland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John_W_-Stafford" data-author-popup="auth-John_W_-Stafford">John W. Stafford</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University College Dublin" /><meta itemprop="address" content="grid.7886.1, 0000000107682743, School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, Ireland" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Thomas-Holz" data-author-popup="auth-Thomas-Holz">Thomas Holz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University College Dublin" /><meta itemprop="address" content="grid.7886.1, 0000000107682743, CLARITY: Centre for Sensor Web Technology, University College Dublin, Belfield, Dublin 4, Ireland" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-G__M__P_-O_Hare" data-author-popup="auth-G__M__P_-O_Hare">G. M. P. O’Hare</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University College Dublin" /><meta itemprop="address" content="grid.7886.1, 0000000107682743, CLARITY: Centre for Sensor Web Technology, University College Dublin, Belfield, Dublin 4, Ireland" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">139</span>–<span itemprop="pageEnd">159</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">949 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0239-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use <i>augmented reality agents</i>, or AuRAs, as their principle interaction and development paradigm. This paper aims to address this new and distinct field of AuRAs by asking three questions: <i>why</i> should AuRAs be researched, <i>when</i> are they a useful paradigm, and <i>how</i> can they be developed? The first question explores the motivation behind applying AuRAs to AR. Specifically, it investigates whether AuRAs are purely an interaction paradigm, or whether they can also serve as a development paradigm, by outlining in which circumstances it is appropriate for a project to use AuRAs and where their addition would only add unnecessary complexity. A navigational experiment, performed in simulated AR, explores the second question of <i>when</i> AuRAs can be a useful concept in AR applications. Results from this experiment suggest that an embodied virtual character allows for faster navigation along a shorter route than directional arrows or marking the target with an AR “bubble”. An exploration of the limitations of the simulated AR environment illuminates how faithfully the experiment recreated the environmental challenges that AuRAs can help to address. Finally, the question of <i>how</i> to develop such applications is addressed through the introduction of the agent factory augmented reality toolkit that allows the rapid prototyping of such applications. Results from a usability study on the toolkit are also presented.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Over the last number of years, numerous research projects have begun to create augmented reality (AR) applications that use agents embodied within a virtual character as their principle interaction and development paradigm. These projects share multiple similarities that give credence to the idea that they represent a new and distinct field of <i>augmented reality agents</i>, or AuRAs. This paper contributes to this field by demonstrating <i>why</i> AuRAs should be developed, exploring <i>when</i> they should be used and, finally, addressing the question of <i>how</i> AuRAs can be developed.</p><p>This paper will firstly discuss the overall notion of AuRAs and demonstrate how the concept has already been used in multiple projects under a number of different guises. After a working definition of an AuRA is provided, we will begin to address the question of <i>why</i> research into AuRAs is important, what factors lead to their adoption, and seek to explore their use as an interaction as well as a design paradigm for AR applications by drawing parallels to the existing literature.</p><p>Having proposed specific guidelines that motivate the use of AuRAs from a theoretical perspective, we will test these assumptions through a concrete AR example application in order to investigate the question of <i>when</i> AuRAs constitute a useful concept in AR applications. Specifically, we conducted a navigational experiment in “simulated AR” (Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Schmalstieg D (2005) Augmented Reality techniques in games. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2005), pp 176–177" href="/article/10.1007/s10055-013-0239-4#ref-CR63" id="ref-link-section-d61414e407">2005</a>), a useful methodology for evaluating AR techniques free of current hardware restrictions in a controllable and repeatable environment. Results from the experiment show that an embodied virtual character allows for faster navigation along a shorter route compared to directional arrows or marking the destination with an AR “bubble”, thus demonstrating advantages of using AuRAs as both design and interaction paradigm.</p><p>Finally, we propose a solution as to <i>how</i> to develop such applications with the introduction of the agent factory augmented reality (AFAR) toolkit, which allows for the rapid prototyping of multi-agent system-based AR applications derived from the NeXuS framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005b" title="O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481–486" href="/article/10.1007/s10055-013-0239-4#ref-CR53" id="ref-link-section-d61414e416">2005b</a>). We discuss its implementation and design principles and report the results from subjecting AFAR to an ISO-recognised user satisfaction test.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Defining AuRAs</h2><div class="c-article-section__content" id="Sec2-content"><p>Before beginning this discussion, we firstly need to define the term “AuRA”, as well as our use of the term “agent” within this paper. Since agent definitions are many and varied, a common understanding of the definition of an agent must therefore be established. Only then can a satisfying definition for an AuRA be given to address the proposed questions about this research area.</p><h3 class="c-article__sub-heading" id="Sec3">A definition of agent</h3><p>At the most fundamental level, an <i>agent</i> can be defined as a computational entity that must sense, deliberate and then act. This, of course, is a very wide definition and one purely based on how an agent operates. Using Wooldridge and Jennings’s methodology (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Wooldridge M, Jennings NR (1994) Intelligent agents: theory and practice. Knowl Eng Rev 10(2):115–152" href="/article/10.1007/s10055-013-0239-4#ref-CR75" id="ref-link-section-d61414e436">1994</a>), an agent can instead be defined as a software-based entity characterised by its attributes, namely those of autonomy, social ability, reactivity and proactivity. Implied in this definition is situatedness, that is, the agent operates within an environment, sensing events in that environment and interacting with it. Wooldridge and Jennings also propose an additional set of attributes to define a stronger notion of agency, which include, but are not limited to, mobility, veracity, benevolence and rationality.</p><p>Many of these stronger notions of agency build upon the work of Shoham (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Shoham Y (1993) Agent-oriented programming. Artif Intell 60(1):51–92" href="/article/10.1007/s10055-013-0239-4#ref-CR65" id="ref-link-section-d61414e442">1993</a>), who defined the concept of agent-oriented programming, and assign mentalistic attitudes such as knowledge, belief, intention and obligation to an agent in order to form the agent’s mental state. Within an agent, these mental attitudes should form components of the agents reasoning, driven by a formal theory with clear semantics and should correspond to the commonsense use of the terms.</p><p>Rao and Georgeff (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Rao AS, Georgeff MP (1995) BDI agents: from theory to practice. In: Proceedings of the first international conference on multi-agent systems (ICMAS 95), pp 312–319" href="/article/10.1007/s10055-013-0239-4#ref-CR58" id="ref-link-section-d61414e448">1995</a>) further this concept by giving agents beliefs, desires and intentions (BDI) based on Michael Bratman’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Bratman ME (1987) Intentions, plans, and practical reason. Harvard University Press, Cambridge" href="/article/10.1007/s10055-013-0239-4#ref-CR5" id="ref-link-section-d61414e451">1987</a>) model of human practical reasoning by the same name. Agents that are situated within an environment and act based upon beliefs, generated through previous and current interactions with the environment and other agents, generate new intentions that are designed to achieve their underlying desires. The AFAR toolkit and the NeXuS framework used within this paper are based on the notion of BDI agents, which they implement using the agent factory framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="O’Hare GMP, Collier R, Conlon J, Abbas S (1998) Agent factory: an environment for constructing and visualising agent communities. In: Proceedings of the ninth Irish conference on artificial intelligence and cognitive science (AICS 98), pp 249–261, Dublin, Ireland" href="/article/10.1007/s10055-013-0239-4#ref-CR50" id="ref-link-section-d61414e454">1998</a>).</p><h3 class="c-article__sub-heading" id="Sec4">A definition of augmented reality agent</h3><p>This paper builds on the concept of a mixed reality agent, or MiRA, which is defined by Holz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Holz T, Campbell AG, O’Hare GMP, Stafford JW, Martin A, Dragone M (2011) MiRA—mixed reality agents. Int J Hum Comput Stud 69:251–268" href="/article/10.1007/s10055-013-0239-4#ref-CR25" id="ref-link-section-d61414e465">2011</a>) as “an agent embodied in a mixed reality environment”. The concept of a MiRA blends the notion of agency (Wooldridge and Jennings <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Wooldridge M, Jennings NR (1994) Intelligent agents: theory and practice. Knowl Eng Rev 10(2):115–152" href="/article/10.1007/s10055-013-0239-4#ref-CR75" id="ref-link-section-d61414e468">1994</a>) with embodiment (Dautenhahn <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Dautenhahn K (1999) Embodiment and interaction in socially intelligent life-like agents. In: Computation for metaphors, analogy, and agents, pp 102–141" href="/article/10.1007/s10055-013-0239-4#ref-CR10" id="ref-link-section-d61414e471">1999</a>; Ziemke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ziemke T (2003) What’s that thing called embodiment. In: Proceedings of the 25th annual meeting of the cognitive science society. Lawrence Erlbaum, Mahwah, pp 1305–1310" href="/article/10.1007/s10055-013-0239-4#ref-CR77" id="ref-link-section-d61414e474">2003</a>) and Milgram’s reality–virtuality continuum (Milgram and Kishino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst (special issue on networked reality), E77-D(12), 1321–1329" href="/article/10.1007/s10055-013-0239-4#ref-CR46" id="ref-link-section-d61414e477">1994</a>). Due to the dual nature of mixed reality environments, Holz et al. further delineate a MiRA’s embodiment by its level of <i>corporeal presence</i> and <i>interactive capacity</i> within both the virtual and physical components of the environment.</p><p>Corporal presence is defined by an agent’s degree of situatedness within its virtual or physical representation. As such, corporeal presence is more than simply visual representation. Rather it is the geometric correspondence between the virtual and physical environments, which is necessary to achieve a successful mixed reality environment, and therefore, it is this basic property that defines corporeal presence. Related characteristics, such as occlusion, collision detection or adherence to other physical laws, increase a MiRA’s corporeal presence.</p><p>Interactive capacity, on the other hand, is defined as the agent’s ability to sense and act within its environment. Therefore, an agent’s virtual interactive capacity is defined as an agent’s ability to sense and act within a virtual environment, whereas its ability to sense and act within the real world defines its physical level of interactive capacity.</p><p>This paper explores the concept of AuRAs, which can be categorised as a MiRA that can both sense and act in the virtual component of their reality but can only sense in the physical. This delineation from the wider MiRA concept helps distinguish AuRAs from mixed reality agents with a primarily physical component, such as robots operating in an AR environment. It is important to note that AuRAs are not simply a heavily anthropomorphised interface, as many agent interfaces traditionally have been (Foner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Foner LN (1993) What’s an agent anyway? A sociological case study. FTP report. MIT Media Lab, Cambridge, MA" href="/article/10.1007/s10055-013-0239-4#ref-CR17" id="ref-link-section-d61414e494">1993</a>). They are rather embodied entities existing within an AR environment.</p><p>Since an AuRAs corporal presence is mainly virtual, it can take a potentially unlimited variety of bodies to suit a given interaction scenario, where each body can have a different set of abilities. This concept of <i>one mind—many bodies</i> has been previously explored by the Agent Chameleons project (Duffy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Duffy B, O’Hare G, Bradley J, Martin A, Schön B (2005) Future reasoning machines: mind and body. Kybernetes 34(9/10):1404–1420" href="/article/10.1007/s10055-013-0239-4#ref-CR15" id="ref-link-section-d61414e504">2005</a>). While Agent Chameleons agents can take different bodies, they can only exist in either the virtual or physical environment at any given time. AuRAs, on the other hand, exist in a shared mixed reality space. They are virtually embodied and can interact with the virtual elements of the environment but can also sense the physical world. This work has also been expanded on by Robert and Breazeal (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Robert D, Breazeal C (2012) Blended reality characters. In: Proceedings of the seventh annual ACM/IEEE international conference on human–robot interaction. ACM, pp 359–366" href="/article/10.1007/s10055-013-0239-4#ref-CR59" id="ref-link-section-d61414e507">2012</a>), whose “blended reality characters” can similarly move between real and virtual environments. In a related matter, agent migration into different bodies can arguably be viewed as a form of mutation (Koay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Koay K, Syrdal D, Walters M, Dautenhahn K (2009) A user study on visualization of agent migration between two companion robots. In: proceedings of 13th International Conference on Human-Computer Interaction (HCII 2009), San Diego, CA, USA" href="/article/10.1007/s10055-013-0239-4#ref-CR35" id="ref-link-section-d61414e510">2009</a>). In all these cases, however, user expectations are of the utmost importance. How a character communicates its identity in spite of mutation and migration is no easy matter (see for example, Syrdal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Syrdal D, Koay K, Walters M, Dautenhahn K (2009) The boy-robot should bark!-children’s impressions of agent migration into diverse embodiments. In: Proceedings of the new frontiers in human–robot interaction, a symposium at the AISB2009 convention" href="/article/10.1007/s10055-013-0239-4#ref-CR69" id="ref-link-section-d61414e513">2009</a> for a discussion).</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Why use AuRAs: embodied interfaces and design paradigm</h2><div class="c-article-section__content" id="Sec5-content"><p>With the definition of AuRA completed, we can begin to address the three questions set out by this paper. First, it is necessary to answer <i>why</i> the concept of AuRAs is a useful one in developing AR applications. As stated previously, the notion of AuRAs takes in the aspects of both interface and a software development paradigm. Arguably, the more common notion of an AuRA is that of an embodied animated character within an AR environment. This section will therefore first discuss the notion of AuRAs as interface paradigm before discussing their usefulness as a design paradigm.</p><h3 class="c-article__sub-heading" id="Sec6">AuRAs as interface paradigm</h3><p>The paradigm of an anthropomorphic interface in human–computer interaction has proven a controversial subject as in some cases it has been clearly demonstrated that its disadvantages as an interface outweigh its advantage in terms of usability, for example, in desktop environments (Shneiderman and Plaisant <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Shneiderman B, Plaisant C (2010) Designing the user interface: strategies for effective human–computer interaction, 5th edn. Addison Wesley, Reading" href="/article/10.1007/s10055-013-0239-4#ref-CR64" id="ref-link-section-d61414e535">2010</a>). Yet, the image of an embodied AuRA interacting with users within an AR environment has been used to popularise the idea behind agents for over 20 years, from the “Software Agent” in the 1990 BBC documentary “Hyperland”, written by Adams (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Adams D (1990) Hyperland. BBC, London" href="/article/10.1007/s10055-013-0239-4#ref-CR1" id="ref-link-section-d61414e538">1990</a>), to the recent Japanese cartoon “Denn<span class="mathjax-tex">\( {\bar{{\text o}}} \)</span> Coil” (Iso <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Iso M (2007) Denno coil. NHK Educational, Geneva, Switzerland" href="/article/10.1007/s10055-013-0239-4#ref-CR28" id="ref-link-section-d61414e572">2007</a>). In some cases, this use of embodied agents as interfaces has produced proven outcomes. For example, placed within a learning scenario, results indicate that embodied agents can increase the performance of the learner (Ju et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ju W, Nickell S, Eng K, Nass C (2005) Influence of colearner agent behavior on learner performance and attitudes. In: Extended abstracts on human factors in computing systems (CHI ’05). ACM, New York, pp 1509–1512" href="/article/10.1007/s10055-013-0239-4#ref-CR29" id="ref-link-section-d61414e575">2005</a>).</p><p>It is thus understandable that research has attempted to recreate this interaction using interface agents in desktop environments (Horvitz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Horvitz E, Breese J, Heckerman D, Hovel D, Rommelse K (1998) The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users. In: Proceedings of the 14th conference on uncertainty in artificial intelligence, pp 256–265" href="/article/10.1007/s10055-013-0239-4#ref-CR26" id="ref-link-section-d61414e581">1998</a>), perhaps the most infamous being Microsoft’s office assistant (Hedberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Hedberg SR (1998) Is AI going mainstream at last? A look inside Microsoft research. IEEE Intell Syst 13(2):21–25" href="/article/10.1007/s10055-013-0239-4#ref-CR24" id="ref-link-section-d61414e584">1998</a>). These assistants proved neither popular nor effective, as they were not real agents, placed within a dynamic environment where they could autonomously sense user activity, learn from it, and then act when the user required it to. Indeed, the level of agency required for an AuRA is an important distinction that needs to be made, as an agent complying with a stronger notion of agency (as defined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec3">2.1</a>) will inevitably be more computationally expensive and may not be necessary for the task at hand.</p><p>One area in which a stronger notion of agency can be of benefit is in achieving behavioural realism, which helps users suspend their disbelief of interacting with an imaginary entity (usually visible to them alone) (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005a" title="O’Hare GM, Campbell AG, Stafford JW, Aiken R (2005a) NeXuS: behavioural realism in mixed reality scenarios through virtual sensing. In: Proceedings of the eighteenth international conference on computer animation and social agents (CASA 2005), Hong Kong" href="/article/10.1007/s10055-013-0239-4#ref-CR51" id="ref-link-section-d61414e593">2005a</a>). One of the earliest examples in this regard is the ALIVE system (Maes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Maes P, Darrell T, Blumberg B, Pentland A (1995) The ALIVE system: full-body interaction with autonomous agents. In: Proceedings of the computer animation" href="/article/10.1007/s10055-013-0239-4#ref-CR45" id="ref-link-section-d61414e596">1995</a>). ALIVE uses a magic mirror metaphor where, rather than placing the user in a virtual environment, a video feed of the real environment is displayed and augmented with a virtual dog character that the user can direct via hand gestures. To allow the user to interact in a behaviourally realistic manner with the virtual dog, Maes and colleagues endow the agent embodied within it with a set of internal needs and motivations. This modelling of the agent’s internal states using folk-psychological concepts (a key notion of stronger agency models) facilitates natural interaction with the agent through the fulfilment of user expectations.</p><p>By dynamically demonstrating their internal beliefs, such agents naturally elicit an emotional response from the user. This concept of behavioural realism was also explored in earlier research using the NeXuS framework (Campbell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Campbell A, Collier R, Dragone M, Görgü L, Holz T, O’GradyM, O’Hare GMP, Sassu A, Stafford J (2012) Facilitating ubiquitous interaction using intelligent agents. In: Human–computer interaction: the agency perspective, pp 303–326" href="/article/10.1007/s10055-013-0239-4#ref-CR7" id="ref-link-section-d61414e602">2012</a>; O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005b" title="O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481–486" href="/article/10.1007/s10055-013-0239-4#ref-CR53" id="ref-link-section-d61414e605">2005b</a>), which explored how emotional responses could be elicited from the user by an agent’s ability to react to a dynamic environment. This effect was also evident in the experiment, described in the next section, where users developed an opinion of the virtual character’s personality simply based on the emergent behaviour of the agent’s reactions to the dynamic environment.</p><p>These emotional responses have been created independently by other researchers using AuRAs for storytelling in AR (Kretschmer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kretschmer U, Coors V, Spierling U, Grasbon D, Schneider K, Rojas I, Malaka R (2001) Meeting the spirit of history. In: Proceedings of the conference on virtual reality, archeology and cultural heritage (VAST ’01), pp 141–152" href="/article/10.1007/s10055-013-0239-4#ref-CR36" id="ref-link-section-d61414e612">2001</a>) as well as augmented virtuality (AV) environments (Charles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Charles F, Cavazza M, Mead S, Martin O, Nandi A, Marichal X (2004) Compelling experiences in mixed reality interactive storytelling. In: Proceedings of the 2004 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, Singapore, pp 32–40" href="/article/10.1007/s10055-013-0239-4#ref-CR8" id="ref-link-section-d61414e615">2004</a>).</p><p>Given the many potential pitfalls and caveats described above, there is a crucial need to assess <i>when</i> AuRAs can be a useful interface paradigm in the development of AR applications and when their addition would only add unnecessary complexity. As the potential applications of AuRAs are only limited by the imagination of AR developers, we cannot hope to provide an exhaustive answer in any meaningful capacity. We therefore address this question by way of example in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a>, where we present the results from an experiment with a navigational context as one situation in which AuRAs as interfaces seem to offer potential benefits.</p><h3 class="c-article__sub-heading" id="Sec7">AuRAs as design paradigm</h3><p>Nagao (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Nagao K (1998) Agent augmented reality: agents integrate the real world with cyberspace. In: Ishida T (ed) Community computing: collaboration over global information networks. Wiley, New York" href="/article/10.1007/s10055-013-0239-4#ref-CR47" id="ref-link-section-d61414e635">1998</a>) was perhaps the first to propose AuRAs as a design paradigm. His proposal, termed “agent augmented reality”, did not envisage AuRAs as embodied interfaces but simply as software agents tasked with delivering relevant content to the user in an AR scenario. In his application ShopNavi, a PDA was used to display the AR environment. AR has progressed since then and, nowadays, it is possible to deploy multi-agent systems on current generation mobile phones (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="O’Hare G, O’Grady M, Muldoon C, Bradley J (2006) Embedded agents: a paradigm for mobile services. Int J Web Grid Serv 2(4):379–405" href="/article/10.1007/s10055-013-0239-4#ref-CR54" id="ref-link-section-d61414e638">2006</a>; Wagner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2010) Real-time detection and tracking for augmented reality on mobile phones. IEEE Trans Vis Comput Graph 16(3):355–368" href="/article/10.1007/s10055-013-0239-4#ref-CR73" id="ref-link-section-d61414e641">2010</a>).</p><p>This use of agents as a software development solution for making sense of the real world is echoed in the areas of ubiquitous computing (O’Hare and O’Grady <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="O’Hare G, O’Grady M (2003) Gulliver’s genie: a multi-agent system for ubiquitous and intelligent content delivery. Comput Commun 26(11):1177–1187" href="/article/10.1007/s10055-013-0239-4#ref-CR49" id="ref-link-section-d61414e647">2003</a>) and robotics (Ferber <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Ferber J (1999) Multi-agent systems: an introduction to distributed artificial intelligence, vol 33. Addison-Wesley, Reading" href="/article/10.1007/s10055-013-0239-4#ref-CR16" id="ref-link-section-d61414e650">1999</a>), where the use of agents has become a prevalent design paradigm. In the AR community, by contrast, agents are so far only used in a relatively small number of projects, and even then primarily as an embodied interface. This imbalance will only change if the concept of AuRAs as design paradigm can take its place as a proven solution for specific situations within AR. We therefore aim to highlight <i>why</i> AuRAs have such great potential by demonstrating that AR environments share many of the properties of other real-world contexts in which agents have shown to be useful. We will do so by examining the challenges of different task environments using properties developed by Russell and Norvig (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Russell SJ, Norvig P (2003) Artificial intelligence: a modern approach, 2nd edn. Prentice Hall, Upper Saddle River" href="/article/10.1007/s10055-013-0239-4#ref-CR60" id="ref-link-section-d61414e656">2003</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Completely versus partially observable environments</h4><p>If an application can have complete knowledge of its environment, it is said to be completely observable; otherwise, the environment is only partially observable. In AR environments, partial observability manifests itself most obviously in the tracking problem. For an AR application to make sense of its environment, it needs to monitor said environment, recognise objects of interest in it, disambiguate between different objects, as well as follow objects over time. However, as the perceived inability of good old-fashioned AI (Brooks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Brooks RA (1991) Intelligence without representation. Artif Intell 47(1–3):139–159" href="/article/10.1007/s10055-013-0239-4#ref-CR6" id="ref-link-section-d61414e666">1991</a>; Haugeland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Haugeland J (1989) Artificial intelligence: the very idea. MIT Press, Cambridge" href="/article/10.1007/s10055-013-0239-4#ref-CR23" id="ref-link-section-d61414e669">1989</a>) to deal with real-world environments demonstrates, a complete observation, and thus representation, is impossible. The number of objects that can be tracked at any one time is necessarily limited, objects might be hidden from view (e.g. in camera-based systems) or not instrumented (e.g. in tag-based systems), not to mention general sensor inaccuracies. Any application will therefore have to cope with being able to obtain only partial knowledge about its environment.</p><p>Multi-agent systems are a natural candidate for this sort of environment, as they are often built with the express notion of acting under partial and uncertain information. This is illustrated, for example, by the fact that the aforementioned BDI model represents knowledge about the environment as <i>beliefs</i> rather than, say, facts, thereby reinforcing the view that any item of knowledge may be incomplete and subject to change. This property is addressed in the experiment in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a> by giving agents only a partial view of the environment to test whether agents as a design paradigm suit this task.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Deterministic versus stochastic environments</h4><p>If every action’s result within the environment can be predicted, it is considered deterministic. Correspondingly, if some of the results are unpredictable, it is considered stochastic in nature. Any real-world scenario can, of course, be considered stochastic as the environment is not under the complete control of the application. AR applications, therefore, need to cope with this inherent unpredictability, be it user behaviour, changes in the environment, or actions of other entities in the environment that affect the current state. In practical terms, the problems of partial observability and randomness can usually be thought of as a single problem of uncertainty about the environment. One particularly successful technique to deal with uncertainty, especially in robotics, has been that of probabilistics (Thrun <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Thrun S (2005) Probabilistic robotics, vol 1. MIT press, Cambridge" href="/article/10.1007/s10055-013-0239-4#ref-CR71" id="ref-link-section-d61414e689">2005</a>). For example, Littman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Littman M, Cassandra A, Pack Kaelbling L (1995) Learning policies for partially observable environments: scaling up. In: International conference on machine learning workshop—then conference, Citeseer, pp 362–370" href="/article/10.1007/s10055-013-0239-4#ref-CR44" id="ref-link-section-d61414e692">1995</a>) successfully applied <i>partially observable Markov decision processes</i> to navigation problems in the area of robotics. Their agents employ belief states, which represent probability distributions over the states of the underlying environment, reflecting the uncertainty of the robot’s knowledge. AuRAs as a design paradigm therefore may offer the same potential to cope with AR environments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Episodic versus sequential environments</h4><p>In an episodic environment, the current state of the environment does not affect any future state, and so any decision-making process can rely solely on the current information about the environment, without taking any knowledge about prior events into account. In sequential environments, on the other hand, any decision taken now could potentially affect all future decisions. While some AR environments could be treated strictly episodical, this would be limited to displaying information based purely on the current snapshot of the environment. The vast majority of applications, however, will need to be aware of past states and of the consequences of their actions on future states of the environment. Many agent-based systems, especially those conforming to the stronger notion of agency, employ a planner component that continually replans the steps the agent needs to perform to achieve its desired goal state. One example of this is the agent factory framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="O’Hare GMP, Collier R, Conlon J, Abbas S (1998) Agent factory: an environment for constructing and visualising agent communities. In: Proceedings of the ninth Irish conference on artificial intelligence and cognitive science (AICS 98), pp 249–261, Dublin, Ireland" href="/article/10.1007/s10055-013-0239-4#ref-CR50" id="ref-link-section-d61414e707">1998</a>), which is employed for both the AuRAs used in the experiment in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a> and the AFAR framework described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec30">5</a>, which aims to address the question of <i>how</i> AuRAs can be developed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Static versus dynamic environments</h4><p>If there is a potential for the environment to change while the agent is deliberating, it is considered to be dynamic. If the agent can rely on the fact that, unless it affects the environment, nothing else will, then it is considered static. An AR environment encompasses the real world which is, by its very nature, dynamic. Agents cope with dynamic environments by continually sensing their environment and adjusting their beliefs and plans based on the most up-to-date information. This can be illustrated with one of the example implementations of the aforementioned NeXus framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005b" title="O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481–486" href="/article/10.1007/s10055-013-0239-4#ref-CR53" id="ref-link-section-d61414e727">2005b</a>), which was a precursor to the AFAR framework discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec30">5</a>, in which a virtual character’s goal is to move away from a virtual flashlight shone by the user. The agent needs to keep track of its current position in the world as well as the position of the light, both of which can randomly change as they are attached to markers that can be moved at will. It therefore continually senses both its position and whether it is currently in the light and adjusts its actions accordingly.</p><p>Other AR applications also dynamically adjust to changing circumstance. Görgü et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Görgü L, Campbell A, McCusker K, Dragone M, O’Grady M, O’Connor N, O’Hare G (2010) FreeGaming: mobile, collaborative, adaptive and augmented ExerGaming. In: Proceedings of the 8th international conference on advances in mobile computing and multimedia. ACM, pp 173–179" href="/article/10.1007/s10055-013-0239-4#ref-CR22" id="ref-link-section-d61414e736">2010</a>), for example, propose an agent-based approach to aid in the creation of AR ExerGaming applications, in which a user can set any location on a map, and the agents will create a virtual treasure hunt to that location. The goal of this treasure hunt, however, is not fixed as both the user and the agents can, at any point, choose to change the final location. In the latter case, an agent can modify the goals to achieve a healthy exercise experience when it senses that the user is exercising too much or too little. In a similar manner, the NAVIG: Guidance system (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Katz B, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M et al (2012) Navig: guidance system for the visually impaired using virtual augmented reality. Technol Disabil 24(2):163–178" href="/article/10.1007/s10055-013-0239-4#ref-CR33" id="ref-link-section-d61414e739">2012</a>) uses agents to dynamically connect to or disconnect from different data streams, depending on the relevance of a data stream at any given point in time.</p><p>Abstracting an agent’s information-gathering abilities through sensors brings with it the advantage that these sensors can be interchangeably virtual or physical. As a consequence, the agents are unaware if they are sensing a virtual or real environment, which facilitates the ability to simulate and thus test an AuRA in an AV environment and only then deploy it in an AR environment. This potential development environment is the basis of the AFAR toolkit outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec30">5</a> and explored in the simulated AR environment presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Discrete versus continuous environments</h4><p>A discrete environment is one that can only take a finite number of states, whereas in a continuous environment, the number of states is infinite with smooth progressions from state to state over time. In AR environments, this challenge manifests itself most plainly in registration errors, as the true position of objects can only be approximated in a digital system. Similarly, while an augmented camera feed could potentially be treated as a sequence of discrete picture events, it is usually easier, and more intuitive, to consider it a continuous image stream. In AR, agents have been used to help visualise continuous streams of current energy use data (Ahrndt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ahrndt S, Fähndrich J, Lützenberger M, Rieger A, Albayrak S (2012) An agent-based augmented reality demonstrator in the domestic energy domain. In: PAAMS ’12, pp 225–228" href="/article/10.1007/s10055-013-0239-4#ref-CR2" id="ref-link-section-d61414e759">2012</a>) as well as the ever-changing information from a users social network community (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Lee Y, Choi J, Kim S, Lee S, Jang S (2011) Social augmented reality for sensor visualization in ubiquitous virtual reality. In: Shumaker R (ed) Virtual and mixed reality: new trends, vol 6773. Lecture Notes in Computer Science. Springer, Berlin, pp 69–75" href="/article/10.1007/s10055-013-0239-4#ref-CR41" id="ref-link-section-d61414e762">2011</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Single- versus multi-agent environments</h4><p>Agents in this context can refer to both software and human agents. In a single-agent environment, the agent in question does not have to account for other agents and the effect of their actions on the environment. In an AR context, however, the user can be seen as another agent whose actions the application needs to account for. Multi-agent systems have been successfully used to each deal with a different aspect of an environment, and the users within it, and to then collaborate to achieve a common goal. An AR example can be seen in Dragone et al.’s “mixed reality agent” (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Dragone M, Holz T, O’Hare GMP (2007) Using mixed reality agents as social interfaces for robots. In: Proceedings of the 16th IEEE international symposium on robot and human interactive communication (RO-MAN 2007). IEEE, pp 1161–1166" href="/article/10.1007/s10055-013-0239-4#ref-CR13" id="ref-link-section-d61414e773">2007</a>), which sees multiple agents cooperating to deliver a consistent AR scene involving multiple robots and virtual characters. Each mixed reality agent consists of a robotic platform, controlled by a <i>robot agent</i>, and an associated virtual persona, controlled by an <i>avatar agent</i>, which collaborate to display a behaviourally consistent persona. A further <i>user interface agent</i> controls the correct displaying of all the virtual information of the AR scene. To this end, it tracks all robotic agents, ensures the correct rendering of the virtual agents and takes care of user input.</p><p>A successful AuRA explicitly dealing with multiple entities is Ubitrack (Pustka et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Pustka D, Huber M, Waechter C, Echtler F, Keitler P, Newman J, Schmalstieg D, Klinker G (2011) Automatic configuration of pervasive sensor networks for augmented reality. IEEE Pervasive Comput 10(3):68–79" href="/article/10.1007/s10055-013-0239-4#ref-CR56" id="ref-link-section-d61414e788">2011</a>), an AR project that uses a distributed middleware component—called the “Ubitrack Middleware Agent”—to help coordinate an AR experience with multiple users. It also has been designed to support virtual software agents as an interface, which leads to the notion that AuRAs as an interface could simply be a modular component of any AR applications. While Ubitrack considers itself a ubiquitous computing application, its development borrows heavily from robotics, and it contains explicit AR components, demonstrating the overlap that results from all these disparate areas dealing with the real world.</p><h3 class="c-article__sub-heading" id="Sec14">Summary</h3><p>This section has outlined how agents are a natural candidate for, and have been successfully applied to, the challenges of providing an embodied, behaviourally realistic interface (AuRAs as interface paradigm) as well as dealing with the complexities of real-world environments (AuRAs as design paradigm).</p><p>To justify AuRAs as a design paradigm, not all of the properties listed above need to be explicitly met, of course, though each one increases the potential benefits. At the core of the AuRAs as design paradigm is the use of an agent architecture within an AR application. Indeed, using the task environment properties outlined, if an agent architecture is not used, the need to implement the same functionality for which agent systems are a proven solution will still arise. For example, if an environment is not fully observable, then part of the application will be developed to deduce what it is unable to observe, and how to deal with that, so that the application can correctly interact with the environment. Therefore, it makes sense to build the application on top of an agent framework from the outset.</p><p>With the question of <i>why</i> a developer would use AuRAs in an AR application both as an interface and as part of the design process addressed, the next question that arises is in which situations to use AuRAs. Developers must ensure that they deal with an appropriate case for AuRAs that justifies the additional computational overhead of using a multi-agent systems approach.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">When to use AuRAs: an experiment in simulated AR</h2><div class="c-article-section__content" id="Sec15-content"><p>In order to address the question of <i>when</i> to use AuRAs as interaction or design paradigm, an experiment was conducted to compare the current state of the art in AR applications to an AuRA-based approach. Participants were asked to collect a set of four items from a simulated supermarket and proceed to the checkout. They were assisted in this task by one of the three navigational aides:<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup>
                </p><ul class="u-list-style-bullet">
                  <li>
                    <p>
                      <i>Layar</i> A 3D sphere, or “bubble”, placed in the target location that can be seen through walls aids participants in navigating around the supermarket. This is currently the most widely used AR navigation approach on today’s smart phones and the one taken, for example, by the Layar mobile browser (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Google (2009) Layar: augmented reality browser. &#xA;                    http://www.google.com/mobile/goggles/&#xA;                    &#xA;                  , accessed Aug 2010" href="/article/10.1007/s10055-013-0239-4#ref-CR21" id="ref-link-section-d61414e846">2009</a>).</p>
                  </li>
                  <li>
                    <p>
                      <i>Arrows</i> Multiple software agents embedded in the supermarket shelves collaborate with their neighbours to find the shortest route from their position to the target. They then generate directional arrows near themselves to point the way.</p>
                  </li>
                  <li>
                    <p>
                      <i>Virtual character</i> A humanoid virtual character leads participants, using movement and speech, along the shortest route to the target object, as determined by the same embodied software agents employed in the arrows condition.</p>
                  </li>
                </ul>
              <p>In this manner, the experiment contrasts a non-AuRA approach (Layar), to an unembodied AuRA approach (Arrows) that uses AuRAs purely as a software design paradigm, as well as an embodied AuRA approach (Virtual Character), which uses the AuRA concept both as an interaction and as design paradigm (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig1">1</a>). Each participant’s task duration and distance covered was recorded and compared between the three groups. In particular, the following hypotheses were tested.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>
                        <b>a</b> Intelligent shelves generate <i>arrows</i> pointing the participants along the shortest route from anywhere in the environment (AuRAs as design paradigm). <b>b</b> The virtual character leads participants along the route set out by the intelligent shelves (AuRAs as interface and design paradigm). <b>c</b> A Layar-style bubble placed in the target location, and visible through other objects, is used as the unintelligent control for the experiment (no AuRAs)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              
                <h3 class="c-article__sub-heading">
                  <b>Hypothesis 1</b> (<i>AuRAs as design paradigm</i>)</h3>
                <p>As the agents embedded in the shelves provide an actual shortest route to the target object, participants following the arrows will accomplish the task faster, and will travel less distance, than those guided by the bubble.</p>
              
                <h3 class="c-article__sub-heading">
                  <b>Hypothesis 2</b> (<i>AuRAs as interface paradigm</i>)</h3>
                <p>The virtual character, despite providing the same information as the directional arrows, will nonetheless outperform the other two conditions due to its anthropomorphic form. Specifically, participants following the virtual character will accomplish the task faster, and travel less distance, than those following either the arrows or the bubble.</p>
              <p>Hypothesis 1 examines if using AuRAs as a development approach benefits the user, the potential advantages of which were outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec7">3.2</a>. To this end, over 25 different agents, placed within individual shelves in the supermarket and each with access to a single sensor, have to cooperate to organise a route for the user to navigate the environment. From a computational perspective, this attentional and computational overhead can only be justified if the resulting task is an improvement on current techniques, such as the bubble approach. Hypothesis 2, on the other hand, seeks to explore if providing an AuRA as an interface (as outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec6">3.1</a>) can improve participant performance while using the same route generated by the agents embodied within the virtual shelves.</p><h3 class="c-article__sub-heading" id="Sec16">Simulated AR</h3><p>As mentioned above, instead of using a real-world AR setup, the experiment was conducted in a simulated AR environment for several reasons.</p><p>Firstly, simulated AR allows for a complete field of view (FOV) for the user, which is not feasible using any current AR display devices, though active research in this area exists (Lingley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Lingley AR, Ali M, Liao Y, Mirjalili R, Klonner M, Sopanen M, Suihkonen S, Shen T, Otis BP, Lipsanen H et al (2011) A single-pixel wireless contact lens display. J Micromech Microeng 21(12):125014" href="/article/10.1007/s10055-013-0239-4#ref-CR43" id="ref-link-section-d61414e954">2011</a>; Saeedi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Saeedi E, Kim S, Parviz BA (2008) Self-assembled crystalline semiconductor optoelectronics on glass and plastic. J Micromech Microeng 18(7):075019" href="/article/10.1007/s10055-013-0239-4#ref-CR61" id="ref-link-section-d61414e957">2008</a>). As previous studies have shown, field of view is vital to navigational tasks, especially for women (Tan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Tan DS, Czerwinski MP, Robertson GG (2006) Large displays enhance optical flow cues and narrow the gender gap in 3-D virtual navigation. Hum Fact 48(2):318–333" href="/article/10.1007/s10055-013-0239-4#ref-CR70" id="ref-link-section-d61414e960">2006</a>) and, as such, is indispensable to this experiment. Additionally, a simulated AR environment gives one complete control over the experimental environment without registration inaccuracies, as all objects are now part of the virtual environment, and the application need only track the user and not the world itself. Finally, it allows for the task to be set in a large indoor environment and for the simulation of multiple sensors. In the context of this experiment, this involved simulating sensors in all of the shelves that detect movement around them. These sensors were given a limited view and were constantly checking their environment without worrying about power requirements. In a real-world scenario, these agents would, of course, have to be aware of power requirements and try to balance sensing the environment with maximising battery life (Jurdak et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Jurdak R, Ruzzelli A, O’Hare G (2010) Radio sleep mode optimization in wireless sensor networks. IEEE Trans Mobile Comput 9(7):955–968" href="/article/10.1007/s10055-013-0239-4#ref-CR30" id="ref-link-section-d61414e963">2010</a>).</p><p>The concept of AR simulation, in which human–computer interaction is examined in virtual reality and lessons learnt are subsequently applied to AR, is not a new one. It was first outlined by Schmalstieg (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Schmalstieg D (2005) Augmented Reality techniques in games. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2005), pp 176–177" href="/article/10.1007/s10055-013-0239-4#ref-CR63" id="ref-link-section-d61414e969">2005</a>). He coined the term “simulated AR” and called for examination of the interaction techniques in modern 3D computer game environments to be used to help educate the design of AR environments. This idea of simulating AR would enable the conducting of research by allowing the creation of a perfect AR environment as a control.</p><p>To achieve a simulated AR environment, he proposes to use an immersive virtual environment. These environments, originating from the concept of the <i>ultimate display</i> by Sutherland (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1965" title="Sutherland IE (1965) The ultimate display. In: Proceedings of the IFIP congress, vol 2. International Federation for Information Processing, Arlington, VA" href="/article/10.1007/s10055-013-0239-4#ref-CR68" id="ref-link-section-d61414e978">1965</a>) and first developed by Cruz-Neira et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72" href="/article/10.1007/s10055-013-0239-4#ref-CR9" id="ref-link-section-d61414e981">1992</a>) under the term cave automatic virtual environment (CAVE), can increase the level of spatial intuition for the user. However, they are usually prohibitively expensive, require a large amount of space and are time-consuming to construct and maintain, as they are often comprised of multiple workstations that are formed into a cluster. Some of these issues have been addressed by the introduction of low-cost versions of the CAVE design, such as the lightweight affordable immersion room (LAIR) (Denby et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Denby B, Campbell A, Carr H, O’Hare G (2009) The LAIR: lightweight affordable immersion room. Presence Teleoper Virtual Environ 18(5), 409–411" href="/article/10.1007/s10055-013-0239-4#ref-CR12" id="ref-link-section-d61414e984">2009</a>) used within this experiment and others such as the definitely affordable virtual environment (DAVE) (Lancelle et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lancelle M, Settgast V, Fellner D (2009) Definitely affordable virtual environment. In: Proceedings of IEEE virtual reality. IEEE, p 1" href="/article/10.1007/s10055-013-0239-4#ref-CR38" id="ref-link-section-d61414e987">2009</a>).</p><p>Simulated AR can offer a complete field of view, unlike current AR applications which use either a head-mounted display with only a limited FOV or a mobile phone, which is only capable of video see-through and whose FOV is even smaller. An ideal AR environment would further allow to precisely ascertain the location of every object within the environment. Registration error would be non-existent, unless, of course, it was of interest to simulate different levels of error.</p><p>Indeed, one of the first AR simulations experimented with different degrees of registration error using a CAVE (Ragan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ragan E, Wilkes C, Bowman DA, Hollerer T (2009) Simulation of augmented reality systems in purely virtual environments. In: Proceedings of the 2009 IEEE virtual reality conference (VR ’09). IEEE Computer Society, Washington, DC, USA, pp 287–288" href="/article/10.1007/s10055-013-0239-4#ref-CR57" id="ref-link-section-d61414e996">2009</a>), thus demonstrating how such an immersive virtual environment can be used for simulated AR. Similar efforts have also been shown to replicate results from previous AR experiments (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee C, Bonebrake S, Hollerer T, Bowman DA (2009) A replication study testing the validity of AR simulation in VR for controlled experiments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2009). IEEE Computer Society, pp 203–204" href="/article/10.1007/s10055-013-0239-4#ref-CR39" id="ref-link-section-d61414e999">2009</a>), while others explore the role of latency in the validity of the simulation (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Lee C, Bonebrake S, Höllerer T, Bowman DA (2010) The role of latency in the validity of AR simulation. In: IEEE virtual reality conference (VR 2010), pp 11–18" href="/article/10.1007/s10055-013-0239-4#ref-CR40" id="ref-link-section-d61414e1002">2010</a>). AR simulation has one further advantage over real AR with regard to human biology. Drascic and Milgram note that there are important differences in human physiology in relation to AR displays (Drascic and Milgram <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Drascic D, Milgram P (1996) Perceptual issues in augmented reality. Stereosc Disp Virtual Real Syst III 2653(1):123–134" href="/article/10.1007/s10055-013-0239-4#ref-CR14" id="ref-link-section-d61414e1005">1996</a>) (e.g. incorrect depth cues), giving rise to increased complexity when trying to produce a consistent AR experience, though current AR research in this area is ongoing (Kruijff and Feiner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kruijff E, Swan JE, Feiner S (2010) Perceptual issues in augmented reality revisited. In: Proceedings of 9th IEEE International Symposium on mixed and augmented reality (ISMAR 2010). IEEE Computer Society, Seoul, Korea, pp 3–12" href="/article/10.1007/s10055-013-0239-4#ref-CR37" id="ref-link-section-d61414e1008">2010</a>), for example, through the development of hardware solutions that would be sufficiently fast and accurate to trick the human eye.</p><p>One additional aspect of AR simulation in terms of the NeXuS framework used within this paper, and its use of a multi-agent system, concerns presence. Each of the agents in NeXuS automatically possesses corporeal presence, as they are not aware of any difference between sensing a virtual simulated input or sensing a real-world input. This adds an advantage to using an agent-based approach, as it only requires changing the perceptors and actuators of an agent but not the fundamental logic behind their actions. The ignorance of agents to their true environment can make them a useful abstraction in AR simulations, as their core functionality can be reused for both environments. Not only are the agents unaware of any differences but it has been found by Obaid et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Obaid M, Niewiadomski R, Pelachaud C (2011) Perception of spatial relations and of coexistence with virtual agents. In: Intelligent virtual agents. Springer, Berlin, pp 363–369" href="/article/10.1007/s10055-013-0239-4#ref-CR48" id="ref-link-section-d61414e1014">2011</a>) “that people interacting with the virtual agent in AR/VR tend to re-use unconsciously their ‘natural’ behaviour patterns learned in ‘real-world’ interactions”.</p><p>While the use of simulated AR technically renders this an Augmented Virtuality application, the aforementioned issues would currently make this experiment infeasible in the real world.</p><h3 class="c-article__sub-heading" id="Sec17">Experimental methods</h3><p>We were mainly interested in two quantitative measures of performance: distance travelled (measured in m) and duration of the task (measured in s). Both were recorded automatically by a software agent for each participant.</p><p>We were further interested in measuring presence in our simulated AR environment in order to compare it to some of our previous experiments using other environments and interaction modalities.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> To this end, presence was measured using a questionnaire devised by Sas and O’Hare (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sas C, O’Hare GMP (2003) Presence equation: an investigation into cognitive factors underlying presence. Presence 12(5):523–537" href="/article/10.1007/s10055-013-0239-4#ref-CR62" id="ref-link-section-d61414e1038">2003</a>). The questionnaire contains 21 statements, each rated on a 7-point Likert scale, and covers the dimensions of <i>being there</i>, <i>not being here</i> and <i>reflective consciousness</i> (awareness of being there). The authors demonstrated the internal reliability for the questionnaire (<i>α</i> = .92) and showed a positive correlation with another well-established questionnaire (Slater et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Slater M, Usoh M, Steed A (1994) Depth of presence in virtual environments. Presence Teleoper Virtual Environ 3:130–144" href="/article/10.1007/s10055-013-0239-4#ref-CR67" id="ref-link-section-d61414e1054">1994</a>).</p><h3 class="c-article__sub-heading" id="Sec18">Experimental design</h3><p>The experiment employed a randomised-block design in which 54 participants (35 males, 19 females), aged 18–38 years (M = 22.17, SD = 4.237) were split into three different groups. Each group used one of three AR navigational aids (Layar, Arrows, Cirtual Character) to achieve a way-finding task in a simulated supermarket. Overall, four participants had to be removed: one got lost in the environment, another failed to proceed to the checkout even after collecting all four items, a further participant had problems interacting with the pressure pad, and one had to be removed due to an application failure. However, as these incidents were observed during the actual experiment, we recruited four more participants in order to maintain participant numbers across groups.</p><h3 class="c-article__sub-heading" id="Sec19">Physical setup</h3><p>The AR environment was simulated via a lightweight affordable immersion room (LAIR), a variation on the traditional CAVE that is designed to be used in spatially and financially constrained situations (Denby et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Denby B, Campbell A, Carr H, O’Hare G (2009) The LAIR: lightweight affordable immersion room. Presence Teleoper Virtual Environ 18(5), 409–411" href="/article/10.1007/s10055-013-0239-4#ref-CR12" id="ref-link-section-d61414e1072">2009</a>). The LAIR consists of four walls, arranged in a square, providing a 360° view. By using front projection and two projectors per wall, the LAIR provides a compact environment which gives users some freedom of movement. In terms of performance and cost, the LAIR can be situated somewhere between a power wall and a CAVE. It is considerably more immersive than a power wall but less so than a CAVE, and comparably moderate in terms of space and cost.</p><p>The layout of the configuration can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig2">2</a>. The occlusion-free region is depicted by the octagon in the figure, and the highlighted projector L1 shows how the projection creates the outer border of the octagon.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Projector layout: a user standing in the central octagonal region will not cast shadows on the walls. <i>F1</i> and <i>F2</i> are the projectors for the front wall. <i>C</i> is a central ceiling mounted projector to be used as an information display</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The LAIR uses “crystal eyes” glasses to provide 3D active stereo graph. A pressure pad on the floor is used for navigating the virtual environment. The LAIR also provides for interaction via mouse and keyboard, but we have found in pilot studies that a number of users tend to focus on only the front projection and ignore the 360° view when using the keyboard. The LAIR uses ARToolkit (Kato and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality" href="/article/10.1007/s10055-013-0239-4#ref-CR32" id="ref-link-section-d61414e1112">1999</a>) markers for head tracking (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig3">3</a>), but instead of the using the marker to track an object in the environment, the transformation matrix is used to place the user correctly within the environment. Future experiments in the LAIR aim to explore the possible use of oculus rift virtual reality HMD and the use of the OMNI Omnidirectional treadmill. Although the oculus rift FOV is limited to 110°, its ability to allow a user to look up and down may prove an important addition in simulated AR environments. The OMNI would easily replace the current pressure pad.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The LAIR uses an ARToolkit marker to track head movement and a pair of crystal eyes glasses for 3D stereo</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec20">Protocol</h3><p>After entering the room, participants were introduced to the equipment, the purpose and protocol of the experiment. Each participant then took part in a short control task where they navigated a different supermarket than the one used in the experiment.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> The participants then collected four objects and moved to the exit, without the use of any navigational aides other than a printed overhead map of the control supermarket.</p><p>The control task served two purposes: (1) it helped participants to get used to the environment, equipment and control interface, and (2) it allowed the matching of participants across the three experiment groups according to their spatial ability. To this end, all participants within a group were ranked according to their performance in the control task and participants of the same rank were then combined into a block.</p><p>Upon completion of the control task, the experimenter explained the respective navigational aid to the participant and instructed them to collect the items as quickly as possible. The experimenter then left the room and the participants started the experiment. As the layout of the supermarket in the main experiment was different, participants’ navigational abilities were not being influenced by completing the control task. Once they successfully collected all four objects and proceeded to the checkout, the interaction ended. The experimenter then reentered the room and the participants finished the experiment by filling in the self-report measures.</p><h3 class="c-article__sub-heading" id="Sec21">Results</h3><p>We present results of our analysis of the distance and duration for both the control and experiment task as well as presence scores of participants for the experiment task. A summary of the main data is presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0239-4#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Descriptive statistics for duration (in s) and distance (in m) of both control and experiment tasks as well as for presence scores (7-point Likert) averaged over all 21 presence items</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0239-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Control task</h4><p>Analysis of the control task aimed to demonstrate two points: (1) that participants were distributed evenly across groups, i.e. the participants of one group did not possess a significantly greater navigational ability than the other groups; and (2) that completion time was a valid matching criteria, i.e. participants that completed the control task quickly also completed the experimental task quickly.</p><p>The first point was demonstrated with a one-way ANOVA, which found no significant difference of task duration between conditions [<i>F</i>(2,51) = .686, <i>p</i> = .508], nor of distance travelled [<i>F</i>(2,42) = .400, <i>p</i> = .673], indicating that participant performance on the control task was similar across groups.<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup>
                  </p><p>A correlation analysis found that duration was positively correlated across the control and experiment task [Pearson’s <i>r</i>(54) = .577,  <i>p</i> &lt; .001], whereas distance was not [Pearson’s <i>r</i>(45) = .080, <i>p</i> = .599]. This demonstrates that completion time of the control task was an adequate predictor of experiment performance and thus justifies it as a valid variable to match participants on. The scatter plot in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig4">4</a> illustrates the relationship between the completion time for the control and the experiment task.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A scatter plot illustrating the relationship between completion time for the control and the experiment task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Experiment task</h4><p>As the experiment employed a random block design, mean duration and distance during the experiment task were analysed using a repeated-measures (RM) ANOVA to find differences between conditions (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0239-4#Tab2">2</a> for an overview of the mean and standard deviation of duration and distance data for all groups).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Mean and standard deviation for duration (in s) and distance (in m) for the different conditions in the experiment task</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0239-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The RM ANOVA found a significant effect of the type of navigational aid on task duration [<i>F</i>(2,16) = 5.135, <i>p</i> = .019]. Post hoc comparisons using paired-samples <i>t</i> tests for all conditions found a significant difference between the Layar and the Arrows condition [<i>t</i>(17) = 2.474, <i>p</i> = .024), and between the Arrows and Virtual Character condition [<i>t</i>(17) = 3.213, <i>p</i> = .005] but not between the Layar and Virtual Character condition [<i>t</i>(17) = .831, <i>p</i> = .418]. This indicates that participants in the Arrows condition accomplished their task significantly slower than those in either the Layar or in the Virtual Character condition. However, our results suggest that completion time did not significantly differ between the Layar and the Virtual Character condition (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig5">5</a>, left). The results show the same pattern of significance even when tested using the Geisser and Greenhouse adjustment (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1958" title="Geisser S, Greenhouse SW (1958) An extension of box’s results on the use of the f distribution in multivariate analysis. Ann Math Stat 29(3):885–891" href="/article/10.1007/s10055-013-0239-4#ref-CR19" id="ref-link-section-d61414e1666">1958</a>) for possible violation of sphericity.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Mean duration (<i>left</i>) and distance (<i>right</i>) for the three experiment groups. Participants in the Arrows condition performed the task significantly slower, whereas they took a significantly longer route in the Layar condition. For both measures, there was no significant difference between the remaining two conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The type of navigational aid had a further significant effect on the distance travelled [<i>F</i>(2,8) = 27.581, <i>p</i> &lt; .001]. Post hoc paired-samples <i>t</i> tests found a significant difference between the Layar and Arrows condition [<i>t</i>(9) = 6.956, <i>p</i> &lt; .001] existed, and between the Layar and Virtual Character condition [<i>t</i>(9) = 7.341, <i>p</i> &lt; .001] but not between the Arrows and Virtual Character condition [<i>t</i>(9) = −.864, <i>p</i> = .410). This indicates that participants in the Layar condition travelled significantly further than those in either the Arrows or in the Virtual Character condition. However, distance did not seem to significantly differ between the Arrows and the Virtual Character condition (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig5">5</a>, right).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Presence</h4><p>As mentioned earlier, we were mainly interested in an overall presence score to serve as an indicator for the level of immersion of the LAIR environment and did not expect a difference in presence scores between conditions. The average presence score over all 21 self-report items had a very wide range (4.04 on a 7-point Likert scale), which is perhaps not surprising for a concept as individual as presence. The overall average was M = 4.580, however, which we consider a satisfactorily high presence score when compared to similar studies we conducted in a desktop environment (M = 2.5). As expected, a repeated-measures ANOVA found no effect of condition on the reported presence score [<i>F</i>(2,16) = .587, <i>p</i> = .568]. That is, participants reported the same level of presence regardless of the navigational aid they were using.</p><h3 class="c-article__sub-heading" id="Sec25">Discussion</h3><p>The following sections will discuss the results with regard to the two hypotheses stated at the beginning of this experiment and their bearing on whether a navigational AR application as presented can constitute a scenario in which AuRAs are a useful design or interaction paradigm. We also discuss potential future avenues of investigation and review the limitations of our simulated AR environment by evaluating how faithfully it managed to recreate a real-world environment, using again the properties introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec7">3.2</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Hypothesis 1: AuRAs as design paradigm</h4><p>Hypothesis 1 was partially supported by the results. The Arrows condition showed that having a large amount of intelligent agents continually sensing a user’s position, coordinating between themselves and informing the user of where to go, did result in subjects taking the optimal route but, contrary to our assumptions, actually increased task completion time over participants in the Layar condition. At first, these results might seem counterintuitive. However, from observation and direct feedback of participants, we believe that the number of arrows led to an overload of information. While participants did consistently follow the arrows correctly, as evidenced by them taking the shortest route, they had to periodically pause to interpret them. These pauses to check that they were still on the correct route may have led to their poor performance in terms of duration.</p><p>The Layar participants, on the other hand, never stopped moving around the environment and, consequently, finished the task quickly, but did not necessarily take an optimal route to the target objects. We believe that, if the arrows had been less ambiguous, participants’ task completion time would also have been significantly shorter than in the Layar condition and would like to test this hypothesis in a future experiment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Hypothesis 2: AuRAs as interface paradigm</h4><p>Only participants of the Virtual Character condition consistently both followed the shortest route and did so quickly, lending some credence to the idea of AuRAs as design paradigm as well as interaction paradigm. However, it needs to be acknowledged that the experiment did manipulate more than one factor with the Virtual Character condition. Firstly, it introduced an embodied agent, in order to test the veracity of Hypothesis 2, but it also made the virtual character the only condition in which participants followed a moving signpost, and the only one containing spoken prompts. Indeed, in direct feedback, some participants reported getting annoyed at the virtual character’s insistent prompts to follow whenever they fell behind.</p><p>While this seems to suggest that participant performance was grounded in the lack of ambiguity (as they did not have to distinguish between and interpret several markers) rather than the virtual character itself, future experiments are needed to tease out exactly which aspects of the virtual character cause the resulting performance gain if real world AR application developers are to use virtual characters as their interaction paradigm. Unfortunately, these effects are difficult to separate, as agent embodiment is tightly bound to user expectations (Slater and Steed <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Slater M, Steed A (2002) Meeting people virtually: experiments in shared virtual environments. The social life of avatars: presence and interaction in shared virtual environments, pp 146–171" href="/article/10.1007/s10055-013-0239-4#ref-CR66" id="ref-link-section-d61414e1773">2002</a>). Based on the character’s bodily shape, users will assume additional abilities that may, in fact, not be present. While it is very possible that, in this type of restricted task, a bubble moving in front of the participants would have lead to similar task completion times, further giving said bubble the ability to speak and ask the user to follow it, would arguably still constitute some aspect of anthropomorphism and might well contradict user expectations. Similarly, users might expect a virtual character to be able to speak and might not follow its directions as well if it does not, not because of the lack of spoken commands but because of it being counterintuitive.</p><p>Fulfilling social expectations is perhaps one of the most difficult aspects of anthropomorphised interfaces. Indeed, anecdotal evidence about the limitations in terms of social interaction was generated during the experiment, as users reported that the virtual character seemed annoyed with them, repeatedly requesting them to follow. Others even expressed that the character was hostile towards them. While the AuRA tried to respond to the needs of the participants, helping them whenever they collided with the virtual environment, how an application provides this assistance is crucial to how the user will respond to it. In the experiment, the virtual character had the ability to talk to the user to help guide them. However, some users expressed negative reactions to the virtual character’s overly direct commands. This supports Lieberman’s recommendation that agents should offer suggestions rather than telling a user what to do (Lieberman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Lieberman H (1997) Autonomous interface agents. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’97). ACM, New York, pp 67–74" href="/article/10.1007/s10055-013-0239-4#ref-CR42" id="ref-link-section-d61414e1779">1997</a>).</p><p>Future experiments could therefore change different aspects of the character’s social interaction with the user. For instance, when the user is falling behind, the character could apologise for going too fast instead of asking the user to keep up. This could be done in a controlled manner by limiting the users mobility within the world to force this scenario to take place.</p><p>In summary, it should be kept in mind that the reasoning for using a virtual character should always be, as Dehn and van Mulken (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Dehn D, Van Mulken S (2000) The impact of animated interface agents: a review of empirical research. Int J Hum Comput Stud 52(1):1–22" href="/article/10.1007/s10055-013-0239-4#ref-CR11" id="ref-link-section-d61414e1788">2000</a>) put it, to show “some behaviour that is functional with regard to the system’s aim”. We believe that an anthropomorphic form can significantly add to the experience in this type of context and will undertake future experiments with a more acceptable virtual character and less confusing arrows to confirm this. Other experiments should also be conducted to investigate whether our results actually generalise to a wider range of tasks and different characters.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Limitations of the simulated AR environment</h4><p>The positive results of experiment must also be examined in the light of the limitations of using a simulated AR environment, and the ways in which these could potentially have affected the outcome. In order to ascertain these limitations in recreating a sufficiently complex representation of a real AR environment, it is important to again use Russell and Norvig’s task environment properties introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec7">3.2</a>.</p><p>
                    <i>Was the environment partially observable</i>? In a simulated AR environment, the partially observable nature of AR is itself simulated, as technically everything is observable. For the purposes of this experiment, an agent embodied within a shelf could only gain information from the other agents embodied in shelves immediately surrounding it. This led to the agents needing to cooperate to generate a map of the supermarket. The shelves used this information to generate arrows to help guide the user. This same information was also used to allow the virtual character to act as a navigational guide. We believe this artificial compartmentalisation of information to reasonably mimic real-world limitations were this type of system to be deployed as an actual AR system, where it is likely that individual sensors with limited sensing and communication range would be deployed in the shelves as part of a store-wide wireless sensor network.</p><p>
                    <i>Was the environment stochastic</i>? The simulated AR environment itself, by virtue of being fully under the control of the experiment, was deterministic, only the participant’s actions were stochastic. In the virtual character condition, the participant was tasked with following the virtual character to guide them around the supermarket. As participants were not always following this goal, the virtual character possessed a set of plans to prompt the participant using voice commands. In this regard, only the user’s actions faithfully represented a corresponding real-world scenario. However, while it would be possible to introduce stochastic elements into the environment itself, this was purposely omitted in this experiment to restrict variability of experiment runs between different subjects.</p><p>
                    <i>Was the environment sequential</i>? The experiment required participants to pick up a number of items. While a fixed order of items was forced on participants, the agent still needed to keep track of the picked up items. Although a dynamic planning of which item to pick up next, based on the current user position, would have illustrated better the agent’s ability to project consequences based on past and current states, this was again kept constant for repeatability between subjects. However, investigating different approaches to dynamically planning the user’s route might well be the focus of future experiments.</p><p>
                    <i>Was the environment dynamic</i>? Similarly to the last point, only the participant’s actions were of a dynamic nature, the environment itself was static. Thus, in the experiment, the agents only had to deliberate about a limited series of choices but, as the user’s actions could change as it deliberated, the environment was still somewhat dynamic in nature. The agent in charge of the virtual character therefore continually sensed the position, direction and speed of the participant and adjusted its behaviour accordingly. For example, if the participant was prevented from following due to colliding with a wall, the virtual character would change its voice prompts to first help the participant move away from the collision.</p><p>
                    <i>Was the environment continuous</i>? An AR environment, as it contains the real world, can have an infinite continuous number of states. The experiment, on the other hand, can theoretically be viewed as a series of discrete states, defined by when objects were picked up. This approach, however, would not take a sufficiently fine-grained view of the problem. One can argue though that the virtual character had to continuously react to the participants’ actions and place itself at a correct distance to show them in which direction to go. Seen from this perspective, the limit of states is only bound by the resources of the machine simulating the positions of the user and the virtual character.</p><p>
                    <i>Was it a multi-agent environment</i>? In the experiment, the participant was the only agent not under control of the application. In a real-world AR test, the user, and the agents assisting him, would have to cope with other actors within the environment, for example, fellow patrons of the store. The advantages of a multi-agent system in dealing with multiple actors, therefore, were not exercised in this experiment. Follow-up experiments that include, for example, other (simulated) shoppers moving about the store would not only incorporate the multi-agent aspect into the simulated environment but would also serve to make the environment more dynamic, stochastic and continuous. For these reasons, such an experiment would be a logical next step in investigating the applicability of AuRAs to this type of system.</p><h3 class="c-article__sub-heading" id="Sec29">Summary</h3><p>This section identified and explored how the use of controlled laboratory studies can begin to address the question of <i>when</i> to use AuRAs. The design paradigm hypothesis was partially supported, as the distance travelled was reduced, though task completion time increased. Although performance measures supported the interface paradigm hypothesis, direct participant feedback indicated that great care must be exercised when developing an embodied interface agent.</p><p>The experiment also introduced and justified the use of simulated AR in examining the suitability of AuRAs for different application scenarios by presenting a controlled testing environment which in many cases would, at present, still be prohibitively expensive to achieve in the real world. Simulated AR is likely, in fact, to become only more inexpensive with the development of affordable immersive headsets, such as the Oculus Rift,<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> and natural interaction devices, such as omnidirectional treadmills.<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> Combined they provide inexpensive tools to aid in simulated AR experiments and offer researchers exciting opportunities that would have been considered unattainable even a mere decade ago.</p><p>With both the theoretical and an exemplary practical applicability of AuRAs to AR systems demonstrated, we now review our own approach to <i>how</i> AuRAs can be developed by introducing the AFAR toolkit, which allows for the rapid prototyping of AuRAs.</p></div></div></section><section aria-labelledby="Sec30"><div class="c-article-section" id="Sec30-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec30">How to create AuRAs: the AFAR NetBeans plugin</h2><div class="c-article-section__content" id="Sec30-content"><p>After justifying <i>why</i> and <i>when</i> to employ AuRAs, the final section of this paper will discuss <i>how</i> AuRAs can be used to develop AR applications. Most current AR projects are normally designed from scratch or are based on previous projects. The software used in the experiment described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a> is no exception in this regard. Starting out originally as an ad hoc implementation, the project was eventually split off and expanded into the AFAR toolkit, a module for the NetBeans development environment that enables developers to create intentional agents for AR applications according to the NeXuS mixed reality framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005b" title="O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481–486" href="/article/10.1007/s10055-013-0239-4#ref-CR53" id="ref-link-section-d61414e1907">2005b</a>). The intention behind AFAR, which is available under the lesser general public license (LGPL) and can be downloaded from <a href="http://sourceforge.net/projects/afar/">http://sourceforge.net/projects/afar/</a>, is to facilitate the development of AuRAs through common development tools.</p><p>Combined with the agent factory NetBeans module, AFAR allows for rapid prototyping of AR scenarios running Agent Factory agents through the use of the NeXuS framework, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig6">6</a>. Before releasing the toolkit online, an evaluation into its usefulness was undertaken using the SUMI questionnaire (Kirakowski and Corbett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Kirakowski J, Corbett M (1993) SUMI: the software usability measurement inventory. Br J Educ Technol 24(3):210–212" href="/article/10.1007/s10055-013-0239-4#ref-CR34" id="ref-link-section-d61414e1924">1993</a>), a set of questions for measuring the quality of software from the perspective of the end-user.<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> Before discussing the results of that questionnaire, a description of the AFAR architecture is needed to understand the workings of the toolkit.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Screenshot of an AFAR agent and the control interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec31">Architecture of AFAR</h3><p>The AFAR architecture (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig7">7</a>) is designed to facilitate the rapid prototyping and deployment of experiments and applications that fuse physical and virtual content into an AR environment through the use of intentional agent technology. AFAR utilises BDI agents in particular, as these imbue AR applications with the reasoning capabilities to behave in a realistic fashion. The design approach of a layered architecture, comprising of a number of customisable hardware and software components, is one that has been inherited and adapted from the NeXuS Framework (O’Hare et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="O’Hare GM, Campbell AG, Stafford JW, Aiken R (2005a) NeXuS: behavioural realism in mixed reality scenarios through virtual sensing. In: Proceedings of the eighteenth international conference on computer animation and social agents (CASA 2005), Hong Kong" href="/article/10.1007/s10055-013-0239-4#ref-CR51" id="ref-link-section-d61414e1964">2004</a>). This architectural approach enhances the flexibility and deployability of AFAR in developing applications, as it facilitates the addition of new components without the requirement of a complete system redesign. The overall maintainability of the system is also enhanced by the architecture’s logical tier separation, as these minimise the potential for implementing complications caused by the modification of pre-existing system components. The three distinct layers into which the AFAR architecture is divided consist of:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <i>Agent platform layer</i> Comprising of the toolset for agent development and the runtime environment on which to deploy said agents. The runtime environment is responsible for processing each agent’s set of beliefs and checking whether they trigger a corresponding commitment rule.</p>
                    </li>
                    <li>
                      <p>
                        <i>Agent layer</i> Containing the application-specific agents. An agent interacts with the world layer via sensors, which perceive the AR environment and generate beliefs about it, and actuators, which when triggered by commitment rules, enact changes upon the world.</p>
                    </li>
                    <li>
                      <p>
                        <i>World layer</i> Representing the AR environment itself. This layer is further subdivided into its physical and virtual components to reflect the duality of this technology.</p>
                    </li>
                  </ul>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Three-tiered architecture of the AFAR toolkit</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The agent platform layer’s run-time environment provides the reasoning engine that continually interprets the high-level mental state constructs of the system agents. This layer’s capability to interpret these high-level statements allows AFAR applications to be implemented in a more abstract fashion.</p><p>The agent layer contains the agents developed and deployed on the underlying agent platform. The agents that inhabit this layer define and control the functionality of every AFAR-implemented application. While all such applications require the deployment of at least one agent, there is no fundamental upper limit imposed by the architecture on the number of agents that a system may utilise. Developers are afforded the freedom to decide the optimal number of required agents based upon traditional considerations, such as the functional requirements of an application and the hardware resources of the platform on which the application is to be deployed.</p><p>The world layer of the AFAR architecture encompasses the AR environment that the system agents inhabit and manipulate. Just as an AR environment is composed of both virtual and physical artefacts, the world layer design is similarly subdivided between its virtual and physical content. The physical content is occupied by the actual hardware components of an AFAR-created application, whereas the virtual content’s subdivision broadly encompasses the software interfaces for these devices. The AFAR architecture is designed to be flexible enough to facilitate the addition of a multitude of hardware I/O devices in order to maximise its overall deployability. At a minimum, this subdivision should contain a video capture device (such as a webcam) and a display device (such as a computer monitor or a head-mounted display). The addition of other hardware devices is dictated by the requirements of the specific application that is being developed with AFAR.</p><h3 class="c-article__sub-heading" id="Sec32">AFAR implementation methodology</h3><p>The implementation of the AFAR toolkit required a number of components to be integrated. This can be conceptually broken down into two areas: (1) creating the agents, or <i>ghosts</i>, behind the AuRAs; and (2) the virtual models they will be embedded within, the <i>shells</i>. Indeed, the concept of ghosts and shells is integral to the underlying philosophy of AFAR. Ghosts embodied in shells allow for the creation of AuRAs as interface agent, as outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec6">3.1</a>. However, not all ghosts need to embody a shell. Such disembodied ghosts can instead be used to address a complex problem without a virtual representation in the AR environment, illustrating the AuRAs as design approach, as outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec7">3.2</a>.</p><p>To create the ghost component, AFAR uses the agent factory framework as the development and runtime environment. Agent Factory allows for the rapid prototyping of agents that follow the BDI paradigm and conform to the FIPA standard (Bellifemine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bellifemine F, Poggi A, Rimassa G (2001) Developing multi-agent systems with a FIPA-compliant agent framework. Softw Pract Exp 31(2):103–128" href="/article/10.1007/s10055-013-0239-4#ref-CR3" id="ref-link-section-d61414e2044">2001</a>), set out to help intra-agent interoperability. This affords the AuRAs constructed in AFAR the potential to interact with other agent-based platforms. AuRAs constructed using AFAR employ the Agent Factory Agent Programming Language version 2 (AFAPL2), a high-level programming language designed for defining the mental states of Agent Factory BDI agents. The syntax of AFAPL2 enables the definition of the following mental attributes: <i>commitments</i>, representing the BDI notion of intentions; <i>plans</i>, comprising a set of activities that an agent can perform; <i>commitment rules</i>, corresponding to the BDI notion of desires, these enable an agent to decide the appropriate plan or action; <i>roles</i>, pre-defined roles that an agent may perform in; and <i>ontologies</i>, the format of the facts and objects that an agent may use in the process of reasoning about its environment.</p><p>AFAR does not follow a strict methodology, as the specific steps that need to be taken to implement individual applications may vary substantially due to conventional software engineering considerations such as scale and complexity. In a broader context, however, one can distinguish five fundamental tasks that must be accomplished in order for an application to be realised with AFAR. We will now introduce and illustrate these tasks using the simple problem of associating a virtual model with a physical marker and creating a control panel to adjust the direction the model moves in.<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup> To this end, a developer has to implement the following five steps:</p><p>
                  <i>Encapsulating 3D data as shell objects</i> This step requires the creation of the underlying 3D models for the AR application and the encapsulation of said models in shell objects, thus making them available to the AR environment. The 3D models, or shells, for the agents are created using Java3D, a scene graph-based wrapper for OpenGL. The scene graph generated by Java3D is an integral part of AFAR, and the shells are realised as Java3D <span class="c-literal">
                    <span class="u-monospace">TransformGroup</span>
                  </span> objects. In concrete terms, a developer has to simply subclass the abstract <span class="c-literal">
                    <span class="u-monospace">Shell</span>
                  </span> class and take care of loading the virtual model.</p><p>
                  <i>Adding shell objects to the AR environment</i> The shell objects need to be assigned fiducial markers, so they can be tracked in the real world, and then registered with the environment. Once this is completed, shells can then be attached to a node within the scene graph, which are ultimately registered correctly with the real world using said fiducial markers to create an AR environment. jARToolKit (Geiger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Geiger C, Reimann C, Sticklein J, Paelke V (2002) JARToolKit: a java binding for ARToolKit. In: The first IEEE international augmented reality toolkit workshop (ART 02). IEEE" href="/article/10.1007/s10055-013-0239-4#ref-CR18" id="ref-link-section-d61414e2096">2002</a>), a Java wrapper for ARToolKit that supports GL4Java, JOGL and Java3D, is the marker toolkit of choice for AFAR.</p><p>
                  <i>Implementing perceptors for sensing the environment</i> Perceptors must be written that can generate beliefs about the world so that the AuRAs can make sense of their environment. The general structure of a perceptor is as follows:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figa_HTML.gif" alt="figurea" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div>
                <p>The above code snippet defines a named perceptor that is mapped to an implementing class and defines beliefs consistent with the given ontology. The concept of ontologies, for historical reasons, is used slightly different than what the term is generally associated with by the wider computer science community. Specifically, ontologies within Agent Factory define the structure of beliefs that a perceptor is allowed to generate. The basic outline of an ontology is given below:
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figb_HTML.gif?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figb_HTML.gif" alt="figureb" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div>
                <p>Constructing a purpose-built ontology for the aforementioned example can be as simple as:
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figc_HTML.gif?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figc_HTML.gif" alt="figurec" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div>
                <p>This determines that a perceptor using the example ontology must generate beliefs that fit the <span class="c-literal">
                    <span class="u-monospace">directionPressed(?direction)</span>
                  </span> template predicate.<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup> In practical terms, this means that the implementing class must contain a statement of the following form within its <span class="c-literal">
                    <span class="u-monospace">perceive</span>
                  </span> method, which every perceptor class must implement.
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-d"><figure><div class="c-article-section__figure-content" id="Figd"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figd_HTML.gif?as=webp"></source><img aria-describedby="figure-d-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figd_HTML.gif" alt="figured" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-d-desc"></div></div></figure></div>
                <p>The <span class="c-literal">
                    <span class="u-monospace">adoptBelief(…)</span>
                  </span> statement enters the given belief into the agent’s active belief set, with the <span class="c-literal">
                    <span class="u-monospace">direction</span>
                  </span> variable being expanded to the direction pressed.</p><p>
                  <i>Implementing actuators for enacting changes upon the environment</i> Once perceptors have been written that generate beliefs, actuators must then be developed that allow AuRAs to interact with the shells within the environment based on those beliefs. The basic template of an actuator is given as follows:
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-e"><figure><div class="c-article-section__figure-content" id="Fige"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fige_HTML.gif?as=webp"></source><img aria-describedby="figure-e-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fige_HTML.gif" alt="figuree" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-e-desc"></div></div></figure></div>
                <p>The action is automatically triggered by the Agent Factory framework according to the agent’s commitment rules (see next step) and supplied with parameters, if specified, by calling the <span class="c-literal">
                    <span class="u-monospace">act()</span>
                  </span> method that every actuator class implements. Within this method, the developer can then execute code that displaces the virtual model relative to the marker’s centre, to stay within the example mentioned above.</p><p>
                  <i>Defining agent behaviours through commitment rules</i> As discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec32">5.2</a>, commitment rules allow the AuRA to take action based on the beliefs generated through its perceptors to bring about a specific goal. The general structure of a commitment rule is as follows:
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-f"><figure><div class="c-article-section__figure-content" id="Figf"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figf_HTML.gif?as=webp"></source><img aria-describedby="figure-f-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figf_HTML.gif" alt="figuref" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-f-desc"></div></div></figure></div>
                <p>The declarative nature of commitment rules allows for the creation of an emergent interaction between the AuRA and its environment without having to explicitly describe every possible combination of actions for it to take. For example, looking at the direction pad visible in the screenshot in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig6">6</a>, a commitment rule that would trigger the model to move based on the direction pressed could look as follows:
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-g"><figure><div class="c-article-section__figure-content" id="Figg"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figg_HTML.gif?as=webp"></source><img aria-describedby="figure-g-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figg_HTML.gif" alt="figureg" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-g-desc"></div></div></figure></div>
                <p>Once the agent’s perceptors cause the agent to adopt a belief that matches the left-hand side of a commitment rule, said rule is triggered and results in the agent executing the action(s) stated within the rule.</p><p>Following all the steps above thus creates an agent that can sense which direction the user presses and take action accordingly. While this is, of course, a very simplified example, we nonetheless believe it to sufficiently illustrate the basic steps involved in developing AuRAs using the AFAR toolkit as well as help to envisage the possibilities for much more complex and autonomous behaviour by the agent, based on the interplay of individually very simple actuators, perceptors and commitment rules.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec33">Implementing the navigational agents in AFAR</h4><p>To give a slightly more complex example, consider the following code which is a hypothetical and nearly complete reimplementation of the navigational functionality used in the experiment in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec15">4</a> using AFAR. Specifically, the below code specifies the ontology, perceptor, action and commitment rules that let an individual agent decide the distance to the target and the direction to point an arrow, based purely on information from its immediate surroundings and collaboration with neighbouring agents.
</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-h"><figure><div class="c-article-section__figure-content" id="Figh"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figh_HTML.gif?as=webp"></source><img aria-describedby="figure-h-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Figh_HTML.gif" alt="figureh" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-h-desc"></div></div></figure></div>
                  <p>Every agent in the supermarket is embedded in a shelf and can sense its neighbours via a <span class="c-literal">
                      <span class="u-monospace">NeighbourPerceptor</span>
                    </span> class that generates a belief of the form <span class="c-literal">
                      <span class="u-monospace">neighbour (?name, ?direction)</span>
                    </span> for every neighbouring agent in the cardinal and ordinal directions. All agents further have a belief of the current target, which corresponds simply to the name of the agent embedded in the shelf that contains the target object. As every agent is aware of its own name, if an agent realises it is the target, it sets the distance to target to zero.</p><p>Upon every cycle update to their mental state, the agent will then perceive its neighbours and their relative location and, if the neighbour is not previously known, exchange distance information with the new neighbour using FIPA messages (not shown). If a neighbour is closer to the target than itself, the agent will update its own distance to target to be one greater than the neighbouring agent and set that agent as new closest neighbour (not shown). If the agent is not already pointing its arrow in the direction of the closest neighbour, the second commitment rule shown above will fire and cause the arrow to point in the correct direction.</p><p>In this manner, the agents collaborate on deciding the shortest route to the target and ensure that each individual agent always points its arrow in the correct direction.</p><h3 class="c-article__sub-heading" id="Sec34">Evaluation of the toolkit</h3><p>To evaluate the usability of AFAR, a study was conducted with 32 individuals who all had at least some experience in one of the following skill sets: coding with the Java programming language, agent-oriented programming, 3D graphics programming, usage of an integrated development environment (IDE) and finally AR development. Candidates who volunteered to participate in the evaluation were requested to perform a series of tasks that illustrate some of the key features of the AFAR system. The tasks were primarily designed to see whether the volunteers could develop a basic AR application by adding a model to a marker and then code a control panel to interact with the AuRA (as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig6">6</a>). Upon finishing these tasks, the participating subjects completed a SUMI questionnaire to establish their opinions regarding the experience.</p><p>SUMI’s item consensual analysis (ICA) is designed to enable the analysis of individual responses and to contrast observed candidate responses to individual items with those that are expected. A chi-square test is used to express the difference between the observed and expected values. The creators of SUMI have constructed normative tables that define a generic standard for software. State-of-the-art software is set at a standard score value of 50. Evaluated software that obtains a score above 50 is considered to be better than the state of the art and, conversely, software that scores below the 50 threshold is regarded as relatively inferior. Feedback data are processed in a manner that converts the raw scores captured in a standard SUMI questionnaire to a scale with a mean of 50. Accordingly it becomes possible for an evaluator to compare feedback from their own experiments with a quantified norm. The standard SUMI questionnaire itself consists of 50 individual statements to which respondents must state whether they are in agreement, disagreement or are uncertain of the expressed consensus.</p><p>AFAR’s profile analysis scores on SUMI’s six-dimension scale are summarised by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0239-4#Fig8">8</a>. A score below 50 indicates the requirement to review a system’s usability. Values above 50 suggest that usability is above average, while a score above 60 indicates that a software system is highly usable. AFAR has a global usability score of 61, a strong indication that many of the evaluation participants considered using AFAR to be a positive experience. The efficiency and affect dimensions also share a score of 61, an indication that candidates found the use of AFAR to be a stimulating experience and that they considered the system functionality to be both fast and effective. A score of 59 for helpfulness suggests a positive perception of AFAR’s presentation and methodology. A score of 55 for both the control and learnability dimensions, respectively, indicate that participants perceived AFAR to be functionally consistent and that they considered themselves to be capable of becoming familiar with the framework’s features after a short period of time.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0239-4/MediaObjects/10055_2013_239_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Results of the SUMI evaluation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0239-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Evaluating the usability of AFAR for programmers was undertaken as a necessary first step in the validation of the toolkit. Assessing code quality of AFAR-produced applications would be a necessary second step but is notoriously difficult (Kanellopoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kanellopoulos Y, Antonellis P, Antoniou D, Makris C, Theodoridis E, Tjortjis C, Tsirakis N (2010) Code quality evaluation methodology using the ISO/IEC 9126 standard. Int J Softw Eng Appl 1(3):17–36" href="/article/10.1007/s10055-013-0239-4#ref-CR31" id="ref-link-section-d61414e2317">2010</a>) and, for comparison with other AR development toolkits, would likely necessitate a standard development methodology, among other restrictions. The task of placing a model on a marker and then developing an interface to control it is admittedly simple but also very universal. In fact, it could conceivably be taken as a common baseline task to enable comparison with other marker-based AR development toolkits.</p><h3 class="c-article__sub-heading" id="Sec35">Summary</h3><p>This section described the AFAR toolkit, which is designed to aid in the development of AuRAs. Its architecture and implementation has been influenced by previous AuRA-based projects discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec4">2.2</a> and the exploration of the AuRA concept as both an interface and design paradigm within Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0239-4#Sec5">3</a>. The outlined AFAR methodology is intended to help developers get started with the toolkit. The user evaluation demonstrated AFAR’s general usability, while the choice of task provides an easily reproducible baseline that could be used to evaluate future AR development toolkits.</p></div></div></section><section aria-labelledby="Sec36"><div class="c-article-section" id="Sec36-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec36">Conclusion</h2><div class="c-article-section__content" id="Sec36-content"><p>The main contribution of this paper lies in its identification of the distinct field of <i>AuRAs</i> and in its exploration of this concept by addressing three central questions regarding this novel area of research:
</p><ul class="u-list-style-bullet">
                  <li>
                    <p>
                      <i>Why should AuRAs be used to develop AR application</i> The discussion of current and past projects that use an AuRA concept as interface or design paradigm demonstrated the potential usefulness of agents in AR. Using Russell and Norvig’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Russell SJ, Norvig P (2003) Artificial intelligence: a modern approach, 2nd edn. Prentice Hall, Upper Saddle River" href="/article/10.1007/s10055-013-0239-4#ref-CR60" id="ref-link-section-d61414e2356">2003</a>) concept of task environment properties, this section highlighted why agents can be beneficial despite initially adding complexity to the development of an AR application.</p>
                  </li>
                  <li>
                    <p>
                      <i>When should developers use AuRAs</i> The evaluation of a simulated AR experiment demonstrated the advantages of using AuRAs in the context of the experimental task. The experiment showed how AuRAs can increase performance at a navigation task over traditional AR solutions. At the same time, the obtained results generated additional questions due to the mixed responses to the employed virtual character.</p>
                  </li>
                  <li>
                    <p>
                      <i>How can AuRAs be developed</i> It is envisaged that toolkits, such as AFAR, which is ready to go and fully integrated with a common agent-based development environment, will allow researchers to easily integrate agents into their projects. As a first step, this paper demonstrated the sufficiency and adequacy of this toolkit to create marker-based AR applications by subjecting AFAR to an ISO-recognised user satisfaction test and reporting the results.</p>
                  </li>
                </ul>
              <p>The true practicality of AuRAs becomes all the more evident when one considers the increasingly pervasive nature of modern computing paradigms. One particular usage scenario that has become relatively commonplace is that of an AR application deployed on a smart phone. Such applications are inherently event driven, as their behaviour is dictated by the asynchronous flow of data from the numerous hardware sensors that are typically found on these devices (e.g. camera, accelerometer, GPS receiver etc.). The agent-oriented approach of AuRAs provides the instrument to define complex behavioural mechanisms that are responsive to this constantly fluctuating environment through the means of high-level mental state definitions.</p><p>Seen in this light, AuRAs can be seen not only as novel concept for the development of AR applications but also as a powerful metaphor. The concept of AuRAs builds on the idea of an aura around the user. This aura may take any shape or form, and defining this form will require creative ideas from artists just as much as the technical skills to produce it. Weiser (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Weiser M (1992) Does ubiquitous computing need interface agents? In: MIT media lab symposium on user interface agents" href="/article/10.1007/s10055-013-0239-4#ref-CR74" id="ref-link-section-d61414e2385">1992</a>) has outlined how much metaphors matter: they can speed up understanding but can just as readily delay progress in a field. We strongly believe the AuRA metaphor to be of the former kind and that answering the questions of why, when and how offers the key to the successful adoption, delivery and deployment of AuRAs.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>While similar systems have been deployed using robots as shopping guides (Black et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Black D, Clemmensen N, Skov M (2009) Supporting the supermarket shopping experience through a context-aware shopping trolley. In: Proceedings of the 21st annual conference of the Australian computer–human interaction special interest group: design: open 24/7. ACM, pp 33–40" href="/article/10.1007/s10055-013-0239-4#ref-CR4" id="ref-link-section-d61414e823">2009</a>; Gharpure and Kulyukin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Gharpure C, Kulyukin V (2008) Robot-assisted shopping for the blind: issues in spatial cognition and product selection. Intell Serv Robot 1(3):237–251" href="/article/10.1007/s10055-013-0239-4#ref-CR20" id="ref-link-section-d61414e826">2008</a>), they are quite common use cases for AR as well [e.g. the aforementioned ShopNavi (Nagao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Nagao K (1998) Agent augmented reality: agents integrate the real world with cyberspace. In: Ishida T (ed) Community computing: collaboration over global information networks. Wiley, New York" href="/article/10.1007/s10055-013-0239-4#ref-CR47" id="ref-link-section-d61414e829">1998</a>) or the more recent PromoPad (Zhu and Owen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhu W, Owen C (2008) Design of the promopad: an automated augmented-reality shopping assistant. J Organ End User Comput (JOEUC) 20(3):41–56" href="/article/10.1007/s10055-013-0239-4#ref-CR76" id="ref-link-section-d61414e832">2008</a>)].</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>While it is an undecided issue whether presence scores can be compared between different experiments (Usoh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Usoh M, Catena E, Arman S, Slater M (2000) Using presence questionnaires in reality. Presence Teleoper Virtual Environ 9(5):497–503" href="/article/10.1007/s10055-013-0239-4#ref-CR72" id="ref-link-section-d61414e1033">2000</a>), we believe they can nonetheless serve as a useful indicator of the level of immersion experienced by users.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>The supermarket used in the experiment was based on blueprints of award-winning designs to prevent any bias in its construction (Pegler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Pegler MM (1990) Market, supermarket and hypermarket design. Visual Reference, New York" href="/article/10.1007/s10055-013-0239-4#ref-CR55" id="ref-link-section-d61414e1148">1990</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>The experiment software failed to record distance for some of the participants, which is why subject numbers and degrees of freedom are different for some of the distance statistics.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.oculusvr.com/">http://www.oculusvr.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.virtuix.com/">http://www.virtuix.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>SUMI is a well-established commercial system that is referenced by the ISO 9241-11 standard (ISO <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="ISO 9241-11 (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidance on usability. International Organization for Standardization, Geneva, Switzerland" href="/article/10.1007/s10055-013-0239-4#ref-CR27" id="ref-link-section-d61414e1931">1998</a>) as a method for testing user satisfaction.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p>This is essentially the same objective given to participants that completed the user evaluation reported in the next section.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9"><span class="c-article-footnote--listed__index">9.</span><div class="c-article-footnote--listed__content"><p>A predicate is the overarching term for a statement of logic that an agent can process, be that a belief, plan or commitment rule.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Adams, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Adams D (1990) Hyperland. BBC, London" /><p class="c-article-references__text" id="ref-CR1">Adams D (1990) Hyperland. BBC, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hyperland&amp;publication_year=1990&amp;author=Adams%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ahrndt S, Fähndrich J, Lützenberger M, Rieger A, Albayrak S (2012) An agent-based augmented reality demonstrat" /><p class="c-article-references__text" id="ref-CR2">Ahrndt S, Fähndrich J, Lützenberger M, Rieger A, Albayrak S (2012) An agent-based augmented reality demonstrator in the domestic energy domain. In: PAAMS ’12, pp 225–228</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Bellifemine, A. Poggi, G. Rimassa, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Bellifemine F, Poggi A, Rimassa G (2001) Developing multi-agent systems with a FIPA-compliant agent framework." /><p class="c-article-references__text" id="ref-CR3">Bellifemine F, Poggi A, Rimassa G (2001) Developing multi-agent systems with a FIPA-compliant agent framework. Softw Pract Exp 31(2):103–128</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20multi-agent%20systems%20with%20a%20FIPA-compliant%20agent%20framework&amp;journal=Softw%20Pract%20Exp&amp;volume=31&amp;issue=2&amp;pages=103-128&amp;publication_year=2001&amp;author=Bellifemine%2CF&amp;author=Poggi%2CA&amp;author=Rimassa%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Black D, Clemmensen N, Skov M (2009) Supporting the supermarket shopping experience through a context-aware sh" /><p class="c-article-references__text" id="ref-CR4">Black D, Clemmensen N, Skov M (2009) Supporting the supermarket shopping experience through a context-aware shopping trolley. In: Proceedings of the 21st annual conference of the Australian computer–human interaction special interest group: design: open 24/7. ACM, pp 33–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="ME. Bratman, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Bratman ME (1987) Intentions, plans, and practical reason. Harvard University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR5">Bratman ME (1987) Intentions, plans, and practical reason. Harvard University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intentions%2C%20plans%2C%20and%20practical%20reason&amp;publication_year=1987&amp;author=Bratman%2CME">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Brooks, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Brooks RA (1991) Intelligence without representation. Artif Intell 47(1–3):139–159" /><p class="c-article-references__text" id="ref-CR6">Brooks RA (1991) Intelligence without representation. Artif Intell 47(1–3):139–159</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0004-3702%2891%2990053-M" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intelligence%20without%20representation&amp;journal=Artif%20Intell&amp;volume=47&amp;issue=1%E2%80%933&amp;pages=139-159&amp;publication_year=1991&amp;author=Brooks%2CRA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Campbell A, Collier R, Dragone M, Görgü L, Holz T, O’GradyM, O’Hare GMP, Sassu A, Stafford J (2012) Facilitati" /><p class="c-article-references__text" id="ref-CR7">Campbell A, Collier R, Dragone M, Görgü L, Holz T, O’GradyM, O’Hare GMP, Sassu A, Stafford J (2012) Facilitating ubiquitous interaction using intelligent agents. In: Human–computer interaction: the agency perspective, pp 303–326</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Charles F, Cavazza M, Mead S, Martin O, Nandi A, Marichal X (2004) Compelling experiences in mixed reality int" /><p class="c-article-references__text" id="ref-CR8">Charles F, Cavazza M, Mead S, Martin O, Nandi A, Marichal X (2004) Compelling experiences in mixed reality interactive storytelling. In: Proceedings of the 2004 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, Singapore, pp 32–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Cruz-Neira, DJ. Sandin, TA. DeFanti, RV. Kenyon, JC. Hart, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic vir" /><p class="c-article-references__text" id="ref-CR9">Cruz-Neira C, Sandin DJ, DeFanti TA, Kenyon RV, Hart JC (1992) The CAVE: audio visual experience automatic virtual environment. Commun ACM 35(6):64–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F129888.129892" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20CAVE%3A%20audio%20visual%20experience%20automatic%20virtual%20environment&amp;journal=Commun%20ACM&amp;volume=35&amp;issue=6&amp;pages=64-72&amp;publication_year=1992&amp;author=Cruz-Neira%2CC&amp;author=Sandin%2CDJ&amp;author=DeFanti%2CTA&amp;author=Kenyon%2CRV&amp;author=Hart%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dautenhahn K (1999) Embodiment and interaction in socially intelligent life-like agents. In: Computation for m" /><p class="c-article-references__text" id="ref-CR10">Dautenhahn K (1999) Embodiment and interaction in socially intelligent life-like agents. In: Computation for metaphors, analogy, and agents, pp 102–141</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Dehn, S. Van Mulken, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Dehn D, Van Mulken S (2000) The impact of animated interface agents: a review of empirical research. Int J Hum" /><p class="c-article-references__text" id="ref-CR11">Dehn D, Van Mulken S (2000) The impact of animated interface agents: a review of empirical research. Int J Hum Comput Stud 52(1):1–22</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.1999.0325" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20impact%20of%20animated%20interface%20agents%3A%20a%20review%20of%20empirical%20research&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=52&amp;issue=1&amp;pages=1-22&amp;publication_year=2000&amp;author=Dehn%2CD&amp;author=Van%20Mulken%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Denby, A. Campbell, H. Carr, G. O’Hare, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Denby B, Campbell A, Carr H, O’Hare G (2009) The LAIR: lightweight affordable immersion room. Presence Teleope" /><p class="c-article-references__text" id="ref-CR12">Denby B, Campbell A, Carr H, O’Hare G (2009) The LAIR: lightweight affordable immersion room. Presence Teleoper Virtual Environ 18(5), 409–411</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20LAIR%3A%20lightweight%20affordable%20immersion%20room&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=18&amp;issue=5&amp;pages=409-411&amp;publication_year=2009&amp;author=Denby%2CB&amp;author=Campbell%2CA&amp;author=Carr%2CH&amp;author=O%E2%80%99Hare%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dragone M, Holz T, O’Hare GMP (2007) Using mixed reality agents as social interfaces for robots. In: Proceedin" /><p class="c-article-references__text" id="ref-CR13">Dragone M, Holz T, O’Hare GMP (2007) Using mixed reality agents as social interfaces for robots. In: Proceedings of the 16th IEEE international symposium on robot and human interactive communication (RO-MAN 2007). IEEE, pp 1161–1166</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Drascic, P. Milgram, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Drascic D, Milgram P (1996) Perceptual issues in augmented reality. Stereosc Disp Virtual Real Syst III 2653(1" /><p class="c-article-references__text" id="ref-CR14">Drascic D, Milgram P (1996) Perceptual issues in augmented reality. Stereosc Disp Virtual Real Syst III 2653(1):123–134</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1117%2F12.237425" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20issues%20in%20augmented%20reality&amp;journal=Stereosc%20Disp%20Virtual%20Real%20Syst%20III&amp;volume=2653&amp;issue=1&amp;pages=123-134&amp;publication_year=1996&amp;author=Drascic%2CD&amp;author=Milgram%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Duffy, G. O’Hare, J. Bradley, A. Martin, B. Schön, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Duffy B, O’Hare G, Bradley J, Martin A, Schön B (2005) Future reasoning machines: mind and body. Kybernetes 34" /><p class="c-article-references__text" id="ref-CR15">Duffy B, O’Hare G, Bradley J, Martin A, Schön B (2005) Future reasoning machines: mind and body. Kybernetes 34(9/10):1404–1420</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1108%2F03684920510614731" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Future%20reasoning%20machines%3A%20mind%20and%20body&amp;journal=Kybernetes&amp;volume=34&amp;issue=9%2F10&amp;pages=1404-1420&amp;publication_year=2005&amp;author=Duffy%2CB&amp;author=O%E2%80%99Hare%2CG&amp;author=Bradley%2CJ&amp;author=Martin%2CA&amp;author=Sch%C3%B6n%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ferber J (1999) Multi-agent systems: an introduction to distributed artificial intelligence, vol 33. Addison-W" /><p class="c-article-references__text" id="ref-CR16">Ferber J (1999) Multi-agent systems: an introduction to distributed artificial intelligence, vol 33. Addison-Wesley, Reading</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Foner LN (1993) What’s an agent anyway? A sociological case study. FTP report. MIT Media Lab, Cambridge, MA" /><p class="c-article-references__text" id="ref-CR17">Foner LN (1993) What’s an agent anyway? A sociological case study. FTP report. MIT Media Lab, Cambridge, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geiger C, Reimann C, Sticklein J, Paelke V (2002) JARToolKit: a java binding for ARToolKit. In: The first IEEE" /><p class="c-article-references__text" id="ref-CR18">Geiger C, Reimann C, Sticklein J, Paelke V (2002) JARToolKit: a java binding for ARToolKit. In: The first IEEE international augmented reality toolkit workshop (ART 02). IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Geisser, SW. Greenhouse, " /><meta itemprop="datePublished" content="1958" /><meta itemprop="headline" content="Geisser S, Greenhouse SW (1958) An extension of box’s results on the use of the f distribution in multivariate" /><p class="c-article-references__text" id="ref-CR19">Geisser S, Greenhouse SW (1958) An extension of box’s results on the use of the <i>f</i> distribution in multivariate analysis. Ann Math Stat 29(3):885–891</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1214%2Faoms%2F1177706545" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0090.35804" aria-label="View reference 19 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=96331" aria-label="View reference 19 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20extension%20of%20box%E2%80%99s%20results%20on%20the%20use%20of%20the%20f%20distribution%20in%20multivariate%20analysis&amp;journal=Ann%20Math%20Stat&amp;volume=29&amp;issue=3&amp;pages=885-891&amp;publication_year=1958&amp;author=Geisser%2CS&amp;author=Greenhouse%2CSW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Gharpure, V. Kulyukin, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Gharpure C, Kulyukin V (2008) Robot-assisted shopping for the blind: issues in spatial cognition and product s" /><p class="c-article-references__text" id="ref-CR20">Gharpure C, Kulyukin V (2008) Robot-assisted shopping for the blind: issues in spatial cognition and product selection. Intell Serv Robot 1(3):237–251</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11370-008-0020-9" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Robot-assisted%20shopping%20for%20the%20blind%3A%20issues%20in%20spatial%20cognition%20and%20product%20selection&amp;journal=Intell%20Serv%20Robot&amp;volume=1&amp;issue=3&amp;pages=237-251&amp;publication_year=2008&amp;author=Gharpure%2CC&amp;author=Kulyukin%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Google (2009) Layar: augmented reality browser. http://www.google.com/mobile/goggles/, accessed Aug 2010" /><p class="c-article-references__text" id="ref-CR21">Google (2009) Layar: augmented reality browser. <a href="http://www.google.com/mobile/goggles/">http://www.google.com/mobile/goggles/</a>, accessed Aug 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Görgü L, Campbell A, McCusker K, Dragone M, O’Grady M, O’Connor N, O’Hare G (2010) FreeGaming: mobile, collabo" /><p class="c-article-references__text" id="ref-CR22">Görgü L, Campbell A, McCusker K, Dragone M, O’Grady M, O’Connor N, O’Hare G (2010) FreeGaming: mobile, collaborative, adaptive and augmented ExerGaming. In: Proceedings of the 8th international conference on advances in mobile computing and multimedia. ACM, pp 173–179</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Haugeland, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Haugeland J (1989) Artificial intelligence: the very idea. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR23">Haugeland J (1989) Artificial intelligence: the very idea. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20intelligence%3A%20the%20very%20idea&amp;publication_year=1989&amp;author=Haugeland%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SR. Hedberg, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Hedberg SR (1998) Is AI going mainstream at last? A look inside Microsoft research. IEEE Intell Syst 13(2):21–" /><p class="c-article-references__text" id="ref-CR24">Hedberg SR (1998) Is AI going mainstream at last? A look inside Microsoft research. IEEE Intell Syst 13(2):21–25</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5254.671087" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20AI%20going%20mainstream%20at%20last%3F%20A%20look%20inside%20Microsoft%20research&amp;journal=IEEE%20Intell%20Syst&amp;volume=13&amp;issue=2&amp;pages=21-25&amp;publication_year=1998&amp;author=Hedberg%2CSR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Holz, AG. Campbell, GMP. O’Hare, JW. Stafford, A. Martin, M. Dragone, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Holz T, Campbell AG, O’Hare GMP, Stafford JW, Martin A, Dragone M (2011) MiRA—mixed reality agents. Int J Hum " /><p class="c-article-references__text" id="ref-CR25">Holz T, Campbell AG, O’Hare GMP, Stafford JW, Martin A, Dragone M (2011) MiRA—mixed reality agents. Int J Hum Comput Stud 69:251–268</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2010.10.001" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=MiRA%E2%80%94mixed%20reality%20agents&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=69&amp;pages=251-268&amp;publication_year=2011&amp;author=Holz%2CT&amp;author=Campbell%2CAG&amp;author=O%E2%80%99Hare%2CGMP&amp;author=Stafford%2CJW&amp;author=Martin%2CA&amp;author=Dragone%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Horvitz E, Breese J, Heckerman D, Hovel D, Rommelse K (1998) The Lumiere Project: Bayesian user modeling for i" /><p class="c-article-references__text" id="ref-CR26">Horvitz E, Breese J, Heckerman D, Hovel D, Rommelse K (1998) The Lumiere Project: Bayesian user modeling for inferring the goals and needs of software users. In: Proceedings of the 14th conference on uncertainty in artificial intelligence, pp 256–265</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ISO 9241-11 (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidan" /><p class="c-article-references__text" id="ref-CR27">ISO 9241-11 (1998) Ergonomic requirements for office work with visual display terminals (VDTs)—part 11: guidance on usability. International Organization for Standardization, Geneva, Switzerland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Iso M (2007) Denno coil. NHK Educational, Geneva, Switzerland" /><p class="c-article-references__text" id="ref-CR28">Iso M (2007) Denno coil. NHK Educational, Geneva, Switzerland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ju W, Nickell S, Eng K, Nass C (2005) Influence of colearner agent behavior on learner performance and attitud" /><p class="c-article-references__text" id="ref-CR29">Ju W, Nickell S, Eng K, Nass C (2005) Influence of colearner agent behavior on learner performance and attitudes. In: Extended abstracts on human factors in computing systems (CHI ’05). ACM, New York, pp 1509–1512</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Jurdak, A. Ruzzelli, G. O’Hare, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Jurdak R, Ruzzelli A, O’Hare G (2010) Radio sleep mode optimization in wireless sensor networks. IEEE Trans Mo" /><p class="c-article-references__text" id="ref-CR30">Jurdak R, Ruzzelli A, O’Hare G (2010) Radio sleep mode optimization in wireless sensor networks. IEEE Trans Mobile Comput 9(7):955–968</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMC.2010.35" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Radio%20sleep%20mode%20optimization%20in%20wireless%20sensor%20networks&amp;journal=IEEE%20Trans%20Mobile%20Comput&amp;volume=9&amp;issue=7&amp;pages=955-968&amp;publication_year=2010&amp;author=Jurdak%2CR&amp;author=Ruzzelli%2CA&amp;author=O%E2%80%99Hare%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Kanellopoulos, P. Antonellis, D. Antoniou, C. Makris, E. Theodoridis, C. Tjortjis, N. Tsirakis, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Kanellopoulos Y, Antonellis P, Antoniou D, Makris C, Theodoridis E, Tjortjis C, Tsirakis N (2010) Code quality" /><p class="c-article-references__text" id="ref-CR31">Kanellopoulos Y, Antonellis P, Antoniou D, Makris C, Theodoridis E, Tjortjis C, Tsirakis N (2010) Code quality evaluation methodology using the ISO/IEC 9126 standard. Int J Softw Eng Appl 1(3):17–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Code%20quality%20evaluation%20methodology%20using%20the%20ISO%2FIEC%209126%20standard&amp;journal=Int%20J%20Softw%20Eng%20Appl&amp;volume=1&amp;issue=3&amp;pages=17-36&amp;publication_year=2010&amp;author=Kanellopoulos%2CY&amp;author=Antonellis%2CP&amp;author=Antoniou%2CD&amp;author=Makris%2CC&amp;author=Theodoridis%2CE&amp;author=Tjortjis%2CC&amp;author=Tsirakis%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferen" /><p class="c-article-references__text" id="ref-CR32">Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings of the 2nd international workshop on augmented reality</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Katz, F. Dramas, G. Parseihian, O. Gutierrez, S. Kammoun, A. Brilhault, L. Brunet, M. Gallay, B. Oriola, M. Auvray, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Katz B, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M et" /><p class="c-article-references__text" id="ref-CR33">Katz B, Dramas F, Parseihian G, Gutierrez O, Kammoun S, Brilhault A, Brunet L, Gallay M, Oriola B, Auvray M et al (2012) Navig: guidance system for the visually impaired using virtual augmented reality. Technol Disabil 24(2):163–178</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navig%3A%20guidance%20system%20for%20the%20visually%20impaired%20using%20virtual%20augmented%20reality&amp;journal=Technol%20Disabil&amp;volume=24&amp;issue=2&amp;pages=163-178&amp;publication_year=2012&amp;author=Katz%2CB&amp;author=Dramas%2CF&amp;author=Parseihian%2CG&amp;author=Gutierrez%2CO&amp;author=Kammoun%2CS&amp;author=Brilhault%2CA&amp;author=Brunet%2CL&amp;author=Gallay%2CM&amp;author=Oriola%2CB&amp;author=Auvray%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kirakowski, M. Corbett, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Kirakowski J, Corbett M (1993) SUMI: the software usability measurement inventory. Br J Educ Technol 24(3):210" /><p class="c-article-references__text" id="ref-CR34">Kirakowski J, Corbett M (1993) SUMI: the software usability measurement inventory. Br J Educ Technol 24(3):210–212</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8535.1993.tb00076.x" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SUMI%3A%20the%20software%20usability%20measurement%20inventory&amp;journal=Br%20J%20Educ%20Technol&amp;volume=24&amp;issue=3&amp;pages=210-212&amp;publication_year=1993&amp;author=Kirakowski%2CJ&amp;author=Corbett%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koay K, Syrdal D, Walters M, Dautenhahn K (2009) A user study on visualization of agent migration between two " /><p class="c-article-references__text" id="ref-CR35">Koay K, Syrdal D, Walters M, Dautenhahn K (2009) A user study on visualization of agent migration between two companion robots. In: proceedings of 13th International Conference on Human-Computer Interaction (HCII 2009), San Diego, CA, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kretschmer U, Coors V, Spierling U, Grasbon D, Schneider K, Rojas I, Malaka R (2001) Meeting the spirit of his" /><p class="c-article-references__text" id="ref-CR36">Kretschmer U, Coors V, Spierling U, Grasbon D, Schneider K, Rojas I, Malaka R (2001) Meeting the spirit of history. In: Proceedings of the conference on virtual reality, archeology and cultural heritage (VAST ’01), pp 141–152</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kruijff E, Swan JE, Feiner S (2010) Perceptual issues in augmented reality revisited. In: Proceedings of 9th I" /><p class="c-article-references__text" id="ref-CR37">Kruijff E, Swan JE, Feiner S (2010) Perceptual issues in augmented reality revisited. In: Proceedings of 9th IEEE International Symposium on mixed and augmented reality (ISMAR 2010). IEEE Computer Society, Seoul, Korea, pp 3–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lancelle M, Settgast V, Fellner D (2009) Definitely affordable virtual environment. In: Proceedings of IEEE vi" /><p class="c-article-references__text" id="ref-CR38">Lancelle M, Settgast V, Fellner D (2009) Definitely affordable virtual environment. In: Proceedings of IEEE virtual reality. IEEE, p 1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee C, Bonebrake S, Hollerer T, Bowman DA (2009) A replication study testing the validity of AR simulation in " /><p class="c-article-references__text" id="ref-CR39">Lee C, Bonebrake S, Hollerer T, Bowman DA (2009) A replication study testing the validity of AR simulation in VR for controlled experiments. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2009). IEEE Computer Society, pp 203–204</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee C, Bonebrake S, Höllerer T, Bowman DA (2010) The role of latency in the validity of AR simulation. In: IEE" /><p class="c-article-references__text" id="ref-CR40">Lee C, Bonebrake S, Höllerer T, Bowman DA (2010) The role of latency in the validity of AR simulation. In: IEEE virtual reality conference (VR 2010), pp 11–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee Y, Choi J, Kim S, Lee S, Jang S (2011) Social augmented reality for sensor visualization in ubiquitous vir" /><p class="c-article-references__text" id="ref-CR41">Lee Y, Choi J, Kim S, Lee S, Jang S (2011) Social augmented reality for sensor visualization in ubiquitous virtual reality. In: Shumaker R (ed) Virtual and mixed reality: new trends, vol 6773. Lecture Notes in Computer Science. Springer, Berlin, pp 69–75</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lieberman H (1997) Autonomous interface agents. In: Proceedings of the SIGCHI conference on human factors in c" /><p class="c-article-references__text" id="ref-CR42">Lieberman H (1997) Autonomous interface agents. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’97). ACM, New York, pp 67–74</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AR. Lingley, M. Ali, Y. Liao, R. Mirjalili, M. Klonner, M. Sopanen, S. Suihkonen, T. Shen, BP. Otis, H. Lipsanen, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Lingley AR, Ali M, Liao Y, Mirjalili R, Klonner M, Sopanen M, Suihkonen S, Shen T, Otis BP, Lipsanen H et al (" /><p class="c-article-references__text" id="ref-CR43">Lingley AR, Ali M, Liao Y, Mirjalili R, Klonner M, Sopanen M, Suihkonen S, Shen T, Otis BP, Lipsanen H et al (2011) A single-pixel wireless contact lens display. J Micromech Microeng 21(12):125014</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1088%2F0960-1317%2F21%2F12%2F125014" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20single-pixel%20wireless%20contact%20lens%20display&amp;journal=J%20Micromech%20Microeng&amp;volume=21&amp;issue=12&amp;publication_year=2011&amp;author=Lingley%2CAR&amp;author=Ali%2CM&amp;author=Liao%2CY&amp;author=Mirjalili%2CR&amp;author=Klonner%2CM&amp;author=Sopanen%2CM&amp;author=Suihkonen%2CS&amp;author=Shen%2CT&amp;author=Otis%2CBP&amp;author=Lipsanen%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Littman M, Cassandra A, Pack Kaelbling L (1995) Learning policies for partially observable environments: scali" /><p class="c-article-references__text" id="ref-CR44">Littman M, Cassandra A, Pack Kaelbling L (1995) Learning policies for partially observable environments: scaling up. In: International conference on machine learning workshop—then conference, Citeseer, pp 362–370</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maes P, Darrell T, Blumberg B, Pentland A (1995) The ALIVE system: full-body interaction with autonomous agent" /><p class="c-article-references__text" id="ref-CR45">Maes P, Darrell T, Blumberg B, Pentland A (1995) The ALIVE system: full-body interaction with autonomous agents. In: Proceedings of the computer animation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst (special issue o" /><p class="c-article-references__text" id="ref-CR46">Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst (special issue on networked reality), E77-D(12), 1321–1329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nagao K (1998) Agent augmented reality: agents integrate the real world with cyberspace. In: Ishida T (ed) Com" /><p class="c-article-references__text" id="ref-CR47">Nagao K (1998) Agent augmented reality: agents integrate the real world with cyberspace. In: Ishida T (ed) Community computing: collaboration over global information networks. Wiley, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Obaid M, Niewiadomski R, Pelachaud C (2011) Perception of spatial relations and of coexistence with virtual ag" /><p class="c-article-references__text" id="ref-CR48">Obaid M, Niewiadomski R, Pelachaud C (2011) Perception of spatial relations and of coexistence with virtual agents. In: Intelligent virtual agents. Springer, Berlin, pp 363–369</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. O’Hare, M. O’Grady, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="O’Hare G, O’Grady M (2003) Gulliver’s genie: a multi-agent system for ubiquitous and intelligent content deliv" /><p class="c-article-references__text" id="ref-CR49">O’Hare G, O’Grady M (2003) Gulliver’s genie: a multi-agent system for ubiquitous and intelligent content delivery. Comput Commun 26(11):1177–1187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0140-3664%2802%2900252-9" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gulliver%E2%80%99s%20genie%3A%20a%20multi-agent%20system%20for%20ubiquitous%20and%20intelligent%20content%20delivery&amp;journal=Comput%20Commun&amp;volume=26&amp;issue=11&amp;pages=1177-1187&amp;publication_year=2003&amp;author=O%E2%80%99Hare%2CG&amp;author=O%E2%80%99Grady%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Hare GMP, Collier R, Conlon J, Abbas S (1998) Agent factory: an environment for constructing and visualising" /><p class="c-article-references__text" id="ref-CR50">O’Hare GMP, Collier R, Conlon J, Abbas S (1998) Agent factory: an environment for constructing and visualising agent communities. In: Proceedings of the ninth Irish conference on artificial intelligence and cognitive science (AICS 98), pp 249–261, Dublin, Ireland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Hare GM, Campbell AG, Stafford JW, Aiken R (2005a) NeXuS: behavioural realism in mixed reality scenarios thr" /><p class="c-article-references__text" id="ref-CR51">O’Hare GM, Campbell AG, Stafford JW, Aiken R (2005a) NeXuS: behavioural realism in mixed reality scenarios through virtual sensing. In: Proceedings of the eighteenth international conference on computer animation and social agents (CASA 2005), Hong Kong</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents." /><p class="c-article-references__text" id="ref-CR53">O’Hare GMP, Campbell AG, Stafford JW (2005b) NeXuS: delivering behavioural realism through intentional agents. In: Proceedings of the 3rd international conference on active media technology (AMT 2005). IEEE Press, Takamatsu, Japan, pp 481–486</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. O’Hare, M. O’Grady, C. Muldoon, J. Bradley, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="O’Hare G, O’Grady M, Muldoon C, Bradley J (2006) Embedded agents: a paradigm for mobile services. Int J Web Gr" /><p class="c-article-references__text" id="ref-CR54">O’Hare G, O’Grady M, Muldoon C, Bradley J (2006) Embedded agents: a paradigm for mobile services. Int J Web Grid Serv 2(4):379–405</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1504%2FIJWGS.2006.011711" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Embedded%20agents%3A%20a%20paradigm%20for%20mobile%20services&amp;journal=Int%20J%20Web%20Grid%20Serv&amp;volume=2&amp;issue=4&amp;pages=379-405&amp;publication_year=2006&amp;author=O%E2%80%99Hare%2CG&amp;author=O%E2%80%99Grady%2CM&amp;author=Muldoon%2CC&amp;author=Bradley%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="MM. Pegler, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Pegler MM (1990) Market, supermarket and hypermarket design. Visual Reference, New York" /><p class="c-article-references__text" id="ref-CR55">Pegler MM (1990) Market, supermarket and hypermarket design. Visual Reference, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Market%2C%20supermarket%20and%20hypermarket%20design&amp;publication_year=1990&amp;author=Pegler%2CMM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Pustka, M. Huber, C. Waechter, F. Echtler, P. Keitler, J. Newman, D. Schmalstieg, G. Klinker, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Pustka D, Huber M, Waechter C, Echtler F, Keitler P, Newman J, Schmalstieg D, Klinker G (2011) Automatic confi" /><p class="c-article-references__text" id="ref-CR56">Pustka D, Huber M, Waechter C, Echtler F, Keitler P, Newman J, Schmalstieg D, Klinker G (2011) Automatic configuration of pervasive sensor networks for augmented reality. IEEE Pervasive Comput 10(3):68–79</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMPRV.2010.50" aria-label="View reference 55">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20configuration%20of%20pervasive%20sensor%20networks%20for%20augmented%20reality&amp;journal=IEEE%20Pervasive%20Comput&amp;volume=10&amp;issue=3&amp;pages=68-79&amp;publication_year=2011&amp;author=Pustka%2CD&amp;author=Huber%2CM&amp;author=Waechter%2CC&amp;author=Echtler%2CF&amp;author=Keitler%2CP&amp;author=Newman%2CJ&amp;author=Schmalstieg%2CD&amp;author=Klinker%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ragan E, Wilkes C, Bowman DA, Hollerer T (2009) Simulation of augmented reality systems in purely virtual envi" /><p class="c-article-references__text" id="ref-CR57">Ragan E, Wilkes C, Bowman DA, Hollerer T (2009) Simulation of augmented reality systems in purely virtual environments. In: Proceedings of the 2009 IEEE virtual reality conference (VR ’09). IEEE Computer Society, Washington, DC, USA, pp 287–288</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rao AS, Georgeff MP (1995) BDI agents: from theory to practice. In: Proceedings of the first international con" /><p class="c-article-references__text" id="ref-CR58">Rao AS, Georgeff MP (1995) BDI agents: from theory to practice. In: Proceedings of the first international conference on multi-agent systems (ICMAS 95), pp 312–319</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Robert D, Breazeal C (2012) Blended reality characters. In: Proceedings of the seventh annual ACM/IEEE interna" /><p class="c-article-references__text" id="ref-CR59">Robert D, Breazeal C (2012) Blended reality characters. In: Proceedings of the seventh annual ACM/IEEE international conference on human–robot interaction. ACM, pp 359–366</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SJ. Russell, P. Norvig, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Russell SJ, Norvig P (2003) Artificial intelligence: a modern approach, 2nd edn. Prentice Hall, Upper Saddle R" /><p class="c-article-references__text" id="ref-CR60">Russell SJ, Norvig P (2003) Artificial intelligence: a modern approach, 2nd edn. Prentice Hall, Upper Saddle River</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20intelligence%3A%20a%20modern%20approach&amp;publication_year=2003&amp;author=Russell%2CSJ&amp;author=Norvig%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Saeedi, S. Kim, BA. Parviz, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Saeedi E, Kim S, Parviz BA (2008) Self-assembled crystalline semiconductor optoelectronics on glass and plasti" /><p class="c-article-references__text" id="ref-CR61">Saeedi E, Kim S, Parviz BA (2008) Self-assembled crystalline semiconductor optoelectronics on glass and plastic. J Micromech Microeng 18(7):075019</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-assembled%20crystalline%20semiconductor%20optoelectronics%20on%20glass%20and%20plastic&amp;journal=J%20Micromech%20Microeng&amp;volume=18&amp;issue=7&amp;publication_year=2008&amp;author=Saeedi%2CE&amp;author=Kim%2CS&amp;author=Parviz%2CBA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Sas, GMP. O’Hare, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Sas C, O’Hare GMP (2003) Presence equation: an investigation into cognitive factors underlying presence. Prese" /><p class="c-article-references__text" id="ref-CR62">Sas C, O’Hare GMP (2003) Presence equation: an investigation into cognitive factors underlying presence. Presence 12(5):523–537</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474603322761315" aria-label="View reference 61">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Presence%20equation%3A%20an%20investigation%20into%20cognitive%20factors%20underlying%20presence&amp;journal=Presence&amp;volume=12&amp;issue=5&amp;pages=523-537&amp;publication_year=2003&amp;author=Sas%2CC&amp;author=O%E2%80%99Hare%2CGMP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schmalstieg D (2005) Augmented Reality techniques in games. In: Proceedings of IEEE/ACM international symposiu" /><p class="c-article-references__text" id="ref-CR63">Schmalstieg D (2005) Augmented Reality techniques in games. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality (ISMAR 2005), pp 176–177</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shneiderman B, Plaisant C (2010) Designing the user interface: strategies for effective human–computer interac" /><p class="c-article-references__text" id="ref-CR64">Shneiderman B, Plaisant C (2010) Designing the user interface: strategies for effective human–computer interaction, 5th edn. Addison Wesley, Reading</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Shoham, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Shoham Y (1993) Agent-oriented programming. Artif Intell 60(1):51–92" /><p class="c-article-references__text" id="ref-CR65">Shoham Y (1993) Agent-oriented programming. Artif Intell 60(1):51–92</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0004-3702%2893%2990034-9" aria-label="View reference 64">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1216896" aria-label="View reference 64 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Agent-oriented%20programming&amp;journal=Artif%20Intell&amp;volume=60&amp;issue=1&amp;pages=51-92&amp;publication_year=1993&amp;author=Shoham%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Slater M, Steed A (2002) Meeting people virtually: experiments in shared virtual environments. The social life" /><p class="c-article-references__text" id="ref-CR66">Slater M, Steed A (2002) Meeting people virtually: experiments in shared virtual environments. The social life of avatars: presence and interaction in shared virtual environments, pp 146–171</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Slater, M. Usoh, A. Steed, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Slater M, Usoh M, Steed A (1994) Depth of presence in virtual environments. Presence Teleoper Virtual Environ " /><p class="c-article-references__text" id="ref-CR67">Slater M, Usoh M, Steed A (1994) Depth of presence in virtual environments. Presence Teleoper Virtual Environ 3:130–144</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Depth%20of%20presence%20in%20virtual%20environments&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=3&amp;pages=130-144&amp;publication_year=1994&amp;author=Slater%2CM&amp;author=Usoh%2CM&amp;author=Steed%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sutherland IE (1965) The ultimate display. In: Proceedings of the IFIP congress, vol 2. International Federati" /><p class="c-article-references__text" id="ref-CR68">Sutherland IE (1965) The ultimate display. In: Proceedings of the IFIP congress, vol 2. International Federation for Information Processing, Arlington, VA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Syrdal D, Koay K, Walters M, Dautenhahn K (2009) The boy-robot should bark!-children’s impressions of agent mi" /><p class="c-article-references__text" id="ref-CR69">Syrdal D, Koay K, Walters M, Dautenhahn K (2009) The boy-robot should bark!-children’s impressions of agent migration into diverse embodiments. In: Proceedings of the new frontiers in human–robot interaction, a symposium at the AISB2009 convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DS. Tan, MP. Czerwinski, GG. Robertson, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Tan DS, Czerwinski MP, Robertson GG (2006) Large displays enhance optical flow cues and narrow the gender gap " /><p class="c-article-references__text" id="ref-CR70">Tan DS, Czerwinski MP, Robertson GG (2006) Large displays enhance optical flow cues and narrow the gender gap in 3-D virtual navigation. Hum Fact 48(2):318–333</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872006777724381" aria-label="View reference 69">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Large%20displays%20enhance%20optical%20flow%20cues%20and%20narrow%20the%20gender%20gap%20in%203-D%20virtual%20navigation&amp;journal=Hum%20Fact&amp;volume=48&amp;issue=2&amp;pages=318-333&amp;publication_year=2006&amp;author=Tan%2CDS&amp;author=Czerwinski%2CMP&amp;author=Robertson%2CGG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thrun S (2005) Probabilistic robotics, vol 1. MIT press, Cambridge" /><p class="c-article-references__text" id="ref-CR71">Thrun S (2005) Probabilistic robotics, vol 1. MIT press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Usoh, E. Catena, S. Arman, M. Slater, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Usoh M, Catena E, Arman S, Slater M (2000) Using presence questionnaires in reality. Presence Teleoper Virtual" /><p class="c-article-references__text" id="ref-CR72">Usoh M, Catena E, Arman S, Slater M (2000) Using presence questionnaires in reality. Presence Teleoper Virtual Environ 9(5):497–503</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474600566989" aria-label="View reference 71">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 71 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20presence%20questionnaires%20in%20reality&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=9&amp;issue=5&amp;pages=497-503&amp;publication_year=2000&amp;author=Usoh%2CM&amp;author=Catena%2CE&amp;author=Arman%2CS&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Wagner, G. Reitmayr, A. Mulloni, T. Drummond, D. Schmalstieg, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2010) Real-time detection and tracking for augment" /><p class="c-article-references__text" id="ref-CR73">Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2010) Real-time detection and tracking for augmented reality on mobile phones. IEEE Trans Vis Comput Graph 16(3):355–368</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2009.99" aria-label="View reference 72">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20detection%20and%20tracking%20for%20augmented%20reality%20on%20mobile%20phones&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=16&amp;issue=3&amp;pages=355-368&amp;publication_year=2010&amp;author=Wagner%2CD&amp;author=Reitmayr%2CG&amp;author=Mulloni%2CA&amp;author=Drummond%2CT&amp;author=Schmalstieg%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Weiser M (1992) Does ubiquitous computing need interface agents? In: MIT media lab symposium on user interface" /><p class="c-article-references__text" id="ref-CR74">Weiser M (1992) Does ubiquitous computing need interface agents? In: MIT media lab symposium on user interface agents</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Wooldridge, NR. Jennings, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Wooldridge M, Jennings NR (1994) Intelligent agents: theory and practice. Knowl Eng Rev 10(2):115–152" /><p class="c-article-references__text" id="ref-CR75">Wooldridge M, Jennings NR (1994) Intelligent agents: theory and practice. Knowl Eng Rev 10(2):115–152</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1017%2FS0269888900008122" aria-label="View reference 74">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intelligent%20agents%3A%20theory%20and%20practice&amp;journal=Knowl%20Eng%20Rev&amp;volume=10&amp;issue=2&amp;pages=115-152&amp;publication_year=1994&amp;author=Wooldridge%2CM&amp;author=Jennings%2CNR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Zhu, C. Owen, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Zhu W, Owen C (2008) Design of the promopad: an automated augmented-reality shopping assistant. J Organ End Us" /><p class="c-article-references__text" id="ref-CR76">Zhu W, Owen C (2008) Design of the promopad: an automated augmented-reality shopping assistant. J Organ End User Comput (JOEUC) 20(3):41–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4018%2Fjoeuc.2008070103" aria-label="View reference 75">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 75 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20of%20the%20promopad%3A%20an%20automated%20augmented-reality%20shopping%20assistant&amp;journal=J%20Organ%20End%20User%20Comput%20%28JOEUC%29&amp;volume=20&amp;issue=3&amp;pages=41-56&amp;publication_year=2008&amp;author=Zhu%2CW&amp;author=Owen%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ziemke T (2003) What’s that thing called embodiment. In: Proceedings of the 25th annual meeting of the cogniti" /><p class="c-article-references__text" id="ref-CR77">Ziemke T (2003) What’s that thing called embodiment. In: Proceedings of the 25th annual meeting of the cognitive science society. Lawrence Erlbaum, Mahwah, pp 1305–1310</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0239-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work is supported by Science Foundation Ireland under Grant No. 07/CE/I1147. The authors would like to thank the reviewers whose suggestions and constructive feedback throughout the review process led to a greatly improved paper.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CLARITY: Centre for Sensor Web Technology, University College Dublin, Belfield, Dublin 4, Ireland</p><p class="c-article-author-affiliation__authors-list">Abraham G. Campbell, Thomas Holz &amp; G. M. P. O’Hare</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, Ireland</p><p class="c-article-author-affiliation__authors-list">John W. Stafford</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Abraham_G_-Campbell"><span class="c-article-authors-search__title u-h3 js-search-name">Abraham G. Campbell</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Abraham G.+Campbell&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Abraham G.+Campbell" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Abraham G.+Campbell%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-John_W_-Stafford"><span class="c-article-authors-search__title u-h3 js-search-name">John W. Stafford</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;John W.+Stafford&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John W.+Stafford" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John W.+Stafford%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Thomas-Holz"><span class="c-article-authors-search__title u-h3 js-search-name">Thomas Holz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Thomas+Holz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thomas+Holz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thomas+Holz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-G__M__P_-O_Hare"><span class="c-article-authors-search__title u-h3 js-search-name">G. M. P. O’Hare</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;G. M. P.+O%E2%80%99Hare&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=G. M. P.+O%E2%80%99Hare" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22G. M. P.+O%E2%80%99Hare%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0239-4/email/correspondent/c1/new">Abraham G. Campbell</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Why%2C%20when%20and%20how%20to%20use%20augmented%20reality%20agents%20%28AuRAs%29&amp;author=Abraham%20G.%20Campbell%20et%20al&amp;contentID=10.1007%2Fs10055-013-0239-4&amp;publication=1359-4338&amp;publicationDate=2013-12-01&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Campbell, A.G., Stafford, J.W., Holz, T. <i>et al.</i> Why, when and how to use augmented reality agents (AuRAs).
                    <i>Virtual Reality</i> <b>18, </b>139–159 (2014). https://doi.org/10.1007/s10055-013-0239-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0239-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-05-14">14 May 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-11-09">09 November 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-12-01">01 December 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06">June 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0239-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0239-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multi-agent systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">AR simulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Interaction techniques</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0239-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=239;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

