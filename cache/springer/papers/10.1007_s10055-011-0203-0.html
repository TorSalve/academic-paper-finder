<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Reducing the gap between Augmented Reality and 3D modeling with real-t"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Whereas 3D surface models are often used for augmented reality (e.g., for occlusion handling or model-based camera tracking), the creation and the use of such dense 3D models in augmented reality..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging"/>

    <meta name="dc.source" content="Virtual Reality 2011 17:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-12-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Whereas 3D surface models are often used for augmented reality (e.g., for occlusion handling or model-based camera tracking), the creation and the use of such dense 3D models in augmented reality applications usually are two separated processes. The 3D surface models are often created in offline preparation steps, which makes it difficult to detect changes and to adapt the 3D model to these changes. This work presents a 3D change detection and model adjustment framework that combines AR techniques with real-time depth imaging to close the loop between dense 3D modeling and augmented reality. The proposed method detects the differences between a scene and a 3D model of the scene in real time. Then, the detected geometric differences are used to update the 3D model, thus bringing AR and 3D modeling closer together. The accuracy of the geometric difference detection depends on the depth measurement accuracy as well as on the accuracy of the intrinsic and extrinsic parameters. To evaluate the influence of these parameters, several experiments were conducted with simulated ground truth data. Furthermore, the evaluation shows the applicability of AR and depth image&#8211;based 3D modeling for model-based camera tracking."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-12-23"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="111"/>

    <meta name="prism.endingPage" content="123"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0203-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0203-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0203-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0203-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2013/06"/>

    <meta name="citation_online_date" content="2011/12/23"/>

    <meta name="citation_firstpage" content="111"/>

    <meta name="citation_lastpage" content="123"/>

    <meta name="citation_article_type" content="SI: Mixed and Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0203-0"/>

    <meta name="DOI" content="10.1007/s10055-011-0203-0"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0203-0"/>

    <meta name="description" content="Whereas 3D surface models are often used for augmented reality (e.g., for occlusion handling or model-based camera tracking), the creation and the use of s"/>

    <meta name="dc.creator" content="Svenja Kahn"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoperators Virtual Environ; citation_title=A survey of augmented reality; citation_author=RT Azuma; citation_volume=6; citation_publication_date=1997; citation_pages=355-385; citation_id=CR1"/>

    <meta name="citation_reference" content="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), 2010, pp 199&#8211;205"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A method for registration of 3-D shapes; citation_author=P Besl, N McKay; citation_volume=14; citation_issue=2; citation_publication_date=1992; citation_pages=239-256; citation_doi=10.1109/34.121791; citation_id=CR3"/>

    <meta name="citation_reference" content="Bleser G (2009) Towards visual-inertial slam for mobile augmented reality. PhD thesis, TU Kaiserslautern"/>

    <meta name="citation_reference" content="Bleser G, Pastamorv Y, Stricker D (2005) Real-time 3d camera tracking for industrial augmented reality applications. In: WSCG, pp 47&#8211;54"/>

    <meta name="citation_reference" content="citation_journal_title=J Real Time Image Proc; citation_title=Real-time vision-based tracking and reconstruction; citation_author=G Bleser, M Becker, D Stricker; citation_volume=2; citation_publication_date=2007; citation_pages=161-175; citation_doi=10.1007/s11554-007-0034-0; citation_id=CR6"/>

    <meta name="citation_reference" content="Bosch&#233; F (2008) Automated recognition of 3d cad model objects in dense laser range point clouds. PhD thesis, University of Waterloo"/>

    <meta name="citation_reference" content="citation_journal_title=Elsevier J Adv Eng Inform; citation_title=Automated recognition of 3D cad model objects in laser scans and calculation of as-built dimensions for dimensional compliance control in construction; citation_author=F Bosch&#233;; citation_volume=24; citation_issue=1; citation_publication_date=2010; citation_pages=107-118; citation_doi=10.1016/j.aei.2009.08.006; citation_id=CR8"/>

    <meta name="citation_reference" content="Bosch&#233; F, Teizer J, Haas CT, Caldas CH (2006) Integrating data from 3d cad and 3d cameras for real-time modeling. In: Proceedings of joint international conference on computing and decision making in civil and building engineering, pp 37&#8211;46"/>

    <meta name="citation_reference" content="Engelhard N (2010) KinectAutoCalibration. 
                    https://github.com/NikolasE/KinectAutoCalibration
                    
                  
                "/>

    <meta name="citation_reference" content="Engelhard N, Endres F, Hess J, Sturm J, Burgard W (2011) Real-time 3d visual slam with a hand-held rgb-d camera. In: Proceedings of the RGB-D workshop on 3D perception in robotics at the European robotics forum, Vasteras, Sweden"/>

    <meta name="citation_reference" content="Franke T, Kahn S, Olbrich M, Jung Y (2011) Enhancing realism of mixed reality applications through real-time depth-imaging devices in x3d. In: Proceedings of the 16th international conference on 3D web technology. ACM, New York, NY, USA, Web3D &#8217;11, pp 71&#8211;79"/>

    <meta name="citation_reference" content="Gall J, Rosenhahn B, Seidel HP (2006) Robust pose estimation with 3D textured models. In: Pacific-Rim symposium on image and video technology (PSIVT), pp 84&#8211;95"/>

    <meta name="citation_reference" content="Garland M, Heckbert PS (1997) Surface simplification using quadric error metrics. In: Siggraph 1997, pp 209&#8211;216"/>

    <meta name="citation_reference" content="Georgel P, Schroeder P, Benhimane S, Hinterstoisser S, Appel M, Navab N (2007) An industrial augmented reality solution for discrepancy check. In: ISMAR 2007: proceedings of the 6th IEEE and ACM international symposium on mixed and augmented reality, pp 1&#8211;4"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Navigation tools for viewing augmented cad models; citation_author=P Georgel, P Schroeder, N Navab; citation_volume=29; citation_issue=6; citation_publication_date=2009; citation_pages=65-73; citation_doi=10.1109/MCG.2009.123; citation_id=CR16"/>

    <meta name="citation_reference" content="Georgel P, Benhimane S, Sotke J, Navab N (2009a) Photo-based industrial augmented reality application using a single keyframe registration procedure. In: ISMAR 2009: proceedings of the 8th IEEE and ACM international symposium on mixed and augmented reality, pp 187&#8211;188"/>

    <meta name="citation_reference" content="Haller M (2004) Photorealism or/and non-photorealism in augmented reality. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry, VRCAI &#8217;04, pp 189&#8211;196"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Intell Syst Technol Appl (IJISTA); citation_title=On-the-fly scene acquisition with a handy multi-sensor system; citation_author=B Huhle, P Jenke, W Stra&#223;er; citation_volume=5; citation_issue=3/4; citation_publication_date=2008; citation_pages=255-263; citation_id=CR19"/>

    <meta name="citation_reference" content="Kahn S, Wuest H, Fellner DW (2010a) Time-of-flight based scene reconstruction with a mesh processing tool for model based camera tracking. In: 5th international conference on computer vision theory and applications (VISAPP), vol 1. pp 302&#8211;309"/>

    <meta name="citation_reference" content="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241&#8211;242"/>

    <meta name="citation_reference" content="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the eigth IEEE and ACM international symposium on mixed and augmented reality (ISMAR&#8217;09), Orlando, pp 83&#8211;86"/>

    <meta name="citation_reference" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of the eurographics (state-of-the-art report), pp 119&#8211;134"/>

    <meta name="citation_reference" content="Lepetit V, Fua P (2005) Monocular model-based 3D tracking of rigid objects: a survey. In: Foundations and trends in computer graphics and vision, vol 1, pp 1&#8211;89"/>

    <meta name="citation_reference" content="Lindner M, Kolb A, Hartmann K (2007) Data-fusion of pmd-based distance-information and high-resolution rgb-images. In: Proceedings of the international symposium on signals, circuits and systems (ISSCS), session on algorithms for 3D TOF-cameras, vol 1, pp 121&#8211;124"/>

    <meta name="citation_reference" content="MesaImaging (2011) Mesa imaging. 
                    http://www.mesa-imaging.ch
                    
                  
                "/>

    <meta name="citation_reference" content="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: IEEE conference on computer vision and pattern recognition (CVPR), 2010, pp 1498&#8211;1505"/>

    <meta name="citation_reference" content="Oggier T, Lustenberger F, Blanc N (2006) Miniature 3D ToF camera for real-time imaging. In: Perception and interactive technologies, pp 212&#8211;216"/>

    <meta name="citation_reference" content="OpenNI (2011) Openni framework. 
                    http://www.openni.org/
                    
                  
                "/>

    <meta name="citation_reference" content="OpenSG (2011) OpenSG. 
                    http://www.opensg.org
                    
                  
                "/>

    <meta name="citation_reference" content="Pan Q, Reitmayr G, Drummond T (2009) Proforma: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British machine vision conference (BMVC), p 11"/>

    <meta name="citation_reference" content="Panagopoulos A, Samaras D, Paragios N (2009) Robust shadow and illumination estimation using a mixture model. In: IEEE conference on computer vision and pattern recognition, 2009. CVPR 2009, pp 651 &#8211;658"/>

    <meta name="citation_reference" content="Schiller I, Beder C, Koch R (2008) Calibration of a pmd-camera using a planar calibration pattern together with a multi-camera setup. In: The international archives of the photogrammetry, remote sensing and spatial information sciences, vol XXI. ISPRS Congress, pp 297&#8211;302"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track. In: IEEE conference on computer vision and pattern recognition (CVPR&#8217;94), pp 593&#8211;600"/>

    <meta name="citation_reference" content="Van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the 2009 8th IEEE international symposium on mixed and augmented reality, ISMAR &#8217;09, pp 107&#8211;110"/>

    <meta name="citation_reference" content="Webel S, Becker M, Stricker D, Wuest H (2007) Identifying differences between cad and physical mock-ups using ar. In: ISMAR 2007: proceedings of the sixth IEEE and ACM international symposium on mixed and augmented reality, pp 281&#8211;282"/>

    <meta name="citation_reference" content="Wuest H (2008) Efficient line and patch feature characterization and management for real-time camera tracking. PhD thesis, TU Darmstadt"/>

    <meta name="citation_reference" content="citation_title=Adaptable model-based tracking using analysis-by-synthesis techniques; citation_inbook_title=Computer analysis of images and patterns, lecture notes in computer science; citation_publication_date=2007; citation_pages=20-27; citation_id=CR38; citation_author=H Wuest; citation_author=F Wientapper; citation_author=D Stricker; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Zhou F, Duh HBL, Billinghurst M (2008) Trends in augmented reality tracking, interaction and display: a review of ten years of ismar. In: ISMAR 2008: IEEE/ACM international symposium on mixed and augmented reality, pp 193&#8211;202"/>

    <meta name="citation_author" content="Svenja Kahn"/>

    <meta name="citation_author_email" content="svenja.kahn@igd.fraunhofer.de"/>

    <meta name="citation_author_institution" content="Fraunhofer IGD, Darmstadt, Germany"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0203-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0203-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging"/>
        <meta property="og:description" content="Whereas 3D surface models are often used for augmented reality (e.g., for occlusion handling or model-based camera tracking), the creation and the use of such dense 3D models in augmented reality applications usually are two separated processes. The 3D surface models are often created in offline preparation steps, which makes it difficult to detect changes and to adapt the 3D model to these changes. This work presents a 3D change detection and model adjustment framework that combines AR techniques with real-time depth imaging to close the loop between dense 3D modeling and augmented reality. The proposed method detects the differences between a scene and a 3D model of the scene in real time. Then, the detected geometric differences are used to update the 3D model, thus bringing AR and 3D modeling closer together. The accuracy of the geometric difference detection depends on the depth measurement accuracy as well as on the accuracy of the intrinsic and extrinsic parameters. To evaluate the influence of these parameters, several experiments were conducted with simulated ground truth data. Furthermore, the evaluation shows the applicability of AR and depth image–based 3D modeling for model-based camera tracking."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0203-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"3D modeling, Augmented Reality, Computer vision, Tracking, Depth imaging, Analysis-by-synthesis","kwrd":["3D_modeling","Augmented_Reality","Computer_vision","Tracking","Depth_imaging","Analysis-by-synthesis"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0203-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0203-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=203;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0203-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0203-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0203-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Mixed and Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-12-23" itemprop="datePublished">23 December 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Svenja-Kahn" data-author-popup="auth-Svenja-Kahn" data-corresp-id="c1">Svenja Kahn<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Fraunhofer IGD" /><meta itemprop="address" content="grid.461618.c, 0000000097308837, Fraunhofer IGD, Darmstadt, Germany" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">111</span>–<span itemprop="pageEnd">123</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">919 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">10 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0203-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Whereas 3D surface models are often used for augmented reality (e.g., for occlusion handling or model-based camera tracking), the creation and the use of such dense 3D models in augmented reality applications usually are two separated processes. The 3D surface models are often created in offline preparation steps, which makes it difficult to detect changes and to adapt the 3D model to these changes. This work presents a 3D change detection and model adjustment framework that combines AR techniques with real-time depth imaging to close the loop between dense 3D modeling and augmented reality. The proposed method detects the differences between a scene and a 3D model of the scene in real time. Then, the detected geometric differences are used to update the 3D model, thus bringing AR and 3D modeling closer together. The accuracy of the geometric difference detection depends on the depth measurement accuracy as well as on the accuracy of the intrinsic and extrinsic parameters. To evaluate the influence of these parameters, several experiments were conducted with simulated ground truth data. Furthermore, the evaluation shows the applicability of AR and depth image–based 3D modeling for model-based camera tracking.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Augmented reality (AR) applications combine real and virtual, are interactive in real time, and registered in 3D (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma RT (1997) A survey of augmented reality. Presence Teleoperators Virtual Environ 6:355–385" href="/article/10.1007/s10055-011-0203-0#ref-CR1" id="ref-link-section-d38709e286">1997</a>). Since Azuma first stated these characteristics of AR applications in 1997, augmented reality has matured remarkably (Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhou F, Duh HBL, Billinghurst M (2008) Trends in augmented reality tracking, interaction and display: a review of ten years of ismar. In: ISMAR 2008: IEEE/ACM international symposium on mixed and augmented reality, pp 193–202" href="/article/10.1007/s10055-011-0203-0#ref-CR39" id="ref-link-section-d38709e289">2008</a>). However, an important bottleneck remains: The availability of dense 3D surface models. Such dense 3D models are of uppermost importance for two different augmented reality aspects. First, a 3D model is needed for a smooth and seamless integration of virtual objects into the camera images. Therefore, the virtual objects should be illuminated in a consistent way with the illumination of the real scene, they should cast shadows, and they should be occluded by parts of the real scene which are closer than the virtual object. Both occlusion handling and shadow calculation require knowledge about the 3D structure of the real scene (Haller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Haller M (2004) Photorealism or/and non-photorealism in augmented reality. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry, VRCAI ’04, pp 189–196" href="/article/10.1007/s10055-011-0203-0#ref-CR18" id="ref-link-section-d38709e292">2004</a>; Panagopoulos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Panagopoulos A, Samaras D, Paragios N (2009) Robust shadow and illumination estimation using a mixture model. In: IEEE conference on computer vision and pattern recognition, 2009. CVPR 2009, pp 651 –658" href="/article/10.1007/s10055-011-0203-0#ref-CR32" id="ref-link-section-d38709e295">2009</a>). Furthermore, dense 3D models are often used for model-based camera tracking, both for the camera pose initialization and for model-based frame-to-frame tracking (Lepetit and Fua <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lepetit V, Fua P (2005) Monocular model-based 3D tracking of rigid objects: a survey. In: Foundations and trends in computer graphics and vision, vol 1, pp 1–89" href="/article/10.1007/s10055-011-0203-0#ref-CR24" id="ref-link-section-d38709e298">2005</a>). The model-based estimation of the camera pose has the advantage that it overcomes the need to prepare the scene with fiducial markers (Gall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gall J, Rosenhahn B, Seidel HP (2006) Robust pose estimation with 3D textured models. In: Pacific-Rim symposium on image and video technology (PSIVT), pp 84–95" href="/article/10.1007/s10055-011-0203-0#ref-CR13" id="ref-link-section-d38709e302">2006</a>).</p><p>Recently, depth cameras have been developed which acquire dense distance measurements in real time. Depth cameras can be used to create realistic AR applications with shadow mapping or occlusion culling even if no 3D model of the real scene exists (Franke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Franke T, Kahn S, Olbrich M, Jung Y (2011) Enhancing realism of mixed reality applications through real-time depth-imaging devices in x3d. In: Proceedings of the 16th international conference on 3D web technology. ACM, New York, NY, USA, Web3D ’11, pp 71–79" href="/article/10.1007/s10055-011-0203-0#ref-CR12" id="ref-link-section-d38709e308">2011</a>). However, these approaches have several drawbacks. First, the user needs to move a bulky dual-camera setup comprising both a depth and a color camera (e.g., the Kinect has a width of 28 cm and the state-of-the-art SR4000 depth camera weights 500 g, without the additional color camera). A 3D model offers better device independence: It can also be used by mobile devices such as UMPCs, which offer powerful processors and built-in 2D cameras and which are thus suited for model-based AR, but which have no integrated depth measurement devices. Another disadvantage of real-time depth imaging without a 3D model is that the 2D camera and the depth camera capture the scene from different viewpoints. Thus due to occlusions, there is no complete mapping between the color pixels and the depth measurements (Lindner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lindner M, Kolb A, Hartmann K (2007) Data-fusion of pmd-based distance-information and high-resolution rgb-images. In: Proceedings of the international symposium on signals, circuits and systems (ISSCS), session on algorithms for 3D TOF-cameras, vol 1, pp 121–124" href="/article/10.1007/s10055-011-0203-0#ref-CR25" id="ref-link-section-d38709e311">2007</a>). Further artifacts arise in fast camera movements if the color and the depth camera capture the images at slightly different points in time. Finally, a 3D model can provide more stable 2D–3D correspondences for model-based camera tracking than depth measurements that are captured on the fly. For example, depth measurement artifacts occur due to motion blur effects if the depth camera is moved quickly. In the 3D model creation process, these artifacts can easily be circumvented by slow camera movements. Therefore, if the user moves the camera quickly during the tracking, acquiring the 3D measurements from the depth camera on the fly would suffer from these artifacts, whereas the 3D model provides 3D surface information which is not influenced by this effect.</p><p>Currently, when it comes to dense 3D modeling, there is a gap between the 3D modeling phase and the use of the 3D models by AR applications (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec2">2</a>). This work presents an augmented reality 3D modeling framework which reduces this gap by combining real-time depth imaging with AR (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec6">3</a>). In a first step, geometric differences between the real scene and the 3D model are detected in real time by aligning dense depth measurements of a real scene with a 3D model of this scene (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec7">4</a>). Then in a second step, the detected geometric differences are used to adapt the 3D model to the real scene, either manually or automatically (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec12">5</a>). The accuracy of the geometric difference detection depends on the depth measurement accuracy as well as on the accuracy of the intrinsic and extrinsic parameters. To evaluate the influence of these parameters, several experiments were conducted with simulated ground truth data. The results of these experiments are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec15">6</a>. They quantify the influence of the camera tracking accuracy and the measurement noise on the AR difference detection method. Further experiments show the applicability of 3D model adjustment for model-based camera tracking.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">State-of-the-art</h2><div class="c-article-section__content" id="Sec2-content"><p>The major challenge of using dense 3D models for AR is that the 3D models need to be adapted whenever some part of the scene changes (e.g., if new objects were added to the tracked scene or objects were moved to another position).</p><h3 class="c-article__sub-heading" id="Sec3">3D modeling for Augmented Reality</h3><p>Most augmented reality techniques that make use of dense 3D models implicitly assume that such a 3D model already exists (Gall et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gall J, Rosenhahn B, Seidel HP (2006) Robust pose estimation with 3D textured models. In: Pacific-Rim symposium on image and video technology (PSIVT), pp 84–95" href="/article/10.1007/s10055-011-0203-0#ref-CR13" id="ref-link-section-d38709e346">2006</a>; Wuest <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wuest H (2008) Efficient line and patch feature characterization and management for real-time camera tracking. PhD thesis, TU Darmstadt" href="/article/10.1007/s10055-011-0203-0#ref-CR37" id="ref-link-section-d38709e349">2008</a>). Therefore, the reconstruction process is often decoupled from the augmented reality application and the reconstruction is done in an offline preparation phase before the AR application can be used (Wuest et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wuest H, Wientapper F, Stricker D (2007) Adaptable model-based tracking using analysis-by-synthesis techniques. In: Kropatsch W, Kampel M, Hanbury A (eds) Computer analysis of images and patterns, lecture notes in computer science, vol 4673, Springer, Berlin, pp 20–27" href="/article/10.1007/s10055-011-0203-0#ref-CR38" id="ref-link-section-d38709e352">2007</a>; Kahn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010a" title="Kahn S, Wuest H, Fellner DW (2010a) Time-of-flight based scene reconstruction with a mesh processing tool for model based camera tracking. In: 5th international conference on computer vision theory and applications (VISAPP), vol 1. pp 302–309" href="/article/10.1007/s10055-011-0203-0#ref-CR20" id="ref-link-section-d38709e355">2010a</a>). This strict separation between the modeling process and the application of the created 3D model for AR causes two major problems: First, the occurrence of a change is often not obvious in the first place. When a 3D model is used for model-based camera tracking, objects that are part of the modeled scene are often displaced partially in the real scene. This can cause drift and instabilities in the camera tracking. However, it is not always obvious that the tracking inaccuracies are due to the fact that the real scene was changed. Furthermore, even when the user is aware that the 3D model does not fit the reality any more, it is often a difficult task to find out how the 3D model needs to be adapted such that it correctly models the real scene again.</p><p>Recently, several methods have been proposed that use augmented reality for 3D reconstruction with a 2D camera. Pan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Pan Q, Reitmayr G, Drummond T (2009) Proforma: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British machine vision conference (BMVC), p 11" href="/article/10.1007/s10055-011-0203-0#ref-CR31" id="ref-link-section-d38709e361">2009</a>) reconstruct a textured 3D model of an object by rotating the object in front of the camera. Whereas this approach provides good results for textured objects which fulfill the condition that they can be rotated in front of the camera, this method requires a static camera position and thus cannot be used for the reconstruction of larger scenes. The system presented by Van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the 2009 8th IEEE international symposium on mixed and augmented reality, ISMAR ’09, pp 107–110" href="/article/10.1007/s10055-011-0203-0#ref-CR35" id="ref-link-section-d38709e364">2009</a>) relies on user input for the reconstruction of textured objects. It estimates the camera pose for each image of a previously recorded video sequence. To reconstruct a captured object, the user manually specifies the 2D vertices of flat polygons that form the contour of the object in the 2D image. Then the 3D positions of these vertices are reconstructed with simultaneous localization and mapping. Whereas this method is well suited for the reconstruction of objects that are composed from planar surfaces, it would be difficult to model scenes with arbitrary, non-planar surfaces. Bastian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), 2010, pp 199–205" href="/article/10.1007/s10055-011-0203-0#ref-CR2" id="ref-link-section-d38709e367">2010</a>) presented a method that reconstructs the surface of an object with silhouette-based voxel carving. This approach is based on the precondition that the boundary of the object can be identified in the 2D image and that the camera can be moved around the object. Scenes that do not fulfill this condition cannot be reconstructed with this approach. For example, it is not possible to reconstruct the walls of a room because the projection of the wall onto the 2D camera image always covers the whole image and no object boundary gets projected onto the 2D image.</p><p>Structure from motion reconstruction algorithms combines 3D reconstruction with camera tracking methods (Bleser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Proc 2:161–175" href="/article/10.1007/s10055-011-0203-0#ref-CR6" id="ref-link-section-d38709e373">2007</a>; Bleser <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Bleser G (2009) Towards visual-inertial slam for mobile augmented reality. PhD thesis, TU Kaiserslautern" href="/article/10.1007/s10055-011-0203-0#ref-CR4" id="ref-link-section-d38709e376">2009</a>; Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the eigth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’09), Orlando, pp 83–86" href="/article/10.1007/s10055-011-0203-0#ref-CR22" id="ref-link-section-d38709e379">2009</a>). Sparse features are reconstructed online while the pose of the camera is tracked with camera pose estimation algorithms. These online reconstruction algorithms for sparse features provide a smooth integration of 3D modeling and AR applications. This is currently often not the case for dense 3D models, which (except by the method presented in Newcombe and Davison (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: IEEE conference on computer vision and pattern recognition (CVPR), 2010, pp 1498–1505" href="/article/10.1007/s10055-011-0203-0#ref-CR27" id="ref-link-section-d38709e382">2010</a>)) usually need to be modeled offline. This is partly due to the fact that in contrast to sparse 3D features dense surfaces can not easily be reconstructed from 2D camera images in real time, especially not if the object surfaces are not well textured. However, currently real-time depth imaging cameras have been developed which can help to bridge this gap.</p><h3 class="c-article__sub-heading" id="Sec4">Real-time depth imaging</h3><p>Time-of-flight cameras acquire dense 3D measurements in real time (Oggier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Oggier T, Lustenberger F, Blanc N (2006) Miniature 3D ToF camera for real-time imaging. In: Perception and interactive technologies, pp 212–216" href="/article/10.1007/s10055-011-0203-0#ref-CR28" id="ref-link-section-d38709e393">2006</a>; Kolb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of the eurographics (state-of-the-art report), pp 119–134" href="/article/10.1007/s10055-011-0203-0#ref-CR23" id="ref-link-section-d38709e396">2009</a>). They emit near-infrared modulated light that gets reflected by the scene and captured by the image sensor of the camera. For every pixel, the distance to the scene is calculated by the phase shift of the reflected light. The distances can be transformed to Cartesian coordinates, yielding a 3D measurement per pixel. Time-of-flight cameras capture 3D measurements as well as an intensity image that depicts the amount of light reflected onto each pixel.</p><p>Furthermore, recently the first low-cost depth camera for the mass market was released by PrimeSense respectively by Microsoft. This depth camera is part of the Kinect device. Initially targeted at the gaming market, the Kinect depth camera can be used to realize AR applications with real-time depth imaging as well. It has a higher resolution than state-of-the-art time-of-flight cameras (SwissRanger 3000: 176 × 144 pixel, Kinect: 640 × 480 pixel). The Kinect contains two cameras (a color camera and the depth camera) as well as an infrared projector that projects an infrared pattern onto the scene. This infrared pattern is detected by the depth camera (which is in fact an infrared camera) and used to calculate the distance of the scene. The measured depth and color images of the Kinect can be accessed with the OpenNI interface (OpenNI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="OpenNI (2011) Openni framework. &#xA;                    http://www.openni.org/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0203-0#ref-CR29" id="ref-link-section-d38709e402">2011</a>).</p><p>Huhle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Huhle B, Jenke P, Straßer W (2008) On-the-fly scene acquisition with a handy multi-sensor system. Int J Intell Syst Technol Appl (IJISTA) 5(3/4):255–263" href="/article/10.1007/s10055-011-0203-0#ref-CR19" id="ref-link-section-d38709e408">2008</a>) as well as Engelhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Engelhard N, Endres F, Hess J, Sturm J, Burgard W (2011) Real-time 3d visual slam with a hand-held rgb-d camera. In: Proceedings of the RGB-D workshop on 3D perception in robotics at the European robotics forum, Vasteras, Sweden" href="/article/10.1007/s10055-011-0203-0#ref-CR11" id="ref-link-section-d38709e411">2011</a>) presented methods that use a depth and a color camera for 3D reconstruction. Both assemble colored 3D point clouds by first aligning the 3D point clouds based on 2D features in the color images. Then, the alignment is refined with 3D–3D geometric registration. Both approaches can be used to reconstruct 3D scenes from scratch. However, they do not cover the issue how information about an existing 3D model (e.g., a triangle mesh that already models a part of the scene) can be incorporated into the reconstruction process. The incorporation of an existing triangle mesh in the reconstruction process requires a method that can detect where the existing mesh differs from the real geometry (geometric difference detection) and where it thus needs to be updated.</p><h3 class="c-article__sub-heading" id="Sec5">Geometric difference detection</h3><p>Kahn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241–242" href="/article/10.1007/s10055-011-0203-0#ref-CR21" id="ref-link-section-d38709e422">2010b</a>) proposed the first 3D difference detection approach for a moving camera. This approach allows to move a camera freely in a scene and to automatically detect 3D differences in real time with a dense 3D discrepancy check. This paper extends the ideas that were first presented in Kahn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241–242" href="/article/10.1007/s10055-011-0203-0#ref-CR21" id="ref-link-section-d38709e425">2010b</a>). Previous solutions for discrepancy check were based on still 2D images: In their pioneering work, Georgel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Georgel P, Schroeder P, Benhimane S, Hinterstoisser S, Appel M, Navab N (2007) An industrial augmented reality solution for discrepancy check. In: ISMAR 2007: proceedings of the 6th IEEE and ACM international symposium on mixed and augmented reality, pp 1–4" href="/article/10.1007/s10055-011-0203-0#ref-CR15" id="ref-link-section-d38709e428">2007</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009a" title="Georgel P, Benhimane S, Sotke J, Navab N (2009a) Photo-based industrial augmented reality application using a single keyframe registration procedure. In: ISMAR 2009: proceedings of the 8th IEEE and ACM international symposium on mixed and augmented reality, pp 187–188" href="/article/10.1007/s10055-011-0203-0#ref-CR17" id="ref-link-section-d38709e431">2009a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Georgel P, Schroeder P, Navab N (2009) Navigation tools for viewing augmented cad models. IEEE Comput Graph Appl 29(6):65–73" href="/article/10.1007/s10055-011-0203-0#ref-CR16" id="ref-link-section-d38709e434">b</a>) presented an augmented reality solution for discrepancy check in the context of construction planning. Their system allows engineers to superimpose still 2D images of a plant with the CAD model developed during the planning phase. Whereas this augmentation of still 2D images with the 3D model is very useful to visually compare the 3D model and the real scene, it is limited to the 2D information contained in the images and provides no possibility to automatically compare the 3D data of the model with the 3D geometry of the real scene. Webel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Webel S, Becker M, Stricker D, Wuest H (2007) Identifying differences between cad and physical mock-ups using ar. In: ISMAR 2007: proceedings of the sixth IEEE and ACM international symposium on mixed and augmented reality, pp 281–282" href="/article/10.1007/s10055-011-0203-0#ref-CR36" id="ref-link-section-d38709e438">2007</a>) presented a system for AR discrepancy check with which the 3D positions of single points in the 3D model and the real scene can be compared: A laser pointer is used to depict a point on the surface of the real scene. The 3D coordinate of the point is reconstructed by triangulation with a stereo camera system. Whereas this approach allows the comparison of single 3D points, it is not suited for dense 3D difference detection.</p><p>Bosché et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bosché F, Teizer J, Haas CT, Caldas CH (2006) Integrating data from 3d cad and 3d cameras for real-time modeling. In: Proceedings of joint international conference on computing and decision making in civil and building engineering, pp 37–46" href="/article/10.1007/s10055-011-0203-0#ref-CR9" id="ref-link-section-d38709e444">2006</a>) transformed the 3D data of a CAD model and 3D data measured with a time-of-flight camera into a common voxel occupancy grid. However, they used a static camera position and thus did not need to solve the registration problem. In recent publications, Bosche addressed the registration problem in the context of object recognition (Bosché <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bosché F (2008) Automated recognition of 3d cad model objects in dense laser range point clouds. PhD thesis, University of Waterloo" href="/article/10.1007/s10055-011-0203-0#ref-CR7" id="ref-link-section-d38709e447">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bosché F (2010) Automated recognition of 3D cad model objects in laser scans and calculation of as-built dimensions for dimensional compliance control in construction. Elsevier J Adv Eng Inform 24(1):107–118" href="/article/10.1007/s10055-011-0203-0#ref-CR8" id="ref-link-section-d38709e450">2010</a>). He used manually specified 3D correspondences between a laser scan of a construction site and a 3D model of the construction site to transform both data sets into a common coordinate system. As the manual selection of 3D correspondences is an offline step (and for the task of discrepancy check, 3D–3D correspondences cannot easily be extracted automatically as the 3D model might differ from the real scene), this approach is not feasible for difference detection with an arbitrary moving camera.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Concept overview</h2><div class="c-article-section__content" id="Sec6-content"><p>Most frameworks for model-based augmented reality use a sequential one-way process for the 3D modeling task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig1">1</a>a). First a 3D model is created in a preparation step. Then, this 3D model is used for the realization of augmented reality applications. These conventional approaches do not offer the possibility to use AR for the 3D modeling step itself.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>
                        <b>a</b> Conventional framework: The 3D model is used as input for the AR application, but not vice versa. <b>b</b> Proposed framework: AR is used to detect geometric differences between the 3D model and the real scene. Then the measured geometric differences are fed back into the 3D modeling pipeline and are used to update the 3D model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>This limitation is overcome by the proposed framework (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig1">1</a>b). Similar to the work presented by Van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the 2009 8th IEEE international symposium on mixed and augmented reality, ISMAR ’09, pp 107–110" href="/article/10.1007/s10055-011-0203-0#ref-CR35" id="ref-link-section-d38709e496">2009</a>) and Bastian et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), 2010, pp 199–205" href="/article/10.1007/s10055-011-0203-0#ref-CR2" id="ref-link-section-d38709e499">2010</a>), it closes the loop between 3D reconstruction and augmented reality by employing AR for the 3D modeling process. In contrast to previous approaches, it furthermore combines AR with real-time depth imaging to detect geometric differences between the real scene and a 3D model of the scene. Thus, the 3D modeling is supported by the AR components of the framework. Vice versa, the reconstructed 3D model can be used for model-based AR applications. With the proposed framework, the user can reconstruct or adjust a 3D model of the scene with a depth-color camera setup. After the AR-based model adjustment step, AR applications that need 3D information about the scene can be realized with a single color camera, as the 3D information can be accessed from the 3D model.</p><p>The basic idea for 3D difference detection was first described in Kahn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241–242" href="/article/10.1007/s10055-011-0203-0#ref-CR21" id="ref-link-section-d38709e505">2010b</a>). In this paper, it is extended for 3D modeling by AR. For the difference detection, the pose of a depth camera is estimated by calculating the pose of a rigidly coupled 2D camera with camera tracking algorithms. The tracked camera pose is then used to register the real depth measurements and the 3D model in a common coordinate system. By rendering the 3D model from the point of view of the depth camera, a simulated depth image can be generated which is compared to the real depth measurements to detect geometric differences (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec7">4</a>).</p><p>In a next step, the measured geometric differences are fed back into the 3D modeling pipeline where they are used to update the 3D model either manually or semi-automatically (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec12">5</a>). This update step benefits from the fact that the previous augmented reality difference detection registers the measured depth values and the 3D model in a common coordinate system. This eases the model adjustment task, both for the user and for (semi-)automatic model adjustment. Continuously updating the 3D model would require very complex algorithms for real-time 3D model processing. Furthermore, these algorithms would have to assure that pose estimation errors or measurement inaccuracies do not distort previously modeled parts. The approach presented in this paper is tailored to semi-automatic model adjustment: Rather than updating the 3D model fully automatically with each single depth measurements, the user takes a few snapshots of a 3D scene. Then the 3D model is automatically updated according to the depth measurements of these snapshots. The AR visualization of the geometric differences supports the user in identifying the viewpoints for new depth images and in judging the accuracy of the current camera pose estimation (which influences the accuracy of the 3D modeling, see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec15">6</a>).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Geometric difference detection</h2><div class="c-article-section__content" id="Sec7-content"><p>To detect geometric differences between a real scene and a 3D model of the scene, a color camera (used for the camera tracking) is rigidly coupled with a depth camera (for the 3D imaging). This section describes why both a 2D and a depth image are needed, how to calibrate the cameras, and how to use camera tracking in combination with analysis-by-synthesis to geometrically compare the real scene and the 3D model.</p><h3 class="c-article__sub-heading" id="Sec8">Combining 2D imaging with real-time depth imaging</h3><p>By combining camera tracking based on 2D images with real-time depth imaging, dense 3D measurements can be acquired and registered with the coordinate system of the tracked scene in real time. This would not be possible if either one of these images (the 2D image or the depth image) was used alone: Whereas the color images of custom 2D cameras can be used to track the camera position, 2D cameras cannot capture dense depth images in real time (e.g., at untextured parts of a scene). Vice versa, depth cameras acquire dense depth images in real time but their pose cannot be tracked robustly with 2D image-based camera tracking algorithms. An alternative possibility to register the measured depth images with the 3D model would be to use geometric alignment, i.e., iteratively approach the 3D points with the iterative closest point algorithm (Besl and McKay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Besl P, McKay N (1992) A method for registration of 3-D shapes. IEEE Trans Pattern Anal Mach Intell 14(2):239–256" href="/article/10.1007/s10055-011-0203-0#ref-CR3" id="ref-link-section-d38709e534">1992</a>). However, this approach would be computationally expensive. Furthermore, differences between the real scene and the 3D model would result in wrong registrations, which should be avoided for geometric difference detection tasks because here the 3D model can differ from the real scene.</p><p>For these reasons, two cameras are used: a custom 2D color camera and a depth camera. This combination of 2D imaging with 3D imaging is not restricted to specific devices. Whereas in Kahn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241–242" href="/article/10.1007/s10055-011-0203-0#ref-CR21" id="ref-link-section-d38709e540">2010b</a>) a time-of-flight depth camera and a color camera were rigidly coupled on a camera rig, the Kinect camera of Microsoft (which contains both a depth and a color camera) can be used as well. Both systems have in common that the depth and the color camera are rigidly coupled and placed at different positions. Thus, the intrinsic parameters of both cameras as well as the relative transformation <span class="mathjax-tex">\((\Updelta R, \Updelta t)\)</span> between the cameras need to be calculated. In contrast to the Kinect depth camera, the time-of-flight camera captures not only the depth image but also an intensity image that depicts the amount of light reprojected at each pixel. This intensity image can also be used to track the pose of the time-of-flight camera without a color camera. However, it has a much lower resolution than custom color cameras (typically 176 × 144 pixels) (Oggier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Oggier T, Lustenberger F, Blanc N (2006) Miniature 3D ToF camera for real-time imaging. In: Perception and interactive technologies, pp 212–216" href="/article/10.1007/s10055-011-0203-0#ref-CR28" id="ref-link-section-d38709e554">2006</a>).</p><h3 class="c-article__sub-heading" id="Sec9">Camera calibration</h3><p>For a setup that combines a time-of-flight camera with a color camera, the MultiCameraCalibration tool (Schiller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Schiller I, Beder C, Koch R (2008) Calibration of a pmd-camera using a planar calibration pattern together with a multi-camera setup. In: The international archives of the photogrammetry, remote sensing and spatial information sciences, vol XXI. ISPRS Congress, pp 297–302" href="/article/10.1007/s10055-011-0203-0#ref-CR33" id="ref-link-section-d38709e565">2008</a>) calculates the relative transformation between the time-of-flight camera and the color camera as well as the intrinsic parameters of both cameras.</p><p>The main challenge for the calibration of a Kinect depth camera is that the 2D checkerboard patterns that are otherwise used for the calibration are not visible in the depth images. Therefore, the calibration of Kinect depth cameras requires a trick which is based on the insight that the depth camera is in fact an infrared camera (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec4">2.2</a>). Instead of the depth image, the raw infrared image is acquired from the OpenNI interface. The infrared projector is covered with opaque tape to remove the infrared pattern from the image, and the chessboard pattern is illuminated with an external infrared lamp. This way the chessboard becomes visible in the infrared images of the depth camera. Then the KinectAutoCalibration can be used to calculate the intrinsic parameters as well as the relative transformation between the color and the depth camera of the Kinect (Engelhard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Engelhard N (2010) KinectAutoCalibration. &#xA;                    https://github.com/NikolasE/KinectAutoCalibration&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0203-0#ref-CR10" id="ref-link-section-d38709e574">2010</a>).</p><h3 class="c-article__sub-heading" id="Sec10">Camera tracking</h3><p>After the 3D model was reconstructed or adjusted to the real scene, it can be used for model-based camera tracking. However, in the 3D model reconstruction phase, model-based camera tracking can be unstable: During the adjustment process, the 3D model does not yet correspond to the real geometry. This can result in errors or inaccuracies of the estimated camera pose. This is why marker tracking and structure from motion are used for the camera pose estimation in the 3D model adjustment phase. The camera pose is initialized with a marker whose coordinates are specified in the coordinate system of the given 3D model. Thus, the world coordinate system is identical to the model coordinate system. Marker-based camera tracking has the drawback that the camera pose can only be calculated if markers are visible in the current camera image. Therefore, we combine markerless camera tracking with structure from motion 3D reconstruction (Bleser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Proc 2:161–175" href="/article/10.1007/s10055-011-0203-0#ref-CR6" id="ref-link-section-d38709e585">2007</a>). While the camera is moved, this approach detects characteristic image features with a Shi–Tomasi corner detector (Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. In: IEEE conference on computer vision and pattern recognition (CVPR’94), pp 593–600" href="/article/10.1007/s10055-011-0203-0#ref-CR34" id="ref-link-section-d38709e588">1994</a>) and tracks these 2D image features with a Lucas Kanade tracker (Wuest <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wuest H (2008) Efficient line and patch feature characterization and management for real-time camera tracking. PhD thesis, TU Darmstadt" href="/article/10.1007/s10055-011-0203-0#ref-CR37" id="ref-link-section-d38709e591">2008</a>). Then, the 3D positions of the tracked features are reconstructed online via triangulation (Bleser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Proc 2:161–175" href="/article/10.1007/s10055-011-0203-0#ref-CR6" id="ref-link-section-d38709e594">2007</a>). Finally, the 2D–3D correspondences between the 2D image coordinates of the tracked features and their reconstructed 3D coordinates are used to calculate the camera pose if no marker is visible.</p><p>The pose (<i>R</i>
                  <sub>
                    <i>D</i>
                  </sub>, <i>t</i>
                  <sub>
                    <i>D</i>
                  </sub>) of the depth camera is calculated from the tracked pose of the color camera (<i>R</i>
                  <sub>
                    <i>C</i>
                  </sub>, <i>t</i>
                  <sub>
                    <i>C</i>
                  </sub>) with Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ1">1</a>. Here <span class="mathjax-tex">\((\Updelta R,\Updelta t)\)</span> is the relative transformation between the two cameras, which was calculated in the offline calibration step.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} R_{D} &amp;= R_{C}\cdot \Updelta R\\ t_{D} &amp;= R_{C}\cdot t_{C} + \Updelta t \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                <h3 class="c-article__sub-heading" id="Sec11">Difference calculation via analysis-by-synthesis</h3><p>To compare the 3D data acquired by the depth camera and the 3D model, the discrepancy between the 3D measurement and the corresponding 3D position in the 3D model is calculated for each pixel via analysis-by-synthesis: First, the 3D model is rendered from the current camera pose with the intrinsic and extrinsic parameters of the depth camera. Then, a synthetic 3D image is calculated from the depth buffer of the graphics card. If <i>P</i> is the 4 × 4 projection matrix used for the rendering of the 3D model, each depth value is converted to the camera coordinate system with <i>P</i>
                  <sup>−1</sup>. After this conversion, the geometric differences between the 3D model and the real measurements are calculated pixelwise by comparing the depth value of the synthetic depth image and the depth measurement of the depth camera at the same pixel.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Model adjustment</h2><div class="c-article-section__content" id="Sec12-content"><p>The difference detection approach described in this paper contributes to geometric modeling in two different ways: On the one hand, differences are visualized to ease the model adaption task for the user. Furthermore, the 3D model can be adapted algorithmically such that its surface better corresponds to the measured scene.</p><h3 class="c-article__sub-heading" id="Sec13">Geometric difference visualization</h3><p>To visualize the discrepancies, the camera image is augmented with a semi-transparent RGBA image whose colors represent the 3D differences. Red pixels show that the real scene is closer than its counterpart in the 3D model, at yellow pixels the real scene is farther away, and blue pixels show parts of the scene that do not exist in the 3D model. If the depth camera could not acquire a depth value at a certain pixel, this pixel is colored in green.</p><p>To visualize the degree of the discrepancies, the transparency α of each pixel in the difference visualization image is set such that pixels visualizing close distances have a higher transparency than pixels at positions where there is a large discrepancy between the 3D model and the real measurements: <span class="mathjax-tex">\(\alpha=(d_{\rm measured} - d_{{\rm 3D} {\rm model}})\cdot o\)</span>. Here, <i>d</i>
                  <sub>measured</sub> and <i>d</i>
                  <sub>3D model</sub> are the depth values. The opacity factor <i>o</i> can be set by the user to change the transparency of the whole discrepancy visualization image.</p><p>The image of the color camera provides more details about the scene than the grayscale-encoded depth image. However, if the color image is augmented, the differences are not well visible anymore because the colors of the 2D camera interfere with the colors of the difference visualization. Therefore, the color image is converted to a grayscale image. This way it is possible to visualize the (grayscale) details of the scene and the detected differences at the same time. The depth measurements of the depth camera need to be mapped onto the color image. Thus, the raw depth values are first transformed to 3D points and then projected onto the 2D image of the color camera. Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ2">2</a> transforms the 1D depth value <i>d</i>
                  <sub>cam</sub> of a pixel (<i>p</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>p</i>
                  <sub>
                    <i>y</i>
                  </sub>) in the 2D image coordinate system of the depth camera to a 3D point <i>p</i>
                  <sub>CCS(D)</sub> in the depth camera coordinate system CCS(D). The horizontal respectively vertical focal length is denoted by (<i>f</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>f</i>
                  <sub>
                    <i>y</i>
                  </sub>) and the principal point by (<i>c</i>
                  <sub>
                    <i>x</i>
                  </sub>, <i>c</i>
                  <sub>
                    <i>y</i>
                  </sub>). In a next step, the 3D points are transformed to the world coordinate system with Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ3">3</a>.</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ p_{\rm CCS(D)} = \left(\begin{array}{l} (p_x - c_x)*\frac{1}{f_x} * d_{\rm cam}\\ (p_y - c_y)*\frac{1}{f_y} * d_{\rm cam} \\ d_{\rm cam} \end{array}\right) $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                  <div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ p_{\rm WCS} = (R_{D})^{-1} \cdot ( p_{\rm CCS(D)} - t_{D}) $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                <p>Finally, Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ4">4</a> is used to project 3D points from the world coordinate system to 2D coordinates <i>p</i>
                  <sub>ICS(C)</sub> in the camera image of the color camera. Here, <i>K</i>
                  <sub>
                    <i>C</i>
                  </sub> is the camera calibration matrix of the color camera which contains the intrinsic parameters.</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ p_{\rm ICS(C)} = K_{C}((R_{C} \cdot p_{\rm WCS}) + t_{C}) $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig2">2</a> shows the difference visualization of a box that was moved to the side. This displacement would cause problems for model-based camera tracking approaches because the box is well textured and thus many features are detected on the real box. If the 3D model does not match the position of the real object, wrong 3D positions would be inferred from the 3D model. With the approach proposed in this paper, the user can use the difference detection and visualization either to move the box back to the modeled position or to model the new box position in the 3D model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Difference visualization (Kinect): in comparison with the 3D model, the <i>box</i> in the real scene was moved to the <i>left</i>. <b>a</b> Depth image augmented with differences. <b>b</b> Grayscale image augmented with differences</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>
                          <b>a</b> Depth and color image acquired by the Kinect. The <i>black pixels</i> in the depth image show pixels where the Kinect could not acquire depth values. <b>b</b> Textured 3D mesh created by mapping the texture from the <i>color</i> camera to the depth image and by converting the depth values to the world coordinate system. The <i>square markers</i> on the brick model are used to initialize and to track the camera pose. The positions of the depth and the <i>color</i> camera are represented by the small camera <i>symbols</i>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec14">Geometric model adjustment</h3><p>This section describes how the 3D model is geometrically adjusted. This is accomplished by inserting triangle meshes in the 3D model which cover the parts of the surface where the real scene was measured closer to the camera than the surface of the 3D model or where no 3D information was available beforehand (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig4">4</a>). The described approach does not imply restrictions on the 3D model but is feasible for any kind of 3D model which can be rendered (e.g., polygonal meshes, implicit surfaces, or CAD models). The only precondition is that it is possible to store an additional triangle mesh in the used data format.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Geometric model adjustment (Kinect): first only the 3D model of the bricks is known. By adding submeshes for missing parts, the 3D model is gradually amended. <i>Right</i>: adapted 3D model after three submeshes were added</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To adapt the 3D model, a triangle submesh is created which contains all the triangles of the depth measurements whose 3D measurements belong to pixels where a change is visualized. Thus when the user wants to adapt the 3D model to the depth measurements of the current viewpoint, he can change the threshold o (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec13">5.1</a>) to increase or decrease the subparts of the measured depth image which are added to the scene. This triangle mesh is then added to the 3D model on the fly: The new triangle mesh is added as a new geometry node to the scenegraph system OpenSG (OpenSG <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="OpenSG (2011) OpenSG. &#xA;                    http://www.opensg.org&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0203-0#ref-CR30" id="ref-link-section-d38709e980">2011</a>) which is used for the rendering of the 3D model.</p><p>To add 3D measurements of the depth camera to the geometry of the 3D model, the depth measurements need to be transformed to the coordinate system of the 3D model. As noted in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec10">4.3</a>, this corresponds to the world coordinate system. Therefore, the depth measurements are transformed to 3D points in the model coordinate system with Eqs. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ2">2</a> and <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-011-0203-0#Equ3">3</a>.</p><p>If the 3D model is a polygonal vertex mesh, the difference detection approach described in this paper can as well be used to remove parts of the 3D model where the time-of-flight camera measures distances that are farther away than the surface of the 3D model. In contrast to the previously described surface insertion technique, this modifies the vertices and the triangle structure of the 3D model. To remove the correct vertices of the 3D model, all vertices <span class="mathjax-tex">\(v_{i} \in V\)</span> (and the polygons adjacent to these vertices) need to be removed which fulfill the following two conditions: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                          <i>v</i>
                          <sub>
                            <i>i</i>
                          </sub> gets projected on one of the pixels where a difference was detected by the analysis-by-synthesis approach.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                          <i>v</i>
                          <sub>
                            <i>i</i>
                          </sub> is not hidden by another surface of the 3D model. This condition prevents the removal of hidden surfaces, for example the back side of the 3D model.</p>
                      
                    </li>
                  </ol>
                <p>The vertices and triangles that fulfill these conditions are identified with a shader. The 3D model is rendered offscreen with a pixel buffer. To detect the visible vertices and triangles, each triangle is drawn with a different color. Then, the rendered color is a mapping to the triangle which is visible at this pixel.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Quantitative evaluation</h2><div class="c-article-section__content" id="Sec15-content"><p>The accuracy of the 3D difference detection depends on the accuracy of the relative transformation between the two cameras, the accuracy of the camera pose estimation of the 2D camera, and the accuracy of the intrinsic calibrations of the 2D camera and the depth camera. When one of these parameters is inaccurately estimated, there is an offset between the camera pose used for the analysis-by-synthesis approach and the real camera pose. To quantify the influence of these parameters, we evaluated the difference detection and the 3D modeling with a simulation providing ground truth data for both the camera pose and the 3D geometry of the scene.</p><h3 class="c-article__sub-heading" id="Sec16">Extrinsic and intrinsic parameters</h3><p>In the first setup, we used a 3D model of the toy brick scene to create simulated depth and color images while varying the parameters of the virtual cameras. For the evaluation, the difference detection was applied on a depth image which was simulated with the correct parameters (reference image) in combination with a depth image which was generated with the modified extrinsic or intrinsic parameters (evaluation image).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig5">5</a> visualizes the results of this evaluation. The top row shows the differences between the reference and the evaluation image which occur due to the errors in the extrinsic and intrinsic camera parameters. The plots beneath show how the residual increases subject to the intrinsic and extrinsic inaccuracies. Here, the residual is the average of the pixelwise difference of both depth images. Each parameter was evaluated independently. Please note that for the evaluation of the rotation offsets, the camera was rotated locally such that its position remained constant. If the extrinsic of the camera is denoted with (<i>R</i>, <i>t</i>), changing the rotation<i> R</i> while keeping the translation parameter<i> t</i> constant would also result in a changed camera position because the rotation <i>R</i> is applied before <i>t</i>. The distance of the camera to the scene was about one meter, and the width of the brick model is 22 cm. The <i>x</i>-axis of the depth camera points to the right, the <i>y</i>-axis to the top and the <i>z</i>-axis points toward the brick model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Influence of inaccurately estimated camera pose on the difference detection. <b>b</b>–<b>e</b> Inaccurate extrinsic parameters. <b>f</b>–<b>g</b> Inaccurate intrinsic parameters. The residual is the average distance between the depth image and the 3D model. <b>a</b> From <i>left</i> to <i>right</i>: inaccurate translation (0.02 m), rotation (2.3°), principal point (10 pixel), focal length (50 pixel). <b>b</b> Residual caused by camera translation (without noise). <b>c</b> Residual caused by camera translation (with noise). <b>d</b> Residual caused by camera rotation (without noise). <b>e</b> Residual caused by camera translation (with noise). <b>f</b> Residual caused by incorrect intrinsic parameters (without noise). <b>g</b> Residual caused by incorrect intrinsic parameters (with noise)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The evaluation shows that for this scene translational errors have rather moderate effects on the accuracy of the difference detection. This is mainly due to the fact that the scene contains flat walls. Therefore, the changes in the depth measurements are not very large when the camera is moved. However, the calculation accuracy depends strongly on an accurate estimation of the camera rotation. Small errors in the camera rotation cause large discrepancies between the reference and the evaluation depth image. In view of the intrinsic parameters, an inaccurate estimation of the principal point causes much larger errors than inaccuracies of the focal length.</p><h3 class="c-article__sub-heading" id="Sec17">Depth measurement noise</h3><p>The depth measurements of depth cameras are affected by random noise as well as by systematic errors (e.g., due to the reflections of specular surfaces). The systematic errors are different for each depth measuring technology and cannot be simulated easily. However, we evaluated the effects of measurement noise: Therefore, Gaussian noise was added to the depth values. In accordance with the specification of SwissRanger depth cameras, the standard deviation of this random noise was set to 1% of the distance to the camera (MesaImaging <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="MesaImaging (2011) Mesa imaging. &#xA;                    http://www.mesa-imaging.ch&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0203-0#ref-CR26" id="ref-link-section-d38709e1174">2011</a>). The right column visualizes the results of tests that were conducted with noisy depth images. If the pose of the depth camera is very accurately estimated, the accuracy of the calculation results is primarily influenced by the random measurement noise. However, when the pose error increases (especially the rotational error), the difference detection accuracy is primarily reduced by the errors that are caused by the camera pose estimation.</p><h3 class="c-article__sub-heading" id="Sec18">Inaccuracies of the 3D model</h3><p>3D models often only approximate real objects. For example, polygonal meshes approximate curved objects with planar triangle surfaces. We used a standard VRML sphere with a radius of one meter to evaluate this effect. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig6">6</a> visualizes the polygonal sphere mesh as well as the differences that are detected when depth images of the sphere are taken from a distance of three meters to the sphere with rotational and translational offsets. Furthermore, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig7">7</a> shows the residual plots of experiments in which the camera was positioned exactly at the center of the sphere. If the sphere model would be smoothly curved, the rotational offset would always be 0. However, the polygonal approximation of the sphere causes differences between the depth images. Adding noise to the depth images reveals a particular interesting effect: The residual is larger than with either the noise or the approximation-based differences alone, but not as much as the sum of both effects. In this case, the noise in the depth values smoothes the polygonal approximation of curved surfaces, thus reducing the error caused by the triangle approximation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>From <i>left to right</i>
                          <b>a</b> polygonal mesh of a 3D sphere <b>b</b> difference visualization of rotational offset <b>c</b> difference visualization of translational offset (sidewards) <b>d</b> difference visualization of translational offset (distance)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Influence of approximated 3D meshes on difference detection. <b>a</b> Rotation: without noise. <b>b</b> Rotation: with noise. <b>c</b> Translation: without noise. <b>d</b> Translation: with noise</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec19">Execution time</h3><p>The execution time was measured with an Intel Core i7 processor with 3.07 Ghz and an NVidia GeForce GTX 470. The CPU-based part of the framework is implemented as a single core C++ implementation.</p><p>The geometric difference detection compares the real measurements with the 3D model (see Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec11">4.4</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0203-0#Sec13">5.1</a>). It is implemented as a GPU fragment shader. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0203-0#Tab1">1</a> shows the execution time of the geometric difference detection. If the 3D model consists of relatively few triangles, the execution time mainly results from the time it takes to copy the data to the graphics card and back to the CPU. This time is constant for a given image size. Even for large 3D models with more than 2 million triangles, the geometric difference detection is very fast. Thus, it can be used in real time in combination with online camera pose estimation. Please note that the time for the camera pose estimation (marker tracking or structure from motion) is not included in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0203-0#Tab1">1</a>, as it depends on the structure and appearance of the tracked scene. The real-time structure from motion algorithm used in our framework was evaluated by Bleser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Proc 2:161–175" href="/article/10.1007/s10055-011-0203-0#ref-CR6" id="ref-link-section-d38709e1279">2007</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Geometric difference detection: execution time (milliseconds)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-011-0203-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The 3D model adjustment substep that adds triangles to the 3D model is implemented on the GPU. The processing time required to add a new triangle submesh depends only on the size of the new submesh (and thus the number of measurements in the depth image), not on the previous size of the 3D model. It is 9 ms for a depth image with 176 × 144 depth measurements, 22 ms for 320 × 240 depth measurements, and 80 ms for 640 × 480 depth measurements. Usually not the whole depth image, but only subparts of it need to be added to the 3D model. In this case, the processing time is reduced accordingly. The identification of the triangles that need to be removed from the 3D model is implemented as a fragment shader. The processing time of this shader corresponds approximately to the execution time of the difference detection shader, which is given in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0203-0#Tab1">1</a>.</p><h3 class="c-article__sub-heading" id="Sec20">Model-based camera tracking</h3><p>To evaluate the applicability of the presented approach for model-based camera tracking, a virtual 3D scene (for which ground truth data were known) was reconstructed with the presented approach. Then, the tracking accuracy of model-based camera tracking was evaluated for the reconstructed 3D model. The virtual 3D scene used for this evaluation corresponds to the real scene shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig2">2</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig3">3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig4">4</a> and is visualized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig8">8</a>. For this evaluation scenario, we assumed that initially a 3D model of the lego bricks and the sideboard was given. However, the box and the small car were not modeled yet. Thus, the user would take four snapshots of the box and the car, from which the surfaces of both objects were reconstructed (Gaussian noise was added to the simulated depth images). This 3D reconstruction step added 56.502 new triangles to the 3D model, which previously consisted of 201.670 triangles.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Ground truth camera path (<i>smooth path</i>) and tracked camera paths. <i>Red</i>: without 3D model adjustment according to the changed box position. <i>Blue</i>: tracked camera pose after the 3D model adjustment step. The sideboard and the brick model were given as input to the reconstruction, whereas the car and the box were added by the reconstruction (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In a next step, the reconstructed 3D model was used for model-based camera tracking. The model-based pose estimation tracks KLT features in the 2D image and acquires the 3D coordinates of these features from the 3D model, see Bleser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bleser G, Pastamorv Y, Stricker D (2005) Real-time 3d camera tracking for industrial augmented reality applications. In: WSCG, pp 47–54" href="/article/10.1007/s10055-011-0203-0#ref-CR5" id="ref-link-section-d38709e1541">2005</a>). The virtual camera was moved 360° around the object, which is a challenging camera path as it requires to continuously acquire new 2D–3D correspondences for the tracking. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig9">9</a>a plots the Euclidean camera pose inaccuracy for the reconstructed 3D model and for a perfect 3D model of the scene. For the perfect 3D model, the average Euclidean distance of the calculated pose to the real camera pose was 3.3 cm, and for the reconstructed 3D model 4.8 cm. To further evaluate the contribution of 3D model adjustment for model-based camera tracking, the box was moved 2 cm to the right. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig9">9</a>b plots the camera pose error without and with a further 3D model adjustment step. Whereas an accuracy of 4.9 cm can be achieved with the 3D model which gets adjusted according to the replacement of the box, the average camera pose error without this adjustment is 16.5 cm. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0203-0#Fig8">8</a> visualizes the estimated camera paths with and without the adjustment step.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0203-0/MediaObjects/10055_2011_203_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Accuracy of camera pose estimation (model-based camera tracking). <b>a</b> Tracked camera pose: perfect and reconstructed 3D model. <b>b</b> Tracked camera pose: with and without 3D model adjustment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0203-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusion and future work</h2><div class="c-article-section__content" id="Sec21-content"><p>The framework presented in this paper reduces the gap between augmented reality and 3D modeling by supporting the 3D modeling task with augmented reality–based difference detection. The combination of real-time depth imaging with camera tracking is the basis for geometric difference detection between a real scene and a 3D model of this scene. In a model adjustment step, the detected differences can be used to adjust the 3D model such that it corresponds to the real scene. This is achieved by adding new submeshes to the 3D model or by removing triangles if geometric differences are detected.</p><p>In future work, the 3D model adjustment could be enhanced in several ways: Currently, the added triangle meshes have a uniform resolution. Thus, the number of triangles in the 3D model increases quite fast when new submeshes are added. The number of triangles can be significantly reduced by mesh decimation algorithms such as quadric edge collapse decimation (Garland and Heckbert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Garland M, Heckbert PS (1997) Surface simplification using quadric error metrics. In: Siggraph 1997, pp 209–216" href="/article/10.1007/s10055-011-0203-0#ref-CR14" id="ref-link-section-d38709e1589">1997</a>). Furthermore, the detection of geometric elements such as planar surfaces in the depth images could help to yield both smaller (in number of triangles) and more accurate surface structures. If a plane is detected, the 3D measurements can be adjusted such that they lie exactly on the plane and the geometry can be modeled by very few triangles.</p><p>The model adjustment could further be improved by edge stitching: In the current version, the triangle meshes are just added to the geometry the same way as they were measured. This causes saw-toothed edges that are not smoothly merged with the other triangles. This could be enhanced by adding additional triangles between the vertices at the border of the new mesh and the closest vertices of the previously reconstructed triangle mesh. In view of the model adjustment itself, adjusting the position of triangles could be an alternative to the appending and removal of triangles. This would require to find a solution for the question: Under which conditions can we assume that an existing object has moved? And in which cases do we rather assume that a new object was added to or removed from the scene?</p><p>In the current implementation, the camera pose estimation process is separated from the geometric modeling. The registration between the depth image and the 3D model could be enhanced with an additional geometric alignment step such as the iterative closest point algorithm (Besl and McKay <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Besl P, McKay N (1992) A method for registration of 3-D shapes. IEEE Trans Pattern Anal Mach Intell 14(2):239–256" href="/article/10.1007/s10055-011-0203-0#ref-CR3" id="ref-link-section-d38709e1597">1992</a>). Furthermore, a combined numerical optimization, which optimizes the poses as well as the 3D measurements, could help to find an optimum for the pose estimation and the geometric registration.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RT. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma RT (1997) A survey of augmented reality. Presence Teleoperators Virtual Environ 6:355–385" /><p class="c-article-references__text" id="ref-CR1">Azuma RT (1997) A survey of augmented reality. Presence Teleoperators Virtual Environ 6:355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Presence%20Teleoperators%20Virtual%20Environ&amp;volume=6&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CRT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: 9th " /><p class="c-article-references__text" id="ref-CR2">Bastian J, Ward B, Hill R, van den Hengel A, Dick A (2010) Interactive modelling for ar applications. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), 2010, pp 199–205</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Besl, N. McKay, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Besl P, McKay N (1992) A method for registration of 3-D shapes. IEEE Trans Pattern Anal Mach Intell 14(2):239–" /><p class="c-article-references__text" id="ref-CR3">Besl P, McKay N (1992) A method for registration of 3-D shapes. IEEE Trans Pattern Anal Mach Intell 14(2):239–256</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.121791" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20method%20for%20registration%20of%203-D%20shapes&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=14&amp;issue=2&amp;pages=239-256&amp;publication_year=1992&amp;author=Besl%2CP&amp;author=McKay%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bleser G (2009) Towards visual-inertial slam for mobile augmented reality. PhD thesis, TU Kaiserslautern" /><p class="c-article-references__text" id="ref-CR4">Bleser G (2009) Towards visual-inertial slam for mobile augmented reality. PhD thesis, TU Kaiserslautern</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bleser G, Pastamorv Y, Stricker D (2005) Real-time 3d camera tracking for industrial augmented reality applica" /><p class="c-article-references__text" id="ref-CR5">Bleser G, Pastamorv Y, Stricker D (2005) Real-time 3d camera tracking for industrial augmented reality applications. In: WSCG, pp 47–54</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Bleser, M. Becker, D. Stricker, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Pr" /><p class="c-article-references__text" id="ref-CR6">Bleser G, Becker M, Stricker D (2007) Real-time vision-based tracking and reconstruction. J Real Time Image Proc 2:161–175</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11554-007-0034-0" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20vision-based%20tracking%20and%20reconstruction&amp;journal=J%20Real%20Time%20Image%20Proc&amp;volume=2&amp;pages=161-175&amp;publication_year=2007&amp;author=Bleser%2CG&amp;author=Becker%2CM&amp;author=Stricker%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bosché F (2008) Automated recognition of 3d cad model objects in dense laser range point clouds. PhD thesis, U" /><p class="c-article-references__text" id="ref-CR7">Bosché F (2008) Automated recognition of 3d cad model objects in dense laser range point clouds. PhD thesis, University of Waterloo</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Bosché, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Bosché F (2010) Automated recognition of 3D cad model objects in laser scans and calculation of as-built dimen" /><p class="c-article-references__text" id="ref-CR8">Bosché F (2010) Automated recognition of 3D cad model objects in laser scans and calculation of as-built dimensions for dimensional compliance control in construction. Elsevier J Adv Eng Inform 24(1):107–118</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.aei.2009.08.006" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automated%20recognition%20of%203D%20cad%20model%20objects%20in%20laser%20scans%20and%20calculation%20of%20as-built%20dimensions%20for%20dimensional%20compliance%20control%20in%20construction&amp;journal=Elsevier%20J%20Adv%20Eng%20Inform&amp;volume=24&amp;issue=1&amp;pages=107-118&amp;publication_year=2010&amp;author=Bosch%C3%A9%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bosché F, Teizer J, Haas CT, Caldas CH (2006) Integrating data from 3d cad and 3d cameras for real-time modeli" /><p class="c-article-references__text" id="ref-CR9">Bosché F, Teizer J, Haas CT, Caldas CH (2006) Integrating data from 3d cad and 3d cameras for real-time modeling. In: Proceedings of joint international conference on computing and decision making in civil and building engineering, pp 37–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Engelhard N (2010) KinectAutoCalibration. https://github.com/NikolasE/KinectAutoCalibration&#xA;                " /><p class="c-article-references__text" id="ref-CR10">Engelhard N (2010) KinectAutoCalibration. <a href="https://github.com/NikolasE/KinectAutoCalibration">https://github.com/NikolasE/KinectAutoCalibration</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Engelhard N, Endres F, Hess J, Sturm J, Burgard W (2011) Real-time 3d visual slam with a hand-held rgb-d camer" /><p class="c-article-references__text" id="ref-CR11">Engelhard N, Endres F, Hess J, Sturm J, Burgard W (2011) Real-time 3d visual slam with a hand-held rgb-d camera. In: Proceedings of the RGB-D workshop on 3D perception in robotics at the European robotics forum, Vasteras, Sweden</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Franke T, Kahn S, Olbrich M, Jung Y (2011) Enhancing realism of mixed reality applications through real-time d" /><p class="c-article-references__text" id="ref-CR12">Franke T, Kahn S, Olbrich M, Jung Y (2011) Enhancing realism of mixed reality applications through real-time depth-imaging devices in x3d. In: Proceedings of the 16th international conference on 3D web technology. ACM, New York, NY, USA, Web3D ’11, pp 71–79</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gall J, Rosenhahn B, Seidel HP (2006) Robust pose estimation with 3D textured models. In: Pacific-Rim symposiu" /><p class="c-article-references__text" id="ref-CR13">Gall J, Rosenhahn B, Seidel HP (2006) Robust pose estimation with 3D textured models. In: Pacific-Rim symposium on image and video technology (PSIVT), pp 84–95</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Garland M, Heckbert PS (1997) Surface simplification using quadric error metrics. In: Siggraph 1997, pp 209–21" /><p class="c-article-references__text" id="ref-CR14">Garland M, Heckbert PS (1997) Surface simplification using quadric error metrics. In: Siggraph 1997, pp 209–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Georgel P, Schroeder P, Benhimane S, Hinterstoisser S, Appel M, Navab N (2007) An industrial augmented reality" /><p class="c-article-references__text" id="ref-CR15">Georgel P, Schroeder P, Benhimane S, Hinterstoisser S, Appel M, Navab N (2007) An industrial augmented reality solution for discrepancy check. In: ISMAR 2007: proceedings of the 6th IEEE and ACM international symposium on mixed and augmented reality, pp 1–4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Georgel, P. Schroeder, N. Navab, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Georgel P, Schroeder P, Navab N (2009) Navigation tools for viewing augmented cad models. IEEE Comput Graph Ap" /><p class="c-article-references__text" id="ref-CR16">Georgel P, Schroeder P, Navab N (2009) Navigation tools for viewing augmented cad models. IEEE Comput Graph Appl 29(6):65–73</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2009.123" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigation%20tools%20for%20viewing%20augmented%20cad%20models&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=29&amp;issue=6&amp;pages=65-73&amp;publication_year=2009&amp;author=Georgel%2CP&amp;author=Schroeder%2CP&amp;author=Navab%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Georgel P, Benhimane S, Sotke J, Navab N (2009a) Photo-based industrial augmented reality application using a " /><p class="c-article-references__text" id="ref-CR17">Georgel P, Benhimane S, Sotke J, Navab N (2009a) Photo-based industrial augmented reality application using a single keyframe registration procedure. In: ISMAR 2009: proceedings of the 8th IEEE and ACM international symposium on mixed and augmented reality, pp 187–188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haller M (2004) Photorealism or/and non-photorealism in augmented reality. In: Proceedings of the 2004 ACM SIG" /><p class="c-article-references__text" id="ref-CR18">Haller M (2004) Photorealism or/and non-photorealism in augmented reality. In: Proceedings of the 2004 ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry, VRCAI ’04, pp 189–196</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Huhle, P. Jenke, W. Straßer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Huhle B, Jenke P, Straßer W (2008) On-the-fly scene acquisition with a handy multi-sensor system. Int J Intell" /><p class="c-article-references__text" id="ref-CR19">Huhle B, Jenke P, Straßer W (2008) On-the-fly scene acquisition with a handy multi-sensor system. Int J Intell Syst Technol Appl (IJISTA) 5(3/4):255–263</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On-the-fly%20scene%20acquisition%20with%20a%20handy%20multi-sensor%20system&amp;journal=Int%20J%20Intell%20Syst%20Technol%20Appl%20%28IJISTA%29&amp;volume=5&amp;issue=3%2F4&amp;pages=255-263&amp;publication_year=2008&amp;author=Huhle%2CB&amp;author=Jenke%2CP&amp;author=Stra%C3%9Fer%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kahn S, Wuest H, Fellner DW (2010a) Time-of-flight based scene reconstruction with a mesh processing tool for " /><p class="c-article-references__text" id="ref-CR20">Kahn S, Wuest H, Fellner DW (2010a) Time-of-flight based scene reconstruction with a mesh processing tool for model based camera tracking. In: 5th international conference on computer vision theory and applications (VISAPP), vol 1. pp 302–309</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. " /><p class="c-article-references__text" id="ref-CR21">Kahn S, Wuest H, Stricker D, Fellner DW (2010b) 3D discrepancy check and visualization via augmented reality. In: 9th IEEE international symposium on mixed and augmented reality (ISMAR), pp 241–242</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the eigth IEEE an" /><p class="c-article-references__text" id="ref-CR22">Klein G, Murray D (2009) Parallel tracking and mapping on a camera phone. In: Proceedings of the eigth IEEE and ACM international symposium on mixed and augmented reality (ISMAR’09), Orlando, pp 83–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of the e" /><p class="c-article-references__text" id="ref-CR23">Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of the eurographics (state-of-the-art report), pp 119–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lepetit V, Fua P (2005) Monocular model-based 3D tracking of rigid objects: a survey. In: Foundations and tren" /><p class="c-article-references__text" id="ref-CR24">Lepetit V, Fua P (2005) Monocular model-based 3D tracking of rigid objects: a survey. In: Foundations and trends in computer graphics and vision, vol 1, pp 1–89</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lindner M, Kolb A, Hartmann K (2007) Data-fusion of pmd-based distance-information and high-resolution rgb-ima" /><p class="c-article-references__text" id="ref-CR25">Lindner M, Kolb A, Hartmann K (2007) Data-fusion of pmd-based distance-information and high-resolution rgb-images. In: Proceedings of the international symposium on signals, circuits and systems (ISSCS), session on algorithms for 3D TOF-cameras, vol 1, pp 121–124</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="MesaImaging (2011) Mesa imaging. http://www.mesa-imaging.ch&#xA;                " /><p class="c-article-references__text" id="ref-CR26">MesaImaging (2011) Mesa imaging. <a href="http://www.mesa-imaging.ch">http://www.mesa-imaging.ch</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: IEEE conference on com" /><p class="c-article-references__text" id="ref-CR27">Newcombe R, Davison A (2010) Live dense reconstruction with a single moving camera. In: IEEE conference on computer vision and pattern recognition (CVPR), 2010, pp 1498–1505</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oggier T, Lustenberger F, Blanc N (2006) Miniature 3D ToF camera for real-time imaging. In: Perception and int" /><p class="c-article-references__text" id="ref-CR28">Oggier T, Lustenberger F, Blanc N (2006) Miniature 3D ToF camera for real-time imaging. In: Perception and interactive technologies, pp 212–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="OpenNI (2011) Openni framework. http://www.openni.org/&#xA;                " /><p class="c-article-references__text" id="ref-CR29">OpenNI (2011) Openni framework. <a href="http://www.openni.org/">http://www.openni.org/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="OpenSG (2011) OpenSG. http://www.opensg.org&#xA;                " /><p class="c-article-references__text" id="ref-CR30">OpenSG (2011) OpenSG. <a href="http://www.opensg.org">http://www.opensg.org</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pan Q, Reitmayr G, Drummond T (2009) Proforma: probabilistic feature-based on-line rapid model acquisition. In" /><p class="c-article-references__text" id="ref-CR31">Pan Q, Reitmayr G, Drummond T (2009) Proforma: probabilistic feature-based on-line rapid model acquisition. In: Proceedings of the 20th British machine vision conference (BMVC), p 11</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Panagopoulos A, Samaras D, Paragios N (2009) Robust shadow and illumination estimation using a mixture model. " /><p class="c-article-references__text" id="ref-CR32">Panagopoulos A, Samaras D, Paragios N (2009) Robust shadow and illumination estimation using a mixture model. In: IEEE conference on computer vision and pattern recognition, 2009. CVPR 2009, pp 651 –658</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schiller I, Beder C, Koch R (2008) Calibration of a pmd-camera using a planar calibration pattern together wit" /><p class="c-article-references__text" id="ref-CR33">Schiller I, Beder C, Koch R (2008) Calibration of a pmd-camera using a planar calibration pattern together with a multi-camera setup. In: The international archives of the photogrammetry, remote sensing and spatial information sciences, vol XXI. ISPRS Congress, pp 297–302</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track. In: IEEE conference on computer vision and pattern recognition " /><p class="c-article-references__text" id="ref-CR34">Shi J, Tomasi C (1994) Good features to track. In: IEEE conference on computer vision and pattern recognition (CVPR’94), pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the 2009 8th " /><p class="c-article-references__text" id="ref-CR35">Van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of the 2009 8th IEEE international symposium on mixed and augmented reality, ISMAR ’09, pp 107–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Webel S, Becker M, Stricker D, Wuest H (2007) Identifying differences between cad and physical mock-ups using " /><p class="c-article-references__text" id="ref-CR36">Webel S, Becker M, Stricker D, Wuest H (2007) Identifying differences between cad and physical mock-ups using ar. In: ISMAR 2007: proceedings of the sixth IEEE and ACM international symposium on mixed and augmented reality, pp 281–282</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wuest H (2008) Efficient line and patch feature characterization and management for real-time camera tracking." /><p class="c-article-references__text" id="ref-CR37">Wuest H (2008) Efficient line and patch feature characterization and management for real-time camera tracking. PhD thesis, TU Darmstadt</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Wuest, F. Wientapper, D. Stricker, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Wuest H, Wientapper F, Stricker D (2007) Adaptable model-based tracking using analysis-by-synthesis techniques" /><p class="c-article-references__text" id="ref-CR38">Wuest H, Wientapper F, Stricker D (2007) Adaptable model-based tracking using analysis-by-synthesis techniques. In: Kropatsch W, Kampel M, Hanbury A (eds) Computer analysis of images and patterns, lecture notes in computer science, vol 4673, Springer, Berlin, pp 20–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20analysis%20of%20images%20and%20patterns%2C%20lecture%20notes%20in%20computer%20science&amp;pages=20-27&amp;publication_year=2007&amp;author=Wuest%2CH&amp;author=Wientapper%2CF&amp;author=Stricker%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhou F, Duh HBL, Billinghurst M (2008) Trends in augmented reality tracking, interaction and display: a review" /><p class="c-article-references__text" id="ref-CR39">Zhou F, Duh HBL, Billinghurst M (2008) Trends in augmented reality tracking, interaction and display: a review of ten years of ismar. In: ISMAR 2008: IEEE/ACM international symposium on mixed and augmented reality, pp 193–202</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0203-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was partially funded by the German BMBF project AVILUSplus (01IM08002).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Fraunhofer IGD, Darmstadt, Germany</p><p class="c-article-author-affiliation__authors-list">Svenja Kahn</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Svenja-Kahn"><span class="c-article-authors-search__title u-h3 js-search-name">Svenja Kahn</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Svenja+Kahn&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Svenja+Kahn" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Svenja+Kahn%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0203-0/email/correspondent/c1/new">Svenja Kahn</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Reducing%20the%20gap%20between%20Augmented%20Reality%20and%203D%20modeling%20with%20real-time%20depth%20imaging&amp;author=Svenja%20Kahn&amp;contentID=10.1007%2Fs10055-011-0203-0&amp;publication=1359-4338&amp;publicationDate=2011-12-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kahn, S. Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging.
                    <i>Virtual Reality</i> <b>17, </b>111–123 (2013). https://doi.org/10.1007/s10055-011-0203-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0203-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-03-16">16 March 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-02">02 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-23">23 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06">June 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0203-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0203-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">3D modeling</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented Reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Computer vision</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Depth imaging</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Analysis-by-synthesis</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0203-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=203;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

