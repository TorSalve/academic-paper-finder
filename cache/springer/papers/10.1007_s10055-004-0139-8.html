<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Taxonomy for visualizing location-based information"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Location-based data is digital information that has a real-world location. Location-based data can be used for many purposes, such as providing additional information on real-world objects or..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Taxonomy for visualizing location-based information"/>

    <meta name="dc.source" content="Virtual Reality 2004 8:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-09-18"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Location-based data is digital information that has a real-world location. Location-based data can be used for many purposes, such as providing additional information on real-world objects or helping a user in a specific task. Access to such data can be provided in many ways, for example, with augmented reality (AR) systems. AR techniques can help its user in various tasks and the AR data can be presented to the user in various ways, depending on the task at hand. The different visualizations that can be used are heavily dependent on the hardware platform and, thus, all technologies are not suitable for every situation. This paper studies two factors that affect the visualization of location-based data. The two factors are the environment model they use, ranging from three dimensions (3D) to no dimensions (0D) at all; and the viewpoint, whether it is a first-person or a third-person view. As a result, we define a taxonomy for visualizing location-based data, where each model&#8211;view (MV) combination is referred to using its MV number. We also present numerous case studies with different MV values."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-09-18"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="71"/>

    <meta name="prism.endingPage" content="82"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0139-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0139-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0139-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0139-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Taxonomy for visualizing location-based information"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2004/09/18"/>

    <meta name="citation_firstpage" content="71"/>

    <meta name="citation_lastpage" content="82"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0139-8"/>

    <meta name="DOI" content="10.1007/s10055-004-0139-8"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0139-8"/>

    <meta name="description" content="Location-based data is digital information that has a real-world location. Location-based data can be used for many purposes, such as providing additional "/>

    <meta name="dc.creator" content="Riku Suomela"/>

    <meta name="dc.creator" content="Juha Lehikoinen"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Wireless Netw; citation_title=Cyberguide: a mobile context-aware tour guide; citation_author=GD Abowd, CG Atkeson, J Hong, S Long, R Kooper, M Pinkerton; citation_volume=3; citation_issue=5; citation_publication_date=1997; citation_pages=421-433; citation_doi=10.1023/A:1019194325861; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=The design of electronic map displays; citation_author=AJ Aretz; citation_volume=33; citation_issue=1; citation_publication_date=1991; citation_pages=85-101; citation_id=CR2"/>

    <meta name="citation_reference" content="Azuma R, Hoff B, Neely H III, Sarfaty R (1999) A motion-stabilized outdoor augmented reality system. In: Proceedings of IEEE virtual reality conference (VR&#8217;99), Houston, Texas, March 1999, pp 252&#8211;259"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot, R Behringer, S Feiner, S Julier, B MacIntyre; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-47; citation_doi=10.1109/38.963459; citation_id=CR4"/>

    <meta name="citation_reference" content="Bahl P, Padmanabhan VN (2000) RADAR: an in-building RF-based user location and tracking system. In: Proceedings of the IEEE conference on computer communications (INFOCOM 2000), Tel-Aviv, Israel, March 2000, vol 2, pp 775&#8211;784"/>

    <meta name="citation_reference" content="Benefon (2003) Home page at 
                    http://www.benefon.com/products/index.htm
                    
                  . Cited 7 Oct 2003"/>

    <meta name="citation_reference" content="citation_title=Readings in information visualization: using vision to think; citation_publication_date=1999; citation_id=CR7; citation_author=SK Card; citation_author=JD Mackinlay; citation_author=B Shneiderman; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Tauber MJ (ed) Proceedings of the conference on human factors in computing systems (CHI&#8217;96), Vancouver, Canada, April 1996, pp 142&#8211;149"/>

    <meta name="citation_reference" content="Darken RP, Cevik H (1999) Map usage in virtual environments: orientation issues. In: Proceedings of the IEEE virtual reality conference (VR&#8217;99), Houston, Texas, March 1999, pp 133&#8211;140"/>

    <meta name="citation_reference" content="Feiner S, MacIntyre B, H&#246;llerer T, Webster A. (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. In: Proceedings of the 1st international symposium on wearable computers (ISWC&#8217;97), Cambridge, Massachusetts, October 1997, pp 74&#8211;81"/>

    <meta name="citation_reference" content="Geiger C, Kleinnjohann B, Reimann C, Stichling D (2001) Mobile AR4ALL. In: Proceedings of the IEEE and ACM international symposium on augmented reality (ISAR 2001), Munich, Germany, October 2000, pp 181&#8211;182"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Spectr; citation_title=Perspective/navigation&#8212;the global positioning system; citation_author=IA Getting; citation_volume=30; citation_issue=12; citation_publication_date=1993; citation_pages=36-38; citation_doi=10.1109/6.272176; citation_id=CR12"/>

    <meta name="citation_reference" content="GPS Pilot (2003) Home page at 
                    http://www.gpspilot.com/Products.htm
                    
                  . Cited 7 Oct 2003"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Location systems for ubiquitous computing; citation_author=J Hightower, G Borriello; citation_volume=34; citation_issue=8; citation_publication_date=2001; citation_pages=57-66; citation_doi=10.1109/2.940014; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Information filtering for mobile augmented reality; citation_author=S Julier, Y Baillot, D Brown; citation_volume=22; citation_issue=5; citation_publication_date=2002; citation_pages=12-15; citation_doi=10.1109/MCG.2002.1028721; citation_id=CR15"/>

    <meta name="citation_reference" content="Krumm J, Williams L, Smith G (2002) SmartMoveX on a graph&#8212;an inexpensive active badge tracker. In: Proceedings of the 4th international conference on ubiquitous computing (UbiComp 2002), G&#246;teborg, Sweden, September/October 2002, pp 299&#8211;307"/>

    <meta name="citation_reference" content="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human&#8211;computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224&#8211;231"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=WalkMap: developing an augmented reality map application for wearable computers; citation_author=J Lehikoinen, R Suomela; citation_volume=6; citation_issue=1; citation_publication_date=2002; citation_pages=33-44; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Personal Ubiquitous Comput; citation_title=Accessing context in wearable computers; citation_author=J Lehikoinen, R Suomela; citation_volume=6; citation_issue=1; citation_publication_date=2002; citation_pages=64-74; citation_doi=10.1007/s007790200006; citation_id=CR19"/>

    <meta name="citation_reference" content="Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearable computers (ISWC 2002), Seattle, Washington, October 2002, pp 171&#8211;178"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Gen; citation_title=Principles of spatial problem solving; citation_author=M Levine, IN Jankovic, M Palij; citation_volume=111; citation_issue=2; citation_publication_date=1982; citation_pages=157-175; citation_doi=10.1037//0096-3445.111.2.157; citation_id=CR21"/>

    <meta name="citation_reference" content="Mobilaris (2003) Home page at 
                    http://www.mobilaris.se
                    
                  . Cited 7 Oct 2003"/>

    <meta name="citation_reference" content="Nokia GPS module (2003) Home page at 
                    http://www.nokia.com
                    
                  . Cited 7 Oct 2003"/>

    <meta name="citation_reference" content="citation_journal_title=Sci J Orienteering; citation_title=What does it take to read a map?; citation_author=T Ottosson; citation_volume=4; citation_publication_date=1988; citation_pages=97-106; citation_id=CR24"/>

    <meta name="citation_reference" content="PFU Systems (2003) Cell computing, online document, available at 
                    http://www.pfusystems.com/
                    
                  . Cited 7 Oct 2003"/>

    <meta name="citation_reference" content="Rakkolainen I, Pulkkinen S, Heinonen A (1998) Visualizing real-time GPS data with internet&#8217;s VRML worlds. In: Proceedings of the 6th international symposium on advances in geographic information systems (ACM-GIS&#8217;98), Washington, District of Columbia, November 1998, pp 52&#8211;56"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Transact Comput Hum Interact; citation_title=Nomadic radio: speech and audio interaction for contextual messaging in nomadic environments; citation_author=N Sawhney, C Schmandt; citation_volume=7; citation_issue=3; citation_publication_date=2000; citation_pages=353-383; citation_doi=10.1145/355324.355327; citation_id=CR27"/>

    <meta name="citation_reference" content="Spoerri A (1993) Novel route guidance displays. In: Proceedings of the IEEE-IEE vehicle navigation and information systems conference, Ottawa, Ontario, October 1993, pp 419&#8211;422"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=A profile of drivers&#8217; map-reading abilities; citation_author=LA Streeter, D Vitello; citation_volume=28; citation_issue=2; citation_publication_date=1986; citation_pages=223-239; citation_id=CR29"/>

    <meta name="citation_reference" content="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147&#8211;154"/>

    <meta name="citation_reference" content="Suomela R, Lehikoinen J, Salminen I (2001) A system for evaluating augmented reality user interfaces in wearable computers. In: Proceedings of the 5th international symposium on wearable computers (ISWC 2001), Zurich, Switzerland, October 2001, pp 77&#8211;84"/>

    <meta name="citation_reference" content="citation_journal_title=Personal Ubiquitous Comput; citation_title=The evolution of perspective view in WalkMap; citation_author=R Suomela, K Roimela, J Lehikoinen; citation_volume=7; citation_issue=5; citation_publication_date=2003; citation_pages=249-262; citation_doi=10.1007/s00779-003-0244-9; citation_id=CR32"/>

    <meta name="citation_reference" content="Thomas B, Demczuk V, Piekarski W, Hepworth D, Gunther B (1998) A wearable computer system with augmented reality to support terrestrial navigation. In: Proceedings of the 2nd international symposium on wearable computers (ISWC&#8217;98), Pittsburgh, Pennsylvania, October 1998, pp 168&#8211;171"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Transact Info Syst; citation_title=The active badge location system; citation_author=R Want, A Hopper, V Falc&#227;o, J Gibbons; citation_volume=10; citation_issue=1; citation_publication_date=1992; citation_pages=91-102; citation_doi=10.1145/128756.128759; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_journal_title=Environ Behav; citation_title=Map alignment in traveling multisegment routes; citation_author=DH Warren, TE Scott; citation_volume=25; citation_issue=5; citation_publication_date=1993; citation_pages=643-666; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Perception of map-environment correspondence: the roles of features and alignment; citation_author=DH Warren, MJ Rossano, TD Wear; citation_volume=2; citation_issue=2; citation_publication_date=1990; citation_pages=131-150; citation_id=CR36"/>

    <meta name="citation_author" content="Riku Suomela"/>

    <meta name="citation_author_email" content="riku.suomela@nokia.com"/>

    <meta name="citation_author_institution" content="Nokia Research Center, Tampere, Finland"/>

    <meta name="citation_author" content="Juha Lehikoinen"/>

    <meta name="citation_author_email" content="juha.lehikoinen@nokia.com"/>

    <meta name="citation_author_institution" content="Nokia Research Center, Tampere, Finland"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0139-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0139-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Taxonomy for visualizing location-based information"/>
        <meta property="og:description" content="Location-based data is digital information that has a real-world location. Location-based data can be used for many purposes, such as providing additional information on real-world objects or helping a user in a specific task. Access to such data can be provided in many ways, for example, with augmented reality (AR) systems. AR techniques can help its user in various tasks and the AR data can be presented to the user in various ways, depending on the task at hand. The different visualizations that can be used are heavily dependent on the hardware platform and, thus, all technologies are not suitable for every situation. This paper studies two factors that affect the visualization of location-based data. The two factors are the environment model they use, ranging from three dimensions (3D) to no dimensions (0D) at all; and the viewpoint, whether it is a first-person or a third-person view. As a result, we define a taxonomy for visualizing location-based data, where each model–view (MV) combination is referred to using its MV number. We also present numerous case studies with different MV values."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Taxonomy for visualizing location-based information | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0139-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Location-based data, Virtual objects, Augmented reality, Visualization, Taxonomy","kwrd":["Location-based_data","Virtual_objects","Augmented_reality","Visualization","Taxonomy"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0139-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0139-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=139;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0139-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Taxonomy for visualizing location-based information
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0139-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0139-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-09-18" itemprop="datePublished">18 September 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Taxonomy for visualizing location-based information</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Riku-Suomela" data-author-popup="auth-Riku-Suomela" data-corresp-id="c1">Riku Suomela<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nokia Research Center" /><meta itemprop="address" content="grid.6533.3, 0000000123048515, Nokia Research Center, P.O. Box 100, Tampere, 33721, Finland" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Juha-Lehikoinen" data-author-popup="auth-Juha-Lehikoinen">Juha Lehikoinen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nokia Research Center" /><meta itemprop="address" content="grid.6533.3, 0000000123048515, Nokia Research Center, P.O. Box 100, Tampere, 33721, Finland" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">71</span>–<span itemprop="pageEnd">82</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">166 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0139-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Location-based data is digital information that has a real-world location. Location-based data can be used for many purposes, such as providing additional information on real-world objects or helping a user in a specific task. Access to such data can be provided in many ways, for example, with augmented reality (AR) systems. AR techniques can help its user in various tasks and the AR data can be presented to the user in various ways, depending on the task at hand. The different visualizations that can be used are heavily dependent on the hardware platform and, thus, all technologies are not suitable for every situation. This paper studies two factors that affect the visualization of location-based data. The two factors are the <i>environment model</i> they use, ranging from three dimensions (3D) to no dimensions (0D) at all; and the <i>viewpoint</i>, whether it is a first-person or a third-person view. As a result, we define a taxonomy for visualizing location-based data, where each model–view (MV) combination is referred to using its MV number. We also present numerous case studies with different MV values.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Location-based data ties digital data to some real-world locations or areas. The real world only has a limited space for information at a certain point in space, but the digital domain has a potentially endless storage capacity for relevant information relating to a location. Location-based data can be the complementary information channel for the real world.</p><p>Location-based information is, typically, a set of virtual objects in a certain area. Virtual objects have a precise location in the real world and they can be presented to the user in many different ways. The data can be presented either in a virtual world, in which case the user sees the data as part of the digital representation of the environment, or it can be shown as part of reality. Similarly, a virtual object may be an object that exists in the virtual world only, or it may represent a real-world object.</p><p>In this paper, we use virtual objects to denote location-based digital objects stored in a database. The user can access this data by downloading a set of relevant objects. One way of choosing the objects is to take all objects inside a sphere that is centered around the user and has a known radius. By altering the radius of the sphere, the user can choose how many objects are shown and how far they are from him or her. The minimal set contains one such object and, in case too many objects are in the set, we need some mechanism for prioritizing the important ones. Some filtering can also be applied to these objects, so only the relevant types of objects are retrieved (see e.g., Julier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Julier S, Baillot Y, Brown D (2002) Information filtering for mobile augmented reality. IEEE Comput Graph Appl 22(5):12–15 " href="/article/10.1007/s10055-004-0139-8#ref-CR15" id="ref-link-section-d40635e300">2002</a>).</p><p>The virtual objects usually have their location presented in either three or two dimensions (2D), depending on e.g., the application needs. Obviously, 2D virtual objects limit the visualization possibilities of the application, as true 3D presentation cannot be achieved.</p><p>There should be many different ways to access the location-based data, mainly due to the variety of different people’s needs. Augmented reality (AR) applications use a 3D model of the environment and they require expensive hardware and may, therefore, not be affordable to the general public. To reach a large number of users, the hardware required for accessing location-based data needs to be inexpensive. The location-based data should remain the same, no matter what the end-user device is. Different applications create different kinds of views to the same digital information space; hence, there should be several different approaches for accessing the location-based data. This is already evidenced by many existing approaches. The fanciest applications use AR techniques (e.g., Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Feiner S, MacIntyre B, Höllerer T, Webster A. (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. In: Proceedings of the 1st international symposium on wearable computers (ISWC’97), Cambridge, Massachusetts, October 1997, pp 74–81" href="/article/10.1007/s10055-004-0139-8#ref-CR10" id="ref-link-section-d40635e309">1997</a>) to merge the real and virtual worlds. Other techniques include digital maps that usually present the environment in 2D, while some location-based services only list the available resources at a certain location.</p><p>Since virtual location-based data is tied to a <i>real-world</i> location, it becomes a part of the reality (even though one cannot perceive it without computational resources). Consequently, the connection point between these two worlds is the <i>location</i>. Therefore, it is imperative to be able to represent this relation as accurately as possible in order to maintain the user’s model of the two combined worlds. A common method for this representation is to use graphical means, i.e., information visualization. We concentrate on this aspect.</p><p>Card et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Card SK, Mackinlay JD, Shneiderman B (1999) Readings in information visualization: using vision to think. Morgan Kaufmann, San Francisco, California, p 6 " href="/article/10.1007/s10055-004-0139-8#ref-CR7" id="ref-link-section-d40635e324">1999</a>) define visualization as follows: “the use of computer-supported, interactive, visual representations of data to amplify cognition.” In accordance with this definition, our task is to assist people to perceive the digital information all around them.</p><p>This paper analyses two essential factors that affect the visualization of location-based data: the environmental model and the view perspective. By using these two factors, we create a taxonomy for the representation of the location-based data.</p><p>The <i>environment model</i> is used to denote how many dimensions the application uses in visualizing the environment. The most complex ones use a 3D model of the environment while the simplest ones have no information of the environment whatsoever. A fully 3D environment model provides the user with most help on the exact location of an object. If no environment model is used, the user does not gain specific location information of an object, except that the object might be somewhere close by. The most obvious environment models are summarized below:</p><ul class="u-list-style-dash">
                  <li>
                    <p><i>3D environment model</i>: these applications have an accurate 3D model of the environment and they place the location-based data onto its actual location in either the virtual or augmented view</p>
                  </li>
                  <li>
                    <p><i>2D environment model</i>: the locations of the virtual objects are accurately projected onto a plane</p>
                  </li>
                  <li>
                    <p><i>1D environment model</i>: application only shows one aspect of the location-based data</p>
                  </li>
                  <li>
                    <p><i>No environment model</i>: the applications present the data to the user but nothing about its location or relation to the user</p>
                  </li>
                </ul><p>Another important aspect of our visualization taxonomy, in addition to the environment model, is the <i>users’ view perspective</i>. The user can either view the environment from the first-person view, that is, the user sees the environment as looking at it from his or her own viewpoint, or the other possibility is to have the user presented in a third-person view, that is, the user sees him/herself as part of the environment.</p><p>The user’s view to the location-based data is one of the two:</p><ul class="u-list-style-dash">
                  <li>
                    <p><i>First person view</i>: the user views the location-based data from a user-centric view, and the location-based data is spread around him or her</p>
                  </li>
                  <li>
                    <p><i>Third person view</i>: the user views both the location-based data and his or her representation</p>
                  </li>
                </ul><p>The visualizations used in accessing location-based data can be classified into these categories based on how complex is the model of the environment that the application uses, and from which perspective it is viewed. The first-person views create a user-centric view to the data while third-person views can be understood as maps of the environment.</p><p>Each of these visualization types is suitable for some situations. Each of them has different hardware requirements and benefits for different usage. A complex environment model helps the user in locating the data, but requires more from the hardware. A 3D environment model places the data at its exact location in the real or virtual world and the user does not need to do too much matching. When less complex environment models are used, the application requires less processing power, but the user needs to process the information more. Less than 3D presentations of the environment are only approximations of the real world and, thus, the user needs to map the data to the real world.</p><p>This paper concentrates on analyzing different visualizations for location-based applications. The visualizations are analyzed based on the environment model and the viewpoint, and the taxonomy is constructed using these two factors. The paper is structured as follows. First, we take a look at maps and navigation in general, as this is a very widely studied area and closely related to visual presentation of location-based information. Next, we look at the different visualization models already in use in various research projects and products, and introduce the different model-view combinations. After that, we present our work on many of the different combinations, followed by discussion about the topic. Finally, the work is concluded.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Maps and navigation</h2><div class="c-article-section__content" id="Sec2-content"><p>Maps are the most common way to visualize environmental information. Traditional paper maps present this information usually from a 2D bird’s eye view (third-person view). With the advent of digital maps displayed on computer screens, however, more versatile ways of presenting maps have emerged. The new visualization methods include, for example, 3D maps that are viewed from an egocentric reference frame (a first-person view).</p><p>Both 2D and 3D maps are used frequently in many situations. For some tasks, the egocentric views are better, while for other tasks, some other views would be preferred (see e.g., Aretz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Aretz AJ (1991) The design of electronic map displays. Hum Factors 33(1):85–101" href="/article/10.1007/s10055-004-0139-8#ref-CR2" id="ref-link-section-d40635e413">1991</a>). Navigational tasks with digital maps can be defined as searching tasks (naïve search and primed search), and exploration tasks (Darken and Sibert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Tauber MJ (ed) Proceedings of the conference on human factors in computing systems (CHI’96), Vancouver, Canada, April 1996, pp 142–149" href="/article/10.1007/s10055-004-0139-8#ref-CR8" id="ref-link-section-d40635e416">1996</a>). A fourth task can be defined as a targeted search (Darken and Cevik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Darken RP, Cevik H (1999) Map usage in virtual environments: orientation issues. In: Proceedings of the IEEE virtual reality conference (VR’99), Houston, Texas, March 1999, pp 133–140" href="/article/10.1007/s10055-004-0139-8#ref-CR9" id="ref-link-section-d40635e419">1999</a>). In a targeted search, the target is shown on the map; in primed search, the target is known, but does not appear on the map; in naïve search, there is no a prior knowledge of the position of the target, and the target is not shown on the map; in exploration, no specific target has been set.</p><p>In addition to mere visualization, maps are also used frequently as the user interface (UI) of the map software application. This approach allows e.g., direct manipulation with both the objects on the map as well as the view parameters, thus, significantly enriching the human–map interaction.</p><p>Maps only require some knowledge of the user’s location and they can have different functionality based on how accurately the user’s location is known. If the location is not known very accurately, only the map is shown. If, on the contrary, the location is known accurately enough, the user can be placed on the scene as a “you-are-here” (YAH) symbol.</p><p>An important aspect concerning maps and navigation is <i>alignment</i>. Basically, this term defines how the user holds the map; that is, it specifies how the map is oriented with respect to the user and the environment. A map may be <i>reader aligned</i>, in which case the orientation of the map remains constant with regard to the reader’s body. A typical example of this is a reader holding the map with the print upright. In these cases, the map is usually, although not necessarily, misaligned with respect to the environment. These maps are often aligned north-up (i.e., up on the map corresponds to the north in the environment). An <i>environment-aligned</i> map, on the contrary, is oriented consistently with regard to the environment; in other words, north on the map always corresponds to north in the environment. This implies that, in order to keep the map aligned, the reader is required to turn the map in relation to him or herself when turning in the environment. Environment-aligned maps are referred to as forward-up (or track-up) aligned maps (i.e., up on the map corresponds to the user’s forward view in the environment).</p><p>It has been found that the most severe problem with using traditional 2D maps is the inability to understand the spatial relationships between the real-world objects, and, therefore, to match the map and terrain model in one’s mind (Ottosson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Ottosson T (1988) What does it take to read a map? Sci J Orienteering 4:97–106" href="/article/10.1007/s10055-004-0139-8#ref-CR24" id="ref-link-section-d40635e442">1988</a>). Another study on map-reading abilities suggests that up to 64% of the population have difficulties in map reading (Streeter and Vitello <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Streeter LA, Vitello D (1986) A profile of drivers’ map-reading abilities. Hum Factors 28(2):223–239" href="/article/10.1007/s10055-004-0139-8#ref-CR29" id="ref-link-section-d40635e445">1986</a>). Therefore, even though a 2D map display is a well known visualization technique, alternative ways for visualizing geographical and environmental information should be examined. This is especially true with digital maps that allow a wide variety of dynamic visualization options. These issues are considered next.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Model and view visualizations</h2><div class="c-article-section__content" id="Sec3-content"><p>In this work, the visualizations are categorized based on two criteria: the environment model and the viewpoint on how the user is presented in the UI. The environment model can be 3D, 2D, 1D, or it can be without any dimensions (0D). The user is presented either from the first- or third-person view. We concentrate on the visual presentation, although other medium may be used as well, such as sound (Sawhney and Schmandt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sawhney N, Schmandt C (2000) Nomadic radio: speech and audio interaction for contextual messaging in nomadic environments. ACM Transact Comput Hum Interact 7(3):353–383" href="/article/10.1007/s10055-004-0139-8#ref-CR27" id="ref-link-section-d40635e456">2000</a>).</p><p>The environment model refers to the visualization only. The underlying location-based data can always be stored in 3D, but we are only interested in how the data is presented, as some dimensions can be discarded and a simpler model can be used. Simpler visualizations can be designed for less powerful devices, which provide a larger number of users with access to this data.</p><p>The first-person view augments data as seen from the user perspective. The third-person views are digital maps that show both the augmented data and the user. With maps, the user is watching him or her in the middle of the location-based data and, thus, needs to match the virtual representation to the real world. The first-person views are not subject to this kind of problem, since at least the direction in which the object lies is shown.</p><p>Every application that uses location-based data needs to determine the location of the user, or the environment needs to know and provide this information. This is needed, among other things, for determining what set of virtual objects is downloaded onto the terminal. However, not all of the models need similar accuracy for the location. The AR applications need to determine the user’s viewpoint very accurately, as they need to know how the real world is aligned to the user. On the other hand, applications that only list the virtual objects do not need to know the location very accurately.</p><p>There are several methods to determine the user’s location with various accuracies, both manually and automatically. Some very accurate locationing methods exist for indoor use, while the more general methods that are usable in a wider area tend to be less accurate. The global positioning system (GPS; see e.g., Getting <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Getting IA (1993) Perspective/navigation—the global positioning system. IEEE Spectr 30(12):36–38, 43–47" href="/article/10.1007/s10055-004-0139-8#ref-CR12" id="ref-link-section-d40635e469">1993</a>) is a widely used method to determine the user’s location. Its accuracy can be measured in meters, but there are several methods to improve it. Some other methods for locating a mobile user include infra red (IR)-based locationing (Want et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Want R, Hopper A, Falcão V, Gibbons J (1992) The active badge location system. ACM Transact Info Syst 10(1):91–102" href="/article/10.1007/s10055-004-0139-8#ref-CR34" id="ref-link-section-d40635e472">1992</a>) and methods using radio frequency (RF) (Bahl and Padmanabhan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Bahl P, Padmanabhan VN (2000) RADAR: an in-building RF-based user location and tracking system. In: Proceedings of the IEEE conference on computer communications (INFOCOM 2000), Tel-Aviv, Israel, March 2000, vol 2, pp 775–784" href="/article/10.1007/s10055-004-0139-8#ref-CR5" id="ref-link-section-d40635e475">2000</a>; Krumm et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Krumm J, Williams L, Smith G (2002) SmartMoveX on a graph—an inexpensive active badge tracker. In: Proceedings of the 4th international conference on ubiquitous computing (UbiComp 2002), Göteborg, Sweden, September/October 2002, pp 299–307" href="/article/10.1007/s10055-004-0139-8#ref-CR16" id="ref-link-section-d40635e478">2002</a>). A good survey of the locationing methods can be found in Hightower and Borriello (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hightower J, Borriello G (2001) Location systems for ubiquitous computing. Computer 34(8):57–66" href="/article/10.1007/s10055-004-0139-8#ref-CR14" id="ref-link-section-d40635e481">2001</a>).</p><p>In addition to location, many visualizations need to know, or can benefit from knowing, the user’s viewpoint as accurately as possible. The user’s viewpoint can be determined with various sensors or with digital camera techniques. There are many different sensors for getting the information, but with accuracy, the price increases as well. When it comes to the present study, we only consider the minimum sensor requirements in each of the different model and view combinations (additional sensors can obviously be used).</p><p>One of our aims was to develop a taxonomy for visualizing location-based data. For this purpose, we define an MV number for each model and view combination. Consequently, each combination can be referred to with its MV number, which is presented in the form MV(<i>m</i>, <i>v</i>). In this notation, <i>m</i> denotes the dimensions of the environment model, and <i>v</i> is the view perspective. As an example, the 3D environmental model with the first-person view can be referred to as MV(3, 1).</p><p>Obviously, with our MV notation, the number of environmental dimensions is not restricted in any way, nor is the viewpoint. However, not all of these combinations are applicable to systems targeting AR, where close resemblance to the real world is imperative. Therefore, in this paper, we restrict ourselves to discussing only the most obvious model and view combinations.</p><h3 class="c-article__sub-heading" id="Sec4">3D environment model: first person view; MV(3, 1)</h3><p>AR applications use a 3D environment model and the environment is viewed from the first-person view. AR gives its user a simultaneous access to both the real and virtual worlds, which are merged into a one seamless combination. AR applications provide a very intuitive UI as the virtual objects are overlaid into the real world at their actual positions. This approach does visualize a digital representation for the environment as the real world is used, but the application still has an accurate model of the environment and objects. A concept image of such a UI is seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A concept image of MV(3, 1), also known as AR. The real and virtual objects are merged into one combined view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Calculations with a 3D model of the environment require a lot of processing power and very accurate sensors are needed to determine the user’s exact viewpoint. The real world is the environment model in which the virtual objects are placed, so it needs to be known with great accuracy in order for the virtual objects to appear, and stay at, their actual locations. Generally, these are very high-end applications that require expensive hardware.</p><p>The “touring machine” (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Feiner S, MacIntyre B, Höllerer T, Webster A. (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. In: Proceedings of the 1st international symposium on wearable computers (ISWC’97), Cambridge, Massachusetts, October 1997, pp 74–81" href="/article/10.1007/s10055-004-0139-8#ref-CR10" id="ref-link-section-d40635e535">1997</a>) is a good example of an AR system. It integrates the real and virtual information in a 3D space for mobile users in large outdoor areas. Another outdoor augmentation was presented by Azuma et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Azuma R, Hoff B, Neely H III, Sarfaty R (1999) A motion-stabilized outdoor augmented reality system. In: Proceedings of IEEE virtual reality conference (VR’99), Houston, Texas, March 1999, pp 252–259" href="/article/10.1007/s10055-004-0139-8#ref-CR3" id="ref-link-section-d40635e538">1999</a>). They address the problems currently occurring in such systems, the main one being that the location is not known accurately enough. Augmenting is easier indoors, as the location can be determined with greater accuracy.</p><p>Another way to create an AR view is to use a digital camera to capture a scene and analyze the image to overlay the augmented data on the image. This requires complex image analysis and it is also a very demanding approach for the hardware. One such system is the AR-PDA (Geiger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Geiger C, Kleinnjohann B, Reimann C, Stichling D (2001) Mobile AR4ALL. In: Proceedings of the IEEE and ACM international symposium on augmented reality (ISAR 2001), Munich, Germany, October 2000, pp 181–182" href="/article/10.1007/s10055-004-0139-8#ref-CR11" id="ref-link-section-d40635e544">2001</a>). This system uses a hand-held device that has a camera attached to it. The system feeds video data produced by the camera to a network server that analyzes this data. The network server feeds the augmented data back to the user terminal. The terminal itself does not need very powerful computing capabilities—only a fast mobile network connection—but the processing power needs to be somewhere.</p><p>To conclude, there are two primary techniques to provide the user with an AR view. Hardware-wise, they are both very demanding tasks; however, in the present paper, we only concentrate on the sensor-based approach. In video capture, the equipment and methods remain the same, while with the sensor approach, we are able to downscale the hardware requirements as the environment dimensions are reduced.</p><p>The biggest problem in AR UIs is the registration (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47" href="/article/10.1007/s10055-004-0139-8#ref-CR4" id="ref-link-section-d40635e553">2001</a>). Registration means that the real and virtual data need to be well aligned with respect to each other. If the virtual objects do not behave as if they were real, the user will find this annoying. Another problem in the AR systems is information overload. If the screen is filled with augmented information, the user might get distracted from the real world.</p><p>Generally, visualizations with a 3D environment model are very hardware intensive. These applications do most of the work for the user, as the data is always placed in the right location in the augmented view. The UIs are intuitive for the user, as the virtual data is a part of the real world. Everyone knows how to interact with the real world, so adding a few virtual objects into it would be an ideal way to access location-based data.</p><p>Applications with a 3D environment model can be used for almost any purpose. Adding virtual data right on top of the real world can seamlessly enhance the life of a user.</p><h3 class="c-article__sub-heading" id="Sec5">3D environment model: third person view; MV(3, 3)</h3><p>The 3D world can be accessed from a third-person view also, which is a 3D map. Several such applications have been developed in the area of virtual reality (VR). One example system of tracking a mobile user who has a GPS unit can be found in Rakkolainen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Rakkolainen I, Pulkkinen S, Heinonen A (1998) Visualizing real-time GPS data with internet’s VRML worlds. In: Proceedings of the 6th international symposium on advances in geographic information systems (ACM-GIS’98), Washington, District of Columbia, November 1998, pp 52–56" href="/article/10.1007/s10055-004-0139-8#ref-CR26" id="ref-link-section-d40635e568">1998</a>). The location-based data is overlaid onto this map and the user sees his or her representation in this map from the third-person perspective. The view can be oriented according to the user’s orientation. A concept image of a 3D map is seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A concept image of MV(3, 3). The user sees him/herself (YAH) with a 3D model of the environment. Virtual objects are at their precise locations in the virtual world</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The third-person views are not as sensor-sensitive as the respective first-person views. The virtual world is not merged with the real world, as the two are separate. The simplest implementation only requires one sensor—the user’s location: no sensor for obtaining the user’s viewpoint is needed. However, by adding an orientation sensor, the user’s view direction can be used to create a forward-up view. The scene is 3D, which is computationally complex and a lot of processing power is needed. The world is recreated virtually and the virtual objects are always at their exact locations in the virtual world.</p><p>A 3D scene presents certain problems to the user. Parts of the scene and the virtual objects may be behind each other, preventing the user from seeing important information about the environment. The problem can be even bigger if a north-up-aligned map is used—the user might not see what is directly in front of him/her. This MV can be used instead of MV(3, 1) if the sensors are not very accurate, but the precise location of the location-based data is important. Still, a fast platform is needed, as building a 3D scene is a complex task.</p><h3 class="c-article__sub-heading" id="Sec6">2D environment model: first person view; MV(2, 1)</h3><p>The location-based data can be projected onto a 2D plane, thus, losing one dimension from the data. The most obvious dimension to lose is the altitude, as it is easy to assume that the users usually move on the surface of the earth. This plane can be projected to the user in different ways. The application can project the virtual objects on a plane in front of the user, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig3">3</a>. The user can see the virtual objects and their direction and distance from him or her, but they are not at their correct locations in the real world. These MV’s are best suited for tasks where the user needs to see ahead of him or her, and the essential information can be projected onto this surface.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A concept image of MV(2, 1). The user sees the virtual objects on a visible or non-visible surface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Another way to project the objects in MV(2, 1) is seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig4">4</a>. In this case, the viewpoint is brought down to the ground level (i.e., the user is viewing the world while lying down on the ground), resulting in a view where the objects appear on the same horizontal level. The objects that are further away from the user are scaled down proportionally.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Another concept image of MV(2, 1). The objects are shown according to the angle at which they are in relation to the user’s orientation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This MV can be implemented with fewer sensors compared to MV(3, 1), as it does not require 3D information of the user’s orientation. An orientation sensor is needed as, otherwise, the user would not be able to see all of the objects, as they would only see objects in front of them. If the horizon of the virtual view is aligned with the real-world horizon, more sensor information is needed.</p><p>MV(2, 1) has been applied to in-car navigation (Spoerri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Spoerri A (1993) Novel route guidance displays. In: Proceedings of the IEEE-IEE vehicle navigation and information systems conference, Ottawa, Ontario, October 1993, pp 419–422" href="/article/10.1007/s10055-004-0139-8#ref-CR28" id="ref-link-section-d40635e652">1993</a>). The system displays a driver-centered view that simulates what the driver sees through the windshield. A car is an ideal platform for this kind of visualization, as the road ahead of the user is always a 2D plane. The system performed well in the tests compared to traditional map views.</p><p>Another example of this kind of an environment model is found in the “map-in-the-hat” (Thomas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Thomas B, Demczuk V, Piekarski W, Hepworth D, Gunther B (1998) A wearable computer system with augmented reality to support terrestrial navigation. In: Proceedings of the 2nd international symposium on wearable computers (ISWC’98), Pittsburgh, Pennsylvania, October 1998, pp 168–171" href="/article/10.1007/s10055-004-0139-8#ref-CR33" id="ref-link-section-d40635e659">1998</a>). The system is designed for use in large outdoor areas and it uses GPS for positioning information. The main focus of the work is on navigation and guiding people from point A to point B. The system uses waypoints to help the user find the final destination. The next waypoint is shown on the user’s see-through head-worn display, increasing in size as the user approaches it.</p><p>We have implemented a navigation system for wearable computer users, called WalkMap, that has many visualizations for location-based data with different environment models (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Lehikoinen J, Suomela R (2002a) WalkMap: developing an augmented reality map application for wearable computers. Virtual Reality 6(1):33–44" href="/article/10.1007/s10055-004-0139-8#ref-CR18" id="ref-link-section-d40635e665">2002a</a>). One essential part of WalkMap is Context Compass, a 1D selection technique with a MV(2, 1) visualization method (Suomela and Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147–154" href="/article/10.1007/s10055-004-0139-8#ref-CR30" id="ref-link-section-d40635e668">2000</a>; Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002b" title="Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74" href="/article/10.1007/s10055-004-0139-8#ref-CR19" id="ref-link-section-d40635e671">2002b</a>). In the Context Compass, the UI presents the direction of objects relative to the user’s current head orientation. The objects are presented as icons, and scaled in size according to their distance from the user.</p><h3 class="c-article__sub-heading" id="Sec7">2D environment model: third person view; MV(2, 3)</h3><p>Two-dimensional representations of the environment are always approximations of the environment. A 2D representation viewed from the third-person perspective is a digital map. 2D maps in general are one of the oldest forms to represent the environment and digital maps are a natural succession with virtual objects and the user overlaid onto the environment representation. A concept image of such visualization without the underlying map is seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>MV(2, 3)—a concept image. The user, along with virtual objects, is visualized on the screen</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The application only needs to know the user’s location; other sensor information is not necessary. Orientation information can be used to make a map that is aligned forward-up continuously, although the benefits of automatic alignment are not unambiguous, as our previous research has shown (Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231" href="/article/10.1007/s10055-004-0139-8#ref-CR17" id="ref-link-section-d40635e705">2001</a>). These kinds of MV’s can be implemented with less accurate sensors compared to the previous views and less powerful hardware. These facts make them potentially available for a wider selection of devices and users, since the hardware costs are low.</p><p>The problems and benefits are again the same as in all maps. The user needs to match the location of the YAH symbol and his or her location with the real world. Increasing the map scale can compensate for an inaccurate location of the user, but if the user’s location is not known accurately, there is no point in showing a YAH.</p><p>Several research projects have studied digital maps and commercial applications are also available on the market. Cyberguide (Abowd et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Abowd GD, Atkeson CG, Hong J, Long S, Kooper R, Pinkerton M (1997) Cyberguide: a mobile context-aware tour guide. Wireless Netw 3(5):421–433" href="/article/10.1007/s10055-004-0139-8#ref-CR1" id="ref-link-section-d40635e713">1997</a>), a handheld tourist assistance system, is an application that assists tourists visiting the GVU Center Lab. It runs on a commercially available PDA and its goals are to provide the user with his or her location information and to assist in finding information.</p><p>Our WalkMap, discussed above, implements a few different MV’s (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Lehikoinen J, Suomela R (2002a) WalkMap: developing an augmented reality map application for wearable computers. Virtual Reality 6(1):33–44" href="/article/10.1007/s10055-004-0139-8#ref-CR18" id="ref-link-section-d40635e720">2002a</a>). There is, for example, an implementation of the standard map. Further, several commercial applications are already in use in this category. Benefon (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Benefon (2003) Home page at &#xA;                    http://www.benefon.com/products/index.htm&#xA;                    &#xA;                  . Cited 7 Oct 2003" href="/article/10.1007/s10055-004-0139-8#ref-CR6" id="ref-link-section-d40635e723">2003</a>) has a mobile phone that has a complete map service built in, while Nokia 9210 Communicator (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nokia GPS module (2003) Home page at &#xA;                    http://www.nokia.com&#xA;                    &#xA;                  . Cited 7 Oct 2003" href="/article/10.1007/s10055-004-0139-8#ref-CR23" id="ref-link-section-d40635e726">2003</a>) and Palm PDAs (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="GPS Pilot (2003) Home page at &#xA;                    http://www.gpspilot.com/Products.htm&#xA;                    &#xA;                  . Cited 7 Oct 2003" href="/article/10.1007/s10055-004-0139-8#ref-CR13" id="ref-link-section-d40635e729">2003</a>) can have a GPS module inserted to enable the map services.</p><p>These applications are very useful when the user needs to know the structure of the environment, such as when planning a route. A 2D map with a third-person view allows the user to see the digital environment clearly in every direction, while other models cannot always guarantee this. The first-person views only show information in front of the user and the real-world objects can block the view ahead. A 3D digital map can have 3D objects blocking the view, depending on the viewpoint. The user has a clear idea of the structure of the environment and he or she knows quite accurately where the location-based data is. This is one of the most popular UIs of the location-based application, as it is fairly easy to implement and people have been using maps for centuries.</p><h3 class="c-article__sub-heading" id="Sec8">1D environment model: first person view; MV(1, 1)</h3><p>In MV(1, 1) visualization, the objects are shown based on only one environment criterion. There are many possibilities for choosing this criterion, as this visualization is a very limited subset of the 3D space. When using only one dimension, instead of using Cartesian coordinates, we should calculate the distance from the user to the object and the angles at which the objects are in relation to the user’s viewpoint and direction. Any one of these values could be used as the criteria for the visualization. The distance allows us to sort the objects according to their distance, while the angle gives information on which direction the objects are in relation to the user’s view orientation. These MV’s do not present the location of the virtual data accurately, and the user does not know exactly where the data is.</p><p>One should note that MV(1, 1) is in very close relation to MV(2, 1). This can be seen clearly when looking back at Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig3">3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig4">4</a>. The case depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig3">3</a> would become MV(1, 1) should the objects be in the same arc, regardless of their distance. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig4">4</a>, on the contrary, the case would be MV(1, 1) if all the objects were of the same size.</p><p>There are other ways to create MV(1, <i>x</i>). The distance to objects could be used, but, obviously, this does not tell their actual location. In some situations, it could be useful; for example if virtual objects of known locations were ordered, the user would know which would be the closest. The direction, on the other hand, can be used to show the direction towards that object.</p><p>These MV’s are very useful for many applications. They can be implemented with no or only one sensor in addition to the location. The objects can be ordered according to their distance without any other sensors and the angle can be computed with the help of an orientation sensor. If a user wants to go to a destination point, MV(1, 1) is sufficient in many cases. Some people have trouble reading the maps, but this visualization only shows the direction to which the user should go, and this might be an easy guidance system. Further, the objects are not scaled down (as is the case with MV(2, 1)), making them easier to read.</p><h3 class="c-article__sub-heading" id="Sec9">1D environment model: third person view; MV(1, 3)</h3><p>There are, again, two possibilities to create this view; either to use the angle or the distance of the objects. If the distance is used, this MV is identical to MV(1, 1) presented above, as both create a list of objects. We only consider the situation where the objects are visualized according to their angle.</p><p>This model has even less hardware requirements than the previous model, as it does not require an orientation sensor. However, without an orientation sensor, a forward-up view cannot be made, which might make it more difficult to use. In this case, the user needs to know a reference point (which could be either a compass point or one of the visible objects) in order to align himself or herself with the objects. Thus, this model can benefit a lot from an orientation sensor.</p><p>In MV(1, 3), the user is presented as a symbol on screen and the data is spread around the user. The user again knows in which direction the objects are from him or her, but not their exact location. A concept image can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig6">6</a>. This visualization looks like a traditional hand-held compass, where, instead of the compass points, the directions to virtual objects are shown.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>MV(1, 3). The user is drawn at the <i>center</i> and either north-up or forward-up alignment can be used. (This is a concept image.)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This kind of an approach implements a compass that does not show the compass points but, rather, the direction to certain important virtual objects. We propose that MV(1, 3) can be used for all the same purposes as MV(1, 1).</p><h3 class="c-article__sub-heading" id="Sec10">No environment model: viewpoint not relevant; MV(0, 0)</h3><p>The simplest UIs for accessing the location-based data do not use any environment model at all. The virtual data can be presented as just a collection of data in the environment, for example, as a list. The user does not gain any specific information on the locations of the objects; the only information is that they are at the vicinity. There is no difference between viewing this information from the first- or third-person perspective, as the environment does not have any dimensions. However, this MV is enough for certain situations, for example, when the user has a good knowledge of the environment and just needs to find information or services close by. A concept image is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig7">7</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb7.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb7.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>A concept image of MV(0, 0). If the objects are sorted, e.g., based on distance, then this is MV(1, <i>x</i>), as environment data is used</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>No alignment problems occur in this model, as the user cannot match the data to the environment. If the user wishes to find the data from the real world, then the MV(0, 0) visualization cannot help at all and the task is the most demanding for the user of all the UIs described above.</p><p>There are already several commercial applications in this category. Yellow pages and friend-finders are typical applications in this category. A typical application is targeted at a mobile phone user, as it already has a huge user base; see, for example, Mobilaris (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Mobilaris (2003) Home page at &#xA;                    http://www.mobilaris.se&#xA;                    &#xA;                  . Cited 7 Oct 2003" href="/article/10.1007/s10055-004-0139-8#ref-CR22" id="ref-link-section-d40635e839">2003</a>).</p><h3 class="c-article__sub-heading" id="Sec11">Summary</h3><p>The different MV’s have different properties and are suitable for different situations. The hardware platform and the task at hand are the main features that affect the MV. If the hardware does not have accurate sensors, the third-person views might be more user-friendly. The user’s viewpoint in third-person views is not dependent on real-world sensor information. The third-person views are maps and they are subject to the common map-reading problem where the user has to match the relationships between real and virtual objects. The first-person views require less from the user when determining the actual location of the virtual objects. At least the direction in which the objects are is shown to the user. The user does not need to do the matching between the virtual and real worlds to locate the objects.</p><p>The environment model used affects how accurately the location-based data is presented in either the virtual or real worlds. A 3D-environment model shows the actual location of the virtual object. In some situations, however, it is not necessary to know the location very accurately, and a simpler environmental model may be considered.</p><p>Referring to Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0139-8#Tab1">1</a>, we can see that MV(<i>x</i>, 3) always requires less sensor information than MV(<i>x</i>, 1). This should be taken into account when designing applications for specific hardware. It can also be seen that MV(3 ,1) is clearly the most demanding combination when it comes to the sensor requirements and application complexity.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Comparison of MV’s. Sensor requirements indicate how much and how accurate the information is needed in addition to the location information of the user</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0139-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Application complexity is the sum of two factors. First, more sensors result in a more complex application, because more data processing is needed. Second, more environment dimensions imply more processing. A rough estimation of the application complexity can be obtained by summing the number of dimensions with the minimum number of sensors required.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Our field experiments</h2><div class="c-article-section__content" id="Sec12-content"><p>We have studied location-based data UIs by concentrating on helping a walking user navigate in the real-world environment (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002a" title="Lehikoinen J, Suomela R (2002a) WalkMap: developing an augmented reality map application for wearable computers. Virtual Reality 6(1):33–44" href="/article/10.1007/s10055-004-0139-8#ref-CR18" id="ref-link-section-d40635e1046">2002a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002b" title="Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74" href="/article/10.1007/s10055-004-0139-8#ref-CR19" id="ref-link-section-d40635e1049">2002b</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002c" title="Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearable computers (ISWC 2002), Seattle, Washington, October 2002, pp 171–178" href="/article/10.1007/s10055-004-0139-8#ref-CR20" id="ref-link-section-d40635e1052">2002c</a>; Suomela and Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147–154" href="/article/10.1007/s10055-004-0139-8#ref-CR30" id="ref-link-section-d40635e1055">2000</a>; Lehikoiken <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231" href="/article/10.1007/s10055-004-0139-8#ref-CR17" id="ref-link-section-d40635e1058">2001</a>; Suomela et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Suomela R, Lehikoinen J, Salminen I (2001) A system for evaluating augmented reality user interfaces in wearable computers. In: Proceedings of the 5th international symposium on wearable computers (ISWC 2001), Zurich, Switzerland, October 2001, pp 77–84" href="/article/10.1007/s10055-004-0139-8#ref-CR31" id="ref-link-section-d40635e1062">2001</a>). The user experiments were made with a wearable computer and several different types of UIs were tested. The test software includes all environment models and both the first- and third-person views. Our wearable computer was not capable of a proper MV(3, 1) visualization, mainly due to the lack of fast enough sensors. We implemented a few test programs that tested the suitability of the hardware for AR, but it was deemed unusable. Based on other research, 3D AR UIs are generally very intuitive to use and the problems are hardware related (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47" href="/article/10.1007/s10055-004-0139-8#ref-CR4" id="ref-link-section-d40635e1065">2001</a>).</p><p>Our tests revealed a lot of interesting data on how people use, and want to use, these UIs. A walking user does not constantly need access to the location-based information when navigating the environment. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0139-8#Tab2">2</a> lists all of the different UI’s that we have implemented or evaluated. The MV(0, 0) visualization is not discussed in detail, as it is a simple list of data.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 The different applications that we have implemented or evaluated</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0139-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">The hardware</h3><p>All of the application UIs in this paper have been designed for a wearable computer. The wearable computer consists of a Plug’n’Run Mobile Pentium II processor and PCMCIA CardPC board by Cell Computing (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="PFU Systems (2003) Cell computing, online document, available at &#xA;                    http://www.pfusystems.com/&#xA;                    &#xA;                  . Cited 7 Oct 2003" href="/article/10.1007/s10055-004-0139-8#ref-CR25" id="ref-link-section-d40635e1193">2003</a>). The operating system is Windows 98, the computer has 128 MB of RAM and a 340 MB IBM MicroDrive hard disk drive. It has two serial ports, one parallel port, and two USB ports to connect the external devices. There are two PCMCIA slots. A Sony Glasstron PLM-S700E is used as the display. The compass is a Leica DMC-SXZ.</p><h3 class="c-article__sub-heading" id="Sec14">Case 1: MV(3, 3)—3D maps</h3><p>A 3D map system was implemented (Suomela et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Suomela R, Roimela K, Lehikoinen J (2003) The evolution of perspective view in WalkMap. Personal Ubiquitous Comput 7(5):249–262" href="/article/10.1007/s10055-004-0139-8#ref-CR32" id="ref-link-section-d40635e1204">2003</a>), but usability tests with the system have not advanced past the pilot tests. A screenshot can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig8">8</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>MV(3, 3). The user is presented in it with an <i>arrow</i>, denoting his or her location. The UI has a forward-up view, where the <i>arrow</i> is pointing towards a waypoint</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The pilot tests suggest that the real world is slightly easier to align with a 3D virtual world than a plain 2D map. The system needs to be thoroughly evaluated in order to verify any results. The screen capture shows some of the problems associated with such a view. The rising buildings block information behind them and the user has to guess whether a road exists behind an object. There are ways to overcome these problems with transparency and other means that alter the scene (Suomela et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Suomela R, Roimela K, Lehikoinen J (2003) The evolution of perspective view in WalkMap. Personal Ubiquitous Comput 7(5):249–262" href="/article/10.1007/s10055-004-0139-8#ref-CR32" id="ref-link-section-d40635e1237">2003</a>).</p><h3 class="c-article__sub-heading" id="Sec15">Case 2: MV(2, 3)—2D maps</h3><p>Two implementations of a traditional 2D digital map are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig9">9</a>. Both north-up and forward-up views were implemented. The forward-up view was made both automatic and user-initiated, which was the subject of the tests made with the system (Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231" href="/article/10.1007/s10055-004-0139-8#ref-CR17" id="ref-link-section-d40635e1252">2001</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>MV(2, 3). The user is presented with an <i>arrow</i> at the <i>center</i> of the image, virtual data is scattered on <i>top</i> of the map</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Previous studies have shown that a map is easier to use if it is aligned forward-up (see e.g., Levine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Levine M, Jankovic IN, Palij M (1982) Principles of spatial problem solving. J Exp Psychol Gen 111(2):157–175" href="/article/10.1007/s10055-004-0139-8#ref-CR21" id="ref-link-section-d40635e1285">1982</a>; Warren et al <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Warren DH, Rossano MJ, Wear TD (1990) Perception of map–environment correspondence: the roles of features and alignment. Ecol Psychol 2(2):131–150" href="/article/10.1007/s10055-004-0139-8#ref-CR36" id="ref-link-section-d40635e1288">1990</a>; Warren and Scott <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Warren DH, Scott TE (1993) Map alignment in traveling multisegment routes. Environ Behav 25(5):643–666" href="/article/10.1007/s10055-004-0139-8#ref-CR35" id="ref-link-section-d40635e1291">1993</a>). In our field experiments (Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231" href="/article/10.1007/s10055-004-0139-8#ref-CR17" id="ref-link-section-d40635e1294">2001</a>), we wanted to compare automatic and manual alignment methods. We had ten subjects in the experiment. Their task was to navigate to 20 real-world locations, one location at a time. Each subject navigated to ten targets with automatic alignment, and ten with manual alignment; half of the users had the automatic tasks first, whereas the other half performed the manual tasks first. The route was about 2 km long, within an area of about 1 km<sup>2</sup>. The target locations were overlaid on a map shown on a see-through head-worn display. The results showed that there are no significant differences between the two methods in performance or in subjective preferences (the user satisfaction was measured quantitatively by detecting performance and errors, and qualitatively by semi-structured final interviews.). Therefore, we concluded that the map application should provide the user with both options.</p><h3 class="c-article__sub-heading" id="Sec16">Case 3: MV(2, 1)—Context Compass</h3><p>Context Compass is a MV(2, 1) visualization that presents the location-based data based on its orientation to the user (Suomela and Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147–154" href="/article/10.1007/s10055-004-0139-8#ref-CR30" id="ref-link-section-d40635e1307">2000</a>). The UI can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig10">10</a>. One should note that Context Compass is a 2D visualization that uses a 1D selection technique.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>MV(2, 1)—Context Compass. The image is constructed by overlaying the screenshot of Context Compass on <i>top</i> of a photo</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We evaluated the Context Compass UI using a quantitative lab experiment with six users (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002b" title="Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74" href="/article/10.1007/s10055-004-0139-8#ref-CR19" id="ref-link-section-d40635e1337">2002b</a>). The users were standing on a marked spot while trying to locate virtual objects with and without respective real-world counterparts (paper sheets attached to the walls). We measured the performance, as well as asking subjective opinions after the tasks were completed. The results revealed that the method is better suited for pointing at virtual objects that have real-world counterparts.</p><p>The UI is very simple and only serves one purpose. Such specialized UIs are suitable for some specific tasks and this UI could be simpler than maps for people with map-reading problems.</p><p>We have also implemented an MV(1, 1) version of Context Compass by removing the object scaling feature.</p><h3 class="c-article__sub-heading" id="Sec17">Case 4: MV(2, 3) and MV(2, 1)</h3><p>Our visualization is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig11">11</a>, which contains two parts: the perspective map (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002c" title="Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearable computers (ISWC 2002), Seattle, Washington, October 2002, pp 171–178" href="/article/10.1007/s10055-004-0139-8#ref-CR20" id="ref-link-section-d40635e1355">2002c</a>) and the Context Compass (Suomela and Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147–154" href="/article/10.1007/s10055-004-0139-8#ref-CR30" id="ref-link-section-d40635e1358">2000</a>; Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002b" title="Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74" href="/article/10.1007/s10055-004-0139-8#ref-CR19" id="ref-link-section-d40635e1361">2002b</a>). The perspective map was first implemented as MV(2, 1), similar to the concept in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig3">3</a>, but we chose to change it slightly. The target of the system is a walking user and we assumed that it would be useful to see a little of what is behind the user as well. The user’s location on the map is slightly changed and the user’s YAH is located quite close to the bottom but not right at the edge, making this view MV(2, 3). This shows how easy it is to move between the views; ultimately, it may be up to the user to decide what he or she prefers.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>The screenshot contains both the perspective map MV(2, 3) and the Context Compass MV(2, 1)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Another aspect seen in the figure is the presence of two MVs. As both of the MVs consume screen real estate sparingly, it is possible to show them simultaneously, thus, providing the user with more options in choosing between the preferred method.</p><p>The field evaluation of the perspective map (Lehikoinen and Suomela <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002c" title="Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearable computers (ISWC 2002), Seattle, Washington, October 2002, pp 171–178" href="/article/10.1007/s10055-004-0139-8#ref-CR20" id="ref-link-section-d40635e1390">2002c</a>) was conducted similarly to the MV(2, 3) map evaluations described above (Lehikoinen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231" href="/article/10.1007/s10055-004-0139-8#ref-CR17" id="ref-link-section-d40635e1393">2001</a>). The purpose of the experiment was to find out whether this viewpoint is better for a walking user than a regular bird’s eye view. The results revealed that this kind of visualization needs a lot of tweaking. Especially for targeted search tasks (which was the case in the experiment), the viewpoint was chosen to be too far behind the walking user. The user can see very far ahead, but this is not the highest priority for a walking user who tries to locate an object nearby. For exploration tasks, however, this viewpoint seems to fit better.</p><h3 class="c-article__sub-heading" id="Sec18">Case 5: MV(1, 3)</h3><p>This MV is a modification to the Context Compass that would be usable in handheld devices. The Context Compass is modified to present the information in a circle without scaling the objects. This mode has been implemented, but not evaluated. A screenshot is seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0139-8#Fig12">12</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0139-8/MediaObjects/s10055-004-0139-8fhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>MV(1, 3)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0139-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The objects are presented forward-up; that is, the objects that are in front of the user are presented on top of the compass circle. The user is presented as an arrow that points forward, denoting the view orientation. This visualization was not tested on our wearable computer, as we feel that it is more usable in a handheld device.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Discussion</h2><div class="c-article-section__content" id="Sec19-content"><p>The different UIs analyzed in this paper are suitable for different location-based applications. Things to consider when designing a location-based application visualization include the task, the available hardware, and the target user group. Some of the MVs are in wide use, as proved by commercial applications and devices already on the market.</p><p>All of the different environment models have suitable applications. The MV(3, <i>x</i>) models have a perfect reproduction of the real world, thus, allowing very precise location-based data to be shown at its perfect location. These views, however, are subject to having the 3D objects blocking other objects behind them. The MV(2,,<i>x</i>) models can overcome this problem by omitting one dimension (e.g., height) from the model. All objects are visible and no object can be behind others. They lose the exact locationing of virtual objects, however. The MV(1, <i>x</i>) models are simple and easy to implement but they lose out on the exact location of virtual objects. They cannot show the exact location of the virtual objects, but only e.g., the direction to them. They are still useful for wayfinding, as they can point in the direction to the next waypoint.</p><p>The different nature of the first-person and third-person views make them suitable for different tasks. The first-person views, MV(<i>x</i>, 1), can help the user in wayfinding and provide additional information on objects. It is easy to show where the next waypoint is or the direction to it, and all visible real-world objects can be digitally augmented with additional information. The third-person views on the other hand can show the user a much wider area in all the directions around the user, as they are not restricted to the user’s current viewpoint and orientation.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusions</h2><div class="c-article-section__content" id="Sec20-content"><p>In this paper, we have proposed a taxonomy for location-based information visualization. The taxonomy is 2D; the complexity of the environment model is considered as well as the user viewpoint. The complexity of the environment refers to how many dimensions were used in the visualization. The user viewpoint means how the system is viewed; either from the first- or third-person perspective.</p><p>The taxonomy is referred to with its MV number. This number is presented in the form MV(<i>m</i>, <i>v</i>), where <i>m</i> denotes the dimensions of the environment model, and <i>v</i> denotes the view perspective. The following cases, the closest equivalents to the real world, have been presented in detail:</p><ul class="u-list-style-dash">
                  <li>
                    <p>3D environment model, viewed from the first-person perspective MV(3, 1)</p>
                  </li>
                  <li>
                    <p>3D environment model, viewed from the third-person perspective MV(3, 3)</p>
                  </li>
                  <li>
                    <p>2D environment model, viewed from the first-person perspective MV(2, 1)</p>
                  </li>
                  <li>
                    <p>2D environment model, viewed from the third-person perspective MV(2, 3)</p>
                  </li>
                  <li>
                    <p>1D environment model, viewed from the first-person perspective MV(1, 1)</p>
                  </li>
                  <li>
                    <p>1D environment model, viewed from the third-person perspective MV(1, 3)</p>
                  </li>
                  <li>
                    <p>No environment model, no viewpoint MV(0, 0)</p>
                  </li>
                </ul><p>The different visualizations have different hardware requirements and, thus, are suitable for different devices. The more complex environment models require more precise information on how the user sees the environment and, thus, more accurate sensors are needed. The task at hand determines what kind of visualization should be constructed.</p><p>In practice, the MV number defines the UI and visualization design space available for the application developers, providing them with a tool for defining the scope and complexity of their applications and systems.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GD. Abowd, CG. Atkeson, J. Hong, S. Long, R. Kooper, M. Pinkerton, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Abowd GD, Atkeson CG, Hong J, Long S, Kooper R, Pinkerton M (1997) Cyberguide: a mobile context-aware tour gui" /><p class="c-article-references__text" id="ref-CR1">Abowd GD, Atkeson CG, Hong J, Long S, Kooper R, Pinkerton M (1997) Cyberguide: a mobile context-aware tour guide. Wireless Netw 3(5):421–433</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1019194325861" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cyberguide%3A%20a%20mobile%20context-aware%20tour%20guide&amp;journal=Wireless%20Netw&amp;volume=3&amp;issue=5&amp;pages=421-433&amp;publication_year=1997&amp;author=Abowd%2CGD&amp;author=Atkeson%2CCG&amp;author=Hong%2CJ&amp;author=Long%2CS&amp;author=Kooper%2CR&amp;author=Pinkerton%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Aretz, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Aretz AJ (1991) The design of electronic map displays. Hum Factors 33(1):85–101" /><p class="c-article-references__text" id="ref-CR2">Aretz AJ (1991) The design of electronic map displays. Hum Factors 33(1):85–101</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20design%20of%20electronic%20map%20displays&amp;journal=Hum%20Factors&amp;volume=33&amp;issue=1&amp;pages=85-101&amp;publication_year=1991&amp;author=Aretz%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Azuma R, Hoff B, Neely H III, Sarfaty R (1999) A motion-stabilized outdoor augmented reality system. In: Proce" /><p class="c-article-references__text" id="ref-CR3">Azuma R, Hoff B, Neely H III, Sarfaty R (1999) A motion-stabilized outdoor augmented reality system. In: Proceedings of IEEE virtual reality conference (VR’99), Houston, Texas, March 1999, pp 252–259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyre, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. " /><p class="c-article-references__text" id="ref-CR4">Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=6&amp;pages=34-47&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY&amp;author=Behringer%2CR&amp;author=Feiner%2CS&amp;author=Julier%2CS&amp;author=MacIntyre%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bahl P, Padmanabhan VN (2000) RADAR: an in-building RF-based user location and tracking system. In: Proceeding" /><p class="c-article-references__text" id="ref-CR5">Bahl P, Padmanabhan VN (2000) RADAR: an in-building RF-based user location and tracking system. In: Proceedings of the IEEE conference on computer communications (INFOCOM 2000), Tel-Aviv, Israel, March 2000, vol 2, pp 775–784</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benefon (2003) Home page at http://www.benefon.com/products/index.htm. Cited 7 Oct 2003" /><p class="c-article-references__text" id="ref-CR6">Benefon (2003) Home page at <a href="http://www.benefon.com/products/index.htm">http://www.benefon.com/products/index.htm</a>. Cited 7 Oct 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SK. Card, JD. Mackinlay, B. Shneiderman, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Card SK, Mackinlay JD, Shneiderman B (1999) Readings in information visualization: using vision to think. Morg" /><p class="c-article-references__text" id="ref-CR7">Card SK, Mackinlay JD, Shneiderman B (1999) Readings in information visualization: using vision to think. Morgan Kaufmann, San Francisco, California, p 6 </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Readings%20in%20information%20visualization%3A%20using%20vision%20to%20think&amp;publication_year=1999&amp;author=Card%2CSK&amp;author=Mackinlay%2CJD&amp;author=Shneiderman%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Tauber MJ (ed) Pr" /><p class="c-article-references__text" id="ref-CR8">Darken RP, Sibert JL (1996) Wayfinding strategies and behaviors in large virtual worlds. In: Tauber MJ (ed) Proceedings of the conference on human factors in computing systems (CHI’96), Vancouver, Canada, April 1996, pp 142–149</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Darken RP, Cevik H (1999) Map usage in virtual environments: orientation issues. In: Proceedings of the IEEE v" /><p class="c-article-references__text" id="ref-CR9">Darken RP, Cevik H (1999) Map usage in virtual environments: orientation issues. In: Proceedings of the IEEE virtual reality conference (VR’99), Houston, Texas, March 1999, pp 133–140</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feiner S, MacIntyre B, Höllerer T, Webster A. (1997) A touring machine: prototyping 3D mobile augmented realit" /><p class="c-article-references__text" id="ref-CR10">Feiner S, MacIntyre B, Höllerer T, Webster A. (1997) A touring machine: prototyping 3D mobile augmented reality systems for exploring the urban environment. In: Proceedings of the 1st international symposium on wearable computers (ISWC’97), Cambridge, Massachusetts, October 1997, pp 74–81</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geiger C, Kleinnjohann B, Reimann C, Stichling D (2001) Mobile AR4ALL. In: Proceedings of the IEEE and ACM int" /><p class="c-article-references__text" id="ref-CR11">Geiger C, Kleinnjohann B, Reimann C, Stichling D (2001) Mobile AR4ALL. In: Proceedings of the IEEE and ACM international symposium on augmented reality (ISAR 2001), Munich, Germany, October 2000, pp 181–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IA. Getting, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Getting IA (1993) Perspective/navigation—the global positioning system. IEEE Spectr 30(12):36–38, 43–47" /><p class="c-article-references__text" id="ref-CR12">Getting IA (1993) Perspective/navigation—the global positioning system. IEEE Spectr 30(12):36–38, 43–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F6.272176" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perspective%2Fnavigation%E2%80%94the%20global%20positioning%20system&amp;journal=IEEE%20Spectr&amp;volume=30&amp;issue=12&amp;pages=36-38&amp;publication_year=1993&amp;author=Getting%2CIA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="GPS Pilot (2003) Home page at http://www.gpspilot.com/Products.htm. Cited 7 Oct 2003" /><p class="c-article-references__text" id="ref-CR13">GPS Pilot (2003) Home page at <a href="http://www.gpspilot.com/Products.htm">http://www.gpspilot.com/Products.htm</a>. Cited 7 Oct 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Hightower, G. Borriello, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hightower J, Borriello G (2001) Location systems for ubiquitous computing. Computer 34(8):57–66" /><p class="c-article-references__text" id="ref-CR14">Hightower J, Borriello G (2001) Location systems for ubiquitous computing. Computer 34(8):57–66</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F2.940014" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Location%20systems%20for%20ubiquitous%20computing&amp;journal=Computer&amp;volume=34&amp;issue=8&amp;pages=57-66&amp;publication_year=2001&amp;author=Hightower%2CJ&amp;author=Borriello%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Julier, Y. Baillot, D. Brown, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Julier S, Baillot Y, Brown D (2002) Information filtering for mobile augmented reality. IEEE Comput Graph Appl" /><p class="c-article-references__text" id="ref-CR15">Julier S, Baillot Y, Brown D (2002) Information filtering for mobile augmented reality. IEEE Comput Graph Appl 22(5):12–15 </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2002.1028721" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20filtering%20for%20mobile%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=22&amp;issue=5&amp;pages=12-15&amp;publication_year=2002&amp;author=Julier%2CS&amp;author=Baillot%2CY&amp;author=Brown%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krumm J, Williams L, Smith G (2002) SmartMoveX on a graph—an inexpensive active badge tracker. In: Proceedings" /><p class="c-article-references__text" id="ref-CR16">Krumm J, Williams L, Smith G (2002) SmartMoveX on a graph—an inexpensive active badge tracker. In: Proceedings of the 4th international conference on ubiquitous computing (UbiComp 2002), Göteborg, Sweden, September/October 2002, pp 299–307</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceeding" /><p class="c-article-references__text" id="ref-CR17">Lehikoinen J (2001) An evaluation of augmented reality navigational maps in head-worn displays. In: Proceedings of the 8th conference on human–computer interaction (INTERACT 2001), Tokyo, Japan, July 2001, pp 224–231</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Lehikoinen, R. Suomela, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lehikoinen J, Suomela R (2002a) WalkMap: developing an augmented reality map application for wearable computer" /><p class="c-article-references__text" id="ref-CR18">Lehikoinen J, Suomela R (2002a) WalkMap: developing an augmented reality map application for wearable computers. Virtual Reality 6(1):33–44</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=WalkMap%3A%20developing%20an%20augmented%20reality%20map%20application%20for%20wearable%20computers&amp;journal=Virtual%20Reality&amp;volume=6&amp;issue=1&amp;pages=33-44&amp;publication_year=2002&amp;author=Lehikoinen%2CJ&amp;author=Suomela%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Lehikoinen, R. Suomela, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74" /><p class="c-article-references__text" id="ref-CR19">Lehikoinen J, Suomela R (2002b) Accessing context in wearable computers. Personal Ubiquitous Comput 6(1):64–74</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs007790200006" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Accessing%20context%20in%20wearable%20computers&amp;journal=Personal%20Ubiquitous%20Comput&amp;volume=6&amp;issue=1&amp;pages=64-74&amp;publication_year=2002&amp;author=Lehikoinen%2CJ&amp;author=Suomela%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearabl" /><p class="c-article-references__text" id="ref-CR20">Lehikoinen J, Suomela R (2002c) Perspective map. In: Proceedings of the 6th international symposium on wearable computers (ISWC 2002), Seattle, Washington, October 2002, pp 171–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Levine, IN. Jankovic, M. Palij, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Levine M, Jankovic IN, Palij M (1982) Principles of spatial problem solving. J Exp Psychol Gen 111(2):157–175" /><p class="c-article-references__text" id="ref-CR21">Levine M, Jankovic IN, Palij M (1982) Principles of spatial problem solving. J Exp Psychol Gen 111(2):157–175</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F0096-3445.111.2.157" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Principles%20of%20spatial%20problem%20solving&amp;journal=J%20Exp%20Psychol%20Gen&amp;volume=111&amp;issue=2&amp;pages=157-175&amp;publication_year=1982&amp;author=Levine%2CM&amp;author=Jankovic%2CIN&amp;author=Palij%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mobilaris (2003) Home page at http://www.mobilaris.se. Cited 7 Oct 2003" /><p class="c-article-references__text" id="ref-CR22">Mobilaris (2003) Home page at <a href="http://www.mobilaris.se">http://www.mobilaris.se</a>. Cited 7 Oct 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nokia GPS module (2003) Home page at http://www.nokia.com. Cited 7 Oct 2003" /><p class="c-article-references__text" id="ref-CR23">Nokia GPS module (2003) Home page at <a href="http://www.nokia.com">http://www.nokia.com</a>. Cited 7 Oct 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Ottosson, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Ottosson T (1988) What does it take to read a map? Sci J Orienteering 4:97–106" /><p class="c-article-references__text" id="ref-CR24">Ottosson T (1988) What does it take to read a map? Sci J Orienteering 4:97–106</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%20does%20it%20take%20to%20read%20a%20map%3F&amp;journal=Sci%20J%20Orienteering&amp;volume=4&amp;pages=97-106&amp;publication_year=1988&amp;author=Ottosson%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="PFU Systems (2003) Cell computing, online document, available at http://www.pfusystems.com/. Cited 7 Oct 2003" /><p class="c-article-references__text" id="ref-CR25">PFU Systems (2003) Cell computing, online document, available at <a href="http://www.pfusystems.com/">http://www.pfusystems.com/</a>. Cited 7 Oct 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rakkolainen I, Pulkkinen S, Heinonen A (1998) Visualizing real-time GPS data with internet’s VRML worlds. In: " /><p class="c-article-references__text" id="ref-CR26">Rakkolainen I, Pulkkinen S, Heinonen A (1998) Visualizing real-time GPS data with internet’s VRML worlds. In: Proceedings of the 6th international symposium on advances in geographic information systems (ACM-GIS’98), Washington, District of Columbia, November 1998, pp 52–56</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Sawhney, C. Schmandt, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sawhney N, Schmandt C (2000) Nomadic radio: speech and audio interaction for contextual messaging in nomadic e" /><p class="c-article-references__text" id="ref-CR27">Sawhney N, Schmandt C (2000) Nomadic radio: speech and audio interaction for contextual messaging in nomadic environments. ACM Transact Comput Hum Interact 7(3):353–383</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F355324.355327" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nomadic%20radio%3A%20speech%20and%20audio%20interaction%20for%20contextual%20messaging%20in%20nomadic%20environments&amp;journal=ACM%20Transact%20Comput%20Hum%20Interact&amp;volume=7&amp;issue=3&amp;pages=353-383&amp;publication_year=2000&amp;author=Sawhney%2CN&amp;author=Schmandt%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spoerri A (1993) Novel route guidance displays. In: Proceedings of the IEEE-IEE vehicle navigation and informa" /><p class="c-article-references__text" id="ref-CR28">Spoerri A (1993) Novel route guidance displays. In: Proceedings of the IEEE-IEE vehicle navigation and information systems conference, Ottawa, Ontario, October 1993, pp 419–422</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LA. Streeter, D. Vitello, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Streeter LA, Vitello D (1986) A profile of drivers’ map-reading abilities. Hum Factors 28(2):223–239" /><p class="c-article-references__text" id="ref-CR29">Streeter LA, Vitello D (1986) A profile of drivers’ map-reading abilities. Hum Factors 28(2):223–239</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20profile%20of%20drivers%E2%80%99%20map-reading%20abilities&amp;journal=Hum%20Factors&amp;volume=28&amp;issue=2&amp;pages=223-239&amp;publication_year=1986&amp;author=Streeter%2CLA&amp;author=Vitello%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable" /><p class="c-article-references__text" id="ref-CR30">Suomela R, Lehikoinen J (2000) Context Compass. In: Proceedings of the 4th international symposium on wearable computers (ISWC 2000), Atlanta, Georgia, October 2000, pp 147–154</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Suomela R, Lehikoinen J, Salminen I (2001) A system for evaluating augmented reality user interfaces in wearab" /><p class="c-article-references__text" id="ref-CR31">Suomela R, Lehikoinen J, Salminen I (2001) A system for evaluating augmented reality user interfaces in wearable computers. In: Proceedings of the 5th international symposium on wearable computers (ISWC 2001), Zurich, Switzerland, October 2001, pp 77–84</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Suomela, K. Roimela, J. Lehikoinen, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Suomela R, Roimela K, Lehikoinen J (2003) The evolution of perspective view in WalkMap. Personal Ubiquitous Co" /><p class="c-article-references__text" id="ref-CR32">Suomela R, Roimela K, Lehikoinen J (2003) The evolution of perspective view in WalkMap. Personal Ubiquitous Comput 7(5):249–262</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00779-003-0244-9" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20evolution%20of%20perspective%20view%20in%20WalkMap&amp;journal=Personal%20Ubiquitous%20Comput&amp;volume=7&amp;issue=5&amp;pages=249-262&amp;publication_year=2003&amp;author=Suomela%2CR&amp;author=Roimela%2CK&amp;author=Lehikoinen%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thomas B, Demczuk V, Piekarski W, Hepworth D, Gunther B (1998) A wearable computer system with augmented reali" /><p class="c-article-references__text" id="ref-CR33">Thomas B, Demczuk V, Piekarski W, Hepworth D, Gunther B (1998) A wearable computer system with augmented reality to support terrestrial navigation. In: Proceedings of the 2nd international symposium on wearable computers (ISWC’98), Pittsburgh, Pennsylvania, October 1998, pp 168–171</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Want, A. Hopper, V. Falcão, J. Gibbons, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Want R, Hopper A, Falcão V, Gibbons J (1992) The active badge location system. ACM Transact Info Syst 10(1):91" /><p class="c-article-references__text" id="ref-CR34">Want R, Hopper A, Falcão V, Gibbons J (1992) The active badge location system. ACM Transact Info Syst 10(1):91–102</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F128756.128759" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20active%20badge%20location%20system&amp;journal=ACM%20Transact%20Info%20Syst&amp;volume=10&amp;issue=1&amp;pages=91-102&amp;publication_year=1992&amp;author=Want%2CR&amp;author=Hopper%2CA&amp;author=Falc%C3%A3o%2CV&amp;author=Gibbons%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DH. Warren, TE. Scott, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Warren DH, Scott TE (1993) Map alignment in traveling multisegment routes. Environ Behav 25(5):643–666" /><p class="c-article-references__text" id="ref-CR35">Warren DH, Scott TE (1993) Map alignment in traveling multisegment routes. Environ Behav 25(5):643–666</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Map%20alignment%20in%20traveling%20multisegment%20routes&amp;journal=Environ%20Behav&amp;volume=25&amp;issue=5&amp;pages=643-666&amp;publication_year=1993&amp;author=Warren%2CDH&amp;author=Scott%2CTE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DH. Warren, MJ. Rossano, TD. Wear, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Warren DH, Rossano MJ, Wear TD (1990) Perception of map–environment correspondence: the roles of features and " /><p class="c-article-references__text" id="ref-CR36">Warren DH, Rossano MJ, Wear TD (1990) Perception of map–environment correspondence: the roles of features and alignment. Ecol Psychol 2(2):131–150</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20of%20map-environment%20correspondence%3A%20the%20roles%20of%20features%20and%20alignment&amp;journal=Ecol%20Psychol&amp;volume=2&amp;issue=2&amp;pages=131-150&amp;publication_year=1990&amp;author=Warren%2CDH&amp;author=Rossano%2CMJ&amp;author=Wear%2CTD">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0139-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Nokia Research Center, P.O. Box 100, Tampere, 33721, Finland</p><p class="c-article-author-affiliation__authors-list">Riku Suomela &amp; Juha Lehikoinen</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Riku-Suomela"><span class="c-article-authors-search__title u-h3 js-search-name">Riku Suomela</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Riku+Suomela&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Riku+Suomela" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Riku+Suomela%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Juha-Lehikoinen"><span class="c-article-authors-search__title u-h3 js-search-name">Juha Lehikoinen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Juha+Lehikoinen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Juha+Lehikoinen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Juha+Lehikoinen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0139-8/email/correspondent/c1/new">Riku Suomela</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Taxonomy%20for%20visualizing%20location-based%20information&amp;author=Riku%20Suomela%20et%20al&amp;contentID=10.1007%2Fs10055-004-0139-8&amp;publication=1359-4338&amp;publicationDate=2004-09-18&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Suomela, R., Lehikoinen, J. Taxonomy for visualizing location-based information.
                    <i>Virtual Reality</i> <b>8, </b>71–82 (2004). https://doi.org/10.1007/s10055-004-0139-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0139-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2002-11-08">08 November 2002</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-08-10">10 August 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-09-18">18 September 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0139-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0139-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Location-based data</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual objects</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visualization</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Taxonomy</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0139-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=139;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

