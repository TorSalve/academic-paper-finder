<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Human perception of a conversational virtual human: an empirical study"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for different contexts and usually for users..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture"/>

    <meta name="dc.source" content="Virtual Reality 2013 17:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-08-30"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for different contexts and usually for users from different countries. The emphasis on a virtual human&#8217;s emotional expression depends on the application; some virtual reality applications need an emotional expression of the virtual human during the speaking phase, some during the listening phase and some during both speaking and listening phases. Although studies have investigated how humans perceive a virtual human&#8217;s emotion during each phase separately, few studies carried out a parallel comparison between the two phases. This study aims to fill this gap, and on top of that, includes an investigation of the cultural interpretation of the virtual human&#8217;s emotion, especially with respect to the emotion&#8217;s valence. The experiment was conducted with both Chinese and non-Chinese participants. These participants were asked to rate the valence of seven different emotional expressions (ranging from negative to neutral to positive during speaking and listening) of a Chinese virtual lady. The results showed that there was a high correlation in valence rating between both groups of participants, which indicated that the valence of the emotional expressions was as easily recognized by people from a different cultural background as the virtual human. In addition, participants tended to perceive the virtual human&#8217;s expressed valence as more intense in the speaking phase than in the listening phase. The additional vocal emotional expression in the speaking phase is put forward as a likely cause for this phenomenon."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-08-30"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="307"/>

    <meta name="prism.endingPage" content="321"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0231-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0231-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0231-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0231-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2013/11"/>

    <meta name="citation_online_date" content="2013/08/30"/>

    <meta name="citation_firstpage" content="307"/>

    <meta name="citation_lastpage" content="321"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0231-z"/>

    <meta name="DOI" content="10.1007/s10055-013-0231-z"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0231-z"/>

    <meta name="description" content="Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for di"/>

    <meta name="dc.creator" content="Chao Qu"/>

    <meta name="dc.creator" content="Willem-Paul Brinkman"/>

    <meta name="dc.creator" content="Yun Ling"/>

    <meta name="dc.creator" content="Pascal Wiggers"/>

    <meta name="dc.creator" content="Ingrid Heynderickx"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction; citation_author=JN Bailenson, N Yee, D Merget, R Schroeder; citation_volume=15; citation_issue=4; citation_publication_date=2006; citation_pages=359-372; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Soc Psychol Rev Off J Soc Pers Soc Psychol; citation_title=Solving the emotion paradox: categorization and the experience of emotion; citation_author=LF Barrett; citation_volume=10; citation_publication_date=2006; citation_pages=20-46; citation_doi=10.1207/s15327957pspr1001_2; citation_id=CR2"/>

    <meta name="citation_reference" content="Bartneck C (2001) Affective expressions of machines. CHI &#8216;01 extended abstracts on human factors in computing systems, pp 189&#8211;190"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The role of emotion in believable agents; citation_author=J Bates; citation_volume=37; citation_issue=7; citation_publication_date=1994; citation_pages=122-125; citation_doi=10.1145/176789.176803; citation_id=CR4"/>

    <meta name="citation_reference" content="Bradley MM, Lang PJ (2007) The international affective digitized sounds (2nd Edition; IADS-2): affective ratings of sounds and instruction manual. Technical report B-3. University of Florida, Gainesville, Fl"/>

    <meta name="citation_reference" content="Brand M (1999) Voice puppetry. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques SIGGRAPH 99, pp 21&#8211;28"/>

    <meta name="citation_reference" content="Bregler C (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH&#8217;97, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=Stud Health Technol Inf; citation_title=Virtual reality to study responses to social environmental stressors in individuals with and without psychosis; citation_author=W-P Brinkman, W Veling, E Dorrestijn, G Sandino, V Vakili, M Gaag; citation_volume=167; citation_publication_date=2011; citation_pages=86-91; citation_id=CR8"/>

    <meta name="citation_reference" content="Brinkman W-P, Hartanto D, Kang N, de Vliegher D, Kampmann IL, Morina N et al (2012) A virtual reality dialogue system for the treatment of social phobia. In: Paper presented at the CHI&#8217;12 extended abstracts on human factors in computing systems"/>

    <meta name="citation_reference" content="citation_journal_title=Affect Comput Intel Interact Workshops; citation_title=Affectbutton: towards a standard for dynamic affective user feedback; citation_author=J Broekens, W-P Brinkman; citation_volume=2009; citation_publication_date=2009; citation_pages=1-8; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=AffectButton: a method for reliable and valid affective self-report; citation_author=J Broekens, W-P Brinkman; citation_volume=71; citation_issue=6; citation_publication_date=2013; citation_pages=641-667; citation_doi=10.1016/j.ijhcs.2013.02.003; citation_id=CR11"/>

    <meta name="citation_reference" content="Broekens J, Pronker A, Neuteboom M (2010) Real time labeling of affect in music using the affectbutton. In: Proceedings of the 3rd international workshop on affective interaction in natural environments, pp 21&#8211;26"/>

    <meta name="citation_reference" content="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer JJ (2011) Validity of a virtual negotiation training. In: IVA&#8217;11 Proceedings of the 11th international conference on intelligent virtual agents, pp 435&#8211;436 
"/>

    <meta name="citation_reference" content="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer J-J (2012) Virtual reality negotiation training increases negotiation knowledge and skill. In: IVA&#8217;12 Proceedings of the 12th international conference on intelligent virtual agents, pp 218&#8211;230"/>

    <meta name="citation_reference" content="Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Interactive Intelligence, Delft University of Technology, pp 1&#8211;30"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Artif Intell; citation_title=The power of a nod and a glance: envelope vs. emotional feedback in animated conversational agents; citation_author=J Cassell, KR Thorisson; citation_volume=13; citation_publication_date=1999; citation_pages=519-538; citation_doi=10.1080/088395199117360; citation_id=CR16"/>

    <meta name="citation_reference" content="Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T et al (1994) Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. In: Proceedings of ACM SIGGRAPH, pp 413&#8211;420"/>

    <meta name="citation_reference" content="Cerezo E, Baldassarri S (2008) Affective embodied conversational agents for natural interaction. In: Or J (ed) Affective computing: emotion modelling, synthesis and recognition, pp 329&#8211;354"/>

    <meta name="citation_reference" content="Chuang E, Bregler C (2002) Performance driven facial animation using blendshape interpolation. Computer Science Technical Report, Stanford University"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Emot; citation_title=Language and organisation of Filipino emotion concepts: comparing emotion concepts and dimensions across cultures; citation_author=T Church, M Katigbak; citation_volume=12; citation_issue=1; citation_publication_date=1998; citation_pages=63-92; citation_doi=10.1080/026999398379781; citation_id=CR20"/>

    <meta name="citation_reference" content="Cohen MM, Massaro DW (1993) Modeling coarticulation in synthetic visual speech. In: Thalman NM, Thalman D (eds) Models and Techniques in Computer Animation. Springer, Verlag, pp 139&#8211;156"/>

    <meta name="citation_reference" content="citation_journal_title=Simulation; citation_title=Teaching negotiation skills through practice and reflection with virtual humans; citation_author=M Core, D Traum, HC Lane, W Swartout, S Marsella, J Gratch, M Lent; citation_volume=82; citation_publication_date=2006; citation_pages=685-701; citation_id=CR22"/>

    <meta name="citation_reference" content="Cowell AJ, Stanney KM (2003) Embodiment and interaction guidelines for designing credible, trustworthy embodied conversational agents. In: 4th international workshop on intelligent virtual agents IVA 2003, 2792, pp 301&#8211;309"/>

    <meta name="citation_reference" content="citation_title=The expression of emotion in man and animals; citation_publication_date=1872; citation_id=CR24; citation_author=C Darwin; citation_publisher=Philosophical Library"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Soc Psychol; citation_title=Virtual prejudice; citation_author=R Dotsch, DHJ Wigboldus; citation_volume=44; citation_issue=4; citation_publication_date=2008; citation_pages=1194-1198; citation_doi=10.1016/j.jesp.2008.03.003; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Psychological Bull; citation_title=Strong evidence for universals in facial expressions: a reply to Russell&#8217;s mistaken critique; citation_author=P Ekman; citation_volume=115; citation_issue=2; citation_publication_date=1994; citation_pages=268-287; citation_doi=10.1037/0033-2909.115.2.268; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Soc; citation_title=Constants across cultures in the face and emotion; citation_author=P Ekman, WV Friesen; citation_volume=17; citation_issue=2; citation_publication_date=1971; citation_pages=124-129; citation_doi=10.1037/h0030377; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Soc Psychol; citation_title=Universals and cultural differences in the judgments of facial expressions of emotion; citation_author=P Ekman, WV Friesen, M O&#8217;Sullivan, A Chan, I Diacoyanni-Tarlatzis, K Heider, P Ricci-Bitti; citation_volume=53; citation_issue=4; citation_publication_date=1987; citation_pages=712-717; citation_doi=10.1037/0022-3514.53.4.712; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Philos Trans Biol Sci; citation_title=Facial expressions of emotion: an old controversy and new findings; citation_author=P Ekman, ET Rolls, DI Perrett, HD Ellis; citation_volume=335; citation_publication_date=1992; citation_pages=63-69; citation_doi=10.1098/rstb.1992.0008; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Human Face; citation_title=Facial action coding system; citation_author=P Ekman, WV Friesen, JC Hager; citation_volume=97; citation_publication_date=2002; citation_pages=4-5; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Curr Dir Psychol Sci; citation_title=Universals and cultural differences in recognizing emotions; citation_author=H Elfenbein; citation_volume=12; citation_issue=5; citation_publication_date=2003; citation_pages=159-164; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Bull; citation_title=Is there an in-group advantage in emotion recognition?; citation_author=HA Elfenbein, N Ambady; citation_volume=128; citation_issue=2; citation_publication_date=2002; citation_pages=243-249; citation_doi=10.1037/0033-2909.128.2.243; citation_id=CR32"/>

    <meta name="citation_reference" content="Elfenbein HA, Beaupre M, Levesque M, Hess U (2007) Toward a dialect theory: cultural differences in the expression and recognition of posed facial expressions. Emotion (Washington, DC) 7(1):131&#8211;146"/>

    <meta name="citation_reference" content="Endrass B, Rehm M, Lipi A (2011) Culture-related differences in aspects of behavior for virtual characters across Germany and Japan. In: Proceedings of AAMAS&#8217;11, 2, pp 441&#8211;448"/>

    <meta name="citation_reference" content="citation_journal_title=Visual Comput; citation_title=Building highly realistic facial modeling and animation: a survey; citation_author=N Ersotelos, F Dong; citation_volume=24; citation_issue=1; citation_publication_date=2008; citation_pages=13-30; citation_doi=10.1007/s00371-007-0175-y; citation_id=CR35"/>

    <meta name="citation_reference" content="Ezzat T, Geiger G, Poggio T (2004) Trainable videorealistic speech animation. In: Sixth IEEE international conference on automatic face and gesture recognition 2004 proceedings, pp 57&#8211;64"/>

    <meta name="citation_reference" content="citation_title=Emotion science cognitive and neuroscientific approaches to understanding human emotions; citation_publication_date=2008; citation_id=CR37; citation_author=E Fox; citation_publisher=Palgrave Macmillan"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Intell Syst; citation_title=Creating interactive virtual humans: some assembly required; citation_author=J Gratch, J Rickel, E Andre, J Cassell, E Petajan, NI Badler, R Jeff; citation_volume=17; citation_issue=4; citation_publication_date=2002; citation_pages=54-63; citation_doi=10.1109/MIS.2002.1024753; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_journal_title=J Nonverbal Behav; citation_title=Methodology for assessing bodily expression of emotion; citation_author=MM Gross, EA Crane, BL Fredrickson; citation_volume=34; citation_publication_date=2010; citation_pages=223-248; citation_doi=10.1007/s10919-010-0094-x; citation_id=CR39"/>

    <meta name="citation_reference" content="Haring M, Bee N, Andre E (2011) Creation and evaluation of emotion expression with body movement, sound and eye color for humanoid robots. In: RO-MAN, 2011 IEEE, pp 204&#8211;209"/>

    <meta name="citation_reference" content="citation_title=Culture&#8217;s consequences: comparing values, behaviors, institutions and organisations across nations; citation_publication_date=2001; citation_id=CR41; citation_author=G Hofstede; citation_publisher=Sage Publications"/>

    <meta name="citation_reference" content="Hudlicka E, Delft TU (2009) Foundations for modelling emotions in game characters: modelling emotion effects on cognition. In: Affective computing and intelligent interaction and workshops, ACII 2009"/>

    <meta name="citation_reference" content="citation_title=PXLab: the psychological experiments laboratory [online]; citation_publication_date=2007; citation_id=CR43; citation_author=H Irtel; citation_publisher=University of Mannheim"/>

    <meta name="citation_reference" content="Isbister K (2006) Better game characters by design: a psychological approach. Education, CRC Press"/>

    <meta name="citation_reference" content="citation_journal_title=Proc Natl Acad Sci; citation_title=Facial expressions of emotion are not culturally universal; citation_author=RE Jack, OGB Garrod, H Yu, R Caldara, PG Schyns; citation_volume=109; citation_issue=19; citation_publication_date=2012; citation_pages=7241-7244; citation_doi=10.1073/pnas.1200155109; citation_id=CR45"/>

    <meta name="citation_reference" content="Jan D, Herrera D, Martinovski B (2007) A computational model of culture-specific conversational behavior. In: IVA &#8216;07 Proceedings of the 7th international conference on intelligent virtual agents, pp 45&#8211;56"/>

    <meta name="citation_reference" content="Kahler K, Haber J, Seidel H-P (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of graphics interface, pp 37&#8211;46"/>

    <meta name="citation_reference" content="Keltner D, Ekman P (2000) Facial expression of emotion. Handbook of emotions, 2nd edn. pp 236&#8211;249"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Affect Comput; citation_title=Affective body expression perception and recognition: a survey; citation_author=A Kleinsmith, N Bianchi-Berthouze; citation_volume=4; citation_publication_date=2013; citation_pages=15-33; citation_doi=10.1109/T-AFFC.2012.16; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=Cross-cultural differences in recognizing affect from body posture; citation_author=A Kleinsmith, PR Silva, N Bianchi-Berthouze; citation_volume=18; citation_issue=6; citation_publication_date=2006; citation_pages=1371-1389; citation_doi=10.1016/j.intcom.2006.04.003; citation_id=CR50"/>

    <meta name="citation_reference" content="Kulms P, Kramer NC, Gratch J, Kang S-H (2011) It&#8217;s in their eyes: a study on female and male virtual humans&#39; gaze. In: IVA&#8217;11 Proceedings of the 11th international conference on intelligent virtual agents, pp 80&#8211;92"/>

    <meta name="citation_reference" content="Lance BJ, Rey MD, Marsella SC (2008) A model of gaze for the purpose of emotional expression in virtual embodied agents. In: AAMAS &#8216;08 proceedings of the 7th international joint conference on autonomous agents and multiagent systems, 1, pp 12&#8211;16"/>

    <meta name="citation_reference" content="citation_journal_title=Am Psychol; citation_title=The emotion probe. Studies of motivation and attention; citation_author=PJ Lang; citation_volume=50; citation_issue=5; citation_publication_date=1995; citation_pages=372-385; citation_doi=10.1037/0003-066X.50.5.372; citation_id=CR53"/>

    <meta name="citation_reference" content="Lang PJ, Bradley MM, Cuthbert BN (1999) International affective picture system (IAPS): technical manual and affective ratings. Psychology. The Center for Research in Psychophysiology, University of Florida, Gainesville, FL"/>

    <meta name="citation_reference" content="Lang PJ, Bradley MM, Cuthbert BN (2008) International affective picture system (IAPS): affective ratings of pictures and instruction manual. Technical Report A-8"/>

    <meta name="citation_reference" content="Lee J, Marsella SC (2012) Modeling speaker behavior: a comparison of two approaches. In: IVA&#8217;12 Proceedings of the 12th international conference on intelligent virtual agents, pp 161&#8211;174"/>

    <meta name="citation_reference" content="Lee Y, Terzopoulos D, Walters K (1995) Realistic modeling for facial animation. In: Proceedings of the 22nd annual conference on computer graphics and interactive techniques SIGGRAPH 95, pp 55&#8211;62"/>

    <meta name="citation_reference" content="Lee J, Prendinger H, Neviarouskaya A, Marsella S (2009) Learning models of speaker head nods with affective information. In: 2009 3rd international conference on affective computing and intelligent interaction and workshops, pp 1&#8211;6"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Hum Behav; citation_title=Accessibility and acceptance of responsive virtual human technology as a survey interviewer training tool; citation_author=M Link, P Armsby, RC Hubal, CI Guinn; citation_volume=22; citation_publication_date=2006; citation_pages=412-426; citation_doi=10.1016/j.chb.2004.09.008; citation_id=CR59"/>

    <meta name="citation_reference" content="Litwinowicz P, Williams L (1994) Animating images with drawings. In: Proceedings of the 21st annual conference on computer graphics and interactive techniques SIGGRAPH 94, pp 409&#8211;412"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Gender differences in the impact of presentational factors in human character animation on decisions in ethical dilemmas; citation_author=KFK MacDorman, JJA Coram, C-CC Ho, H Patel; citation_volume=19; citation_issue=3; citation_publication_date=2010; citation_pages=213-229; citation_id=CR61"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Bull; citation_title=Methodological requirements to test a possible in-group advantage in judging emotions across cultures: comment on Elfenbein and Ambady (2002) and evidence; citation_author=D Matsumoto; citation_volume=128; citation_issue=2; citation_publication_date=2002; citation_pages=236-242; citation_doi=10.1037/0033-2909.128.2.236; citation_id=CR62"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Psychol; citation_title=Emotion judgments do not differ as a function of perceived nationality; citation_author=D Matsumoto; citation_volume=42; citation_issue=3; citation_publication_date=2007; citation_pages=207-214; citation_doi=10.1080/00207590601050926; citation_id=CR63"/>

    <meta name="citation_reference" content="citation_title=An approach to environmental psychology; citation_publication_date=1974; citation_id=CR64; citation_author=A Mehrabian; citation_author=JA Russell; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="Melo Cd, Carnevale P, Gratch J (2011) The effect of expression of anger and happiness in computer agents on negotiations with humans. In: The tenth international conference on autonomous agents and multiagent systems, pp 2&#8211;6"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Affect Comput; citation_title=Affective learning: empathetic agents with emotional facial and tone of voice expressions; citation_author=CN Moridis, AA Economides; citation_volume=3; citation_issue=3; citation_publication_date=2012; citation_pages=260-272; citation_doi=10.1109/T-AFFC.2012.6; citation_id=CR66"/>

    <meta name="citation_reference" content="citation_journal_title=J Advert Res; citation_title=Observations: SAM the self-assessment manikin an efficient cross-cultural measurement of emotional response; citation_author=JD Morris; citation_volume=35; citation_issue=6; citation_publication_date=1995; citation_pages=63-68; citation_id=CR67"/>

    <meta name="citation_reference" content="citation_journal_title=Int J ManMachine Stud; citation_title=The effects of 10 communication modes on the behavior of teams during co-operative problem-solving; citation_author=RB Ochsman, A Chapanis; citation_volume=6; citation_issue=5; citation_publication_date=1974; citation_pages=579-619; citation_doi=10.1016/S0020-7373(74)80019-2; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_journal_title=Depression Anxiety; citation_title=Virtual reality exposure therapy in anxiety disorders: a quantitative meta-analysis; citation_author=D Opris, S Pintea, A Garcia-Palacios, CM Botella, S Szamoskozi, D David; citation_volume=29; citation_publication_date=2012; citation_pages=85-93; citation_doi=10.1002/da.20910; citation_id=CR69"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ACM Annu Conf; citation_title=Computer generated animation of faces; citation_author=FI Parke; citation_volume=1; citation_publication_date=1972; citation_pages=451-457; citation_id=CR70"/>

    <meta name="citation_reference" content="Parke FI (1974) A parametric model for human faces. The University of Utah, Doctoral Dissertation"/>

    <meta name="citation_reference" content="Petrushin V (1999) Emotion in speech: recognition and application to call centers. In: Artificial neural network in engineering (ANNIE&#8217;99), pp 7&#8211;10"/>

    <meta name="citation_reference" content="Picard RW (1998) Toward agents that recognize emotion. In: Actes proceedings IMAGINA, pp 153&#8211;165"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Toward machine emotional intelligence: analysis of affective physiological state; citation_author=RW Picard, E Vyzas, J Healey; citation_volume=23; citation_issue=10; citation_publication_date=2001; citation_pages=1175-1191; citation_doi=10.1109/34.954607; citation_id=CR74"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGGRAPH Comput Graph; citation_title=Animating facial expressions; citation_author=SM Platt, NI Badler; citation_volume=15; citation_issue=3; citation_publication_date=1981; citation_pages=245-252; citation_doi=10.1145/965161.806812; citation_id=CR75"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Human-Comput Interact; citation_title=Online consumer trust and live help interfaces: the effects of text-to-speech voice and three-dimensional avatars; citation_author=L Qiu, I Benbasat; citation_volume=19; citation_publication_date=2005; citation_pages=37-41; citation_id=CR76"/>

    <meta name="citation_reference" content="citation_title=The media equation; citation_publication_date=1996; citation_id=CR77; citation_author=B Reeves; citation_author=C Nass; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Stud Health Technol Inf; citation_title=An intelligent virtual human system for providing healthcare information and support; citation_author=A Rizzo, B Lange, J Buckwalter, E Forbell, J Kim, K Sagae, P Kenny; citation_volume=163; citation_publication_date=2011; citation_pages=503-509; citation_id=CR78"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Bull; citation_title=Culture and the categorization of emotions; citation_author=JA Russell; citation_volume=110; citation_publication_date=1991; citation_pages=426-450; citation_doi=10.1037/0033-2909.110.3.426; citation_id=CR79"/>

    <meta name="citation_reference" content="Ruttkay Z, Pelachaud C (2005) From brows to trust: evaluating embodied conversational agents. Springer, Berlin"/>

    <meta name="citation_reference" content="citation_journal_title=J Voice Off J Voice Found; citation_title=Expression of emotion in voice and music; citation_author=KR Scherer; citation_volume=9; citation_issue=3; citation_publication_date=1995; citation_pages=235-248; citation_doi=10.1016/S0892-1997(05)80231-0; citation_id=CR81"/>

    <meta name="citation_reference" content="citation_journal_title=Speech Commun; citation_title=Vocal communication of emotion: a review of research paradigms; citation_author=KR Scherer; citation_volume=40; citation_publication_date=2003; citation_pages=227-256; citation_doi=10.1016/S0167-6393(02)00084-5; citation_id=CR82"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ACM CHI; citation_title=Face to interface: facial affect in (hu)man and machine; citation_author=DJ Schiano, SM Ehrlich, K Rahardja, K Sheridan; citation_volume=2000; citation_publication_date=2000; citation_pages=193-200; citation_id=CR83"/>

    <meta name="citation_reference" content="Schroder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. Research Report of the Institute of Phonetics"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graphics Appl; citation_title=Public speaking in virtual reality: facing an audience of avatars; citation_author=M Slater, D-P Pertaub, A Steed; citation_volume=19; citation_issue=2; citation_publication_date=1999; citation_pages=6-9; citation_doi=10.1109/38.749116; citation_id=CR85"/>

    <meta name="citation_reference" content="citation_title=Affect, imagery, consciousness: vol 1. The positive affects; citation_publication_date=1962; citation_id=CR86; citation_author=SS Tomkins; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Affect, imagery, consciousness: vol 2. The negative affects; citation_publication_date=1963; citation_id=CR87; citation_author=SS Tomkins; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Tsapatsoulis N, Raouzaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs. In; MPEG-4 facial animation the standard implementations applications"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph SIGGRAPH Proc; citation_title=A muscle model for animating three-dimensional facial expression; citation_author=K Waters; citation_volume=21; citation_issue=4; citation_publication_date=1987; citation_pages=17-24; citation_doi=10.1145/37402.37405; citation_id=CR89"/>

    <meta name="citation_reference" content="citation_title=Emotions across languages and cultures: diversity and universals; citation_publication_date=1995; citation_id=CR90; citation_author=A Wierzbicka; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="Wong JW-E, McGee K (2012) Frown more, talk more: effects of facial expressions in establishing conversational rapport with virtual agents. In: IVA&#8217;12 Proceedings of the 12th international conference on intelligent virtual agents, pp 419&#8211;425"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Entertain; citation_title=Can local avatars satisfy a global audience? A case study of high-fidelity 3D facial avatar animation in subject identification and emotion perception by US and international groups; citation_author=C Yun, Z Deng, M Hiscock; citation_volume=7; citation_issue=2; citation_publication_date=2009; citation_pages=1-25; citation_doi=10.1145/1541895.1541901; citation_id=CR92"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Visual Comput Graphics; citation_title=Geometry-driven photorealistic facial expression synthesis; citation_author=QZQ Zhang, Z Liu, GQG Quo, D Terzopoulos, H-YSH-Y Shum; citation_volume=12; citation_issue=1; citation_publication_date=2006; citation_pages=48-60; citation_doi=10.1109/TVCG.2006.9; citation_id=CR93"/>

    <meta name="citation_author" content="Chao Qu"/>

    <meta name="citation_author_email" content="aquchaos@gmail.com"/>

    <meta name="citation_author_institution" content="Delft University of Technology, Delft, The Netherlands"/>

    <meta name="citation_author" content="Willem-Paul Brinkman"/>

    <meta name="citation_author_institution" content="Delft University of Technology, Delft, The Netherlands"/>

    <meta name="citation_author" content="Yun Ling"/>

    <meta name="citation_author_institution" content="Delft University of Technology, Delft, The Netherlands"/>

    <meta name="citation_author" content="Pascal Wiggers"/>

    <meta name="citation_author_institution" content="Delft University of Technology, Delft, The Netherlands"/>

    <meta name="citation_author" content="Ingrid Heynderickx"/>

    <meta name="citation_author_institution" content="Delft University of Technology, Delft, The Netherlands"/>

    <meta name="citation_author_institution" content="Philips Research Laboratories, Eindhoven, The Netherlands"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0231-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0231-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture"/>
        <meta property="og:description" content="Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for different contexts and usually for users from different countries. The emphasis on a virtual human’s emotional expression depends on the application; some virtual reality applications need an emotional expression of the virtual human during the speaking phase, some during the listening phase and some during both speaking and listening phases. Although studies have investigated how humans perceive a virtual human’s emotion during each phase separately, few studies carried out a parallel comparison between the two phases. This study aims to fill this gap, and on top of that, includes an investigation of the cultural interpretation of the virtual human’s emotion, especially with respect to the emotion’s valence. The experiment was conducted with both Chinese and non-Chinese participants. These participants were asked to rate the valence of seven different emotional expressions (ranging from negative to neutral to positive during speaking and listening) of a Chinese virtual lady. The results showed that there was a high correlation in valence rating between both groups of participants, which indicated that the valence of the emotional expressions was as easily recognized by people from a different cultural background as the virtual human. In addition, participants tended to perceive the virtual human’s expressed valence as more intense in the speaking phase than in the listening phase. The additional vocal emotional expression in the speaking phase is put forward as a likely cause for this phenomenon."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0231-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Virtual human, Emotion, Affective computing, Culture","kwrd":["Virtual_reality","Virtual_human","Emotion","Affective_computing","Culture"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0231-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0231-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=231;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0231-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0231-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0231-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-08-30" itemprop="datePublished">30 August 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chao-Qu" data-author-popup="auth-Chao-Qu" data-corresp-id="c1">Chao Qu<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Delft University of Technology" /><meta itemprop="address" content="grid.5292.c, 0000000120974740, Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Willem_Paul-Brinkman" data-author-popup="auth-Willem_Paul-Brinkman">Willem-Paul Brinkman</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Delft University of Technology" /><meta itemprop="address" content="grid.5292.c, 0000000120974740, Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yun-Ling" data-author-popup="auth-Yun-Ling">Yun Ling</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Delft University of Technology" /><meta itemprop="address" content="grid.5292.c, 0000000120974740, Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Pascal-Wiggers" data-author-popup="auth-Pascal-Wiggers">Pascal Wiggers</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Delft University of Technology" /><meta itemprop="address" content="grid.5292.c, 0000000120974740, Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ingrid-Heynderickx" data-author-popup="auth-Ingrid-Heynderickx">Ingrid Heynderickx</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Delft University of Technology" /><meta itemprop="address" content="grid.5292.c, 0000000120974740, Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Philips Research Laboratories" /><meta itemprop="address" content="grid.417284.c, 0000000403989387, Philips Research Laboratories, High Tech Campus 34, 5656 AE, Eindhoven, The Netherlands" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">307</span>–<span itemprop="pageEnd">321</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">791 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0231-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for different contexts and usually for users from different countries. The emphasis on a virtual human’s emotional expression depends on the application; some virtual reality applications need an emotional expression of the virtual human during the speaking phase, some during the listening phase and some during both speaking and listening phases. Although studies have investigated how humans perceive a virtual human’s emotion during each phase separately, few studies carried out a parallel comparison between the two phases. This study aims to fill this gap, and on top of that, includes an investigation of the cultural interpretation of the virtual human’s emotion, especially with respect to the emotion’s valence. The experiment was conducted with both Chinese and non-Chinese participants. These participants were asked to rate the valence of seven different emotional expressions (ranging from negative to neutral to positive during speaking and listening) of a Chinese virtual lady. The results showed that there was a high correlation in valence rating between both groups of participants, which indicated that the valence of the emotional expressions was as easily recognized by people from a different cultural background as the virtual human. In addition, participants tended to perceive the virtual human’s expressed valence as more intense in the speaking phase than in the listening phase. The additional vocal emotional expression in the speaking phase is put forward as a likely cause for this phenomenon.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>To create a feeling of being “present” in virtual reality is essential to the success of many virtual reality applications such as training (Broekens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer JJ (2011) Validity of a virtual negotiation training. In: IVA’11 Proceedings of the 11th international conference on intelligent virtual agents, pp 435–436 &#xA;" href="/article/10.1007/s10055-013-0231-z#ref-CR13" id="ref-link-section-d20543e352">2011</a>), coaching (Rizzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Rizzo A, Lange B, Buckwalter J, Forbell E, Kim J, Sagae K, Kenny P (2011) An intelligent virtual human system for providing healthcare information and support. Stud Health Technol Inf 163:503–509" href="/article/10.1007/s10055-013-0231-z#ref-CR78" id="ref-link-section-d20543e355">2011</a>), therapy (Brinkman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Brinkman W-P, Hartanto D, Kang N, de Vliegher D, Kampmann IL, Morina N et al (2012) A virtual reality dialogue system for the treatment of social phobia. In: Paper presented at the CHI’12 extended abstracts on human factors in computing systems" href="/article/10.1007/s10055-013-0231-z#ref-CR9" id="ref-link-section-d20543e358">2012</a>) and games (Isbister <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Isbister K (2006) Better game characters by design: a psychological approach. Education, CRC Press" href="/article/10.1007/s10055-013-0231-z#ref-CR44" id="ref-link-section-d20543e361">2006</a>). A feeling of being “present” in virtual reality may be achieved by making the virtual reality environment as natural as possible. Human–computer interaction, including human–virtual human interaction, is inherently natural and social (Reeves and Nass <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Reeves B, Nass C (1996) The media equation. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-013-0231-z#ref-CR77" id="ref-link-section-d20543e364">1996</a>), and so, is an essential component in the realism of the virtual environment. Without proper behavior of the virtual human, users may not be able to “suspend disbelief” and the effectiveness of the virtual reality application will decrease.</p><p>Considering the importance of emotion in human–human communication, emotion may also help people to establish a better relationship with virtual human (Reeves and Nass <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Reeves B, Nass C (1996) The media equation. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-013-0231-z#ref-CR77" id="ref-link-section-d20543e370">1996</a>). As Picard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Picard RW, Vyzas E, Healey J (2001) Toward machine emotional intelligence: analysis of affective physiological state. IEEE Trans Pattern Anal Mach Intell 23(10):1175–1191" href="/article/10.1007/s10055-013-0231-z#ref-CR74" id="ref-link-section-d20543e373">2001</a>) argues, without some emotional skills, machines will not appear intelligent when interacting with people. Therefore, multiple technologies to give virtual human the abilities of generating human acceptable expressions have been developed in recent decades (Ersotelos and Dong <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ersotelos N, Dong F (2008) Building highly realistic facial modeling and animation: a survey. Visual Comput 24(1):13–30" href="/article/10.1007/s10055-013-0231-z#ref-CR35" id="ref-link-section-d20543e376">2008</a>).</p><p>Different applications require different levels of emphasis on how the virtual human express their emotions. Even when implemented in only part of the application, emotional expressions can be effective. In a health coach application, for example, the virtual human might mainly need to speak to motivate the user, and emotional expressions during speaking are most important. In a virtual reality exposure therapy for fear of public speaking, the virtual human only needs to listen, and so, emotional expressions while listening are most important. In some applications, such as for a role playing game or a negotiation simulator, the full range of speaking and listening is used and might benefit from emotional expressions. Studies on generating and evaluating the emotional agents normally only focus on either listening (Slater et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Slater M, Pertaub D-P, Steed A (1999) Public speaking in virtual reality: facing an audience of avatars. IEEE Comput Graphics Appl 19(2):6–9" href="/article/10.1007/s10055-013-0231-z#ref-CR85" id="ref-link-section-d20543e382">1999</a>; Wong and McGee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Wong JW-E, McGee K (2012) Frown more, talk more: effects of facial expressions in establishing conversational rapport with virtual agents. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 419–425" href="/article/10.1007/s10055-013-0231-z#ref-CR91" id="ref-link-section-d20543e385">2012</a>) or speaking (MacDorman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="MacDorman KFK, Coram JJA, Ho C-CC, Patel H (2010) Gender differences in the impact of presentational factors in human character animation on decisions in ethical dilemmas. Presence Teleoper Virtual Environ 19(3):213–229" href="/article/10.1007/s10055-013-0231-z#ref-CR61" id="ref-link-section-d20543e388">2010</a>; Qiu and Benbasat <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Qiu L, Benbasat I (2005) Online consumer trust and live help interfaces: the effects of text-to-speech voice and three-dimensional avatars. Int J Human-Comput Interact 19:37–41" href="/article/10.1007/s10055-013-0231-z#ref-CR76" id="ref-link-section-d20543e391">2005</a>). Studies that do include both speaking and listening,(e.g., Core et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Core M, Traum D, Lane HC, Swartout W, Marsella S, Gratch J, Van Lent M (2006) Teaching negotiation skills through practice and reflection with virtual humans. Simulation 82:685–701" href="/article/10.1007/s10055-013-0231-z#ref-CR22" id="ref-link-section-d20543e394">2006</a>), Broekens et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012a" title="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer J-J (2012) Virtual reality negotiation training increases negotiation knowledge and skill. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 218–230" href="/article/10.1007/s10055-013-0231-z#ref-CR14" id="ref-link-section-d20543e398">2012a</a>), Link et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Link M, Armsby P, Hubal RC, Guinn CI (2006) Accessibility and acceptance of responsive virtual human technology as a survey interviewer training tool. Comput Hum Behav 22:412–426. doi:&#xA;                    10.1016/j.chb.2004.09.008&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0231-z#ref-CR59" id="ref-link-section-d20543e401">2006</a>)) focus mainly on the conversation and communication as a whole and do not separately investigate the speaking and listening phases of the whole conversation. To our knowledge, no study has directly compared the impact of emotional expressions during speaking and listening in virtual reality. In the current study, the virtual human’s valence state was manipulated, while she was speaking and listening from negative to positive, and the impact on the participants’ perception was examined.</p><p>Besides the difference between listening and speaking, culture might also be an important factor for a designer to consider as many applications are used all across the world nowadays. Especially for some virtual reality applications, such as virtual reality exposure therapy for patients with social phobia (Brinkman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Brinkman W-P, Hartanto D, Kang N, de Vliegher D, Kampmann IL, Morina N et al (2012) A virtual reality dialogue system for the treatment of social phobia. In: Paper presented at the CHI’12 extended abstracts on human factors in computing systems" href="/article/10.1007/s10055-013-0231-z#ref-CR9" id="ref-link-section-d20543e407">2012</a>), it is crucial to understand how people with a different cultural background perceive the affective behavior of virtual humans. Several studies have already focused on the effect of cultural differences on evaluating virtual human’s emotions. For example, Jack et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jack RE, Garrod OGB, Yu H, Caldara R, Schyns PG (2012) Facial expressions of emotion are not culturally universal. Proc Natl Acad Sci 109(19):7241–7244" href="/article/10.1007/s10055-013-0231-z#ref-CR45" id="ref-link-section-d20543e410">2012</a>) showed that facial expressions of emotion are culture specific. However, Yun et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Yun C, Deng Z, Hiscock M (2009) Can local avatars satisfy a global audience? A case study of high-fidelity 3D facial avatar animation in subject identification and emotion perception by US and international groups. Comput Entertain 7(2):1–25" href="/article/10.1007/s10055-013-0231-z#ref-CR92" id="ref-link-section-d20543e413">2009</a>) found that cultural background has little effect on emotion perception. Kleinsmith et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kleinsmith A, De Silva PR, Bianchi-Berthouze N (2006) Cross-cultural differences in recognizing affect from body posture. Interact Comput 18(6):1371–1389" href="/article/10.1007/s10055-013-0231-z#ref-CR50" id="ref-link-section-d20543e416">2006</a>) evaluated cultural impact on perception of emotion and found that emotions are both universal- and cultural-specific. Therefore, similar to the studies for perceiving emotional expressions of real humans, the universality of emotion perception of virtual human seems also still inconclusive. In addition, most research is only limited to the investigation of head-only virtual human with facial expressions and far less research is devoted to emotional expressions from a 3D virtual human which expresses its emotional state also via gaze, head movement or voice intonation.</p><p>In summary, this study involves two research questions: (1) whether emotional expressions of virtual human are perceived differently depending on the cultural background of the perceiver, and (2) whether a person is more perceptive to emotional expressions in one of the two phases (the speaking phase and listening phase) or whether a person treats these two phases as equally important when rating the virtual human’s emotions? To answer these research questions, we designed a virtual human representing a Chinese lady at an age around 25. She had the ability to show multiple emotional states in multiple nonverbal and verbal ways: i.e., through facial expression, head movement, gaze and voice intonation. During the listening phase, the virtual human’s emotional behavior was expressed by nonverbal communication only, while during the speaking phase, the emotional behavior was expressed by both verbal (i.e., intonation) and nonverbal communications. To avoid a possible emotional bias from the content of the conversation, a relatively neutral topic, i.e., conference attendance, was selected in this experiment. Petrushin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Petrushin V (1999) Emotion in speech: recognition and application to call centers. In: Artificial neural network in engineering (ANNIE’99), pp 7–10" href="/article/10.1007/s10055-013-0231-z#ref-CR72" id="ref-link-section-d20543e423">1999</a>) pointed out that humans are not perfect in decoding manifest emotions such as anger and happiness in voice intonation only. Therefore, as a first step, only three basic emotional valence states (positive, neutral and negative) were used in this study. In order to test the effect of cultural influence on the perception of the emotions expressed by the virtual human, two groups of participants were recruited: from the same culture as the virtual human and from other cultures. We chose to compare Chinese versus non-Chinese participants, because it is known from cultural models (Hofstede <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hofstede G (2001) Culture’s consequences: comparing values, behaviors, institutions and organisations across nations. Sage Publications, Thousand Oaks" href="/article/10.1007/s10055-013-0231-z#ref-CR41" id="ref-link-section-d20543e426">2001</a>) that the difference in cultural values is significant between these two groups, and since two of the authors experienced these differences while living in Europe. Moreover, the background of these authors facilitated the recruitment of Chinese participants.</p><p>Based on knowledge available in the literature, we envision the following two hypotheses related to our research questions.</p>
                <h3 class="c-article__sub-heading">Hypothesis 1</h3>
                <p>Individuals with the same cultural background as the virtual human perceive the valence state of the virtual human differently from individuals with a different cultural background.</p>
              <p>Especially, as the virtual human was speaking Chinese, participants with a different cultural background could not understand what the virtual human said during verbal communication. Hence, participants with a different cultural background from the virtual human are expected to perceive her emotion differently from participants with the same cultural background.</p>
                <h3 class="c-article__sub-heading">Hypothesis 2</h3>
                <p>The virtual human’s expressed valence is perceived as more intense in the speaking phase than in the listening phase.</p>
              <p>Since the speaking phase also allows including verbal expression of the emotion, it seems likely that compared to the listening phase, the emotion expressed in the speaking phase is perceived as more intense.</p><p>The rest of the paper is structured as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0231-z#Sec2">2</a> provides theoretical background on how a virtual human can express emotion through facial expression, gaze, head movements and voice intonation. In addition, it discusses cultural differences in emotion recognition and various emotional models, needed to understand the rest of the paper. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0231-z#Sec8">3</a> provides a description of the apparatus, validation of the stimuli material and the procedure of the experiment, and its results are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0231-z#Sec14">4</a>. Finally, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0231-z#Sec17">5</a>, the findings of the study are discussed and conclusions are drawn.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Theoretical background</h2><div class="c-article-section__content" id="Sec2-content"><p>No matter what roles virtual humans play in a virtual world, they need to elicit an anthropomorphic interaction with their human users. This requires vast knowledge of various human aspects including facial expression, gaze, head movement, voice expression and their cultural difference in order to make the virtual human believable, responsive and interpretable.</p><h3 class="c-article__sub-heading" id="Sec3">Facial expression of a virtual human</h3><p>Facial expression is one of the options to express human emotion and as such plays a substantial role in depicting human characters. Started in the early 70s and 80s (Parke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1972" title="Parke FI (1972) Computer generated animation of faces. Proc ACM Annu Conf 1:451–457" href="/article/10.1007/s10055-013-0231-z#ref-CR70" id="ref-link-section-d20543e479">1972</a>; Platt and Badler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Platt SM, Badler NI (1981) Animating facial expressions. ACM SIGGRAPH Comput Graph 15(3):245–252" href="/article/10.1007/s10055-013-0231-z#ref-CR75" id="ref-link-section-d20543e482">1981</a>), face modeling and animation have been a continuous research topic for many years. From early 2000s, more flexible emotion representations were created with MPEG-4-based facial animation (Tsapatsoulis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Tsapatsoulis N, Raouzaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs. In; MPEG-4 facial animation the standard implementations applications" href="/article/10.1007/s10055-013-0231-z#ref-CR88" id="ref-link-section-d20543e485">2002</a>). Recent advances in facial animation that allow to produce a rich set of effects on synthetic humans already had their impact on the industry (Ersotelos and Dong <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ersotelos N, Dong F (2008) Building highly realistic facial modeling and animation: a survey. Visual Comput 24(1):13–30" href="/article/10.1007/s10055-013-0231-z#ref-CR35" id="ref-link-section-d20543e488">2008</a>).</p><p>Multiple approaches have been proposed to create naturally looking facial expressions; they can be categorized as follows: (1) simulation or physically based models, which try to model the anatomical structure of the face as well as the underlying dynamics (Kahler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kahler K, Haber J, Seidel H-P (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of graphics interface, pp 37–46" href="/article/10.1007/s10055-013-0231-z#ref-CR47" id="ref-link-section-d20543e494">2001</a>; Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Lee Y, Terzopoulos D, Walters K (1995) Realistic modeling for facial animation. In: Proceedings of the 22nd annual conference on computer graphics and interactive techniques SIGGRAPH 95, pp 55–62" href="/article/10.1007/s10055-013-0231-z#ref-CR57" id="ref-link-section-d20543e497">1995</a>; Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Waters K (1987) A muscle model for animating three-dimensional facial expression. Comput Graph SIGGRAPH Proc 21(4):17–24" href="/article/10.1007/s10055-013-0231-z#ref-CR89" id="ref-link-section-d20543e500">1987</a>), (2) performance-driven models, which reassemble frames from video footage or motion capture data of a real person to yield the desired facial expression (Brand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Brand M (1999) Voice puppetry. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques SIGGRAPH 99, pp 21–28" href="/article/10.1007/s10055-013-0231-z#ref-CR6" id="ref-link-section-d20543e503">1999</a>; Bregler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bregler C (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH’97, pp 1–8" href="/article/10.1007/s10055-013-0231-z#ref-CR7" id="ref-link-section-d20543e506">1997</a>; Chuang and Bregler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Chuang E, Bregler C (2002) Performance driven facial animation using blendshape interpolation. Computer Science Technical Report, Stanford University" href="/article/10.1007/s10055-013-0231-z#ref-CR19" id="ref-link-section-d20543e510">2002</a>; Ezzat et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ezzat T, Geiger G, Poggio T (2004) Trainable videorealistic speech animation. In: Sixth IEEE international conference on automatic face and gesture recognition 2004 proceedings, pp 57–64" href="/article/10.1007/s10055-013-0231-z#ref-CR36" id="ref-link-section-d20543e513">2004</a>; Litwinowicz and Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Litwinowicz P, Williams L (1994) Animating images with drawings. In: Proceedings of the 21st annual conference on computer graphics and interactive techniques SIGGRAPH 94, pp 409–412" href="/article/10.1007/s10055-013-0231-z#ref-CR60" id="ref-link-section-d20543e516">1994</a>) and (3) parameterized-based models, which assign weights to the vertices of meshes representing the face, such that during animation the vertices are moved according to the weights (Cohen and Massaro <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Cohen MM, Massaro DW (1993) Modeling coarticulation in synthetic visual speech. In: Thalman NM, Thalman D (eds) Models and Techniques in Computer Animation. Springer, Verlag, pp 139–156" href="/article/10.1007/s10055-013-0231-z#ref-CR21" id="ref-link-section-d20543e519">1993</a>; Parke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Parke FI (1974) A parametric model for human faces. The University of Utah, Doctoral Dissertation" href="/article/10.1007/s10055-013-0231-z#ref-CR71" id="ref-link-section-d20543e522">1974</a>; Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zhang QZQ, Liu Z, Quo GQG, Terzopoulos D, Shum H-YSH-Y (2006) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Visual Comput Graphics 12(1):48–60" href="/article/10.1007/s10055-013-0231-z#ref-CR93" id="ref-link-section-d20543e525">2006</a>). Considering the high computational load required for the simulation or physically based models and the high costs for the motion capture equipment needed for performance-driven models, we decided to choose an easily repeated facial expression animation based on a parameterized model for this study.</p><h3 class="c-article__sub-heading" id="Sec4">Head movement and gaze of a virtual human</h3><p>Besides facial expressions, also head and eye movements were implemented in the virtual human used in our experiment. Head movements and eye gaze are two important sources of emotional feedback in interaction (Cassell and Thorisson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cassell J, Thorisson KR (1999) The power of a nod and a glance: envelope vs. emotional feedback in animated conversational agents. Appl Artif Intell 13:519–538" href="/article/10.1007/s10055-013-0231-z#ref-CR16" id="ref-link-section-d20543e536">1999</a>; Lee and Marsella <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Lee J, Marsella SC (2012) Modeling speaker behavior: a comparison of two approaches. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 161–174" href="/article/10.1007/s10055-013-0231-z#ref-CR56" id="ref-link-section-d20543e539">2012</a>; Ruttkay and Pelachaud <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ruttkay Z, Pelachaud C (2005) From brows to trust: evaluating embodied conversational agents. Springer, Berlin" href="/article/10.1007/s10055-013-0231-z#ref-CR80" id="ref-link-section-d20543e542">2005</a>). They are essential to embody interactive conversational systems (Cassell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T et al (1994) Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. In: Proceedings of ACM SIGGRAPH, pp 413–420" href="/article/10.1007/s10055-013-0231-z#ref-CR17" id="ref-link-section-d20543e545">1994</a>), and it is relatively simple to create primarily nods and glances toward or away from the user. Still the correct timing is essential (Cassell and Thorisson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cassell J, Thorisson KR (1999) The power of a nod and a glance: envelope vs. emotional feedback in animated conversational agents. Appl Artif Intell 13:519–538" href="/article/10.1007/s10055-013-0231-z#ref-CR16" id="ref-link-section-d20543e548">1999</a>). Research of Lance et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lance BJ, Rey MD, Marsella SC (2008) A model of gaze for the purpose of emotional expression in virtual embodied agents. In: AAMAS ‘08 proceedings of the 7th international joint conference on autonomous agents and multiagent systems, 1, pp 12–16" href="/article/10.1007/s10055-013-0231-z#ref-CR52" id="ref-link-section-d20543e552">2008</a>) and Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee J, Prendinger H, Neviarouskaya A, Marsella S (2009) Learning models of speaker head nods with affective information. In: 2009 3rd international conference on affective computing and intelligent interaction and workshops, pp 1–6" href="/article/10.1007/s10055-013-0231-z#ref-CR58" id="ref-link-section-d20543e555">2009</a>) shows how head movements and gaze can be embedded into a virtual character.</p><h3 class="c-article__sub-heading" id="Sec5">Voice expression of a virtual human</h3><p>Along with the nonverbal emotional expressions, emotion can also be expressed by voice intonation when the virtual human is talking. Speech was once considered as the main channel to carry most, or even all, the necessary information in a conversation (Ochsman and Chapanis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behavior of teams during co-operative problem-solving. Int J ManMachine Stud 6(5):579–619" href="/article/10.1007/s10055-013-0231-z#ref-CR68" id="ref-link-section-d20543e566">1974</a>). This idea has been countered by a growing body of research on believable, lifelike embodied conversational agents (Bates <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Bates J (1994) The role of emotion in believable agents. Commun ACM 37(7):122–125" href="/article/10.1007/s10055-013-0231-z#ref-CR4" id="ref-link-section-d20543e569">1994</a>). Still, the importance of the voice in emotion expression cannot be denied (Scherer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Scherer KR (1995) Expression of emotion in voice and music. J Voice Off J Voice Found 9(3):235–248" href="/article/10.1007/s10055-013-0231-z#ref-CR81" id="ref-link-section-d20543e572">1995</a>). Many studies have investigated emotional effects in voice and speech (Bailenson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bailenson JN, Yee N, Merget D, Schroeder R (2006) The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction. Presence Teleoper Virtual Environ 15(4):359–372" href="/article/10.1007/s10055-013-0231-z#ref-CR1" id="ref-link-section-d20543e575">2006</a>; Petrushin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Petrushin V (1999) Emotion in speech: recognition and application to call centers. In: Artificial neural network in engineering (ANNIE’99), pp 7–10" href="/article/10.1007/s10055-013-0231-z#ref-CR72" id="ref-link-section-d20543e578">1999</a>; Scherer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Scherer KR (2003) Vocal communication of emotion: a review of research paradigms. Speech Commun 40:227–256" href="/article/10.1007/s10055-013-0231-z#ref-CR82" id="ref-link-section-d20543e582">2003</a>), and emotion expressed in the voice of virtual humans (Cerezo and Baldassarri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Cerezo E, Baldassarri S (2008) Affective embodied conversational agents for natural interaction. In: Or J (ed) Affective computing: emotion modelling, synthesis and recognition, pp 329–354" href="/article/10.1007/s10055-013-0231-z#ref-CR18" id="ref-link-section-d20543e585">2008</a>; Moridis and Economides <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Moridis CN, Economides AA (2012) Affective learning: empathetic agents with emotional facial and tone of voice expressions. IEEE Trans Affect Comput 3(3):260–272" href="/article/10.1007/s10055-013-0231-z#ref-CR66" id="ref-link-section-d20543e588">2012</a>). The intonation of the voice was therefore also considered as an important aspect of the virtual human’s emotional expression in this study.</p><h3 class="c-article__sub-heading" id="Sec6">Cultural difference</h3><p>Culture, such as age, gender, posture and context, is one of the many factors affecting emotion expression (Picard <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Picard RW (1998) Toward agents that recognize emotion. In: Actes proceedings IMAGINA, pp 153–165" href="/article/10.1007/s10055-013-0231-z#ref-CR73" id="ref-link-section-d20543e600">1998</a>). A long-time question in the study of human emotion is the extent to which emotional expressions are universal or culturally determined (Elfenbein et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Elfenbein HA, Beaupre M, Levesque M, Hess U (2007) Toward a dialect theory: cultural differences in the expression and recognition of posed facial expressions. Emotion (Washington, DC) 7(1):131–146" href="/article/10.1007/s10055-013-0231-z#ref-CR33" id="ref-link-section-d20543e603">2007</a>). Cultural background may influence the rate of emotion recognition (Matsumoto <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Matsumoto D (2002) Methodological requirements to test a possible in-group advantage in judging emotions across cultures: comment on Elfenbein and Ambady (2002) and evidence. Psychol Bull 128(2):236–242" href="/article/10.1007/s10055-013-0231-z#ref-CR62" id="ref-link-section-d20543e606">2002</a>). When an expresser of an emotion and the perceiver of the emotion have the same cultural background, the perceiver’s recognition rate is found to be higher than when the expresser and perceiver have a different cultural background (Elfenbein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Elfenbein H (2003) Universals and cultural differences in recognizing emotions. Curr Dir Psychol Sci 12(5):159–164" href="/article/10.1007/s10055-013-0231-z#ref-CR31" id="ref-link-section-d20543e609">2003</a>; Elfenbein and Ambady <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Elfenbein HA, Ambady N (2002) Is there an in-group advantage in emotion recognition? Psychol Bull 128(2):243–249" href="/article/10.1007/s10055-013-0231-z#ref-CR32" id="ref-link-section-d20543e612">2002</a>; Elfenbein et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Elfenbein HA, Beaupre M, Levesque M, Hess U (2007) Toward a dialect theory: cultural differences in the expression and recognition of posed facial expressions. Emotion (Washington, DC) 7(1):131–146" href="/article/10.1007/s10055-013-0231-z#ref-CR33" id="ref-link-section-d20543e616">2007</a>). However, Darwin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1872" title="Darwin C (1872) The expression of emotion in man and animals. Philosophical Library, New York" href="/article/10.1007/s10055-013-0231-z#ref-CR24" id="ref-link-section-d20543e619">1872</a>) and Tomkins (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1962" title="Tomkins SS (1962) Affect, imagery, consciousness: vol 1. The positive affects. Springer, New York" href="/article/10.1007/s10055-013-0231-z#ref-CR86" id="ref-link-section-d20543e622">1962</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1963" title="Tomkins SS (1963) Affect, imagery, consciousness: vol 2. The negative affects. Springer, New York" href="/article/10.1007/s10055-013-0231-z#ref-CR87" id="ref-link-section-d20543e625">1963</a>) argue that universal emotions do exist, studies also show universality in the facial expression of emotion and its perception, and attribute only little effect of cultural background on emotion perception from facial expressions (Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ekman P (1994) Strong evidence for universals in facial expressions: a reply to Russell’s mistaken critique. Psychological Bull 115(2):268–287" href="/article/10.1007/s10055-013-0231-z#ref-CR26" id="ref-link-section-d20543e628">1994</a>; Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Ekman P, Friesen WV (1971) Constants across cultures in the face and emotion. J Pers Soc 17(2):124–129" href="/article/10.1007/s10055-013-0231-z#ref-CR27" id="ref-link-section-d20543e631">1971</a>; Ekman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Ekman P, Friesen WV, O’Sullivan M, Chan A, Diacoyanni-Tarlatzis I, Heider K, Ricci-Bitti P (1987) Universals and cultural differences in the judgments of facial expressions of emotion. J Pers Soc Psychol 53(4):712–717" href="/article/10.1007/s10055-013-0231-z#ref-CR28" id="ref-link-section-d20543e635">1987</a>; Matsumoto <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Matsumoto D (2002) Methodological requirements to test a possible in-group advantage in judging emotions across cultures: comment on Elfenbein and Ambady (2002) and evidence. Psychol Bull 128(2):236–242" href="/article/10.1007/s10055-013-0231-z#ref-CR62" id="ref-link-section-d20543e638">2002</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Matsumoto D (2007) Emotion judgments do not differ as a function of perceived nationality. Int J Psychol 42(3):207–214" href="/article/10.1007/s10055-013-0231-z#ref-CR63" id="ref-link-section-d20543e641">2007</a>).</p><p>The question of impact of cultural background can be extended to human–virtual human interaction. Although various studies show that people can correctly identify emotions expressed by embodied agents in general (Bartneck <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bartneck C (2001) Affective expressions of machines. CHI ‘01 extended abstracts on human factors in computing systems, pp 189–190" href="/article/10.1007/s10055-013-0231-z#ref-CR3" id="ref-link-section-d20543e647">2001</a>; Schiano et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Schiano DJ, Ehrlich SM, Rahardja K, Sheridan K (2000) Face to interface: facial affect in (hu)man and machine. Proc ACM CHI 2000:193–200" href="/article/10.1007/s10055-013-0231-z#ref-CR83" id="ref-link-section-d20543e650">2000</a>), how good this performance is retained in different cultures needs to be considered. Clear indications support the statement that culture can shape the expression and interpretation of emotions (Keltner and Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Keltner D, Ekman P (2000) Facial expression of emotion. Handbook of emotions, 2nd edn. pp 236–249" href="/article/10.1007/s10055-013-0231-z#ref-CR48" id="ref-link-section-d20543e653">2000</a>). Culture as a factor has also been studied in the interaction with computers. For example, Dotsch and Wigboldus (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dotsch R, Wigboldus DHJ (2008) Virtual prejudice. J Exp Soc Psychol 44(4):1194–1198" href="/article/10.1007/s10055-013-0231-z#ref-CR25" id="ref-link-section-d20543e656">2008</a>) and Brinkman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Brinkman W-P, Veling W, Dorrestijn E, Sandino G, Vakili V, van der Gaag M (2011) Virtual reality to study responses to social environmental stressors in individuals with and without psychosis. Stud Health Technol Inf 167:86–91" href="/article/10.1007/s10055-013-0231-z#ref-CR8" id="ref-link-section-d20543e659">2011</a>) have found a difference in emotional reaction to a virtual human with ethnic appearance that match or did not match the person’s ethnicity. Endrass et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Endrass B, Rehm M, Lipi A (2011) Culture-related differences in aspects of behavior for virtual characters across Germany and Japan. In: Proceedings of AAMAS’11, 2, pp 441–448" href="/article/10.1007/s10055-013-0231-z#ref-CR34" id="ref-link-section-d20543e663">2011</a>) show that in German and Japanese cultures, the user’s perception of an agent conversation can be enhanced by a culturally prototypical performance of gestures and body postures. Kleinsmith et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kleinsmith A, De Silva PR, Bianchi-Berthouze N (2006) Cross-cultural differences in recognizing affect from body posture. Interact Comput 18(6):1371–1389" href="/article/10.1007/s10055-013-0231-z#ref-CR50" id="ref-link-section-d20543e666">2006</a>) worked on the cross-cultural difference of recognizing affect from virtual human’s body posture and suggest to consider culture as one specific factor for the implementation of agents. Meanwhile, Jan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Jan D, Herrera D, Martinovski B (2007) A computational model of culture-specific conversational behavior. In: IVA ‘07 Proceedings of the 7th international conference on intelligent virtual agents, pp 45–56" href="/article/10.1007/s10055-013-0231-z#ref-CR46" id="ref-link-section-d20543e669">2007</a>) mention that in Arabian and US American cultures, gaze, proximity and turn-taking behavior are all culture related. These results reveal that participants perceive behavior that is in line with their own cultural background differently from behavior that is typical for a different cultural background. In the work presented in this paper, cultural background is considered as a variable which is expected to influence how people perceive the emotional expression of the virtual human.</p><h3 class="c-article__sub-heading" id="Sec7">Dimensional emotion model</h3><p>For facial expressions, six universal basic emotions exist (Ekman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Ekman P, Rolls ET, Perrett DI, Ellis HD (1992) Facial expressions of emotion: an old controversy and new findings. Philos Trans Biol Sci 335:63–69" href="/article/10.1007/s10055-013-0231-z#ref-CR29" id="ref-link-section-d20543e680">1992</a>). However, for language, people’s categorization of verbal labels to describe their everyday life emotions vary between languages and cultures (Russell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Russell JA (1991) Culture and the categorization of emotions. Psychol Bull 110:426–450" href="/article/10.1007/s10055-013-0231-z#ref-CR79" id="ref-link-section-d20543e683">1991</a>). Instead of placing these expressed emotions in categories, i.e., a discrete emotional approach, others suggest placing them in a multi-dimensional space, i.e., a dimensional approach (Fox <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Fox E (2008) Emotion science cognitive and neuroscientific approaches to understanding human emotions. Palgrave Macmillan, UK" href="/article/10.1007/s10055-013-0231-z#ref-CR37" id="ref-link-section-d20543e686">2008</a>). Three broad dimensions have often been proposed to describe affect (Mehrabian and Russell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1974" title="Mehrabian A, Russell JA (1974) An approach to environmental psychology. MIT Press, Cambridge, MA" href="/article/10.1007/s10055-013-0231-z#ref-CR64" id="ref-link-section-d20543e689">1974</a>): i.e., valence, arousal and dominance. Valence is variously referred to as positive and negative affect or as pleasant and unpleasant feelings. The arousal dimension ranges emotions from deep sleep to frenetic excitement. Dominance focuses on the expression of social control and aggression and varies between submissive and dominant (Schroder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Schroder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. Research Report of the Institute of Phonetics" href="/article/10.1007/s10055-013-0231-z#ref-CR84" id="ref-link-section-d20543e692">2004</a>). Compared to the discrete emotional approach, the dimensional approach often uses subjective reports of feelings as its main dependent variable. As such, it has a strong empirical base. Support for the existence of these dimensions has come from research into subjective reports, physiological responses, neural circuits and cognitive appraisal (Barrett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Barrett LF (2006) Solving the emotion paradox: categorization and the experience of emotion. Pers Soc Psychol Rev Off J Soc Pers Soc Psychol 10:20–46. doi:&#xA;                    10.1207/s15327957pspr1001_2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0231-z#ref-CR2" id="ref-link-section-d20543e696">2006</a>; Fox <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Fox E (2008) Emotion science cognitive and neuroscientific approaches to understanding human emotions. Palgrave Macmillan, UK" href="/article/10.1007/s10055-013-0231-z#ref-CR37" id="ref-link-section-d20543e699">2008</a>). Furthermore, Wierzbicka (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Wierzbicka A (1995) Emotions across languages and cultures: diversity and universals. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-013-0231-z#ref-CR90" id="ref-link-section-d20543e702">1995</a>) and Church and Katigbak (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Church T, Katigbak M (1998) Language and organisation of Filipino emotion concepts: comparing emotion concepts and dimensions across cultures. Cogn Emot 12(1):63–92" href="/article/10.1007/s10055-013-0231-z#ref-CR20" id="ref-link-section-d20543e705">1998</a>) also investigated the cross-cultural universality of the emotional dimensions. Their results showed the universality of the valence and arousal dimensions. The study presented in this paper focuses on the valence dimension only. Although participants were asked to rate the virtual human’s emotion on all the three dimensions, only the valence dimension was used for data analysis.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Experiment</h2><div class="c-article-section__content" id="Sec8-content"><h3 class="c-article__sub-heading" id="Sec9">Participants</h3><p>Twelve Chinese (7 females and 5 males) and twelve non-Chinese (5 females and 7 males) students from the Delft University of Technology participated in the experiment. Their age ranged from 24 to 38 years with a mean of 27.8 (SD = 3.4) years. All participants were naive with respect to the hypotheses. Written informed consent forms were obtained from all the participants. The experiment was approved by the university ethic committee.</p><h3 class="c-article__sub-heading" id="Sec10">Creating the virtual human</h3><p>Although Cowell and Stanney (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Cowell AJ, Stanney KM (2003) Embodiment and interaction guidelines for designing credible, trustworthy embodied conversational agents. In: 4th international workshop on intelligent virtual agents IVA 2003, 2792, pp 301–309" href="/article/10.1007/s10055-013-0231-z#ref-CR23" id="ref-link-section-d20543e728">2003</a>) found that users generally prefer to interact with a youthful character matching their ethnicity, they found no significant preference for character gender. Furthermore, Kulms et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kulms P, Kramer NC, Gratch J, Kang S-H (2011) It’s in their eyes: a study on female and male virtual humans' gaze. In: IVA’11 Proceedings of the 11th international conference on intelligent virtual agents, pp 80–92" href="/article/10.1007/s10055-013-0231-z#ref-CR51" id="ref-link-section-d20543e731">2011</a>) showed that actual behavior and its evaluation are more important for the evaluation than gender stereotypes. Therefore, a Chinese virtual lady aged around 25 years was specially created for this study.</p><p>The model of the virtual human was created by FaceGen and 3Ds MAX. All main factors which were considered to contribute to emotion expression were combined; the virtual human’s facial expression, her head and eye movements and her voice intonation were manipulated to express emotion during the conversation. To create facial expressions, an easily repeated facial expression animation method was used. This method rigged the face mesh into 22 action units with 18 features (Gratch et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Gratch J, Rickel J, Andre E, Cassell J, Petajan E, Badler NI, Jeff R (2002) Creating interactive virtual humans: some assembly required. IEEE Intell Syst 17(4):54–63" href="/article/10.1007/s10055-013-0231-z#ref-CR38" id="ref-link-section-d20543e737">2002</a>), where each feature was an anchor point attached to a set of vertices of the face. A model for the face dynamics that was able to control the intensity of the expression, its onset, peak and decay was defined. As such, the virtual human had the ability to show any intensity and any combination of the six basic Ekman facial expressions (Ekman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Ekman P, Friesen WV, Hager JC (2002) Facial action coding system. Human Face 97:4–5" href="/article/10.1007/s10055-013-0231-z#ref-CR30" id="ref-link-section-d20543e740">2002</a>). The validation of this approach was shown by Broekens et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012b" title="Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Interactive Intelligence, Delft University of Technology, pp 1–30" href="/article/10.1007/s10055-013-0231-z#ref-CR15" id="ref-link-section-d20543e743">2012b</a>). By setting the values for the three emotional dimensions (i.e., the valence, arousal and dominance) and for the expression duration, any emotion could be expressed by the virtual human. The facial expressions from neutral to negative or from neutral to positive, used by the virtual human in our experiment, are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Emotions expressed by moving some action units (i.e., the small squares in the figure) of the face mesh. <i>Left column</i> emotions changing from neutral to negative; <i>right column</i> emotions changing from neutral to positive</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The participants were asked to judge the emotional state of the virtual human, and so, there was no interaction between the participant and the virtual human. The participant was told that the scene contained a virtual lady talking with a human, but that the human voice was removed. Therefore, problems related to timing (i.e., whether the virtual human should or should not show an expression at a certain point of time) were avoided, and the participant could focus on the emotional behavior of the virtual human herself.</p><p>Seven conditions were included in the experiment, all varying in the emotional states of the virtual human. Since the scenario was conversation based, two continuously alternating phases could be identified, i.e., one in which the virtual human was speaking and one in which she was listening. These phases allowed the virtual human to express her emotion differently in the two phases. In the speaking phase, the virtual human used voice and nonverbal communications to express her emotions, while in the listening phase, the virtual human only used nonverbal communication to express her emotions. Three emotional states were created for both phases (i.e., positive, neutral and negative states), and they formed the basis for the seven different conditions, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig2">2</a>. As the combination of positive (negative) listening and negative (positive) speaking included contradictory emotional information of the virtual human in the speaking and listening phases, these combinations were considered unnatural and so were excluded from the experiment. Taking the neutral attitude in both the speaking and listening phases as the baseline, it was expected that participants would give a higher valence score when the virtual human responded positively either in the listening or speaking phase. Assuming that there would be no interaction between the speaking and listening phase and that both phases would have a similar impact on the expressed valence intensity, the seven conditions could be ordered into five groups: highly negative (<i>S</i> − <i>L</i>−), lowly negative (<i>S</i> − <i>L0</i>, <i>S0L</i>−), neutral (<i>S0L0</i>), lowly positive (<i>S</i> + <i>L0</i>, <i>S0L</i>+) and highly positive (<i>S</i> + <i>L</i>+). If the intensity of the expressions with a negative or positive valence would be equal, these five groups could be projected on a single valence scale as is done in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig2">2</a> (shown as the predicted valence value axis). Comparing the actual valence values obtained in the experiment to the predicted valence values would make it possible to study hypothesis 2 about the experience of the valence intensity in the two phases of the conversation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Seven conditions, existing of combinations of an emotional state in the speaking and listening phase of a conversation, as used in the experiment and their corresponding predicted valence intensity</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The participants were asked to sit in front of the virtual human (displayed only above her chest on a computer screen), right at the place where the virtual human’s “conversational partner” would sit. With this setup, the participants could well perceive the virtual human’s emotional state, expressed by her vocal expression, facial expression and eyes and head movements. When expressing a positive emotional state, the virtual human would show a happy facial expression and once in a while would nod her head to agree with her conversation partner. Her eyes would mainly look at her conversational partner, only occasionally look away (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig3">3</a>c). When expressing a negative emotional state, the virtual human would have an angry facial expression and would continuously look away showing limited interest in her conversation partner (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig3">3</a>a). The intensity of both the positive (happy) and negative (angry) emotional expressions was evaluated in a previous study (Broekens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012b" title="Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Interactive Intelligence, Delft University of Technology, pp 1–30" href="/article/10.1007/s10055-013-0231-z#ref-CR15" id="ref-link-section-d20543e848">2012b</a>) to ensure that they both could be identified by individuals. The neutral expression was the default facial expression of FaceGen, with the six Ekman basic emotion (Ekman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Ekman P, Rolls ET, Perrett DI, Ellis HD (1992) Facial expressions of emotion: an old controversy and new findings. Philos Trans Biol Sci 335:63–69" href="/article/10.1007/s10055-013-0231-z#ref-CR29" id="ref-link-section-d20543e851">1992</a>) parameters set to zero and with all other morph modifiers removed when generating the face model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Different emotional states of the virtual human in her listening phase. <b>a</b> Negative: angry facial expression, only looking at her conversation partner at the beginning, gradually losing interest and starting to look around. <b>b</b> Neutral: neutral facial expression while constantly looking at her conversation partner with slight eye movements. <b>c</b> Positive: happy facial expression while constantly looking at her conversation partner, showing some slight eye movements and occasionally nodding her head</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the speaking phase, the virtual human would look directly at her conversation partner. In the negative speaking condition, she would have a negative facial expression (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig4">4</a>a), while in the positive speaking condition, she would have a positive facial expression (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig4">4</a>c). In addition, speech with either a negative or positive intonation was added to the virtual human.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Different emotional states of the virtual human in her speaking phase. <b>a</b> Negative: angry facial expression while looking at her conversation partner, and speaking with a negative voice intonation. <b>b</b> Neutral: neutral facial expression while constantly looking at her conversation partner, and speaking with a neutral voice intonation. <b>c</b> Positive: happy facial expression while constantly looking at her conversation partner, and speaking with a positive voice intonation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec11">Emotion validation</h3><p>As mentioned already above, for the speaking phase, verbal communication was added to the virtual human. The voice of the virtual human was recorded in Chinese by a Chinese linguistics student. Her voice was recorded 3 times, each time expressing a different emotional state: positive, neutral and negative. A small separate study, in which 6 Chinese participants, 3 males and 3 females with an average age of 27 (SD = 0.5) years, were asked to rate the valence of the recorded voice on a scale from 1 (negative) to 9 (positive), showed that the emotion in the recorded voice was indeed perceived as intended, <i>F</i>(2,10) = 25.29, <i>p</i> &lt; .001. The negative voice was significantly lower than the neutral voice, <i>t</i>(5) = 3.87, <i>p</i> = .012, and the positive voice, <i>t</i>(5) = 6.52, <i>p</i> &lt; .001. Further, the positive voice was significantly higher than the neutral voice, <i>t</i>(5) = 3.61, <i>p</i> = .015. The means and standard deviations of the scores on the positive, the neutral and the negative voice were <i>M</i> = 7.8, SD = 1.9; <i>M</i> = 5.7, SD = 2.0; <i>M</i> = 1.7; SD = .8, respectively.</p><p>Making a fair comparison between the listening and speaking phases requires that the intensity of the nonverbal communication is similar in both phases. For example, the virtual human’s facial and body expression in the lowly negative speaking phase and lowly negatively listening phase (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig2">2</a>) should have a similar impact on the valence intensity. To test this, an additional small study was conducted. In this study, twelve participants, 5 males and 7 females with an average age of 27 years (SD = 1.8), were presented simultaneously with two video clips of the virtual human including both the listening and speaking phases. Half of the participants were Chinese. The participants were asked to rate how easily they could see the difference between the two videos on a scale from very easy (0) to very difficult (100). The participants were explicitly asked not to rate the valence, but only the easiness with which differences were perceived, representing the intensity of the emotion. The videos were presented without sound. The participants were asked to rate 12 pairs in total (<i>S</i> − <i>L0</i>/<i>S0L0</i>, <i>S0L</i>−/<i>S0L0</i>, <i>S</i> + <i>L0</i>/<i>S0L0</i>, <i>S0L</i>+/<i>S0L0</i>, <i>S</i> − <i>L</i>−/<i>S</i> + <i>L</i>+, <i>S</i> − <i>L</i>−/<i>S0L0</i>, <i>S</i> + <i>L</i>+/<i>S0L0</i>, <i>S0L0</i>/<i>S0L0</i>, <i>S</i> + <i>L0</i>/<i>S</i> + <i>L0</i>, <i>S</i> − <i>L0</i>/<i>S</i> − <i>L0</i>, <i>S0L</i>+/<i>S0L</i>+, <i>S0L</i>−/<i>S0L</i>−), presented to each participant in a different random order. Before they rated the pairs, the participants were shown all the possible behaviors of the virtual human so that they could establish an overall frame of reference.</p><p>The first step of the analysis was to see whether the more intense stimuli were easier to distinguish from the neutral reference video (<i>S0L0</i>) and whether the positive and negative videos were equally distinctive. Therefore, a MANOVA with repeated measures was conducted with the intensity of the video stimuli (high versus low intensity) and the valence direction (positive versus negative) as independent variables. The analysis was conducted on the rating for highly positive (<i>S</i> + <i>L</i>+/<i>S0L0</i>) and negative (<i>S</i> − <i>L</i>−/<i>S0L0</i>) videos, and the mean rating for lowly positive (<i>S</i> + <i>L0</i>/<i>S0L0</i> and <i>S0L</i>+/<i>S0L0</i>) and negative (<i>S</i> − <i>L0</i>/<i>S0L0</i> and <i>S0L</i>−/<i>S0L0</i>) videos across the speaking and listening phases. The analysis found a significant main effect (<i>F</i>(1, 11) = 21.91, <i>p</i> = 0.001) for intensity, in that the highly positive or negative videos (<i>M</i> = 32, SD = 17) were rated as easier to be distinguished than the lowly positive or negative videos (<i>M</i> = 44, SD = 15). Also a significant (<i>F</i>(1, 11) = 15.63, <i>p</i> = 0.002) main effect was found for direction. The positive videos (<i>M</i> = 25, SD = 15) were rated as more easily to be distinguished from the neutral video than the negative videos (<i>M</i> = 50, SD = 23). The analysis found no significant (<i>F</i>(1, 11) = 1.60, <i>p</i> = 0.23) two-way interaction effect, which suggests that the two main effects were constant across the conditions.</p><p>The next analysis focused on the question whether, compared to the neutral reference video, the positive or negative differences in the listing or speaking phase were equally distinguishable, and whether this was the same for the positive and negative videos. Therefore, a second MANOVA with repeated measures was conducted with the valence direction and the phase (speaking versus listening) as independent variables. The analysis used the rating for lowly positive speaking (<i>S</i> + <i>L0</i>/<i>S0L0</i>) and lowly positive listening (<i>S0L</i>+/<i>S0L0</i>) phases and the rating for the lowly negative (<i>S</i> − <i>L0</i>/<i>S0L0</i>) speaking and lowly listening (<i>S0L</i>−/<i>S0L0</i>) phases. The analysis again revealed that the positive videos (<i>M</i> = 28, SD = 16) were significantly (<i>F</i>(1, 11) = 16.91, <i>p</i> = 0.002) rated as more easily to be distinguished than the negative videos (<i>M</i> = 59, SD = 24) from the neutral reference video. No significant difference was found between the listening and speaking phases (<i>F</i>(1, 11) = 0.14, <i>p</i> = 0.71), and also, no significant two-way interaction effect was found (<i>F</i>(1, 11) = 0.44, <i>p</i> = 0.52). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig5">5</a> shows the videos with their predicted valence and the estimated valence. The latter is the <i>z</i> score of the rating for the video subtracted from the rating of the neutral reference video (<i>S0L0/S0L0</i>), whereby the rating of negative videos was multiplied by −1. Both the two lowly negative and the two lowly positive videos are positioned closely together. In other words, the intensity of the nonverbal communication seems similar in the listening and speaking phases. Furthermore, because of the significant difference in rating between negative and positive videos, the neutral reference video seems to be positioned closer to the negative videos than to the positive videos. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig5">5</a>, the predicted and estimated valence values for the videos do not follow a linear function, but rather a cubic function. By using a fitted inverted cubic function, the intensity weighted predicted valence values for the videos were calculated from the estimated valence values, thereby creating values of intended valence intensity to be compared with the perceived valence rating of videos later in the paper.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Predicted valence plotted against the estimated valence fitted with a cubic function</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec12">Measurements</h3><p>There are various ways to quantitatively measure the three emotional dimensions (i.e., valence, arousal and dominance). To ensure the reliability of the emotion measurement, two subjective self-reporting instruments were included in this study: the Self-Assessment Manikin Questionnaire (SAM) (Lang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Lang PJ (1995) The emotion probe. Studies of motivation and attention. Am Psychol 50(5):372–385" href="/article/10.1007/s10055-013-0231-z#ref-CR53" id="ref-link-section-d20543e1260">1995</a>) and the AffectButton (AFB) (Broekens and Brinkman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Broekens J, Brinkman W-P (2009) Affectbutton: towards a standard for dynamic affective user feedback. Affect Comput Intel Interact Workshops 2009:1–8" href="/article/10.1007/s10055-013-0231-z#ref-CR10" id="ref-link-section-d20543e1263">2009</a>).</p><p>The SAM questionnaire consists of a series of manikin figures to judge the affective quality (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig6">6</a>). As a nonverbal rating system, the SAM questionnaire represents the intensity value of the three dimensions of emotion: valence, arousal and dominance (Lang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Lang PJ (1995) The emotion probe. Studies of motivation and attention. Am Psychol 50(5):372–385" href="/article/10.1007/s10055-013-0231-z#ref-CR53" id="ref-link-section-d20543e1272">1995</a>). The first row of SAM manikin figures ranges from unhappy to happy on the valence dimension. The second row represents the arousal dimension, ranging from relaxed to excited. The last row ranges from dominated to controlling, representing the dominance dimension. When instructed on how to use the SAM questionnaire according to the detailed explanation, provided in the instruction manual of Lang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lang PJ, Bradley MM, Cuthbert BN (2008) International affective picture system (IAPS): affective ratings of pictures and instruction manual. Technical Report A-8" href="/article/10.1007/s10055-013-0231-z#ref-CR55" id="ref-link-section-d20543e1275">2008</a>), participants can select one of the nine figures on each row to express their feelings about the emotional stimulus. The manikin figures were taken from the PXLab (Irtel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Irtel H (2007) PXLab: the psychological experiments laboratory [online]. University of Mannheim, Mannheim" href="/article/10.1007/s10055-013-0231-z#ref-CR43" id="ref-link-section-d20543e1278">2007</a>). Various studies show that the SAM questionnaire accurately measures emotional reactions to imagery (Lang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lang PJ, Bradley MM, Cuthbert BN (1999) International affective picture system (IAPS): technical manual and affective ratings. Psychology. The Center for Research in Psychophysiology, University of Florida, Gainesville, FL" href="/article/10.1007/s10055-013-0231-z#ref-CR54" id="ref-link-section-d20543e1281">1999</a>; Morris <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Morris JD (1995) Observations: SAM the self-assessment manikin an efficient cross-cultural measurement of emotional response. J Advert Res 35(6):63–68" href="/article/10.1007/s10055-013-0231-z#ref-CR67" id="ref-link-section-d20543e1285">1995</a>), sounds (Bradley and Lang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bradley MM, Lang PJ (2007) The international affective digitized sounds (2nd Edition; IADS-2): affective ratings of sounds and instruction manual. Technical report B-3. University of Florida, Gainesville, Fl" href="/article/10.1007/s10055-013-0231-z#ref-CR5" id="ref-link-section-d20543e1288">2007</a>), robot gesture expression (Haring et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Haring M, Bee N, Andre E (2011) Creation and evaluation of emotion expression with body movement, sound and eye color for humanoid robots. In: RO-MAN, 2011 IEEE, pp 204–209" href="/article/10.1007/s10055-013-0231-z#ref-CR40" id="ref-link-section-d20543e1291">2011</a>), etc.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Self-Assessment Manikin Questionnaire, three rows representing the valence, arousal and dominance dimension, respectively. Copyright © 2001–2006, Hans Irtel. Distributed under the MIT License as certified by the Open Source Initiative</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The AffectButton (AFB) offers a flexible and dynamic way to collect users’ explicit affective feedback (Broekens and Brinkman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Broekens J, Brinkman W-P (2009) Affectbutton: towards a standard for dynamic affective user feedback. Affect Comput Intel Interact Workshops 2009:1–8" href="/article/10.1007/s10055-013-0231-z#ref-CR10" id="ref-link-section-d20543e1315">2009</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Broekens J, Brinkman W-P (2013) AffectButton: a method for reliable and valid affective self-report. Int J Hum Comput Stud 71(6):641–667" href="/article/10.1007/s10055-013-0231-z#ref-CR11" id="ref-link-section-d20543e1318">2013</a>). The AFB is a button like input interface (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig7">7</a>). In essence, the AFB can be regarded as a navigation tool through a large set of facial expressions. The user can freely move the cursor over the face to change its affective state. Similar to the SAM questionnaire, the AFB returns feedback on the valence, arousal and dominance dimensions. Designed with the intention to be a quick and user-friendly explicit emotion measurement instrument, the reliability and validation of the AFB have been studied on measuring emotional reactions to words, feelings and music (Broekens and Brinkman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Broekens J, Brinkman W-P (2009) Affectbutton: towards a standard for dynamic affective user feedback. Affect Comput Intel Interact Workshops 2009:1–8" href="/article/10.1007/s10055-013-0231-z#ref-CR10" id="ref-link-section-d20543e1324">2009</a>; Broekens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Broekens J, Pronker A, Neuteboom M (2010) Real time labeling of affect in music using the affectbutton. In: Proceedings of the 3rd international workshop on affective interaction in natural environments, pp 21–26" href="/article/10.1007/s10055-013-0231-z#ref-CR12" id="ref-link-section-d20543e1327">2010</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>AffectButton (<i>left</i>) and its corresponding appearance samples (<i>right</i>) while moving the cursor (<i>the cross</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec13">Procedure</h3><p>Prior to the experiment, participants were provided with an information sheet, and the procedure was explained to them. They were then asked to sign an informed consent form. The experiment was setup as a within-subject design, comprising seven conditions with different emotional expressions both in the listening and speaking phases. In each condition, the participants were asked to watch a short clip (around 1 min) of a conversation about going to conferences between a Chinese virtual lady and a person. In each clip, the virtual human spoke 10 sentences in total and was silent in between each sentence, listening to her conversational partner talking. The total length of the virtual human’s speaking phase was around 15 s, and the rest of the 45 s was counted as the virtual human’s listening phase. The conversation was in Chinese, and the participants could hear what the virtual human said during the speaking phase; during the listening phase, there was no sound of the virtual human’s conversational partner. The participants were asked to rate the virtual human’s emotional state using both SAM and AFB when they finished watching a clip. The order in which the video clips were shown was randomized across the participants.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Results</h2><div class="c-article-section__content" id="Sec14-content"><p>The experiment had seven conditions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig2">2</a>), with two different measurements and two groups of participants (Chinese and non-Chinese). The data recorded by the SAM questionnaire were integers ranging from 0 to 8, while the data recorded by the AFB were floating-point numbers ranging from −1 to 1. To compare these two measurements, the data were first normalized into <i>z</i> scores per measurement for each participant across the seven conditions.</p><p>The means for the SAM questionnaire and AFB on the valence emotional dimension are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig8">8</a>. A repeated measures MANOVA was conducted to test the difference between SAM and AFB scores, thereby using condition, type of measurement and cultural background as three independent variables, and the <i>z</i> scores on valence as dependent variable. The analysis also included all two-way and three-way interactions. The results showed no significant difference between SAM and AFB measurement, <i>F</i>(1,22) = 1.30, <i>p</i> = .26, and also no significant interaction effect.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Means and standard deviations of SAM and AFB <i>z</i> scores for the valence dimension for each of the seven experimental conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To test the relationship between these two measurements, a correlation analysis between SAM and AFB scores on the valence dimension was performed. The average scores across all participants for the seven conditions were used. The results showed that SAM and AFB were highly correlated with each other on the valence dimension (<i>r</i>(7) = 0.995, <i>p</i> &lt; .001). The valence scores collected by these two measurements could therefore be regarded as consistent. This made it possible to only focus on the average of the SAM and AFB <i>z</i> scores in the remaining analyses.</p><h3 class="c-article__sub-heading" id="Sec15">Chinese versus non-Chinese</h3><p>To test the effect of cultural background on the valence rating of the emotional expressions, a mixed MANOVA was conducted using condition as a within-subjects independent variable, cultural background as a between-subjects independent variable and averaged valence score of both measurements as a dependent variable. The results showed no significant main effect for the cultural background on valence score, <i>F</i>(1,22) = 1.23, <i>p</i> = .64, and no significant interaction between cultural background and condition, <i>F</i>(6,17) = 0.72, <i>p</i> = .28.</p><p>Instead of looking for a difference between participants from different cultural backgrounds, the next step of the analysis focused on similarity in the ratings between these two groups. To examine the relationship between the ratings of Chinese and non-Chinese participants, we performed a correlation analysis based on the means for the seven conditions. The results showed that the scores on valence of the Chinese participants are significantly correlated with those of the non-Chinese participants <i>r</i>(7) = .98, <i>p</i> &lt; .001. Although a difference in cultural background was expected, the result showed a high consistency in the evaluation of the emotional state between participants from different cultures. Hence, the results of the two groups of participants were grouped in the rest of the data analyses.</p><h3 class="c-article__sub-heading" id="Sec16">Positive versus neutral versus negative emotional state</h3><p>Participants were asked to rate seven conditions (i.e., different combinations of a positive, negative and neutral emotional state during the virtual human’s speaking and listening phases). A repeated measures MANOVA was conducted to study the effect of these conditions on averaged valence score of the SAM and AFB <i>z</i> scores. The results showed a significant effect of condition on the valence rating, <i>F</i>(6,18) = 59.50, <i>p</i> &lt; .001. Next, to run a priori comparisons, paired-sample <i>t</i> tests were performed using the averaged valence scores of the SAM and AFB <i>z</i> scores in all the conditions as paired variables. The results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0231-z#Tab1">1</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Mean, SD and mean difference of the valence rating of the different conditions. H<sub>0</sub>: μ<sub>1</sub> = μ<sub>2</sub>, * <i>p</i> &lt; 0.05</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0231-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To test whether the subjective valence score was correlated with the intensity weighted predicted valence values (see chapter 3.3 and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig5">5</a> and hereafter abbreviated as weighted valence values) for each condition, we calculated the Pearson correlation coefficient between the weighted valence values and the subjective scores averaged over the participants across the seven experimental conditions. This correlation was relatively high, <i>r</i>(7) = .93, <i>p</i> = .002. The following step in the analysis was to determine the deviation between the subjective valence scores and their corresponding expected valence value per experimental condition. To do so, we fitted a line through the three data points: <i>S</i> + <i>L</i>+, <i>S0L0</i> and <i>S</i> − <i>L</i>− using least-squares regression. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig9">9</a> shows this line, including the mean subjective valence scores of the remaining four conditions. Deviations of perceived valence from this line (for the lowly negative and positive videos) show to what extent the perceived valence is different from what is expected in case of an equal intensity in valence between the speaking and listening phases (noted as expected valence value in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig9">9</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0231-z/MediaObjects/10055_2013_231_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The relationship between intensity weighted predicted valence and the averaged subjective valence</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0231-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>One-sample <i>t</i> tests revealed that when the virtual human showed neutral listening, both a positive (<i>S</i> + <i>L0</i>) and negative (<i>S</i> − <i>L0</i>) emotional expressions during speaking had a more extreme valence than expected, i.e., the subjective score was more positive than the expected valence value in case of the positive emotional expression (<i>t</i>(23) = 2.69, <i>p</i> = .013) and more negative than the expected valence value in case of a negative emotional expression during speaking (<i>t</i>(23) = −6.14, <i>p</i> &lt; .001). The opposite was seen for the impact of the listening phase. Considering the speaking phase with a neutral emotional expression, the subjective valence score for listening with a positive emotional expression (i.e., the <i>S0L</i>+ condition) was significantly less positive than expected (<i>t</i>(23) = − 3.08, <i>p</i> = .005). Similarly, the subjective valence score for listening with a negative emotional expression (i.e., the <i>S0L</i>− condition) was significantly less negative than the expected valence value (<i>t</i>(23) = 3.38, <i>p</i> = .003).</p><p>Moreover, the subjective valence score for the <i>S0L</i>− condition was almost equal (<i>t</i>(23) = 0.059, <i>p</i> = .95) to the subjective valence score for the <i>S0L0</i> condition (i.e., speaking with a neutral emotional expression and listening with a neutral emotional expression). Still, the subjective valence score of the <i>S0L</i>+ condition (i.e., speaking with a neutral emotional expression and listening with a positive emotional expression) was significantly more positive than that for the <i>S0L0</i> condition ((<i>t</i>(23) = 2.92, <i>p</i> = .008). A direct comparison of the lowly positive or negative conditions provided a similar pattern. The subjective valence value for the <i>S</i> − <i>L0</i> condition (i.e., speaking with a negative emotional expression and listening with a neutral emotional expression) was significantly more negative than the subjective valence value for the <i>S0L</i>− condition (i.e., speaking with a neutral emotional expression and listening with a negative emotional expression), <i>t</i>(23) = 4.97, <i>p</i> &lt; .001. Similarly, the subjective valance value for the <i>S</i> + <i>L0</i> condition (i.e., speaking with a positive emotional expression and listening with a neutral emotional expression) was significantly more positive than the subjective valence value for the <i>S0L</i>+ condition (i.e., speaking with a neutral emotional expression and listening with a positive emotional expression), <i>t</i>(23) = 4.01, <i>p</i> = .001.</p><p>Together these observations imply that people do not perceive much difference between the virtual human showing neutral or negative listening behavior, but they do perceive a difference with the virtual human showing positive listening behavior. In conclusion, all these results support hypothesis 2, stating that the valence of the emotional expression during the listening phase of a conversation is perceived as less impactful compared to the emotional expression during the speaking phase.</p><p>Finally, we also compared the more extreme emotional conditions with the <i>S0L0</i> condition. The <i>S</i> + <i>L</i>+ condition (<i>t</i>(23) = −9.00, <i>p</i> &lt; .001) or <i>S</i> − <i>L</i>− condition (<i>t</i>(23) = 5.16, <i>p</i> &lt; .001) with positive or negative emotional expressions in both the listening and speaking phases, respectively, strongly impact the perceived valence in the expected way.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Discussion and conclusion</h2><div class="c-article-section__content" id="Sec17-content"><p>The experiment described in this paper is a human perception study on positive and negative emotions of a virtual human and how cultural background might affect the perception of these emotions. In a sense, this study can be seen as a re-confirmation in virtual reality of what is known about human–human interaction in the actual world. Still, this is an important validation step as conversations with virtual humans are increasingly used as part of gaming (e.g., Hudlicka and Delft (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Hudlicka E, Delft TU (2009) Foundations for modelling emotions in game characters: modelling emotion effects on cognition. In: Affective computing and intelligent interaction and workshops, ACII 2009" href="/article/10.1007/s10055-013-0231-z#ref-CR42" id="ref-link-section-d20543e2215">2009</a>), training (e.g., Broekens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference (2012)" title="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer J-J (2012) Virtual reality negotiation training increases negotiation knowledge and skill. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 218–230" href="/article/10.1007/s10055-013-0231-z#ref-CR14" id="ref-link-section-d20543e2218">(2012)</a> or psychotherapy (e.g., Opris et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Opris D, Pintea S, Garcia-Palacios A, Botella CM, Szamoskozi S, David D (2012) Virtual reality exposure therapy in anxiety disorders: a quantitative meta-analysis. Depression Anxiety 29:85–93. doi:&#xA;                    10.1002/da.20910&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0231-z#ref-CR69" id="ref-link-section-d20543e2221">2012</a>).</p><p>The study found that both Chinese and non-Chinese participants could perceive the valence of the virtual human’s emotional states, and no significant difference between these two groups was found. Instead, the ratings of these two groups were highly correlated. The results show that the valence of the emotional states of the virtual human can be easily recognized by all participants independent of their cultural backgrounds. Hypothesis 1 is therefore not confirmed. On the contrary, our results support the idea of universality of the facial expression of emotion (Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Ekman P (1994) Strong evidence for universals in facial expressions: a reply to Russell’s mistaken critique. Psychological Bull 115(2):268–287" href="/article/10.1007/s10055-013-0231-z#ref-CR26" id="ref-link-section-d20543e2227">1994</a>; Matsumoto <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Matsumoto D (2007) Emotion judgments do not differ as a function of perceived nationality. Int J Psychol 42(3):207–214" href="/article/10.1007/s10055-013-0231-z#ref-CR63" id="ref-link-section-d20543e2230">2007</a>), and question the need for tailored made virtual reality applications which target different cultural groups or have multi-cultural users. Still, the results of this study may not be generally applicable to all cultures, since we here only evaluated possible differences in emotion perception between Chinese and non-Chinese people. Further studies are needed to extend our conclusion of universality of emotion perception of virtual human to people with other cultural backgrounds.</p><p>In addition, comparing the difference between conditions, it seems that the participants’ perception of the valence was more influenced by the emotion of the virtual human while speaking than while listening; and so, this supports Hypothesis 2. Comparing the subjectively perceived valence scores with the expected valence values (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0231-z#Fig9">9</a>), the valence perceived by the participants in the conditions where the listening was neutral, the speaking performed with a positive or negative emotional expression was significantly more extreme than what was predicted from equal intensity between speaking and listening. Similarly, the perceived valence was less extreme than the weighted valence value when the speaking was neutral, but the listening performed with a positive or negative emotional expression. This shows the additional influence of verbal communication on valence recognition during a human–virtual human conversation. These findings seem to be in contrast to reports of Melo, Carnevale, and Gratch (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Melo Cd, Carnevale P, Gratch J (2011) The effect of expression of anger and happiness in computer agents on negotiations with humans. In: The tenth international conference on autonomous agents and multiagent systems, pp 2–6" href="/article/10.1007/s10055-013-0231-z#ref-CR65" id="ref-link-section-d20543e2239">2011</a>), who claim that there is no difference in emotion perception between verbal and nonverbal communications. Their study however used text typing as verbal communication means between human and virtual human, which might explain the different finding. It seems not surprising that the combination of both verbal and nonverbal communications transfers more emotional information than the nonverbal communication only. Furthermore, the influence of the voice can be regarded as content independent because of the high consistency found between the Chinese and non-Chinese participants in this experiment. In other words, the results suggest that affective aspects can be conveyed in the speech even if the language is not understood.</p><p>The finding that the perceived valence of the emotion of the virtual human is more intense in the speaking phase than in the listening phase of a conversation may be extended with new research on how to control the level of emotion during these separate phases. Applications such as virtual reality exposure therapy for patients suffering from social phobia may be designed in a way to manipulate the potential phobic stressor using the virtual human’s emotional behavior. Further studies may exploit the difference in valence perception between the speaking and listening phases and explore how to further optimize the persuasive power during these two phases, which may be beneficial for the design of many virtual applications involving human–virtual human conversation. Besides, this study only focuses on how individuals perceive the performance of a virtual human. It is also interesting to test the emotional influence on a human during a human–virtual human conversation. Whether the virtual human’s emotion could lead or alter the content of the conversation could be an appealing topic in the persuasive computing area.</p><p>Two main conclusions may be drawn from the experiment, but there are also still a number of limitations. First, the virtual human only showed her upper body, and no gestures were used to express emotion. However, in recent decades, more insights have become available on body expression (Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Gross MM, Crane EA, Fredrickson BL (2010) Methodology for assessing bodily expression of emotion. J Nonverbal Behav 34:223–248. doi:&#xA;                    10.1007/s10919-010-0094-x&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0231-z#ref-CR39" id="ref-link-section-d20543e2248">2010</a>; Kleinsmith and Bianchi-Berthouze <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kleinsmith A, Bianchi-Berthouze N (2013) Affective body expression perception and recognition: a survey. IEEE Trans Affect Comput 4:15–33. doi:&#xA;                    10.1109/T-AFFC.2012.16&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0231-z#ref-CR49" id="ref-link-section-d20543e2251">2013</a>). It would therefore be interesting to examine how our findings would be affected when the virtual human used its full body to express emotions. Second, the position of the virtual human was fixed in the current study. It would be interesting to test the emotional impact of manipulating the virtual human’s position, for example, far away versus nearby (Broekens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012b" title="Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Interactive Intelligence, Delft University of Technology, pp 1–30" href="/article/10.1007/s10055-013-0231-z#ref-CR15" id="ref-link-section-d20543e2254">2012b</a>). Third, the face model of the virtual human we used in this study was generated by FaceGen with the ethnicity parameter set at Southeast Asia. However, no empirical validation was done to confirm the ethnic appearance of the virtual human. Fourth, the study described in this paper only focused on the valence dimension of the emotion, neglecting so far the other two dimensions of emotion, namely arousal and dominance. Including the additional two dimensions would allow to study more complex emotions, for example, fear, surprise, etc. Despite of the limitations, the results of this paper suggest a superior impact on perceiving the virtual human’s emotional state during its speaking phase, and a potential independence of the perceived valence of the virtual human’s emotion with cultural background. These findings could help designers to focus their attention upon creating and evaluating virtual human with appropriate emotional expressions, which may help to improve the overall experience of virtual environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JN. Bailenson, N. Yee, D. Merget, R. Schroeder, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Bailenson JN, Yee N, Merget D, Schroeder R (2006) The effect of behavioral realism and form realism of real-ti" /><p class="c-article-references__text" id="ref-CR1">Bailenson JN, Yee N, Merget D, Schroeder R (2006) The effect of behavioral realism and form realism of real-time avatar faces on verbal disclosure, nonverbal disclosure, emotion recognition, and copresence in dyadic interaction. Presence Teleoper Virtual Environ 15(4):359–372</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20behavioral%20realism%20and%20form%20realism%20of%20real-time%20avatar%20faces%20on%20verbal%20disclosure%2C%20nonverbal%20disclosure%2C%20emotion%20recognition%2C%20and%20copresence%20in%20dyadic%20interaction&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=15&amp;issue=4&amp;pages=359-372&amp;publication_year=2006&amp;author=Bailenson%2CJN&amp;author=Yee%2CN&amp;author=Merget%2CD&amp;author=Schroeder%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LF. Barrett, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Barrett LF (2006) Solving the emotion paradox: categorization and the experience of emotion. Pers Soc Psychol " /><p class="c-article-references__text" id="ref-CR2">Barrett LF (2006) Solving the emotion paradox: categorization and the experience of emotion. Pers Soc Psychol Rev Off J Soc Pers Soc Psychol 10:20–46. doi:<a href="https://doi.org/10.1207/s15327957pspr1001_2">10.1207/s15327957pspr1001_2</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327957pspr1001_2" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20the%20emotion%20paradox%3A%20categorization%20and%20the%20experience%20of%20emotion&amp;journal=Pers%20Soc%20Psychol%20Rev%20Off%20J%20Soc%20Pers%20Soc%20Psychol&amp;doi=10.1207%2Fs15327957pspr1001_2&amp;volume=10&amp;pages=20-46&amp;publication_year=2006&amp;author=Barrett%2CLF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bartneck C (2001) Affective expressions of machines. CHI ‘01 extended abstracts on human factors in computing " /><p class="c-article-references__text" id="ref-CR3">Bartneck C (2001) Affective expressions of machines. CHI ‘01 extended abstracts on human factors in computing systems, pp 189–190</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Bates, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Bates J (1994) The role of emotion in believable agents. Commun ACM 37(7):122–125" /><p class="c-article-references__text" id="ref-CR4">Bates J (1994) The role of emotion in believable agents. Commun ACM 37(7):122–125</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F176789.176803" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20emotion%20in%20believable%20agents&amp;journal=Commun%20ACM&amp;volume=37&amp;issue=7&amp;pages=122-125&amp;publication_year=1994&amp;author=Bates%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bradley MM, Lang PJ (2007) The international affective digitized sounds (2nd Edition; IADS-2): affective ratin" /><p class="c-article-references__text" id="ref-CR5">Bradley MM, Lang PJ (2007) The international affective digitized sounds (2nd Edition; IADS-2): affective ratings of sounds and instruction manual. Technical report B-3. University of Florida, Gainesville, Fl</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brand M (1999) Voice puppetry. In: Proceedings of the 26th annual conference on computer graphics and interact" /><p class="c-article-references__text" id="ref-CR6">Brand M (1999) Voice puppetry. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques SIGGRAPH 99, pp 21–28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bregler C (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH’97, pp 1–8" /><p class="c-article-references__text" id="ref-CR7">Bregler C (1997) Video rewrite: driving visual speech with audio. In: Proceedings of SIGGRAPH’97, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W-P. Brinkman, W. Veling, E. Dorrestijn, G. Sandino, V. Vakili, M. Gaag, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Brinkman W-P, Veling W, Dorrestijn E, Sandino G, Vakili V, van der Gaag M (2011) Virtual reality to study resp" /><p class="c-article-references__text" id="ref-CR8">Brinkman W-P, Veling W, Dorrestijn E, Sandino G, Vakili V, van der Gaag M (2011) Virtual reality to study responses to social environmental stressors in individuals with and without psychosis. Stud Health Technol Inf 167:86–91</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20to%20study%20responses%20to%20social%20environmental%20stressors%20in%20individuals%20with%20and%20without%20psychosis&amp;journal=Stud%20Health%20Technol%20Inf&amp;volume=167&amp;pages=86-91&amp;publication_year=2011&amp;author=Brinkman%2CW-P&amp;author=Veling%2CW&amp;author=Dorrestijn%2CE&amp;author=Sandino%2CG&amp;author=Vakili%2CV&amp;author=Gaag%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brinkman W-P, Hartanto D, Kang N, de Vliegher D, Kampmann IL, Morina N et al (2012) A virtual reality dialogue" /><p class="c-article-references__text" id="ref-CR9">Brinkman W-P, Hartanto D, Kang N, de Vliegher D, Kampmann IL, Morina N et al (2012) A virtual reality dialogue system for the treatment of social phobia. In: Paper presented at the CHI’12 extended abstracts on human factors in computing systems</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Broekens, W-P. Brinkman, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Broekens J, Brinkman W-P (2009) Affectbutton: towards a standard for dynamic affective user feedback. Affect C" /><p class="c-article-references__text" id="ref-CR10">Broekens J, Brinkman W-P (2009) Affectbutton: towards a standard for dynamic affective user feedback. Affect Comput Intel Interact Workshops 2009:1–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affectbutton%3A%20towards%20a%20standard%20for%20dynamic%20affective%20user%20feedback&amp;journal=Affect%20Comput%20Intel%20Interact%20Workshops&amp;volume=2009&amp;pages=1-8&amp;publication_year=2009&amp;author=Broekens%2CJ&amp;author=Brinkman%2CW-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Broekens, W-P. Brinkman, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Broekens J, Brinkman W-P (2013) AffectButton: a method for reliable and valid affective self-report. Int J Hum" /><p class="c-article-references__text" id="ref-CR11">Broekens J, Brinkman W-P (2013) AffectButton: a method for reliable and valid affective self-report. Int J Hum Comput Stud 71(6):641–667</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2013.02.003" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=AffectButton%3A%20a%20method%20for%20reliable%20and%20valid%20affective%20self-report&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=71&amp;issue=6&amp;pages=641-667&amp;publication_year=2013&amp;author=Broekens%2CJ&amp;author=Brinkman%2CW-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Broekens J, Pronker A, Neuteboom M (2010) Real time labeling of affect in music using the affectbutton. In: Pr" /><p class="c-article-references__text" id="ref-CR12">Broekens J, Pronker A, Neuteboom M (2010) Real time labeling of affect in music using the affectbutton. In: Proceedings of the 3rd international workshop on affective interaction in natural environments, pp 21–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer JJ (2011) Validity of a virtual negotiat" /><p class="c-article-references__text" id="ref-CR13">Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer JJ (2011) Validity of a virtual negotiation training. In: IVA’11 Proceedings of the 11th international conference on intelligent virtual agents, pp 435–436 
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer J-J (2012) Virtual reality negotiation t" /><p class="c-article-references__text" id="ref-CR14">Broekens J, Harbers M, Brinkman W-P, Jonker C, Van den Bosch K, Meyer J-J (2012) Virtual reality negotiation training increases negotiation knowledge and skill. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 218–230</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Intera" /><p class="c-article-references__text" id="ref-CR15">Broekens J, Qu C, Brinkman W-P (2012) Dynamic facial expression of emotion made easy. Technical report. Interactive Intelligence, Delft University of Technology, pp 1–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Cassell, KR. Thorisson, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cassell J, Thorisson KR (1999) The power of a nod and a glance: envelope vs. emotional feedback in animated co" /><p class="c-article-references__text" id="ref-CR16">Cassell J, Thorisson KR (1999) The power of a nod and a glance: envelope vs. emotional feedback in animated conversational agents. Appl Artif Intell 13:519–538</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F088395199117360" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20power%20of%20a%20nod%20and%20a%20glance%3A%20envelope%20vs.%20emotional%20feedback%20in%20animated%20conversational%20agents&amp;journal=Appl%20Artif%20Intell&amp;volume=13&amp;pages=519-538&amp;publication_year=1999&amp;author=Cassell%2CJ&amp;author=Thorisson%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T et al (1994) Animated conversation: Rule-base" /><p class="c-article-references__text" id="ref-CR17">Cassell J, Pelachaud C, Badler N, Steedman M, Achorn B, Becket T et al (1994) Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. In: Proceedings of ACM SIGGRAPH, pp 413–420</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cerezo E, Baldassarri S (2008) Affective embodied conversational agents for natural interaction. In: Or J (ed)" /><p class="c-article-references__text" id="ref-CR18">Cerezo E, Baldassarri S (2008) Affective embodied conversational agents for natural interaction. In: Or J (ed) Affective computing: emotion modelling, synthesis and recognition, pp 329–354</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chuang E, Bregler C (2002) Performance driven facial animation using blendshape interpolation. Computer Scienc" /><p class="c-article-references__text" id="ref-CR19">Chuang E, Bregler C (2002) Performance driven facial animation using blendshape interpolation. Computer Science Technical Report, Stanford University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Church, M. Katigbak, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Church T, Katigbak M (1998) Language and organisation of Filipino emotion concepts: comparing emotion concepts" /><p class="c-article-references__text" id="ref-CR20">Church T, Katigbak M (1998) Language and organisation of Filipino emotion concepts: comparing emotion concepts and dimensions across cultures. Cogn Emot 12(1):63–92</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F026999398379781" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Language%20and%20organisation%20of%20Filipino%20emotion%20concepts%3A%20comparing%20emotion%20concepts%20and%20dimensions%20across%20cultures&amp;journal=Cogn%20Emot&amp;volume=12&amp;issue=1&amp;pages=63-92&amp;publication_year=1998&amp;author=Church%2CT&amp;author=Katigbak%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen MM, Massaro DW (1993) Modeling coarticulation in synthetic visual speech. In: Thalman NM, Thalman D (eds" /><p class="c-article-references__text" id="ref-CR21">Cohen MM, Massaro DW (1993) Modeling coarticulation in synthetic visual speech. In: Thalman NM, Thalman D (eds) Models and Techniques in Computer Animation. Springer, Verlag, pp 139–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Core, D. Traum, HC. Lane, W. Swartout, S. Marsella, J. Gratch, M. Lent, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Core M, Traum D, Lane HC, Swartout W, Marsella S, Gratch J, Van Lent M (2006) Teaching negotiation skills thro" /><p class="c-article-references__text" id="ref-CR22">Core M, Traum D, Lane HC, Swartout W, Marsella S, Gratch J, Van Lent M (2006) Teaching negotiation skills through practice and reflection with virtual humans. Simulation 82:685–701</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Teaching%20negotiation%20skills%20through%20practice%20and%20reflection%20with%20virtual%20humans&amp;journal=Simulation&amp;volume=82&amp;pages=685-701&amp;publication_year=2006&amp;author=Core%2CM&amp;author=Traum%2CD&amp;author=Lane%2CHC&amp;author=Swartout%2CW&amp;author=Marsella%2CS&amp;author=Gratch%2CJ&amp;author=Lent%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cowell AJ, Stanney KM (2003) Embodiment and interaction guidelines for designing credible, trustworthy embodie" /><p class="c-article-references__text" id="ref-CR23">Cowell AJ, Stanney KM (2003) Embodiment and interaction guidelines for designing credible, trustworthy embodied conversational agents. In: 4th international workshop on intelligent virtual agents IVA 2003, 2792, pp 301–309</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Darwin, " /><meta itemprop="datePublished" content="1872" /><meta itemprop="headline" content="Darwin C (1872) The expression of emotion in man and animals. Philosophical Library, New York" /><p class="c-article-references__text" id="ref-CR24">Darwin C (1872) The expression of emotion in man and animals. Philosophical Library, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20expression%20of%20emotion%20in%20man%20and%20animals&amp;publication_year=1872&amp;author=Darwin%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Dotsch, DHJ. Wigboldus, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Dotsch R, Wigboldus DHJ (2008) Virtual prejudice. J Exp Soc Psychol 44(4):1194–1198" /><p class="c-article-references__text" id="ref-CR25">Dotsch R, Wigboldus DHJ (2008) Virtual prejudice. J Exp Soc Psychol 44(4):1194–1198</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jesp.2008.03.003" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20prejudice&amp;journal=J%20Exp%20Soc%20Psychol&amp;volume=44&amp;issue=4&amp;pages=1194-1198&amp;publication_year=2008&amp;author=Dotsch%2CR&amp;author=Wigboldus%2CDHJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Ekman P (1994) Strong evidence for universals in facial expressions: a reply to Russell’s mistaken critique. P" /><p class="c-article-references__text" id="ref-CR26">Ekman P (1994) Strong evidence for universals in facial expressions: a reply to Russell’s mistaken critique. Psychological Bull 115(2):268–287</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-2909.115.2.268" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Strong%20evidence%20for%20universals%20in%20facial%20expressions%3A%20a%20reply%20to%20Russell%E2%80%99s%20mistaken%20critique&amp;journal=Psychological%20Bull&amp;volume=115&amp;issue=2&amp;pages=268-287&amp;publication_year=1994&amp;author=Ekman%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, WV. Friesen, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="Ekman P, Friesen WV (1971) Constants across cultures in the face and emotion. J Pers Soc 17(2):124–129" /><p class="c-article-references__text" id="ref-CR27">Ekman P, Friesen WV (1971) Constants across cultures in the face and emotion. J Pers Soc 17(2):124–129</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fh0030377" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Constants%20across%20cultures%20in%20the%20face%20and%20emotion&amp;journal=J%20Pers%20Soc&amp;volume=17&amp;issue=2&amp;pages=124-129&amp;publication_year=1971&amp;author=Ekman%2CP&amp;author=Friesen%2CWV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, WV. Friesen, M. O’Sullivan, A. Chan, I. Diacoyanni-Tarlatzis, K. Heider, P. Ricci-Bitti, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Ekman P, Friesen WV, O’Sullivan M, Chan A, Diacoyanni-Tarlatzis I, Heider K, Ricci-Bitti P (1987) Universals a" /><p class="c-article-references__text" id="ref-CR28">Ekman P, Friesen WV, O’Sullivan M, Chan A, Diacoyanni-Tarlatzis I, Heider K, Ricci-Bitti P (1987) Universals and cultural differences in the judgments of facial expressions of emotion. J Pers Soc Psychol 53(4):712–717</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0022-3514.53.4.712" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Universals%20and%20cultural%20differences%20in%20the%20judgments%20of%20facial%20expressions%20of%20emotion&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=53&amp;issue=4&amp;pages=712-717&amp;publication_year=1987&amp;author=Ekman%2CP&amp;author=Friesen%2CWV&amp;author=O%E2%80%99Sullivan%2CM&amp;author=Chan%2CA&amp;author=Diacoyanni-Tarlatzis%2CI&amp;author=Heider%2CK&amp;author=Ricci-Bitti%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, ET. Rolls, DI. Perrett, HD. Ellis, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Ekman P, Rolls ET, Perrett DI, Ellis HD (1992) Facial expressions of emotion: an old controversy and new findi" /><p class="c-article-references__text" id="ref-CR29">Ekman P, Rolls ET, Perrett DI, Ellis HD (1992) Facial expressions of emotion: an old controversy and new findings. Philos Trans Biol Sci 335:63–69</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1098%2Frstb.1992.0008" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20expressions%20of%20emotion%3A%20an%20old%20controversy%20and%20new%20findings&amp;journal=Philos%20Trans%20Biol%20Sci&amp;volume=335&amp;pages=63-69&amp;publication_year=1992&amp;author=Ekman%2CP&amp;author=Rolls%2CET&amp;author=Perrett%2CDI&amp;author=Ellis%2CHD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Ekman, WV. Friesen, JC. Hager, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Ekman P, Friesen WV, Hager JC (2002) Facial action coding system. Human Face 97:4–5" /><p class="c-article-references__text" id="ref-CR30">Ekman P, Friesen WV, Hager JC (2002) Facial action coding system. Human Face 97:4–5</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20action%20coding%20system&amp;journal=Human%20Face&amp;volume=97&amp;pages=4-5&amp;publication_year=2002&amp;author=Ekman%2CP&amp;author=Friesen%2CWV&amp;author=Hager%2CJC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Elfenbein, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Elfenbein H (2003) Universals and cultural differences in recognizing emotions. Curr Dir Psychol Sci 12(5):159" /><p class="c-article-references__text" id="ref-CR31">Elfenbein H (2003) Universals and cultural differences in recognizing emotions. Curr Dir Psychol Sci 12(5):159–164</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Universals%20and%20cultural%20differences%20in%20recognizing%20emotions&amp;journal=Curr%20Dir%20Psychol%20Sci&amp;volume=12&amp;issue=5&amp;pages=159-164&amp;publication_year=2003&amp;author=Elfenbein%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HA. Elfenbein, N. Ambady, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Elfenbein HA, Ambady N (2002) Is there an in-group advantage in emotion recognition? Psychol Bull 128(2):243–2" /><p class="c-article-references__text" id="ref-CR32">Elfenbein HA, Ambady N (2002) Is there an in-group advantage in emotion recognition? Psychol Bull 128(2):243–249</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-2909.128.2.243" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20there%20an%20in-group%20advantage%20in%20emotion%20recognition%3F&amp;journal=Psychol%20Bull&amp;volume=128&amp;issue=2&amp;pages=243-249&amp;publication_year=2002&amp;author=Elfenbein%2CHA&amp;author=Ambady%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Elfenbein HA, Beaupre M, Levesque M, Hess U (2007) Toward a dialect theory: cultural differences in the expres" /><p class="c-article-references__text" id="ref-CR33">Elfenbein HA, Beaupre M, Levesque M, Hess U (2007) Toward a dialect theory: cultural differences in the expression and recognition of posed facial expressions. Emotion (Washington, DC) 7(1):131–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Endrass B, Rehm M, Lipi A (2011) Culture-related differences in aspects of behavior for virtual characters acr" /><p class="c-article-references__text" id="ref-CR34">Endrass B, Rehm M, Lipi A (2011) Culture-related differences in aspects of behavior for virtual characters across Germany and Japan. In: Proceedings of AAMAS’11, 2, pp 441–448</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Ersotelos, F. Dong, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Ersotelos N, Dong F (2008) Building highly realistic facial modeling and animation: a survey. Visual Comput 24" /><p class="c-article-references__text" id="ref-CR35">Ersotelos N, Dong F (2008) Building highly realistic facial modeling and animation: a survey. Visual Comput 24(1):13–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-007-0175-y" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Building%20highly%20realistic%20facial%20modeling%20and%20animation%3A%20a%20survey&amp;journal=Visual%20Comput&amp;volume=24&amp;issue=1&amp;pages=13-30&amp;publication_year=2008&amp;author=Ersotelos%2CN&amp;author=Dong%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ezzat T, Geiger G, Poggio T (2004) Trainable videorealistic speech animation. In: Sixth IEEE international con" /><p class="c-article-references__text" id="ref-CR36">Ezzat T, Geiger G, Poggio T (2004) Trainable videorealistic speech animation. In: Sixth IEEE international conference on automatic face and gesture recognition 2004 proceedings, pp 57–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. Fox, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Fox E (2008) Emotion science cognitive and neuroscientific approaches to understanding human emotions. Palgrav" /><p class="c-article-references__text" id="ref-CR37">Fox E (2008) Emotion science cognitive and neuroscientific approaches to understanding human emotions. Palgrave Macmillan, UK</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20science%20cognitive%20and%20neuroscientific%20approaches%20to%20understanding%20human%20emotions&amp;publication_year=2008&amp;author=Fox%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Gratch, J. Rickel, E. Andre, J. Cassell, E. Petajan, NI. Badler, R. Jeff, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Gratch J, Rickel J, Andre E, Cassell J, Petajan E, Badler NI, Jeff R (2002) Creating interactive virtual human" /><p class="c-article-references__text" id="ref-CR38">Gratch J, Rickel J, Andre E, Cassell J, Petajan E, Badler NI, Jeff R (2002) Creating interactive virtual humans: some assembly required. IEEE Intell Syst 17(4):54–63</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMIS.2002.1024753" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Creating%20interactive%20virtual%20humans%3A%20some%20assembly%20required&amp;journal=IEEE%20Intell%20Syst&amp;volume=17&amp;issue=4&amp;pages=54-63&amp;publication_year=2002&amp;author=Gratch%2CJ&amp;author=Rickel%2CJ&amp;author=Andre%2CE&amp;author=Cassell%2CJ&amp;author=Petajan%2CE&amp;author=Badler%2CNI&amp;author=Jeff%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MM. Gross, EA. Crane, BL. Fredrickson, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Gross MM, Crane EA, Fredrickson BL (2010) Methodology for assessing bodily expression of emotion. J Nonverbal " /><p class="c-article-references__text" id="ref-CR39">Gross MM, Crane EA, Fredrickson BL (2010) Methodology for assessing bodily expression of emotion. J Nonverbal Behav 34:223–248. doi:<a href="https://doi.org/10.1007/s10919-010-0094-x">10.1007/s10919-010-0094-x</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10919-010-0094-x" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Methodology%20for%20assessing%20bodily%20expression%20of%20emotion&amp;journal=J%20Nonverbal%20Behav&amp;doi=10.1007%2Fs10919-010-0094-x&amp;volume=34&amp;pages=223-248&amp;publication_year=2010&amp;author=Gross%2CMM&amp;author=Crane%2CEA&amp;author=Fredrickson%2CBL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haring M, Bee N, Andre E (2011) Creation and evaluation of emotion expression with body movement, sound and ey" /><p class="c-article-references__text" id="ref-CR40">Haring M, Bee N, Andre E (2011) Creation and evaluation of emotion expression with body movement, sound and eye color for humanoid robots. In: RO-MAN, 2011 IEEE, pp 204–209</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Hofstede, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hofstede G (2001) Culture’s consequences: comparing values, behaviors, institutions and organisations across n" /><p class="c-article-references__text" id="ref-CR41">Hofstede G (2001) Culture’s consequences: comparing values, behaviors, institutions and organisations across nations. Sage Publications, Thousand Oaks</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Culture%E2%80%99s%20consequences%3A%20comparing%20values%2C%20behaviors%2C%20institutions%20and%20organisations%20across%20nations&amp;publication_year=2001&amp;author=Hofstede%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hudlicka E, Delft TU (2009) Foundations for modelling emotions in game characters: modelling emotion effects o" /><p class="c-article-references__text" id="ref-CR42">Hudlicka E, Delft TU (2009) Foundations for modelling emotions in game characters: modelling emotion effects on cognition. In: Affective computing and intelligent interaction and workshops, ACII 2009</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Irtel, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Irtel H (2007) PXLab: the psychological experiments laboratory [online]. University of Mannheim, Mannheim" /><p class="c-article-references__text" id="ref-CR43">Irtel H (2007) PXLab: the psychological experiments laboratory [online]. University of Mannheim, Mannheim</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=PXLab%3A%20the%20psychological%20experiments%20laboratory%20%5Bonline%5D&amp;publication_year=2007&amp;author=Irtel%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Isbister K (2006) Better game characters by design: a psychological approach. Education, CRC Press" /><p class="c-article-references__text" id="ref-CR44">Isbister K (2006) Better game characters by design: a psychological approach. Education, CRC Press</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RE. Jack, OGB. Garrod, H. Yu, R. Caldara, PG. Schyns, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Jack RE, Garrod OGB, Yu H, Caldara R, Schyns PG (2012) Facial expressions of emotion are not culturally univer" /><p class="c-article-references__text" id="ref-CR45">Jack RE, Garrod OGB, Yu H, Caldara R, Schyns PG (2012) Facial expressions of emotion are not culturally universal. Proc Natl Acad Sci 109(19):7241–7244</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1073%2Fpnas.1200155109" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20expressions%20of%20emotion%20are%20not%20culturally%20universal&amp;journal=Proc%20Natl%20Acad%20Sci&amp;volume=109&amp;issue=19&amp;pages=7241-7244&amp;publication_year=2012&amp;author=Jack%2CRE&amp;author=Garrod%2COGB&amp;author=Yu%2CH&amp;author=Caldara%2CR&amp;author=Schyns%2CPG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jan D, Herrera D, Martinovski B (2007) A computational model of culture-specific conversational behavior. In: " /><p class="c-article-references__text" id="ref-CR46">Jan D, Herrera D, Martinovski B (2007) A computational model of culture-specific conversational behavior. In: IVA ‘07 Proceedings of the 7th international conference on intelligent virtual agents, pp 45–56</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kahler K, Haber J, Seidel H-P (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of g" /><p class="c-article-references__text" id="ref-CR47">Kahler K, Haber J, Seidel H-P (2001) Geometry-based muscle modeling for facial animation. In: Proceedings of graphics interface, pp 37–46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Keltner D, Ekman P (2000) Facial expression of emotion. Handbook of emotions, 2nd edn. pp 236–249" /><p class="c-article-references__text" id="ref-CR48">Keltner D, Ekman P (2000) Facial expression of emotion. Handbook of emotions, 2nd edn. pp 236–249</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kleinsmith, N. Bianchi-Berthouze, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Kleinsmith A, Bianchi-Berthouze N (2013) Affective body expression perception and recognition: a survey. IEEE " /><p class="c-article-references__text" id="ref-CR49">Kleinsmith A, Bianchi-Berthouze N (2013) Affective body expression perception and recognition: a survey. IEEE Trans Affect Comput 4:15–33. doi:<a href="https://doi.org/10.1109/T-AFFC.2012.16">10.1109/T-AFFC.2012.16</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FT-AFFC.2012.16" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affective%20body%20expression%20perception%20and%20recognition%3A%20a%20survey&amp;journal=IEEE%20Trans%20Affect%20Comput&amp;doi=10.1109%2FT-AFFC.2012.16&amp;volume=4&amp;pages=15-33&amp;publication_year=2013&amp;author=Kleinsmith%2CA&amp;author=Bianchi-Berthouze%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kleinsmith, PR. Silva, N. Bianchi-Berthouze, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Kleinsmith A, De Silva PR, Bianchi-Berthouze N (2006) Cross-cultural differences in recognizing affect from bo" /><p class="c-article-references__text" id="ref-CR50">Kleinsmith A, De Silva PR, Bianchi-Berthouze N (2006) Cross-cultural differences in recognizing affect from body posture. Interact Comput 18(6):1371–1389</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.intcom.2006.04.003" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cross-cultural%20differences%20in%20recognizing%20affect%20from%20body%20posture&amp;journal=Interact%20Comput&amp;volume=18&amp;issue=6&amp;pages=1371-1389&amp;publication_year=2006&amp;author=Kleinsmith%2CA&amp;author=Silva%2CPR&amp;author=Bianchi-Berthouze%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kulms P, Kramer NC, Gratch J, Kang S-H (2011) It’s in their eyes: a study on female and male virtual humans' g" /><p class="c-article-references__text" id="ref-CR51">Kulms P, Kramer NC, Gratch J, Kang S-H (2011) It’s in their eyes: a study on female and male virtual humans' gaze. In: IVA’11 Proceedings of the 11th international conference on intelligent virtual agents, pp 80–92</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lance BJ, Rey MD, Marsella SC (2008) A model of gaze for the purpose of emotional expression in virtual embodi" /><p class="c-article-references__text" id="ref-CR52">Lance BJ, Rey MD, Marsella SC (2008) A model of gaze for the purpose of emotional expression in virtual embodied agents. In: AAMAS ‘08 proceedings of the 7th international joint conference on autonomous agents and multiagent systems, 1, pp 12–16</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PJ. Lang, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Lang PJ (1995) The emotion probe. Studies of motivation and attention. Am Psychol 50(5):372–385" /><p class="c-article-references__text" id="ref-CR53">Lang PJ (1995) The emotion probe. Studies of motivation and attention. Am Psychol 50(5):372–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0003-066X.50.5.372" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20emotion%20probe.%20Studies%20of%20motivation%20and%20attention&amp;journal=Am%20Psychol&amp;volume=50&amp;issue=5&amp;pages=372-385&amp;publication_year=1995&amp;author=Lang%2CPJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lang PJ, Bradley MM, Cuthbert BN (1999) International affective picture system (IAPS): technical manual and af" /><p class="c-article-references__text" id="ref-CR54">Lang PJ, Bradley MM, Cuthbert BN (1999) International affective picture system (IAPS): technical manual and affective ratings. Psychology. The Center for Research in Psychophysiology, University of Florida, Gainesville, FL</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lang PJ, Bradley MM, Cuthbert BN (2008) International affective picture system (IAPS): affective ratings of pi" /><p class="c-article-references__text" id="ref-CR55">Lang PJ, Bradley MM, Cuthbert BN (2008) International affective picture system (IAPS): affective ratings of pictures and instruction manual. Technical Report A-8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee J, Marsella SC (2012) Modeling speaker behavior: a comparison of two approaches. In: IVA’12 Proceedings of" /><p class="c-article-references__text" id="ref-CR56">Lee J, Marsella SC (2012) Modeling speaker behavior: a comparison of two approaches. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 161–174</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee Y, Terzopoulos D, Walters K (1995) Realistic modeling for facial animation. In: Proceedings of the 22nd an" /><p class="c-article-references__text" id="ref-CR57">Lee Y, Terzopoulos D, Walters K (1995) Realistic modeling for facial animation. In: Proceedings of the 22nd annual conference on computer graphics and interactive techniques SIGGRAPH 95, pp 55–62</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee J, Prendinger H, Neviarouskaya A, Marsella S (2009) Learning models of speaker head nods with affective in" /><p class="c-article-references__text" id="ref-CR58">Lee J, Prendinger H, Neviarouskaya A, Marsella S (2009) Learning models of speaker head nods with affective information. In: 2009 3rd international conference on affective computing and intelligent interaction and workshops, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Link, P. Armsby, RC. Hubal, CI. Guinn, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Link M, Armsby P, Hubal RC, Guinn CI (2006) Accessibility and acceptance of responsive virtual human technolog" /><p class="c-article-references__text" id="ref-CR59">Link M, Armsby P, Hubal RC, Guinn CI (2006) Accessibility and acceptance of responsive virtual human technology as a survey interviewer training tool. Comput Hum Behav 22:412–426. doi:<a href="https://doi.org/10.1016/j.chb.2004.09.008">10.1016/j.chb.2004.09.008</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.chb.2004.09.008" aria-label="View reference 59">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Accessibility%20and%20acceptance%20of%20responsive%20virtual%20human%20technology%20as%20a%20survey%20interviewer%20training%20tool&amp;journal=Comput%20Hum%20Behav&amp;doi=10.1016%2Fj.chb.2004.09.008&amp;volume=22&amp;pages=412-426&amp;publication_year=2006&amp;author=Link%2CM&amp;author=Armsby%2CP&amp;author=Hubal%2CRC&amp;author=Guinn%2CCI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Litwinowicz P, Williams L (1994) Animating images with drawings. In: Proceedings of the 21st annual conference" /><p class="c-article-references__text" id="ref-CR60">Litwinowicz P, Williams L (1994) Animating images with drawings. In: Proceedings of the 21st annual conference on computer graphics and interactive techniques SIGGRAPH 94, pp 409–412</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KFK. MacDorman, JJA. Coram, C-CC. Ho, H. Patel, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="MacDorman KFK, Coram JJA, Ho C-CC, Patel H (2010) Gender differences in the impact of presentational factors i" /><p class="c-article-references__text" id="ref-CR61">MacDorman KFK, Coram JJA, Ho C-CC, Patel H (2010) Gender differences in the impact of presentational factors in human character animation on decisions in ethical dilemmas. Presence Teleoper Virtual Environ 19(3):213–229</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20differences%20in%20the%20impact%20of%20presentational%20factors%20in%20human%20character%20animation%20on%20decisions%20in%20ethical%20dilemmas&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=19&amp;issue=3&amp;pages=213-229&amp;publication_year=2010&amp;author=MacDorman%2CKFK&amp;author=Coram%2CJJA&amp;author=Ho%2CC-CC&amp;author=Patel%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Matsumoto, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Matsumoto D (2002) Methodological requirements to test a possible in-group advantage in judging emotions acros" /><p class="c-article-references__text" id="ref-CR62">Matsumoto D (2002) Methodological requirements to test a possible in-group advantage in judging emotions across cultures: comment on Elfenbein and Ambady (2002) and evidence. Psychol Bull 128(2):236–242</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-2909.128.2.236" aria-label="View reference 62">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Methodological%20requirements%20to%20test%20a%20possible%20in-group%20advantage%20in%20judging%20emotions%20across%20cultures%3A%20comment%20on%20Elfenbein%20and%20Ambady%20%282002%29%20and%20evidence&amp;journal=Psychol%20Bull&amp;volume=128&amp;issue=2&amp;pages=236-242&amp;publication_year=2002&amp;author=Matsumoto%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Matsumoto, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Matsumoto D (2007) Emotion judgments do not differ as a function of perceived nationality. Int J Psychol 42(3)" /><p class="c-article-references__text" id="ref-CR63">Matsumoto D (2007) Emotion judgments do not differ as a function of perceived nationality. Int J Psychol 42(3):207–214</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F00207590601050926" aria-label="View reference 63">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 63 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20judgments%20do%20not%20differ%20as%20a%20function%20of%20perceived%20nationality&amp;journal=Int%20J%20Psychol&amp;volume=42&amp;issue=3&amp;pages=207-214&amp;publication_year=2007&amp;author=Matsumoto%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Mehrabian, JA. Russell, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Mehrabian A, Russell JA (1974) An approach to environmental psychology. MIT Press, Cambridge, MA" /><p class="c-article-references__text" id="ref-CR64">Mehrabian A, Russell JA (1974) An approach to environmental psychology. MIT Press, Cambridge, MA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 64 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20approach%20to%20environmental%20psychology&amp;publication_year=1974&amp;author=Mehrabian%2CA&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Melo Cd, Carnevale P, Gratch J (2011) The effect of expression of anger and happiness in computer agents on ne" /><p class="c-article-references__text" id="ref-CR65">Melo Cd, Carnevale P, Gratch J (2011) The effect of expression of anger and happiness in computer agents on negotiations with humans. In: The tenth international conference on autonomous agents and multiagent systems, pp 2–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CN. Moridis, AA. Economides, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Moridis CN, Economides AA (2012) Affective learning: empathetic agents with emotional facial and tone of voice" /><p class="c-article-references__text" id="ref-CR66">Moridis CN, Economides AA (2012) Affective learning: empathetic agents with emotional facial and tone of voice expressions. IEEE Trans Affect Comput 3(3):260–272</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FT-AFFC.2012.6" aria-label="View reference 66">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affective%20learning%3A%20empathetic%20agents%20with%20emotional%20facial%20and%20tone%20of%20voice%20expressions&amp;journal=IEEE%20Trans%20Affect%20Comput&amp;volume=3&amp;issue=3&amp;pages=260-272&amp;publication_year=2012&amp;author=Moridis%2CCN&amp;author=Economides%2CAA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JD. Morris, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Morris JD (1995) Observations: SAM the self-assessment manikin an efficient cross-cultural measurement of emot" /><p class="c-article-references__text" id="ref-CR67">Morris JD (1995) Observations: SAM the self-assessment manikin an efficient cross-cultural measurement of emotional response. J Advert Res 35(6):63–68</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 67 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Observations%3A%20SAM%20the%20self-assessment%20manikin%20an%20efficient%20cross-cultural%20measurement%20of%20emotional%20response&amp;journal=J%20Advert%20Res&amp;volume=35&amp;issue=6&amp;pages=63-68&amp;publication_year=1995&amp;author=Morris%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RB. Ochsman, A. Chapanis, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behavior of teams during co-operati" /><p class="c-article-references__text" id="ref-CR68">Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behavior of teams during co-operative problem-solving. Int J ManMachine Stud 6(5):579–619</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0020-7373%2874%2980019-2" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%2010%20communication%20modes%20on%20the%20behavior%20of%20teams%20during%20co-operative%20problem-solving&amp;journal=Int%20J%20ManMachine%20Stud&amp;volume=6&amp;issue=5&amp;pages=579-619&amp;publication_year=1974&amp;author=Ochsman%2CRB&amp;author=Chapanis%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Opris, S. Pintea, A. Garcia-Palacios, CM. Botella, S. Szamoskozi, D. David, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Opris D, Pintea S, Garcia-Palacios A, Botella CM, Szamoskozi S, David D (2012) Virtual reality exposure therap" /><p class="c-article-references__text" id="ref-CR69">Opris D, Pintea S, Garcia-Palacios A, Botella CM, Szamoskozi S, David D (2012) Virtual reality exposure therapy in anxiety disorders: a quantitative meta-analysis. Depression Anxiety 29:85–93. doi:<a href="https://doi.org/10.1002/da.20910">10.1002/da.20910</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fda.20910" aria-label="View reference 69">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20exposure%20therapy%20in%20anxiety%20disorders%3A%20a%20quantitative%20meta-analysis&amp;journal=Depression%20Anxiety&amp;doi=10.1002%2Fda.20910&amp;volume=29&amp;pages=85-93&amp;publication_year=2012&amp;author=Opris%2CD&amp;author=Pintea%2CS&amp;author=Garcia-Palacios%2CA&amp;author=Botella%2CCM&amp;author=Szamoskozi%2CS&amp;author=David%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FI. Parke, " /><meta itemprop="datePublished" content="1972" /><meta itemprop="headline" content="Parke FI (1972) Computer generated animation of faces. Proc ACM Annu Conf 1:451–457" /><p class="c-article-references__text" id="ref-CR70">Parke FI (1972) Computer generated animation of faces. Proc ACM Annu Conf 1:451–457</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 70 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20generated%20animation%20of%20faces&amp;journal=Proc%20ACM%20Annu%20Conf&amp;volume=1&amp;pages=451-457&amp;publication_year=1972&amp;author=Parke%2CFI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parke FI (1974) A parametric model for human faces. The University of Utah, Doctoral Dissertation" /><p class="c-article-references__text" id="ref-CR71">Parke FI (1974) A parametric model for human faces. The University of Utah, Doctoral Dissertation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Petrushin V (1999) Emotion in speech: recognition and application to call centers. In: Artificial neural netwo" /><p class="c-article-references__text" id="ref-CR72">Petrushin V (1999) Emotion in speech: recognition and application to call centers. In: Artificial neural network in engineering (ANNIE’99), pp 7–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Picard RW (1998) Toward agents that recognize emotion. In: Actes proceedings IMAGINA, pp 153–165" /><p class="c-article-references__text" id="ref-CR73">Picard RW (1998) Toward agents that recognize emotion. In: Actes proceedings IMAGINA, pp 153–165</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RW. Picard, E. Vyzas, J. Healey, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Picard RW, Vyzas E, Healey J (2001) Toward machine emotional intelligence: analysis of affective physiological" /><p class="c-article-references__text" id="ref-CR74">Picard RW, Vyzas E, Healey J (2001) Toward machine emotional intelligence: analysis of affective physiological state. IEEE Trans Pattern Anal Mach Intell 23(10):1175–1191</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.954607" aria-label="View reference 74">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20machine%20emotional%20intelligence%3A%20analysis%20of%20affective%20physiological%20state&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=23&amp;issue=10&amp;pages=1175-1191&amp;publication_year=2001&amp;author=Picard%2CRW&amp;author=Vyzas%2CE&amp;author=Healey%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SM. Platt, NI. Badler, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Platt SM, Badler NI (1981) Animating facial expressions. ACM SIGGRAPH Comput Graph 15(3):245–252" /><p class="c-article-references__text" id="ref-CR75">Platt SM, Badler NI (1981) Animating facial expressions. ACM SIGGRAPH Comput Graph 15(3):245–252</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F965161.806812" aria-label="View reference 75">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 75 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Animating%20facial%20expressions&amp;journal=ACM%20SIGGRAPH%20Comput%20Graph&amp;volume=15&amp;issue=3&amp;pages=245-252&amp;publication_year=1981&amp;author=Platt%2CSM&amp;author=Badler%2CNI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Qiu, I. Benbasat, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Qiu L, Benbasat I (2005) Online consumer trust and live help interfaces: the effects of text-to-speech voice a" /><p class="c-article-references__text" id="ref-CR76">Qiu L, Benbasat I (2005) Online consumer trust and live help interfaces: the effects of text-to-speech voice and three-dimensional avatars. Int J Human-Comput Interact 19:37–41</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1085.05053" aria-label="View reference 76 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 76 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Online%20consumer%20trust%20and%20live%20help%20interfaces%3A%20the%20effects%20of%20text-to-speech%20voice%20and%20three-dimensional%20avatars&amp;journal=Int%20J%20Human-Comput%20Interact&amp;volume=19&amp;pages=37-41&amp;publication_year=2005&amp;author=Qiu%2CL&amp;author=Benbasat%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Reeves, C. Nass, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Reeves B, Nass C (1996) The media equation. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR77">Reeves B, Nass C (1996) The media equation. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 77 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20media%20equation&amp;publication_year=1996&amp;author=Reeves%2CB&amp;author=Nass%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rizzo, B. Lange, J. Buckwalter, E. Forbell, J. Kim, K. Sagae, P. Kenny, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Rizzo A, Lange B, Buckwalter J, Forbell E, Kim J, Sagae K, Kenny P (2011) An intelligent virtual human system " /><p class="c-article-references__text" id="ref-CR78">Rizzo A, Lange B, Buckwalter J, Forbell E, Kim J, Sagae K, Kenny P (2011) An intelligent virtual human system for providing healthcare information and support. Stud Health Technol Inf 163:503–509</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 78 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20intelligent%20virtual%20human%20system%20for%20providing%20healthcare%20information%20and%20support&amp;journal=Stud%20Health%20Technol%20Inf&amp;volume=163&amp;pages=503-509&amp;publication_year=2011&amp;author=Rizzo%2CA&amp;author=Lange%2CB&amp;author=Buckwalter%2CJ&amp;author=Forbell%2CE&amp;author=Kim%2CJ&amp;author=Sagae%2CK&amp;author=Kenny%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JA. Russell, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Russell JA (1991) Culture and the categorization of emotions. Psychol Bull 110:426–450" /><p class="c-article-references__text" id="ref-CR79">Russell JA (1991) Culture and the categorization of emotions. Psychol Bull 110:426–450</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-2909.110.3.426" aria-label="View reference 79">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 79 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Culture%20and%20the%20categorization%20of%20emotions&amp;journal=Psychol%20Bull&amp;volume=110&amp;pages=426-450&amp;publication_year=1991&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruttkay Z, Pelachaud C (2005) From brows to trust: evaluating embodied conversational agents. Springer, Berlin" /><p class="c-article-references__text" id="ref-CR80">Ruttkay Z, Pelachaud C (2005) From brows to trust: evaluating embodied conversational agents. Springer, Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KR. Scherer, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Scherer KR (1995) Expression of emotion in voice and music. J Voice Off J Voice Found 9(3):235–248" /><p class="c-article-references__text" id="ref-CR81">Scherer KR (1995) Expression of emotion in voice and music. J Voice Off J Voice Found 9(3):235–248</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0892-1997%2805%2980231-0" aria-label="View reference 81">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 81 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Expression%20of%20emotion%20in%20voice%20and%20music&amp;journal=J%20Voice%20Off%20J%20Voice%20Found&amp;volume=9&amp;issue=3&amp;pages=235-248&amp;publication_year=1995&amp;author=Scherer%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KR. Scherer, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Scherer KR (2003) Vocal communication of emotion: a review of research paradigms. Speech Commun 40:227–256" /><p class="c-article-references__text" id="ref-CR82">Scherer KR (2003) Vocal communication of emotion: a review of research paradigms. Speech Commun 40:227–256</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0167-6393%2802%2900084-5" aria-label="View reference 82">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1006.68948" aria-label="View reference 82 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 82 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vocal%20communication%20of%20emotion%3A%20a%20review%20of%20research%20paradigms&amp;journal=Speech%20Commun&amp;volume=40&amp;pages=227-256&amp;publication_year=2003&amp;author=Scherer%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Schiano, SM. Ehrlich, K. Rahardja, K. Sheridan, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Schiano DJ, Ehrlich SM, Rahardja K, Sheridan K (2000) Face to interface: facial affect in (hu)man and machine." /><p class="c-article-references__text" id="ref-CR83">Schiano DJ, Ehrlich SM, Rahardja K, Sheridan K (2000) Face to interface: facial affect in (hu)man and machine. Proc ACM CHI 2000:193–200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 83 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Face%20to%20interface%3A%20facial%20affect%20in%20%28hu%29man%20and%20machine&amp;journal=Proc%20ACM%20CHI&amp;volume=2000&amp;pages=193-200&amp;publication_year=2000&amp;author=Schiano%2CDJ&amp;author=Ehrlich%2CSM&amp;author=Rahardja%2CK&amp;author=Sheridan%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schroder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach t" /><p class="c-article-references__text" id="ref-CR84">Schroder M (2004) Speech and emotion research: an overview of research frameworks and a dimensional approach to emotional speech synthesis. Research Report of the Institute of Phonetics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Slater, D-P. Pertaub, A. Steed, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Slater M, Pertaub D-P, Steed A (1999) Public speaking in virtual reality: facing an audience of avatars. IEEE " /><p class="c-article-references__text" id="ref-CR85">Slater M, Pertaub D-P, Steed A (1999) Public speaking in virtual reality: facing an audience of avatars. IEEE Comput Graphics Appl 19(2):6–9</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.749116" aria-label="View reference 85">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 85 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Public%20speaking%20in%20virtual%20reality%3A%20facing%20an%20audience%20of%20avatars&amp;journal=IEEE%20Comput%20Graphics%20Appl&amp;volume=19&amp;issue=2&amp;pages=6-9&amp;publication_year=1999&amp;author=Slater%2CM&amp;author=Pertaub%2CD-P&amp;author=Steed%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SS. Tomkins, " /><meta itemprop="datePublished" content="1962" /><meta itemprop="headline" content="Tomkins SS (1962) Affect, imagery, consciousness: vol 1. The positive affects. Springer, New York" /><p class="c-article-references__text" id="ref-CR86">Tomkins SS (1962) Affect, imagery, consciousness: vol 1. The positive affects. Springer, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 86 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%2C%20imagery%2C%20consciousness%3A%20vol%201.%20The%20positive%20affects&amp;publication_year=1962&amp;author=Tomkins%2CSS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SS. Tomkins, " /><meta itemprop="datePublished" content="1963" /><meta itemprop="headline" content="Tomkins SS (1963) Affect, imagery, consciousness: vol 2. The negative affects. Springer, New York" /><p class="c-article-references__text" id="ref-CR87">Tomkins SS (1963) Affect, imagery, consciousness: vol 2. The negative affects. Springer, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affect%2C%20imagery%2C%20consciousness%3A%20vol%202.%20The%20negative%20affects&amp;publication_year=1963&amp;author=Tomkins%2CSS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tsapatsoulis N, Raouzaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis base" /><p class="c-article-references__text" id="ref-CR88">Tsapatsoulis N, Raouzaiou A, Kollias S, Cowie R, Douglas-Cowie E (2002) Emotion recognition and synthesis based on MPEG-4 FAPs. In; MPEG-4 facial animation the standard implementations applications</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Waters, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Waters K (1987) A muscle model for animating three-dimensional facial expression. Comput Graph SIGGRAPH Proc 2" /><p class="c-article-references__text" id="ref-CR89">Waters K (1987) A muscle model for animating three-dimensional facial expression. Comput Graph SIGGRAPH Proc 21(4):17–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1713020" aria-label="View reference 89 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F37402.37405" aria-label="View reference 89">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 89 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20muscle%20model%20for%20animating%20three-dimensional%20facial%20expression&amp;journal=Comput%20Graph%20SIGGRAPH%20Proc&amp;volume=21&amp;issue=4&amp;pages=17-24&amp;publication_year=1987&amp;author=Waters%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Wierzbicka, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Wierzbicka A (1995) Emotions across languages and cultures: diversity and universals. Cambridge University Pre" /><p class="c-article-references__text" id="ref-CR90">Wierzbicka A (1995) Emotions across languages and cultures: diversity and universals. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 90 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotions%20across%20languages%20and%20cultures%3A%20diversity%20and%20universals&amp;publication_year=1995&amp;author=Wierzbicka%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wong JW-E, McGee K (2012) Frown more, talk more: effects of facial expressions in establishing conversational " /><p class="c-article-references__text" id="ref-CR91">Wong JW-E, McGee K (2012) Frown more, talk more: effects of facial expressions in establishing conversational rapport with virtual agents. In: IVA’12 Proceedings of the 12th international conference on intelligent virtual agents, pp 419–425</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Yun, Z. Deng, M. Hiscock, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Yun C, Deng Z, Hiscock M (2009) Can local avatars satisfy a global audience? A case study of high-fidelity 3D " /><p class="c-article-references__text" id="ref-CR92">Yun C, Deng Z, Hiscock M (2009) Can local avatars satisfy a global audience? A case study of high-fidelity 3D facial avatar animation in subject identification and emotion perception by US and international groups. Comput Entertain 7(2):1–25</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1541895.1541901" aria-label="View reference 92">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 92 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Can%20local%20avatars%20satisfy%20a%20global%20audience%3F%20A%20case%20study%20of%20high-fidelity%203D%20facial%20avatar%20animation%20in%20subject%20identification%20and%20emotion%20perception%20by%20US%20and%20international%20groups&amp;journal=Comput%20Entertain&amp;volume=7&amp;issue=2&amp;pages=1-25&amp;publication_year=2009&amp;author=Yun%2CC&amp;author=Deng%2CZ&amp;author=Hiscock%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="QZQ. Zhang, Z. Liu, GQG. Quo, D. Terzopoulos, H-YSH-Y. Shum, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Zhang QZQ, Liu Z, Quo GQG, Terzopoulos D, Shum H-YSH-Y (2006) Geometry-driven photorealistic facial expression" /><p class="c-article-references__text" id="ref-CR93">Zhang QZQ, Liu Z, Quo GQG, Terzopoulos D, Shum H-YSH-Y (2006) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Visual Comput Graphics 12(1):48–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2006.9" aria-label="View reference 93">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 93 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometry-driven%20photorealistic%20facial%20expression%20synthesis&amp;journal=IEEE%20Trans%20Visual%20Comput%20Graphics&amp;volume=12&amp;issue=1&amp;pages=48-60&amp;publication_year=2006&amp;author=Zhang%2CQZQ&amp;author=Liu%2CZ&amp;author=Quo%2CGQG&amp;author=Terzopoulos%2CD&amp;author=Shum%2CH-YSH-Y">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0231-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This study is supported in part by the Chinese Scholarship Council (No. 2008609199) and the COMMIT project—Interaction for Universal Access.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Delft University of Technology, Mekelweg 4, 2628 CD, Delft, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Chao Qu, Willem-Paul Brinkman, Yun Ling, Pascal Wiggers &amp; Ingrid Heynderickx</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Philips Research Laboratories, High Tech Campus 34, 5656 AE, Eindhoven, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Ingrid Heynderickx</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Chao-Qu"><span class="c-article-authors-search__title u-h3 js-search-name">Chao Qu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Chao+Qu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chao+Qu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chao+Qu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Willem_Paul-Brinkman"><span class="c-article-authors-search__title u-h3 js-search-name">Willem-Paul Brinkman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Willem-Paul+Brinkman&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Willem-Paul+Brinkman" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Willem-Paul+Brinkman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Yun-Ling"><span class="c-article-authors-search__title u-h3 js-search-name">Yun Ling</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yun+Ling&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yun+Ling" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yun+Ling%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Pascal-Wiggers"><span class="c-article-authors-search__title u-h3 js-search-name">Pascal Wiggers</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Pascal+Wiggers&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Pascal+Wiggers" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Pascal+Wiggers%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ingrid-Heynderickx"><span class="c-article-authors-search__title u-h3 js-search-name">Ingrid Heynderickx</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ingrid+Heynderickx&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ingrid+Heynderickx" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ingrid+Heynderickx%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0231-z/email/correspondent/c1/new">Chao Qu</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Human%20perception%20of%20a%20conversational%20virtual%20human%3A%20an%20empirical%20study%20on%20the%20effect%20of%20emotion%20and%20culture&amp;author=Chao%20Qu%20et%20al&amp;contentID=10.1007%2Fs10055-013-0231-z&amp;publication=1359-4338&amp;publicationDate=2013-08-30&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Qu, C., Brinkman, W., Ling, Y. <i>et al.</i> Human perception of a conversational virtual human: an empirical study on the effect of emotion and culture.
                    <i>Virtual Reality</i> <b>17, </b>307–321 (2013). https://doi.org/10.1007/s10055-013-0231-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0231-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-01-17">17 January 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-20">20 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-30">30 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-11">November 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0231-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0231-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual human</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Emotion</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Affective computing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Culture</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0231-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=231;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

