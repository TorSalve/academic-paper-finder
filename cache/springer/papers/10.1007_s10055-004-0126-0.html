<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Comparing the roles of 3D representations in audio and audio-visual co"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This study investigates the effects of performance and communication within audio-visual (shared representations) and audio-only conditions. Two three-dimensional (3D) representations were..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/7/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Comparing the roles of 3D representations in audio and audio-visual collaborations"/>

    <meta name="dc.source" content="Virtual Reality 2004 7:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-05-19"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This study investigates the effects of performance and communication within audio-visual (shared representations) and audio-only conditions. Two three-dimensional (3D) representations were presented in each communication condition. The goal of the study was to examine both explicit and implicit references made during verbal interactions, and to gather subjective usability evaluations of each representation. Sixty dyads performed a series of problem solving tasks in three experimental conditions: mixed, 3D cylinder and 3D helix representations. Assessment measures included overall performance time and accuracy, and user attitudes pertaining to the usability of the displays. Although no differences in task performance were observed, qualitative measures revealed differences between representation and communication groups. User preferences for 3D cylinder and 3D helix representations were observed, with disparate strategies being adopted between groups. In general, the analyses indicated that the presence of shared visual information enhances collaborative problem solving."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-05-19"/>

    <meta name="prism.volume" content="7"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="148"/>

    <meta name="prism.endingPage" content="163"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0126-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0126-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0126-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0126-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Comparing the roles of 3D representations in audio and audio-visual collaborations"/>

    <meta name="citation_volume" content="7"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2004/05/19"/>

    <meta name="citation_firstpage" content="148"/>

    <meta name="citation_lastpage" content="163"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0126-0"/>

    <meta name="DOI" content="10.1007/s10055-004-0126-0"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0126-0"/>

    <meta name="description" content="This study investigates the effects of performance and communication within audio-visual (shared representations) and audio-only conditions. Two three-dime"/>

    <meta name="dc.creator" content="Martin Hicks"/>

    <meta name="dc.creator" content="Sarah Nichols"/>

    <meta name="dc.creator" content="Claire O&#8217;Malley"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ainsworth SE, Peevers GJ (2003) The interaction between informational and computational properties on problem-solving and learning. In: Proceedings of the 25th annual conference of the Cognitive Science Society, Boston, Massachusetts, July/August 2003"/>

    <meta name="citation_reference" content="citation_journal_title=Sci Am; citation_author=null Chapanis; citation_volume=232; citation_publication_date=1975; citation_pages=3; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Educ Dev; citation_author=null Chou; citation_volume=21; citation_publication_date=2001; citation_pages=293; citation_doi=10.1016/S0738-0593(00)00003-1; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Appl; citation_author=null Doherty-Sneddon; citation_volume=3; citation_publication_date=1997; citation_pages=105; citation_doi=10.1037//1076-898X.3.2.105; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Inform Technol; citation_author=null Hicks; citation_volume=22; citation_publication_date=2003; citation_pages=185; citation_doi=10.1080/0144929031000117080; citation_id=CR5"/>

    <meta name="citation_reference" content="Hutchins E (1995) Cognition in the wild. MIT Press, Cambridge"/>

    <meta name="citation_reference" content="citation_journal_title=Cognitive Psychol; citation_author=null Kotovsky; citation_volume=22; citation_publication_date=1990; citation_pages=143; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Cognitive Sci; citation_author=null Larkin; citation_volume=11; citation_publication_date=1987; citation_pages=65; citation_id=CR8"/>

    <meta name="citation_reference" content="Meister D (1986) Human factors testing and evaluation. Elsevier, New York"/>

    <meta name="citation_reference" content="Monk A, McCarthy J, Watts L, Daly-Jones O (1997) Measures of process. In: MacLeod M, Murray D (eds) Evaluation for CSCW. Springer, Berlin Heidelberg New York, pp 1&#8211;7"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput St; citation_author=null Neale; citation_volume=55; citation_publication_date=2001; citation_pages=167; citation_doi=10.1006/ijhc.2001.0475; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Man Mach Stud; citation_author=null Ochsman; citation_volume=6; citation_publication_date=1974; citation_pages=579; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Cognitive Sci; citation_author=null Okada; citation_volume=21; citation_publication_date=1997; citation_pages=109; citation_doi=10.1016/S0364-0213(99)80020-2; citation_id=CR13"/>

    <meta name="citation_reference" content="Perkins DN (1993) Person-plus: A distributed view of think and learning. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge , pp 88&#8211;111"/>

    <meta name="citation_reference" content="Salomon G (1993) No distribution without individuals&#8217; cognition: A dynamic interactional view. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge, pp 111&#8211;138"/>

    <meta name="citation_reference" content="citation_journal_title=Cognitive Sci; citation_author=null Zhang; citation_volume=21; citation_publication_date=1997; citation_pages=179; citation_doi=10.1016/S0364-0213(99)80022-6; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=J Am Soc Inform Sci; citation_author=null Zhang; citation_volume=49; citation_publication_date=1998; citation_pages=801; citation_doi=10.1002/(SICI)1097-4571(199807)49:9&lt;801::AID-ASI5&gt;3.0.CO;2-W; citation_id=CR17"/>

    <meta name="citation_author" content="Martin Hicks"/>

    <meta name="citation_author_email" content="mjh@psychology.nottingham.ac.uk"/>

    <meta name="citation_author_institution" content="School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK"/>

    <meta name="citation_author_institution" content="Chimera, Institute for Socio-Technical Innovation and Research, University of Essex, UK"/>

    <meta name="citation_author_institution" content="pp 1 Ross Building, Adastral Park, Martlesham Heath, Ipswich, UK"/>

    <meta name="citation_author" content="Sarah Nichols"/>

    <meta name="citation_author_institution" content="School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK"/>

    <meta name="citation_author" content="Claire O&#8217;Malley"/>

    <meta name="citation_author_institution" content="School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0126-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0126-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Comparing the roles of 3D representations in audio and audio-visual collaborations"/>
        <meta property="og:description" content="This study investigates the effects of performance and communication within audio-visual (shared representations) and audio-only conditions. Two three-dimensional (3D) representations were presented in each communication condition. The goal of the study was to examine both explicit and implicit references made during verbal interactions, and to gather subjective usability evaluations of each representation. Sixty dyads performed a series of problem solving tasks in three experimental conditions: mixed, 3D cylinder and 3D helix representations. Assessment measures included overall performance time and accuracy, and user attitudes pertaining to the usability of the displays. Although no differences in task performance were observed, qualitative measures revealed differences between representation and communication groups. User preferences for 3D cylinder and 3D helix representations were observed, with disparate strategies being adopted between groups. In general, the analyses indicated that the presence of shared visual information enhances collaborative problem solving."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Comparing the roles of 3D representations in audio and audio-visual collaborations | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0126-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"3D representations, Information visualisation, Collaborative problem solving","kwrd":["3D_representations","Information_visualisation","Collaborative_problem_solving"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0126-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0126-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=126;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0126-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Comparing the roles of 3D representations in audio and audio-visual collaborations
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0126-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0126-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-05-19" itemprop="datePublished">19 May 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Comparing the roles of 3D representations in audio and audio-visual collaborations</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Martin-Hicks" data-author-popup="auth-Martin-Hicks" data-corresp-id="c1">Martin Hicks<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Nottingham" /><meta itemprop="address" content="grid.4563.4, 0000000419368868, School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Essex" /><meta itemprop="address" content="grid.8356.8, 0000000109426946, Chimera, Institute for Socio-Technical Innovation and Research, University of Essex, UK" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden" data-present-affiliation="true"><meta itemprop="name" content="pp 1 Ross Building, Adastral Park" /><meta itemprop="address" content="pp 1 Ross Building, Adastral Park, Martlesham Heath, Ipswich, Suffolk, IP5 3RE, UK" /></span></sup><sup class="u-js-hide"> <a href="#nAff3">nAff3</a></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Sarah-Nichols" data-author-popup="auth-Sarah-Nichols">Sarah Nichols</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Nottingham" /><meta itemprop="address" content="grid.4563.4, 0000000419368868, School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Claire-O_Malley" data-author-popup="auth-Claire-O_Malley">Claire O’Malley</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Nottingham" /><meta itemprop="address" content="grid.4563.4, 0000000419368868, School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 7</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">148</span>–<span itemprop="pageEnd">163</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">92 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0126-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This study investigates the effects of performance and communication within audio-visual (shared representations) and audio-only conditions. Two three-dimensional (3D) representations were presented in each communication condition. The goal of the study was to examine both explicit and implicit references made during verbal interactions, and to gather subjective usability evaluations of each representation. Sixty dyads performed a series of problem solving tasks in three experimental conditions: mixed, 3D cylinder and 3D helix representations. Assessment measures included overall performance time and accuracy, and user attitudes pertaining to the usability of the displays. Although no differences in task performance were observed, qualitative measures revealed differences between representation and communication groups. User preferences for 3D cylinder and 3D helix representations were observed, with disparate strategies being adopted between groups. In general, the analyses indicated that the presence of shared visual information enhances collaborative problem solving.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Introduction</h2><div class="c-article-section__content" id="Sec2-content"><p>For a number of years, substantial research has been conducted on investigating the role of external representations in problem solving, and how such representations can affect problem solving efficiency [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Kotovsky K, Simon HA (1990) What makes some problems really hard: Explorations in the problem space difficulty. Cognitive Psychol 22:143–183" href="/article/10.1007/s10055-004-0126-0#ref-CR7" id="ref-link-section-d68104e324">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Larkin JH, Simon HA (1987) Why a diagram is (sometimes) worth ten thousand words. Cognitive Sci 11:65–99" href="/article/10.1007/s10055-004-0126-0#ref-CR8" id="ref-link-section-d68104e327">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Zhang J (1997) The nature of external representations in problem solving. Cognitive Sci 21(2):179–217" href="/article/10.1007/s10055-004-0126-0#ref-CR16" id="ref-link-section-d68104e330">16</a>]. Arguably, a key factor underlying the advantages afforded by multiple external representations (MERs), in terms of their problem solving capacity, relates to computational non-equivalence [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Ainsworth SE, Peevers GJ (2003) The interaction between informational and computational properties on problem-solving and learning. In: Proceedings of the 25th annual conference of the Cognitive Science Society, Boston, Massachusetts, July/August 2003" href="/article/10.1007/s10055-004-0126-0#ref-CR1" id="ref-link-section-d68104e333">1</a>]. Representations containing identical information (information equivalence), but exhibiting different representational characteristics, produces differences in ease of information extraction. Larkin and Simon [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Larkin JH, Simon HA (1987) Why a diagram is (sometimes) worth ten thousand words. Cognitive Sci 11:65–99" href="/article/10.1007/s10055-004-0126-0#ref-CR8" id="ref-link-section-d68104e336">8</a>] contrasted interpretations of graphical and textual information in terms of search, recognition and inference, claiming that one principal benefit of diagrammatic representations is that “diagrams can group together information [to be] used together thus avoiding large amounts of search for the elements needed to make a problem solving inference.” Our prior work contrasting informationally equivalent, but computationally distinct 2D and 3D representations revealed that differences in representational characteristics have significant effects on the amount of cognitive effort required to perform tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising telecommunications usage. Behav Inform Technol 22(3):185–201" href="/article/10.1007/s10055-004-0126-0#ref-CR5" id="ref-link-section-d68104e340">5</a>].</p><p>In addition to individual cognition, another broad level of research considers that an understanding of the interaction between representational characteristics and cognition is developed through social interaction and activity. Collaborative learning research underlines that the interaction of the cognitive processes of several people is qualitatively different to that of an individual [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Okada T, Simon HA (1997) Collaborative discovery in a scientific domain. Cognitive Sci 21(2):109–146" href="/article/10.1007/s10055-004-0126-0#ref-CR13" id="ref-link-section-d68104e346">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Perkins DN (1993) Person-plus: A distributed view of think and learning. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge , pp 88–111" href="/article/10.1007/s10055-004-0126-0#ref-CR14" id="ref-link-section-d68104e349">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Salomon G (1993) No distribution without individuals’ cognition: A dynamic interactional view. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge, pp 111–138" href="/article/10.1007/s10055-004-0126-0#ref-CR15" id="ref-link-section-d68104e352">15</a>]. Hutchins [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Hutchins E (1995) Cognition in the wild. MIT Press, Cambridge" href="/article/10.1007/s10055-004-0126-0#ref-CR6" id="ref-link-section-d68104e355">6</a>] broadly defines the effects of group-level cognitive properties, stating that these properties are produced by interactions between structures both internal to individuals and external to individuals (e.g. external representations).</p><h3 class="c-article__sub-heading" id="Sec3">Effect of communication medium on collaborative problem solving</h3><p>Visual information presented in audio-only contexts forces collaborators to verbalise tacit knowledge which can be misinterpreted. Shared representations can mediate collaborative problem solving discourse, without the need for complex verbal descriptions, through the provision of a means to articulate emerging knowledge and solutions within a medium visible to all participants. Generally, research findings concerning the effects of visual signals on communicative processes appear to be mixed [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Chapanis A (1975). Interactive human communication. Sci Am 232:3–42" href="/article/10.1007/s10055-004-0126-0#ref-CR2" id="ref-link-section-d68104e365">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behaviour of teams during co-operative problem-solving. Int J Man Mach Stud 6:579–619" href="/article/10.1007/s10055-004-0126-0#ref-CR12" id="ref-link-section-d68104e368">12</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Doherty-Sneddon G, O’Malley C, Garrod S, Anderson A, Langton S, Bruce V (1997) Face-to-face and video-mediated communication: A comparison of dialogue structure and task performance. J Exp Psychol Appl 3(2):105–125" href="/article/10.1007/s10055-004-0126-0#ref-CR4" id="ref-link-section-d68104e371">4</a>].</p><p>These studies compared performance and effects on communication in face-to-face, video-mediated and audio-only conditions. Our aim is to investigate the effects of performance and communication incorporating audio-visual (shared representations) and audio-only conditions, following the assumption that visually shared representations optimise collaborative problem solving. Moreover, we aim to examine both explicit and implicit references made during interactions, but prohibit non-verbal communication, except via the computer interface. In addition, we aim to gather subjective measures as a usability assessment to evaluate the representations.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Properties of 3D representations in desktop virtual reality (VR) environments</h2><div class="c-article-section__content" id="Sec4-content"><p>This study compares two graphical representations depicted in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig2">2</a>: 3D cylinder and 3D helix, respectively. The representations were presented on web browsers including Virtual Reality Modelling Language (VRML) plug-in controls (e.g. CosmoPlayer) in desktop VR environments incorporating audio-visual (shared representations) and audio-only communication. The development of representations within desktop VR environments offers certain advantages—e.g. desktop and web-based VR environments can potentially distribute data visualisations over the Internet, allowing multiple users to access the information concurrently. The development of desktop VR is also more cost-effective than equivalent immersive environments [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Chou C, Tsai C-C, Tsai H-F (2001) Developing a networked VRML learning system for health science education in Taiwan. Int J Educ Dev 21:293–303" href="/article/10.1007/s10055-004-0126-0#ref-CR3" id="ref-link-section-d68104e391">3</a>].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p> 3D cylinder representation. Telephone calls made by six groups over a one-month period (with pop-up message, spin button, expanded group key and ‘Start View’)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p> 3D helix representation. Telephone calls made by six groups over a one-month period (with pop-up message, spin button, expanded group key and ‘Start View’)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Both representations include identical telephone usage data, incorporating call usage for six discrete groups during a one-month period. The usage data for each group is represented by colour-coded data points or cylinders on the representations. Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig1">1</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig2">2</a> highlight shared VR functionality included with each representation, notably:</p><ul class="u-list-style-dash">
                  <li>
                    <p>A collapsible colour key for six groups with associated pictures</p>
                  </li>
                  <li>
                    <p>A pop-up message (showing associated group picture and usage information) briefly displayed in the top-left corner whenever a data point is selected</p>
                  </li>
                  <li>
                    <p>A spin button rotating the representation clockwise whenever the spin button is selected</p>
                  </li>
                  <li>
                    <p>A ‘Start View’ (from viewpoint list) to return to the default front view orientation</p>
                  </li>
                </ul><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab1">1</a> summarises the design characteristics for both 3D representations.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 
Representational design characteristics</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Both representations share identical databases and design characteristics, and are considered to have informational and computational equivalence. The representations consist of a number of circular spirals, each rotating through 360°. Each spiral represents one week, and is further divided into seven equal parts, equating to seven days. A darker shaded segment denotes night, with the centre of the segment representing midnight. The two coloured segments within each spiral denote weekends. The 3D cylinder in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig1">1</a> presents consecutive weekly intervals of usage data across each spiral; the earliest week represented by the innermost spiral. With the 3D helix in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig2">2</a>, the days of the week are incremented along each spiral being rotated in a clockwise direction; the earliest week represented by the foremost spiral. Both representations consist of spirals which have a finite number of potential cylinders—each cylinder representing the call duration for a particular group during a two-hour period.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Behavioural usage data and information processing tasks</h2><div class="c-article-section__content" id="Sec5-content"><p>The telephone usage data has been created and manipulated to create differences in call usage and patterns between six groups (based on typical groups representing actual BT customers). These groups, each with associated call usage, include: family with three children (heaviest call usage), mother with two children (medium call usage, daytime calls only), mother with infant (light call usage, only one evening call), father with two children (lightest call usage), male aged 33 (evening calls only, mostly at 8 pm) and student (evening calls, mostly at 10 pm, with occasional weekend and early morning calls).</p><p>A set of five analytical tasks (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab2">2</a>) were devised, with each task divided into two parts, A and B. Tasks 1a and 3a query which group demonstrates both the highest and the lowest call usage (call duration and frequency) for the entire month. Task 2a is included to test whether specific call patterns can be derived from the representations. The data distributions have been manipulated so that four of the six groups exhibit specific call patterns (e.g. when approximately 50% or more of total call usage occurs during specific diurnal or weekly periods). Task 4a specifically tests whether a series of ‘shared’ cylinders (six in total) can be identified (‘shared’ cylinders are divided into two colours, denoting two groups with identical calling times and durations). Finally, task 5a tests which group called at a specific diurnal period throughout the month.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 
Analytical tasks for telephone usage for six groups during a one-month period</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Part B components are included to encourage collaborators to discuss whether any particular representational properties facilitated their search for answers to part A tasks. Moreover, part B tasks are designed to elicit the mutual exchange of representational characteristics between the participants, to derive qualitative feedback to inform usability assessments for the representations and to gain insights into how different representations contribute to collaborative problem solving.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Expert walkthrough and predictions</h2><div class="c-article-section__content" id="Sec6-content"><p>An expert walkthrough (e.g. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Meister D (1986) Human factors testing and evaluation. Elsevier, New York" href="/article/10.1007/s10055-004-0126-0#ref-CR9" id="ref-link-section-d68104e811">9</a>]) was performed by the first author, using task examples with both representations, to identify strategies for attaining sub-goals (most and least effective methods) and cognitive processes. Within the strategies identified, a combination of physical actions and VRML plug-in control selections were possible, including Seek, Zoom, Rotate, Pan, Undo, Redo, Straighten or the ‘Start View’ viewpoint selection from the Viewpoint List.</p><p>During the expert walkthrough, three high-level, sequential cognitive processes were identified and associated with each stage of the five tasks:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Scanning the representation to identify target cylindrical data points</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Selectively grouping target cylindrical data points according to task requirements, e.g.:<dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>(a):</dfn></dt><dd class="c-abbreviation_list__description">
                              <p>by group colour or name, and cylindrical size and frequency (for usage estimations and comparisons, e.g. tasks 1a and 3a), and/or</p>
                            </dd><dt class="c-abbreviation_list__term"><dfn>(b):</dfn></dt><dd class="c-abbreviation_list__description">
                              <p>by group colour or name, and cylindrical spatial location (for temporal patterns and comparisons, e.g., tasks 2a, 4a and 5a)</p>
                            </dd></dl>
</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Encoding target group(s) by writing or remembering group name(s) or colour(s)</p>
                    
                  </li>
                </ol>
<p>These processes may be repeated, with the number of iterations being determined by task difficulty and the efficiency of the strategy adopted. The main factor contributing to lower mental effort is attributable to the degree of computational offloading onto each representation. The amount of offloading achievable is dependent on the efficiency of the strategy selected and its appropriateness for both representation and task.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig3">3</a> shows the 3D helix representation at the front, or ‘Start View’. The simplest strategy is to retain this view while scanning the data and clicking on the cylinders to read the pop-up messages upon mouse selection. However, in order to avoid data occlusion, the representation should be rotated off-centre (e.g., see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig4">4</a>), so that cylinders and spirals are segregated.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p> 3D helix representation (at ‘Start View’). Telephone calls made by six groups over a one-month period (with expanded group key)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p> 3D helix representation (rotated and offset left). Telephone calls made by six groups over a one-month period (with expanded group key)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Perceptual distortions can occur with the 3D helix for tasks requiring data size comparisons (tasks 1a and 3a), as cylindrical height-cues are dissipated around the spirals. By contrast, the 3D cylinder representation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig5">5</a>) optimally supports data size comparisons as the spirals are horizontally aligned, although some perceptual distortions still occur, as cylinders are positioned around the spirals at different distances from the user.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p> 3D cylinder representation (view zoomed in slightly). Telephone calls made by six groups over a one-month period (with group key collapsed)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To overcome these distortions, the 3D cylinder can be spun around so that distant cylinders are presented in the foreground, or rotated upwards so that the representation appears horizontally flat with the cylinders vertically aligned (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig6">6</a>). It is then possible to spin the representation to estimate cylindrical heights as the representation turns.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p> 3D cylinder representation (rotated up to eye-level view). Telephone calls made by six groups over a one-month period (with group key collapsed)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Both representations share representational characteristics, promoting higher offloading and task efficiency. The distribution of cylinders around the spirals at regular intervals enables temporal patterns to be perceived when the cylinders are aligned across the spirals. However, one noticeable difference between the representations is that the shaded and orange segments around the spirals are less distinguishable on the 3D cylinder (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig5">5</a>), appearing more prominent around the helical form of the 3D helix (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig3">3</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig4">4</a>). This effect is primarily due to the horizontal alignment of spirals with the 3D cylinder. As a result, computational offloading may be higher with the 3D helix for tasks requiring the comparison of targets at specific diurnal periods (task 5a).</p><h3 class="c-article__sub-heading" id="Sec7">Experimental predictions</h3><p>The study follows the assumption that visually shared representations optimise collaborative problem solving, predicting a performance advantage for audio-visual groups. The expert walkthrough revealed differences in representational characteristics, which may produce differences in performance measures between representations for certain tasks. To summarise, these hypotheses predict that an overall performance advantage will occur:</p><ul class="u-list-style-dash">
                    <li>
                      <p>For audio-visual groups, as forced verbalisation is unnecessary due to the presence of shared visual information</p>
                    </li>
                    <li>
                      <p>For 3D cylinder groups for data size comparisons (e.g. tasks 1a and 3a)</p>
                    </li>
                    <li>
                      <p>For 3D helix groups for tasks requiring discerning between diurnal periods, weekdays or weekends (e.g. task 5a)</p>
                    </li>
                  </ul>
<p>It is also predicted that lower usability ratings will be submitted by audio-only groups (via questionnaire, see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0126-0#Sec12">5.4</a>), as participants are forced to verbalise tacit knowledge and representational characteristics.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Method</h2><div class="c-article-section__content" id="Sec8-content"><h3 class="c-article__sub-heading" id="Sec9">Participants</h3><p>A total of 120 participants (58 males and 62 females, aged between 17 and 47), comprising undergraduate and postgraduate students at the University of Nottingham, were tested in a between-subjects design. Each participant received a £5 incentive payment.</p><h3 class="c-article__sub-heading" id="Sec10">Materials</h3><p>Both 3D representations were presented on the screens of two Pentium 4 1.4 GHz IBM-compatible personal computers incorporating Matrox Millenium 550 dual head graphics cards. The representations were displayed on desktop web browsers with VRML plug-in controls installed (CosmoPlayer). Direct screen capture video recording (using two Tandberg 6000 video codecs linked to a Sony DRS20 desktop digital recorder) was used to record participants’ verbal comments. Screen desktop information was output to video codecs via the PC graphics cards. Two desk microphones were used to capture audio. In addition, two Sony digital camcorders recorded participants’ head movements. Two test rooms were set up for video conferencing, allowing the transfer of audio-visual (PC screen desktop) information between the two rooms. Screen desktop information output from each local PC was displayed remotely via two Loewe 29” CRT televisions. After completing the tasks, each participant completed a brief twenty-statement scale-based questionnaire to measure subjective judgements about the representations (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0126-0#Sec12">5.4</a>).</p><h3 class="c-article__sub-heading" id="Sec11">Procedure</h3><p>All 120 participants were randomly assigned to dyads (where participants did not formerly know each other personally). The study was conducted in two parts. In the first part, 30 dyads were allocated to the audio-only communication group. In the second part, a further 30 dyads were allocated to the audio-visual communication group. All dyads performed an identical series of five two-part tasks in one of three conditions (incorporating mixed, 3D cylinder or 3D helix representations).</p><p>All participants were seated in separate rooms. The experiment was preceded by a practice session in all test conditions. Participants were each given a set of written instructions to familiarise themselves with the plug-in controls and representational functionality, and three practice questions to familiarise themselves with the nature of the tasks. Prior to each trial, participants were instructed to agree on answers to each task, providing each answer verbally via a spokesperson (the identity of the spokesperson was decided between participants). Participants were also informed of the nature of the two-part tasks (e.g., the answers to part A of each task could be derived from the representation, whereas the answers to part B were subjective), and were instructed that they may or may not have the same 3D representation as their collaborator (participants with audio-visual communication were further informed that they could view their collaborator’s 3D representation and on-screen behaviours via the television screen). Participants were reminded to discuss what they were seeing, doing and why, what they found easy or hard, and what they thought of how the representation worked, including likes and dislikes, etc., and were informed that any device interactions (e.g. keystrokes, mouse manipulations, etc.) and verbal communication during the trials would be recorded for later analysis, and that there was a maximum of 30 min allocated to complete all tasks.</p><p>A final brief set of instructions was verbally given to each participant, informing them that they could communicate with their collaborator via the desk microphone, and that they would be able to hear their collaborator’s comments through the television speaker. All participants were reminded to agree the identity of the spokesperson before continuing, and that written notes could be taken if required. The dyads then proceeded to answer the five two-part tasks as presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab2">2</a>. All responses were given verbally via the nominated spokesperson. Following the set of tasks, participants completed a brief scale-based usability questionnaire without consulting with their collaborator.</p><h3 class="c-article__sub-heading" id="Sec12">Usability questionnaire</h3><p>The usability questionnaire was originally developed for a pilot study preceding our prior work to assess participants’ attitudes towards each of the representations reviewed (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising telecommunications usage. Behav Inform Technol 22(3):185–201" href="/article/10.1007/s10055-004-0126-0#ref-CR5" id="ref-link-section-d68104e1059">5</a>]). The original usability questionnaire has been retained and an additional dimension incorporating four statements has been added. The questionnaire now comprises 20 statements, measuring attitudes over five dimensions including System Performance, User Control, Affective Experience, Usability and Communication Process. The questionnaire uses a five-point Likert scale, requiring responses ranging from ‘Strongly Agree’ to ‘Strongly Disagree’. Three further questions on the original questionnaire measuring interaction strategies (requiring a dichotomous yes/no response) were reduced to one, mutually exclusive question. Reliability analyses were conducted to ensure that all the statements within each dimension were measuring a single concept. Cronbach’s alpha is an overall correlation coefficient that indicates a high level of consistency in a test. Items were tested together within each questionnaire usability dimension. A corrected item-total correlation was then observed for each item and no items with low correlations (e.g. below 0.15) were identified from the test. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab3">3</a> presents the revised usability questionnaire.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 
Revised representation usability questionnaire incorporating additional ‘Communication Process’ dimension</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Results</h2><div class="c-article-section__content" id="Sec13-content"><p>The presentation and discussion of results is organised into three sections: the first section presents the performance measures derived from part A task responses, the second discusses the qualitative subjective measures derived from the part B task responses, while the third presents the post-test questionnaire analysis.</p><h3 class="c-article__sub-heading" id="Sec14">Part A task responses</h3><p>The mean completion times for each task were calculated by separating each part A response from the total completion times recorded for all sequential part A and B responses. To test for equality of variances, Levene tests based on each of the mean part A task performance times were not significant (<i>p</i>&gt;0.05), indicating equal variances. Between subjects (2×3) factorial analysis of variances (ANOVAs) (Communication: audio-only, audio-visual; Representation: mixed, 3D helix and 3D cylinder) were based on the mean performance times for each part A task response. The tests revealed no main effects of communication or representation, with no significant interaction between these factors for any of the task responses (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab4">4</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 
Non-significant results for five mixed ANOVAs based on mean performance times for tasks 1a–5a</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab5">5</a> presents the mean completion times across representations and between communication groups for all part A responses. Faster completion times were observed for task 1a, as all dyads quickly identified the group demonstrating the heaviest call usage, due to the obvious density and length of the dark green cylinders. The inferential difficulty of other tasks (characterised by selecting, encoding and comparing potential target cylinders) resulted in slower completion times. Slower task 2a and 4a responses, overall, demonstrate that these tasks are indicative of increased mental effort. Despite the lack of significant differences, the observed benefits for certain tasks incorporating audio-visual communication (e.g. tasks 2a, 3a and 4a) offers some support to the prediction for a performance advantage with shared visual information, especially with identical representations.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 
Mean time (minutes) to complete tasks 1a-5a by Representation within Communication groups</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To test the accuracy of part A task responses, they were first scored according to the task specifications and data distributions (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0126-0#Sec5">3</a>). Three of the five tasks (e.g. 1a, 3a, and 5a) required single item responses, with one score marked for each correct response. Tasks 2a and 4a consisted of multi-part responses, with one score marked for each correct response given. A maximum of four scores were achievable for task 2a (four groups demonstrate specific calling patterns), whereas six scores were achievable for task 4a (six groups share calling patterns).</p><p>Ceiling effects were observed for tasks 1a and 5a. All dyads tested produced error-free responses for task 1a, and only one incorrect response was obtained for task 5a (from 3D cylinder group with audio-visual communication). Further analysis tested differences between groups in the accuracy of responses for the remaining three tasks. Levene tests based on multi-part mean error responses (e.g. tasks 2a and 4a) were significant (<i>p&lt;</i>0.05), indicating unequal variances. Non-parametric Kruskal-Wallis one-way ANOVAs based on tasks 2a and 4a mean errors revealed no significant differences between communication type (<i>p</i>&gt;0.05), although differences between representation groups for task 4a mean errors achieved near significance (<i>df</i>=2,<i> Χ</i>
<sup>2</sup>=5.023,<i> p</i>&lt;0.09), probably due to the higher mean errors produced by the audio-only mixed representations group (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab6">6</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 
Mean errors for tasks 2a and 4a by Representation within Communication groups</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab7">7</a> shows the number of dyads committing errors for task 3a by representation group within communication group. The 3D helix representation groups achieved greater accuracy than the other representation groups, with no differences in observed errors between communication groups. Chi-square analysis based on differences in errors committed between representation group and communication group was not significant (<i>Χ</i>
<sup><i>2</i></sup>=1.42,<i> p</i>&gt;0.05).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 
Number of dyads committing errors by representation group for task 3a (total number of dyads in parentheses)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec15">Part B task responses</h3><p>The analysis of the part B task responses partly follows the method prescribed by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Neale H, Nichols S (2001) Theme-based content analysis: a flexible method for virtual environment evaluation. Int J Hum Comput St 55:167–189" href="/article/10.1007/s10055-004-0126-0#ref-CR11" id="ref-link-section-d68104e2576">11</a>], namely, theme-based content analysis (TBCA). Using case studies, the TBCA method was devised as a usability tool to evaluate a variety of virtual reality technologies, including desktop environments. TBCA is a qualitative method providing information regarding user attitudes and behaviours, and indications of results in the user population by grouping data into useful categories (see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Neale H, Nichols S (2001) Theme-based content analysis: a flexible method for virtual environment evaluation. Int J Hum Comput St 55:167–189" href="/article/10.1007/s10055-004-0126-0#ref-CR11" id="ref-link-section-d68104e2579">11</a>] for a full review).</p><p>Following the method, the first author derived raw data themes directly from the transcribed part B responses. Higher level themes were devised, summarising the raw data themes at a broader level. The part B task responses were classified according to participants’ explicit verbal references to items, and implicit actions (also partly derived from part A responses). Two separate classification tables were produced, summarising these explicit references and implicit actions. To check for consistency and robustness, the tables were independently verified by two qualitative researchers based at Chimera, Institute for Socio-Technical Innovation and Research, University of Essex.</p><p>Frequencies were calculated from statements and actions falling into specific themes. Simple frequencies can identify differences or commonalities between participant groups in their approaches to completing certain tasks, task strategies and reveal representational usability problems. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab8">8</a> presents the TBCA of dyads’ explicit verbal references.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 
TBCA of dyads’ explicit references (<i>Reps</i>=number of repetitions)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/8"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The higher order theme ‘domain talk’, codes discourse about the domain (e.g. “Probably because they′re attending lectures during the day”, “That’s ‘cause she′s going out clubbing”). ‘Tool talk’, ‘representational characteristics’ and ‘data characteristics’ identify segments describing useful representational and data attributes. ‘Tool talk’ codes segments referring to plug-in tools, or representational functionality (e.g. “I used Pan and Rotate”, “The spin function just helped me”). ‘Representational characteristics codes segments referring to representational properties (e.g. “OK, it’s the layout of the representation”, “Just by looking at the light grey and darker bits”), while ‘data characteristics’ segments refer to data properties (e.g. “It was the length of the bars that helped us”, “It was obviously the different colours that helped us”). ‘Representational envy/Representational boasting’ applies to the mixed representations group with audio-visual communication, referring to segments where participants express a preference for either their collaborator’s representation displayed on the TV screen (‘envy’: “I think yours is easier to understand than mine”) or their own representation (‘boasting’: “I think mine is easier to see than yours”). ‘Representational equivalence’ applies to audio-only groups, highlighting segments where participants have attempted to establish whether they share identical representations (e.g. “How does it look because we don’t necessarily have the same representation”), or where participants have assumed that the representations are identical (e.g. “I’m assuming you’ve got the same as me”). ‘Representational uncertainty’ is applied to segments where participants are unsure how to interpret part of the representation (e.g. “What do you think for that region, is that what represents the weekend?”) and where they are unsure how to complete a sub-goal (e. g. “Where do you click the rotate?”).</p><p>The remaining higher order themes code participants’ references to task strategies, usability problems experienced with the representations and the data, suggested new functionality and include comments relating to task difficulty.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab9">9</a> presents the TBCA of participants’ implicit comments and actions. The higher order theme ‘misinterpretation of action/comments’ codes actions or discourse where participants have misinterpreted functionality suggested by their collaborator (e.g. using spin function instead of Cosmo rotate control), or where participants have misinterpreted their collaborator’s explanations (e.g. exploring inner most spiral when their collaborator was referring to the outer most spiral). ‘Task collaboration’ is characterised by three raw data themes: ‘hypothesis statements’, ‘instructive comments/advice’ and ‘splitting activities/sub-goal’, whereas ‘non-collaboration’, refers to periods where participants work alone (e.g. characterised by extended periods without dialogue) only conversing to agree a response.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 
TBCA of dyads implicit actions (<i>Reps</i>=number of repetitions)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/9"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Qualitative observations</h4><p>For ‘tool talk’ (apart from VRML plug-in references), pop-up messages were often cited as a useful function, especially with groups utilising audio-visual communication. All dyads using the 3D cylinder representation in the audio-visual condition also cited the utility of the spin function. The most cited ‘representational characteristic’ across groups was the light/dark shading on the spirals, whereas popular ‘data characteristics’ included cylindrical length and frequency and, to a lesser extent, cylindrical colours.</p><p>Participants in the audio-visual mixed representations group witnessed their collaborator’s manipulations via the TV screen, and a number of participants made judgements about their collaborator’s representation. Some participants using the 3D cylinder declared ‘representational envy’ for the 3D helix (more so than vice versa), stating that the information appeared clearer, or that it appeared easier to use. One participant elaborated on comments she had made earlier about her collaborator’s representation: “Yeah, I think it’s easier to use, the one that K’s using (3D helix), I would actually look at that rather than look at my screen for that one, ‘cause my one’s too messed up, well too obscured (3D cylinder).” Other related comments (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab10">10</a>) offer support for the prediction that shaded and coloured segments appear more prominent around the helical form of the 3D helix (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0126-0#Sec7">4.1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10 
Mixed representations (audio-visual communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/10"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>A number of participants also demonstrated ‘representational boasting’, stating that the information in their own representation was more legible, or that it appeared more usable than their collaborator’s, e.g. “... I think I can see that easier on my representation... so it’s easier to see on mine, I think (3D helix).”</p><p>‘Representational equivalence’ applies to groups incorporating audio-only communication. Of these groups, participants using mixed representations made more explicit attempts to establish whether they were using identical displays, although three participants erroneously assumed that they were sharing the same representation. Of the groups using identical representations, four participants using the 3D helix correctly assumed that they were sharing the same representation, based on their collaborator’s descriptions.</p><p>Low frequencies of ‘representational uncertainty’ occurred, overall. Typically, when participants stated that they were unsure what part of the representation meant, or were unsure how to achieve a sub-goal, their collaborator intervened, offering instructive comments and advice. These exchanges represent integral components of task collaboration (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab11">11</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b id="Tab11" data-test="table-caption">Table 11 
Mixed representations (audio-visual communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/11"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Certain strategies cited by dyads revealed some striking differences between representational groups in the actual strategies adopted. Most groups stated that they adopted the strategy of counting cylinders. ‘Lining up data by colour or position’ was also a popular strategy. However, the strategy of ‘observing patterns along the spirals’ was more predominant with both 3D helix groups. This strategy is indicative of high computational offloading, but was cited less by the mixed or 3D cylinder groups.</p><p>Data occlusion remained a frequently cited representational usability problem, particularly with groups in audio-visual contexts. Audio-only groups cited data usability issues, including indistinct light/dark green colours, or one cylindrical colour standing out more than the other colours. Suggestions for new functionality were occasionally raised, while the 3D helix group incorporating audio-visual communication were more verbose regarding the theme ‘task difficulty’.</p><p>‘Misinterpretation of actions/comments’ represents implicit actions demonstrated by audio-only groups. A common mistake made by participants involved using the Cosmo rotate tool as their collaborator referred to the spin function (or vice versa, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab12">12</a>). Only one dyad using audio-visual communication misinterpreted functionality.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-12"><figure><figcaption class="c-article-table__figcaption"><b id="Tab12" data-test="table-caption">Table 12 
3D helix representations (audio-only communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/12"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Finally, all dyads demonstrated elements of task collaboration, frequently proposing hypothesis statements and instructive comments or advice. The most frequent instances of splitting activities or sub-goals occurred with the mixed representations group with audio-only communication. Evidence of non-collaboration was more frequently demonstrated by both 3D cylinder groups, and by the mixed representations group with audio-only communication.</p><h3 class="c-article__sub-heading" id="Sec17">Questionnaire ratings</h3><p>To test the prediction that lower questionnaire ratings would be submitted by the audio-only groups (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0126-0#Sec7">4.1</a>), the means for the five questionnaire dimensions (System Performance, User Control, Affective Experience, Usability and Communication Process) were calculated from all questionnaire ratings. Levene tests based on mean questionnaire dimension ratings were not significant, indicating equal variances. A 2×3 between factorial analysis of variance (ANOVA) (Communication: audio only, and audio and visual; Representation: mixed, 3D helix and 3D cylinder) revealed a significant main effect of communication for the Usability dimension (<i>df</i>=1, 114,<i> F</i>=4.650,<i> p</i>&lt;0.05), and significant main effects of representation for Affective Experience (<i>df</i>=2, 114,<i> F</i>=5.571,<i> p</i>&lt;0.05) and Communication Process dimensions (<i>df</i>=2, 114,<i> F</i>=3.517,<i> p</i>&lt;0.05). No significant interactions were observed between communication and representation groups. Independent<i> t</i>-tests revealed significant differences in Usability mean dimension ratings between both groups using 3D cylinder representations, with higher ratings observed for the group incorporating audio-only communication. Further, Tukey tests also revealed significantly higher Affective Experience mean dimension ratings (<i>p</i>&lt;0.05) for groups using 3D helix than mixed representations, and significantly higher Communication Process mean dimension ratings (<i>p</i>&lt;0.05) for groups using 3D cylinder than mixed representations.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig7">7</a> presents the mean ratings for each of the five questionnaire dimensions. High rating scores contribute to a high overall user experience score.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb7.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb7.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p> Mean ratings for System Performance, User Control, Affective Experience, Usability and Communication Process questionnaire dimensions (error bars represent standard error of means)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Further analysis included the binary responses for the question relating to interaction strategies (e.g. writing or remembering information, or neither). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0126-0#Fig8">8</a> shows the number of participants by communication and representation group indicating their strategy choices. A large proportion of the 3D cylinders group with audio-only communication indicated that they relied on writing, rather than remembering information, which may be indicative of less computational offloading afforded by the representations. This trend was also apparent, although less dramatically, with the mixed representations groups. A quarter of both groups indicated no particular strategy preference. The strategies adopted by the 3D helix group with audio-only communication was roughly equal, whereas in the audio-visual condition, the group relied less on remembering information.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0126-0/MediaObjects/s10055-004-0126-0fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p> Number of participants indicating interaction strategy during task performance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0126-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Discussion</h2><div class="c-article-section__content" id="Sec18-content"><p>The prediction for an overall performance advantage for groups utilising audio-visual communication was based on the assumption that forced verbalisations were unnecessary, due to the presence of shared visual information. However, the results confirm that this hypothesis was partly confounded by the ceiling effects observed for tasks 1a and 5a, where all dyads tested achieved virtually error-free performance. For task 1a, all dyads quickly and correctly perceived the group demonstrating the heaviest usage, due to the obvious density and length of the dark green cylinders. Similarly, all but one of the 60 dyads tested, correctly determined which group called at 10 pm weekdays (task 5a). One participant offered an insightful explanation: “... if for that question you had chosen, I don’t know, 2 o’clock in the afternoon, or something, it would have been a lot, lot harder to pick out 2 o’clock in the afternoon, but because there’s just one person calling all the time at that time (10 o’clock) you can sort of pick it out.” This comment illustrates that lighter data distribution at certain diurnal periods allows call usage to be perceived more easily.</p><p>Of the three remaining tasks (tasks 2a, 3a, &amp; 4a), some benefits were observed when audio-visual communication was adopted. Faster completion times were observed, especially when identical representations were shared, although differences were not significant. For task 4a, virtually error-free performance was achieved by groups using mixed and helix representations incorporating audio-visual communication.</p><p>Representing a methodological point, certain authors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Monk A, McCarthy J, Watts L, Daly-Jones O (1997) Measures of process. In: MacLeod M, Murray D (eds) Evaluation for CSCW. Springer, Berlin Heidelberg New York, pp 1–7" href="/article/10.1007/s10055-004-0126-0#ref-CR10" id="ref-link-section-d68104e7375">10</a>] maintain that a full evaluation of communication technology requires measures of both task outcome and communication process. Following this approach, we also consider qualitative measures derived from TBCA and post-test questionnaire analysis. The outcome of the TBCA provides an insight into the usefulness of certain representational and data attributes, and participants’ strategies, assumptions, and problems encountered during collaborative problem solving.</p><p>The TBCA exposed differences in the strategies adopted by participants between representation groups. The majority of dyads in both 3D cylinder groups positively reflected on use of the spin function, compared to only 3 dyads from both 3D helix groups. The first author found that spinning the 3D cylinder representation was an effective method in overcoming data occlusion during the expert walkthrough. A similar strategy involves positioning the 3D cylinder so that it appears horizontally flat while spinning the representation, as confirmed by one participant: “... ‘cause yes, sometimes they overlap, but when you get it at the right angle, and I found if I spin it around, then it’s quite easy to tell if they’re overlapping.”</p><p>The representational characteristic most frequently cited as helpful was the light/dark shading on the spirals. This characteristic affords the observation of patterns along the spirals, a strategy predominantly reported by groups using the helix representation, as reported in our prior work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising telecommunications usage. Behav Inform Technol 22(3):185–201" href="/article/10.1007/s10055-004-0126-0#ref-CR5" id="ref-link-section-d68104e7384">5</a>]. One participant manipulating the 3D helix offered an insightful comment, which suggests that shading and patterns along the spirals enhances computational offloading: “... I like the dark bits that tells you when it’s the night... and that you can click on it that tells you... it’s quite difficult to add up in your head, but I suppose you can see the pattern of where they are in the dark bits, so you don’t really need to write anything down.”</p><p>Another participant confirmed that data occlusion occurs if the 3D helix representation is viewed from the front, or ‘Start View’: “I just find it difficult to distinguish between the weeks, especially when it’s straight on, because some bars (cylinders) look like... you can’t see whether it’s a continuation or not (data occlusion).” Data occlusion was the most frequently cited usability problem, particularly by dyads adopting audio-visual communication. Arguably, shared visual information affords discussion, including usability and other issues.</p><p>The higher order theme ‘representational envy’ was specifically applicable to the mixed representations group in the audio-visual condition. The 3D helix was particularly ‘envied’ by participants using the 3D cylinder representation (one segment supports the prediction that the shaded and coloured segments appear more prominent on the 3D helix, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab10">10</a>). In some instances, both participants preferred or envied each others’ representations, illustrated in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab13">13</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-13"><figure><figcaption class="c-article-table__figcaption"><b id="Tab13" data-test="table-caption">Table 13 
Mixed representations (audio-visual communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/13"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Dyads with mixed representations appeared to make general assumptions about their collaborator’s display. One explanation is that participants dislike their own representation, preferring their collaborator’s instead, or that their collaborator is demonstrating better control of their representation. The segment in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab14">14</a> illustrates two participants reaching a consensus that the 3D helix is preferable.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-14"><figure><figcaption class="c-article-table__figcaption"><b id="Tab14" data-test="table-caption">Table 14 
Mixed representations (audio-visual communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/14"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>For the higher order theme ‘representational equivalence’, the audio-only mixed representation group instigated the highest number of attempts where participants attempted to establish whether representations were identical. The group also demonstrated more task splitting activities compared to other groups. After establishing representational incongruence, participants may have divided sub-goals to achieve more effective task collaboration. However, the similarity of the two representations may have confounded these findings. Unless a detailed verbal description is provided, the representations could be assumed to be similar, or identical, as illustrated at Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0126-0#Tab15">15</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-15"><figure><figcaption class="c-article-table__figcaption"><b id="Tab15" data-test="table-caption">Table 15 
Mixed representations (audio-only communication) transcript sample</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/tables/15"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>A further benefit for the presence of shared visual information was demonstrated by participants correctly inferring their collaborator’s actions and explanations. A common mistake made by participants incorporating audio-only communication involved misinterpreting their collaborator’s references to representational functionality (e.g., confusing spin function with Cosmo rotate tool).</p><p>Shared visual information appears to have made an impact on certain questionnaire dimensions. Participants in the audio-visual groups provided higher ratings for Communication Process, as the presence of shared visual information aided communication between participants (e.g. forced verbalisation was unnecessary). In addition, participants using identical 3D helix and 3D cylinder representations in visually shared contexts expressed higher Affective Experience ratings. Generally, mixed representations attracted lower ratings overall for all five questionnaire dimensions, demonstrating significantly lower Affective Experience ratings compared to 3D helix groups, and lower Communication Process ratings compared to 3D cylinder groups.</p><p>However, the presence of shared visual information also supports discourse concerning other issues, such as usability problems. Groups incorporating audio-only communication provided higher Usability ratings than those in audio-visual groups. This finding was confirmed by the TBCA, where usability problems, such as data occlusion, were more frequently cited by audio-visual groups.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusions</h2><div class="c-article-section__content" id="Sec19-content"><p>It is evident from the findings presented that the presence or absence of shared visual information influences collaborative problem solving. As highlighted by other studies (e.g. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising telecommunications usage. Behav Inform Technol 22(3):185–201" href="/article/10.1007/s10055-004-0126-0#ref-CR5" id="ref-link-section-d68104e7690">5</a>]), by looking at task performance alone, one cannot conclude that differences exist between audio-visual and audio-only conditions. However, qualitative measures exposed by the TBCA and post-test questionnaire analysis revealed differences between representation and communication groups. The analysis revealed that certain representational properties facilitated participants’ task completion, also highlighting that different strategies were adopted between representation groups. The 3D helix groups demonstrated computational offloading by observing patterns along the spirals, whereas the majority of the audio-only 3D cylinders group indicated (via their questionnaire responses) that they had relied on writing notes, which is indicative of lower computational offloading afforded by the representation.</p><p>The presence of shared visual information promotes discourse. Audio-visual groups more frequently cited usability issues such as occlusion problems, resulting in a lower Usability questionnaire score than audio-only groups. Audio-visual groups also submitted higher Communication Process questionnaire ratings, indicating an overall preference for shared information. An important benefit of presenting visually shared information within collaborative environments is that it provides the capacity to correctly infer actions and explanations. Audio-only contexts place greater emphasis on verbal references, forcing participants to verbalise tacit knowledge which can, in turn, be easily misconstrued.</p><p>Although the TBCA indicates that all representation groups demonstrated task collaboration, participants in the mixed representations groups submitted lower questionnaire ratings, overall, with significantly lower Affective Experience ratings compared to 3D helix groups, and lower Communication Process ratings compared to 3D cylinder groups. The reasons why these groups should provide lower subjective ratings is not entirely clear. It is possible that some participants in the audio-only group were influenced by the knowledge that they were not sharing identical representations, or were frustrated by their collaborator’s verbal descriptions. Similarly, it is feasible that some participants in the audio-visual group submitted lower ratings for their own representation, based on their impressions of their collaborator’s 3D display (representational envy).</p><p>To summarise, there are clear indications that the presence of shared visual information enhances collaborative problem solving. The inclusion of mixed representations in collaborative environments does not, necessarily, contribute to the communication process, whereas the inclusion of identical representations, particularly in a shared workspace, offers a more familiar setting for participation.</p><p>Further studies could examine problem solving behaviours under different shared distributed representations, for example, complementary representations (i.e. each participant accesses complementary subsets of the data set). The trade-off with complementary representations is that, although cognitive effort is arguably reduced as each collaborator makes inferences and judgements from reduced data sets, certain contextual information necessary to inform decisions is absent, forcing communication between collaborators in order to find task solutions. This concept follows the paradigm that the abstract task space of a group problem solving task can be distributed across individual representations in different ways [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Zhang J (1998) A distributed representation approach to group problem solving. J Am Soc Inform Sci 49(9):801–809" href="/article/10.1007/s10055-004-0126-0#ref-CR17" id="ref-link-section-d68104e7703">17</a>], yielding different problem solving behaviours, performance measures and qualitative information.</p><p>Further work is also required to address some of the usability issues highlighted in this study—the main problem being data occlusion. Possible solutions could include data filtering, enabling subsets of data to be displayed at any one time in order to reduce data density. Although both representations were presented offline, either representation could be developed in desktop web-based VR environments to distribute data visualisations over the Internet, allowing multiple users to access the information concurrently.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ainsworth SE, Peevers GJ (2003) The interaction between informational and computational properties on problem-" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Ainsworth SE, Peevers GJ (2003) The interaction between informational and computational properties on problem-solving and learning. In: Proceedings of the 25th annual conference of the Cognitive Science Society, Boston, Massachusetts, July/August 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Chapanis, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Chapanis A (1975). Interactive human communication. Sci Am 232:3–42" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Chapanis A (1975). Interactive human communication. Sci Am 232:3–42</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Sci%20Am&amp;volume=232&amp;publication_year=1975&amp;author=Chapanis%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Chou, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Chou C, Tsai C-C, Tsai H-F (2001) Developing a networked VRML learning system for health science education in " /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Chou C, Tsai C-C, Tsai H-F (2001) Developing a networked VRML learning system for health science education in Taiwan. Int J Educ Dev 21:293–303</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0738-0593%2800%2900003-1" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Int%20J%20Educ%20Dev&amp;volume=21&amp;publication_year=2001&amp;author=Chou%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Doherty-Sneddon, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Doherty-Sneddon G, O’Malley C, Garrod S, Anderson A, Langton S, Bruce V (1997) Face-to-face and video-mediated" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Doherty-Sneddon G, O’Malley C, Garrod S, Anderson A, Langton S, Bruce V (1997) Face-to-face and video-mediated communication: A comparison of dialogue structure and task performance. J Exp Psychol Appl 3(2):105–125</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F1076-898X.3.2.105" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J%20Exp%20Psychol%20Appl&amp;volume=3&amp;publication_year=1997&amp;author=Doherty-Sneddon%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Hicks, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising tele" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Hicks M, O’Malley C, Nichols S, Anderson B (2003) Comparison of 2D and 3D representations for visualising telecommunications usage. Behav Inform Technol 22(3):185–201</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F0144929031000117080" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Behav%20Inform%20Technol&amp;volume=22&amp;publication_year=2003&amp;author=Hicks%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hutchins E (1995) Cognition in the wild. MIT Press, Cambridge" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Hutchins E (1995) Cognition in the wild. MIT Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Kotovsky, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Kotovsky K, Simon HA (1990) What makes some problems really hard: Explorations in the problem space difficulty" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Kotovsky K, Simon HA (1990) What makes some problems really hard: Explorations in the problem space difficulty. Cognitive Psychol 22:143–183</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cognitive%20Psychol&amp;volume=22&amp;publication_year=1990&amp;author=Kotovsky%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Larkin, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Larkin JH, Simon HA (1987) Why a diagram is (sometimes) worth ten thousand words. Cognitive Sci 11:65–99" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Larkin JH, Simon HA (1987) Why a diagram is (sometimes) worth ten thousand words. Cognitive Sci 11:65–99</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cognitive%20Sci&amp;volume=11&amp;publication_year=1987&amp;author=Larkin%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Meister D (1986) Human factors testing and evaluation. Elsevier, New York" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Meister D (1986) Human factors testing and evaluation. Elsevier, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Monk A, McCarthy J, Watts L, Daly-Jones O (1997) Measures of process. In: MacLeod M, Murray D (eds) Evaluation" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Monk A, McCarthy J, Watts L, Daly-Jones O (1997) Measures of process. In: MacLeod M, Murray D (eds) Evaluation for CSCW. Springer, Berlin Heidelberg New York, pp 1–7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Neale, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Neale H, Nichols S (2001) Theme-based content analysis: a flexible method for virtual environment evaluation. " /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Neale H, Nichols S (2001) Theme-based content analysis: a flexible method for virtual environment evaluation. Int J Hum Comput St 55:167–189</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.2001.0475" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0971.68568" aria-label="View reference 11 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Int%20J%20Hum%20Comput%20St&amp;volume=55&amp;publication_year=2001&amp;author=Neale%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Ochsman, " /><meta itemprop="datePublished" content="1974" /><meta itemprop="headline" content="Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behaviour of teams during co-operat" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Ochsman RB, Chapanis A (1974) The effects of 10 communication modes on the behaviour of teams during co-operative problem-solving. Int J Man Mach Stud 6:579–619</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Int%20J%20Man%20Mach%20Stud&amp;volume=6&amp;publication_year=1974&amp;author=Ochsman%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Okada, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Okada T, Simon HA (1997) Collaborative discovery in a scientific domain. Cognitive Sci 21(2):109–146" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Okada T, Simon HA (1997) Collaborative discovery in a scientific domain. Cognitive Sci 21(2):109–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0364-0213%2899%2980020-2" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cognitive%20Sci&amp;volume=21&amp;publication_year=1997&amp;author=Okada%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Perkins DN (1993) Person-plus: A distributed view of think and learning. In: Salomon G (ed) Distributed cognit" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Perkins DN (1993) Person-plus: A distributed view of think and learning. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge , pp 88–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salomon G (1993) No distribution without individuals’ cognition: A dynamic interactional view. In: Salomon G (" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Salomon G (1993) No distribution without individuals’ cognition: A dynamic interactional view. In: Salomon G (ed) Distributed cognition: Psychological and educational considerations. Cambridge University Press, Cambridge, pp 111–138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Zhang, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Zhang J (1997) The nature of external representations in problem solving. Cognitive Sci 21(2):179–217" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Zhang J (1997) The nature of external representations in problem solving. Cognitive Sci 21(2):179–217</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0364-0213%2899%2980022-6" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1015.68687" aria-label="View reference 16 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Cognitive%20Sci&amp;volume=21&amp;publication_year=1997&amp;author=Zhang%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Zhang, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Zhang J (1998) A distributed representation approach to group problem solving. J Am Soc Inform Sci 49(9):801–8" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Zhang J (1998) A distributed representation approach to group problem solving. J Am Soc Inform Sci 49(9):801–809</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2F%28SICI%291097-4571%28199807%2949%3A9%3C801%3A%3AAID-ASI5%3E3.0.CO%3B2-W" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J%20Am%20Soc%20Inform%20Sci&amp;volume=49&amp;publication_year=1998&amp;author=Zhang%2C">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0126-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by a research studentship funded by University of Nottingham and BT.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><span class="c-article-author-information__subtitle u-visually-hidden" id="author-notes">Author notes</span><ol class="c-article-author-information__list"><li class="c-article-author-information__item" id="nAff3"><p class="c-article-author-information__authors-list">Martin Hicks</p><p class="js-present-address">Present address: pp 1 Ross Building, Adastral Park, Martlesham Heath, Ipswich, Suffolk, IP5 3RE, UK</p></li></ol><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Mechanical, Materials, Manufacturing Engineering &amp; Management and School of Psychology, University of Nottingham, Nottingham, UK</p><p class="c-article-author-affiliation__authors-list">Martin Hicks, Sarah Nichols &amp; Claire O’Malley</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Chimera, Institute for Socio-Technical Innovation and Research, University of Essex, UK</p><p class="c-article-author-affiliation__authors-list">Martin Hicks</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Martin-Hicks"><span class="c-article-authors-search__title u-h3 js-search-name">Martin Hicks</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Martin+Hicks&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Martin+Hicks" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Martin+Hicks%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Sarah-Nichols"><span class="c-article-authors-search__title u-h3 js-search-name">Sarah Nichols</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Sarah+Nichols&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Sarah+Nichols" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Sarah+Nichols%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Claire-O_Malley"><span class="c-article-authors-search__title u-h3 js-search-name">Claire O’Malley</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Claire+O%E2%80%99Malley&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Claire+O%E2%80%99Malley" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Claire+O%E2%80%99Malley%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0126-0/email/correspondent/c1/new">Martin Hicks</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Comparing%20the%20roles%20of%203D%20representations%20in%20audio%20and%20audio-visual%20collaborations&amp;author=Martin%20Hicks%20et%20al&amp;contentID=10.1007%2Fs10055-004-0126-0&amp;publication=1359-4338&amp;publicationDate=2004-05-19&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Hicks, M., Nichols, S. &amp; O’Malley, C. Comparing the roles of 3D representations in audio and audio-visual collaborations.
                    <i>Virtual Reality</i> <b>7, </b>148–163 (2004). https://doi.org/10.1007/s10055-004-0126-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0126-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-12">12 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-03-29">29 March 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-05-19">19 May 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0126-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0126-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">3D representations</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Information visualisation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative problem solving</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0126-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=126;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

