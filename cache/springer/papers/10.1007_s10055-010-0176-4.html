<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Virtual and augmented reality for cultural computing and heritage: a c"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The paper presents different issues dealing with both the preservation of cultural heritage using virtual reality (VR) and augmented reality (AR) technologies in a cultural context. While the VR/AR..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint)"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-10-29"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The paper presents different issues dealing with both the preservation of cultural heritage using virtual reality (VR) and augmented reality (AR) technologies in a cultural context. While the VR/AR technologies are mentioned, the attention is paid to the 3D visualization, and 3D interaction modalities illustrated through three different demonstrators: the VR demonstrators (immersive and semi-immersive) and the AR demonstrator including tangible user interfaces. To show the benefits of the VR and AR technologies for studying and preserving cultural heritage, we investigated the visualisation and interaction with reconstructed underwater archaeological sites. The base idea behind using VR and AR techniques is to offer archaeologists and general public new insights on the reconstructed archaeological sites allowing archaeologists to study directly from within the virtual site and allowing the general public to immersively explore a realistic reconstruction of the sites. Both activities are based on the same VR engine, but drastically differ in the way they present information and exploit interaction modalities. The visualisation and interaction techniques developed through these demonstrators are the results of the ongoing dialogue between the archaeological requirements and the technological solutions developed."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-10-29"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="311"/>

    <meta name="prism.endingPage" content="327"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0176-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0176-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0176-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0176-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint)"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2011/11"/>

    <meta name="citation_online_date" content="2010/10/29"/>

    <meta name="citation_firstpage" content="311"/>

    <meta name="citation_lastpage" content="327"/>

    <meta name="citation_article_type" content="SI: Cultural Technology"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0176-4"/>

    <meta name="DOI" content="10.1007/s10055-010-0176-4"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0176-4"/>

    <meta name="description" content="The paper presents different issues dealing with both the preservation of cultural heritage using virtual reality (VR) and augmented reality (AR) technolog"/>

    <meta name="dc.creator" content="Mahmoud Haydar"/>

    <meta name="dc.creator" content="David Roussel"/>

    <meta name="dc.creator" content="Madjid Ma&#239;di"/>

    <meta name="dc.creator" content="Samir Otmane"/>

    <meta name="dc.creator" content="Malik Mallem"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Acevedo D, Vote E, Laidlaw DH, Joukowsky MS (2001) Archaeological data visualization in VR: analysis of lamp finds at the great temple of Petra, a case study. In: Proceedings of the 12th IEEE conference on visualization (VIS &#8217;01). IEEE Computer Society, Washington, DC, pp 493&#8211;496"/>

    <meta name="citation_reference" content="ART (2009) Advanced realtime tracking"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot, R Behringer, S Feiner, S Julier, B MacIntyr; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-47; citation_doi=10.1109/38.963459; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=British Archaeol Rep; citation_title=Visualizing what might be: an introduction to virtual reality techniques in archaeology. Virtual reality in archaeology; citation_author=JA Barcel&#243;; citation_volume=S843; citation_publication_date=2000; citation_pages=9-36; citation_id=CR4"/>

    <meta name="citation_reference" content="Bell B, H&#246;llerer T, Feiner S (2002) An annotated situation-awareness aid for augmented reality. In: UIST &#8217;02: proceedings of the 15th annual ACM symposium on user interface software and technology. ACM, New York, pp 213&#8211;216"/>

    <meta name="citation_reference" content="Benko H, Ishak EW, Feiner S (2004) Collaborative mixed reality visualization of an archaeological excavation. In: ISMAR &#8217;04: proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality. IEEE Computer Society, Washington, DC, pp 132&#8211;140"/>

    <meta name="citation_reference" content="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: VRAIS &#8217;98: proceedings of the virtual reality annual international symposium. IEEE Computer Society, Washington, DC, pages 20"/>

    <meta name="citation_reference" content="Billinghurst M, Kato H, Poupyrev I (2001) Collaboration with tangible augmented reality interfaces. In: HCI &#8217;2001: international conference on human computer interaction, New Orleans"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces: theory and practice; citation_publication_date=2005; citation_id=CR9; citation_author=AD Bowman; citation_author=E Kruijff; citation_author=I Laviola; citation_author=J Poupyrev; citation_publisher=Addison Wesley Longman Publishing Co., Inc."/>

    <meta name="citation_reference" content="Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance with different display devices for virtual environments. In: Proceedings of the human factors and ergonomics society annual meeting (HFES&#8217; 02). Human Factors and Ergonomics Society, pp 2134&#8211;2138"/>

    <meta name="citation_reference" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: VRAIS &#8217;97: proceedings of the 1997 virtual reality annual international symposium (VRAIS &#8217;97). IEEE Computer Society, Washington, DC, page 45"/>

    <meta name="citation_reference" content="Burns D, Osfield R (2004) Open scene graph a: introduction, b: examples and applications. In: VR &#8217;04: proceedings of the IEEE virtual reality 2004. IEEE Computer Society, Washington, DC, page 265"/>

    <meta name="citation_reference" content="CC (2009) The cultural computing program"/>

    <meta name="citation_reference" content="Cosmas, John, Take I, Damian G, Edward G, Fred W, Luc VG, Alexy Z, Desi V, Franz L, Markus G, Konrad S, Konrad K, Michael G, Stefan H, Marc W, Marc P, Roland D, Robert S, Martin K (2001) 3D MURALE: a multimedia system for archaeology. In: VAST &#8217;01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 297&#8211;306"/>

    <meta name="citation_reference" content="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the CAVE. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, ACM, New York, pp 135&#8211;142"/>

    <meta name="citation_reference" content="Domingues C, Otmane S, Davesne F, Mallem M (2008) Creating 3D interaction technique empirical evaluation with the use of a knowledge database of interaction experiments. In: Human system interactions, 2008 Conference on, pp 170&#8211;175"/>

    <meta name="citation_reference" content="citation_journal_title=J Photogramm Remote Sensing; citation_title=A digital photogrammetric workstation on the WEB; citation_author=P Drap, P Grussenmeyer; citation_volume=55; citation_issue=1; citation_publication_date=2000; citation_pages=48-58; citation_doi=10.1016/S0924-2716(99)00038-6; citation_id=CR17"/>

    <meta name="citation_reference" content="Drap P, Long L (2001) Towards a digital excavation data management system: the &#8220;Grand Ribaud F&#8221; Estruscan deep-water wreck. In: VAST &#8217;01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 17&#8211;26"/>

    <meta name="citation_reference" content="Drap P, Nedir M, Seinturier J, Papini O, Chapman P, Boucault F, Viant W, Vannini G, Nuccioti M (2006) Toward a photogrammetry and virtual reality based heritage information system: a case study of shawbak castle in jordan. In: Joint event conference of the37th CIPA international workshop dedicated on e-documentation and standardisation in cultural heritage, 7th VAST international symposium on virtual reality, archaeology and cultural heritage, 4th eurographics workshop on graphics and cultural heritage and 1st Euro-med conference on IT in cultural heritage"/>

    <meta name="citation_reference" content="D&#252;nser A, Grasset R, Billinghurst M (2008) A survey of evaluation techniques used in augmented reality studies. In: international conference on computer graphics and interactive techniques. ACM, New York"/>

    <meta name="citation_reference" content="Gaitatzes A, Christopoulos D, Roussou M (2001) Reviving the past: cultural heritage meets virtual reality. In: VAST &#8217;01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 103&#8211;110"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=A multi-view VR interface for 3D GIS; citation_author=R Germs, G Van Maren, E Verbree, FW Jansen; citation_volume=23; citation_issue=4; citation_publication_date=1999; citation_pages=497-506; citation_doi=10.1016/S0097-8493(99)00069-2; citation_id=CR22"/>

    <meta name="citation_reference" content="Gorbet MG, Orth M, Ishii H (1998) Triangles: tangible interface for manipulation and exploration of digital information topography. In: CHI &#8217;98: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 49&#8211;56"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Nautical Archaeol; citation_title=Presenting archaeology on the Web: The La Salle Shipwreck Project; citation_author=A Hall Rebecca, W Hall Andrew, J Barto Arnold; citation_volume=26; citation_issue=3; citation_publication_date=1997; citation_pages=247-251; citation_id=CR24"/>

    <meta name="citation_reference" content="Haydar M, Ma&#239;di M, Roussel D, Mallem M (2009) A new navigation method for 3D virtual environment exploration. In: AIP (ed) The 2nd Mediterranean conference on intelligent systems and automation (CISA 2009), vol 1107. AIP, Zarzis (Tunisia), pp 190&#8211;195"/>

    <meta name="citation_reference" content="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassell N (1997) Usability analysis of 3D rotation techniques. In: Proceedings of the 10th annual ACM symposium on user interface software and technology. ACM, New York, pp 1&#8211;10"/>

    <meta name="citation_reference" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI &#8217;97: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 234&#8211;241"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Blanding B, May R (1999) ARToolKit (Technical Report). Technical report, Hiroshima City University"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: ISAR&#8217; 00 : proceedings of the international symposium on augmented reality, Munich, pp 111&#8211;119"/>

    <meta name="citation_reference" content="Looser J, Billinghurst M, Grasset R, Cockburn A (2007) An evaluation of virtual lenses for object selection in augmented reality. In: Proceedings of the 5th international conference on computer graphics and interactive techniques in Australia and Southeast Asia. ACM, New York, pp 203&#8211;210"/>

    <meta name="citation_reference" content="Looser J, Grasset R, Seichter H, Billinghurst M (2006) OSGART&#8212;a pragmatic approach to MR. In: International symposium of mixed and augmented reality (ISMAR 2006), Santa Barbara"/>

    <meta name="citation_reference" content="McMahan RP, Bowman DA (2007) An empirical comparison of task sequences for immersive virtual environments. In: IEEE symposium on 3D user interfaces"/>

    <meta name="citation_reference" content="Mine Mark R Jr., Brooks Frederick P, Sequin Carlo H (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: SIGGRAPH &#8217;97: Proceedings of the 24th annual conference on computer graphics and interactive techniques, vol 31. ACM Press/Addison-Wesley Publishing Co, pp 19&#8211;26"/>

    <meta name="citation_reference" content="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guide as an apparatus for augmented reality based telemanipulation system on the Internet. In: Proceedings of the 33rd annual simulation symposium, (SS 2000). IEEE Computer Society"/>

    <meta name="citation_reference" content="Poupyrev, Ivan, Tan Desney S, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2001) Tiles: a mixed reality authoring interface. In: INTERACT 2001 conference on human computer interaction, Tokyo, pp 334&#8211;341"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real J; citation_title=On study of design and implementation of virtual fixtures; citation_author=R Prada, S Payandeh; citation_volume=13; citation_issue=2; citation_publication_date=2009; citation_pages=117-129; citation_doi=10.1007/s10055-009-0115-4; citation_id=CR36"/>

    <meta name="citation_reference" content="Reichlen BA (1993) Sparcchair: a one hundred million pixel display. In: 1993 IEEE virtual reality annual international symposium, pp 300&#8211;307"/>

    <meta name="citation_reference" content="citation_title=Virtual reality; citation_publication_date=1991; citation_id=CR38; citation_author=H Rheingold; citation_publisher=Summit Books"/>

    <meta name="citation_reference" content="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In: Proceedings of IEEE virtual reality international symposium, pp 76&#8211;82"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators Virtual Environ; citation_title=Navigating large-scale virtual environments: what differences occur between helmet-mounted and desk-top displays?; citation_author=RA Ruddle, SJ Payne, DM Jones; citation_volume=8; citation_issue=2; citation_publication_date=1999; citation_pages=157-168; citation_doi=10.1162/105474699566143; citation_id=CR40"/>

    <meta name="citation_reference" content="Stoakley R, Conway Mathew J, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: CHI &#8217;95: Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 265&#8211;272"/>

    <meta name="citation_reference" content="Taylor II, Russell M, Hudson Thomas C, Seeger A, Weber H, Juliano J, Helser Aron T (2001) VRPN: a device-independent, network-transparent VR peripheral system. In: VRST &#8217;01: proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 55&#8211;61"/>

    <meta name="citation_reference" content="citation_journal_title=Lect Notes Comput Sci; citation_title=Cultural computing with context-aware application: ZENetic computer; citation_author=N Tosa, S Matsuoka, B Ellis, H Ueda, R Nakatsu; citation_volume=3711; citation_publication_date=2009; citation_pages=13-23; citation_doi=10.1007/11558651_2; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Grap; citation_title=Experiments in immersive virtual reality for scientific visualization; citation_author=A Van Dam, DH Laidlaw, RM Simpson; citation_volume=26; citation_issue=4; citation_publication_date=2002; citation_pages=535-555; citation_doi=10.1016/S0097-8493(02)00113-9; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Grap Appl; citation_title=Archeoguide: an augmented reality guide for archaeological sites; citation_author=V Vlahakis, N Ioannidis, J Karigiannis, M Tsotros, M Gounaris, D Stricker, T Gleue, P Daehne, L Almeida; citation_volume=22; citation_issue=5; citation_publication_date=2002; citation_pages=52-60; citation_doi=10.1109/MCG.2002.1028726; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Discovering petra: archaeological analysis in VR; citation_author=E Vote, DA Feliz, DH Laidlaw, MS Joukowsky; citation_volume=22; citation_issue=5; citation_publication_date=2002; citation_pages=38-50; citation_doi=10.1109/MCG.2002.1028725; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Intell Syst; citation_title=Is culture computable?; citation_author=F-Y Wang; citation_volume=24; citation_issue=2; citation_publication_date=2009; citation_pages=2-3; citation_doi=10.1109/MIS.2009.31; citation_id=CR47"/>

    <meta name="citation_reference" content="Watts Gordon P, Kurt Knoerl T (2007) Out of the blue&#8212;public interpretation of maritime cultural resources, chapter entering the virtual world of underwater archaeology. Springer, pp 223&#8211;239"/>

    <meta name="citation_reference" content="Zendjebil IM, Ababsa F, Didier J, Vairon J, Frauciel L, Hachet M, Guitton P, Delmont R (2008) Outdoor augmented reality: state of the art and issues. In: Virtual reality international conference, pp 177&#8211;187"/>

    <meta name="citation_author" content="Mahmoud Haydar"/>

    <meta name="citation_author_email" content="mahmoud.haydar@ibisc.fr"/>

    <meta name="citation_author_institution" content="Laboratoire IBISC, Universit&#233; d&#8217;Evry, Evry, France"/>

    <meta name="citation_author" content="David Roussel"/>

    <meta name="citation_author_email" content="david.roussel@ibisc.fr"/>

    <meta name="citation_author_institution" content="Laboratoire IBISC, Universit&#233; d&#8217;Evry, Evry, France"/>

    <meta name="citation_author" content="Madjid Ma&#239;di"/>

    <meta name="citation_author_email" content="madjid.maidi@ibisc.fr"/>

    <meta name="citation_author_institution" content="Laboratoire IBISC, Universit&#233; d&#8217;Evry, Evry, France"/>

    <meta name="citation_author" content="Samir Otmane"/>

    <meta name="citation_author_email" content="samir.otmane@ibisc.fr"/>

    <meta name="citation_author_institution" content="Laboratoire IBISC, Universit&#233; d&#8217;Evry, Evry, France"/>

    <meta name="citation_author" content="Malik Mallem"/>

    <meta name="citation_author_email" content="malik.mallem@ibisc.fr"/>

    <meta name="citation_author_institution" content="Laboratoire IBISC, Universit&#233; d&#8217;Evry, Evry, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0176-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0176-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint)"/>
        <meta property="og:description" content="The paper presents different issues dealing with both the preservation of cultural heritage using virtual reality (VR) and augmented reality (AR) technologies in a cultural context. While the VR/AR technologies are mentioned, the attention is paid to the 3D visualization, and 3D interaction modalities illustrated through three different demonstrators: the VR demonstrators (immersive and semi-immersive) and the AR demonstrator including tangible user interfaces. To show the benefits of the VR and AR technologies for studying and preserving cultural heritage, we investigated the visualisation and interaction with reconstructed underwater archaeological sites. The base idea behind using VR and AR techniques is to offer archaeologists and general public new insights on the reconstructed archaeological sites allowing archaeologists to study directly from within the virtual site and allowing the general public to immersively explore a realistic reconstruction of the sites. Both activities are based on the same VR engine, but drastically differ in the way they present information and exploit interaction modalities. The visualisation and interaction techniques developed through these demonstrators are the results of the ongoing dialogue between the archaeological requirements and the technological solutions developed."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint) | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0176-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Underwater archaeology, Mixed reality, Virtual reality, Augmented reality, Cultural heritage, Cultural computing","kwrd":["Underwater_archaeology","Mixed_reality","Virtual_reality","Augmented_reality","Cultural_heritage","Cultural_computing"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0176-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0176-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-b0018c9f69.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-c02f1b37f0.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=176;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0176-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint)
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0176-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0176-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Cultural Technology</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-10-29" itemprop="datePublished">29 October 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint)</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mahmoud-Haydar" data-author-popup="auth-Mahmoud-Haydar" data-corresp-id="c1">Mahmoud Haydar<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Laboratoire IBISC, Université d’Evry" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-David-Roussel" data-author-popup="auth-David-Roussel">David Roussel</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Laboratoire IBISC, Université d’Evry" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Madjid-Ma_di" data-author-popup="auth-Madjid-Ma_di">Madjid Maïdi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Laboratoire IBISC, Université d’Evry" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Samir-Otmane" data-author-popup="auth-Samir-Otmane">Samir Otmane</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Laboratoire IBISC, Université d’Evry" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Malik-Mallem" data-author-popup="auth-Malik-Mallem">Malik Mallem</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Laboratoire IBISC, Université d’Evry" /><meta itemprop="address" content="grid.8390.2, 0000000121805818, Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">311</span>–<span itemprop="pageEnd">327</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1472 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">21 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0176-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The paper presents different issues dealing with both the preservation of cultural heritage using virtual reality (VR) and augmented reality (AR) technologies in a cultural context. While the VR/AR technologies are mentioned, the attention is paid to the 3D visualization, and 3D interaction modalities illustrated through three different demonstrators: the VR demonstrators (immersive and semi-immersive) and the AR demonstrator including tangible user interfaces. To show the benefits of the VR and AR technologies for studying and preserving cultural heritage, we investigated the visualisation and interaction with reconstructed underwater archaeological sites. The base idea behind using VR and AR techniques is to offer archaeologists and general public new insights on the reconstructed archaeological sites allowing archaeologists to study directly from within the virtual site and allowing the general public to immersively explore a realistic reconstruction of the sites. Both activities are based on the same VR engine, but drastically differ in the way they present information and exploit interaction modalities. The visualisation and interaction techniques developed through these demonstrators are the results of the ongoing dialogue between the archaeological requirements and the technological solutions developed.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Cultural computing (CC) implies the application of computer technology in the field of culture, arts, humanities or social sciences. It is an emerging field, and the answer to the computability of culture is not clear as mentioned by Wang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wang F-Y (2009) Is culture computable?. IEEE Intell Syst 24(2):2–3" href="/article/10.1007/s10055-010-0176-4#ref-CR47" id="ref-link-section-d39760e349">2009</a>). For Tosa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2009) Cultural computing with context-aware application: ZENetic computer. Lect Notes Comput Sci 3711:13–23" href="/article/10.1007/s10055-010-0176-4#ref-CR43" id="ref-link-section-d39760e352">2009</a>), the CC is a method for cultural translation that uses scientific methods to represent the essential aspects of culture. An other definition of CC can be given as a computer technology that can enhance, extend and transform human creative products and processes (CC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="CC (2009) The cultural computing program" href="/article/10.1007/s10055-010-0176-4#ref-CR13" id="ref-link-section-d39760e355">2009</a>). Virtual and augmented reality technologies will provide new means to create and transform culture. On the one hand, VR technology provides us with the possibility of immersion within multimodal interactions (audio, video and haptics) to enhance user presence in digitalised culture (digital theatre, digital dance, digital music, digital heritage, etc.). On the other hand, AR or mixed reality technology provides us with the possibility to extend, transform and combine different cultures in the same mixed environment (for example combine object from digital dance with others from digital music over space and time). However, we need to develop new interfaces and new interaction metaphors to allow 3D visualisation of culture and 3D interaction with different objects of such culture. In this paper, we focus our contribution on the domain of archaeology by developing interfaces and new interactions for both presentation of existing archaeological underwater sites including interactions with artefacts in a cultural context as well as developing tools for preserving cultural heritage.</p><p>Most of the work mentioned in this paper is related to research &amp; development performed within the VENUS project (Virtual Exploration of Underwater Sites), sponsored by the European Community. The main goal of the VENUS project is to provide scientific methodologies and technological tools for the virtual exploration of deep underwater archaeological sites. Such sites are generally out of reach for divers and require new technologies and tools in order to be surveyed by archaeologists. The first step of the proposed methodology consists in performing a bathymetric and photogrammetric survey of the site with remote operated or autonomous underwater vehicle. Bathymetric and photogrammetric data are then used to reconstruct the seabed while photogrammetric data are processed in the “Arpenteur” photogrammetric tool
<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> that measures points on artefacts (in our case Amphorae) from several geolocalised points of view in order to reconstruct artefacts shapes, dimensions and location which are then stored in an archaeological database for further examination by archaeologists. Our role in the project consists in gathering the reconstructed elements in an immersive virtual environment (VE) providing tools for archaeologists to survey such virtual sites in the most natural and easy way as possible. We have developed several “demonstrators” in order to assess the benefits of virtual and augmented reality (VR &amp; AR) in the field of underwater archaeology. All the demonstrators are based on the same VR engine described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0176-4#Sec11">3.2.1</a>, but drastically differ in the input and output modalities: The most simple demonstrator is called the “low end” demonstrator and offers all the functionalities on a simple laptop. The virtual reality demonstrators (respectively the semi-immersive and immersive demonstrators) use large screen or head-mounted stereo display to enhance immersion and allow interaction with 3D joysticks. Finally, the augmented reality demonstrator features a real map of the site augmented with virtual elements where interaction is provided through the use of various tangible tools related to the corresponding tools of the VE.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Drap and Long (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Drap P, Long L (2001) Towards a digital excavation data management system: the “Grand Ribaud F” Estruscan deep-water wreck. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 17–26" href="/article/10.1007/s10055-010-0176-4#ref-CR18" id="ref-link-section-d39760e381">2001</a>) mentioned that for many years Geographic Information Systems have become common tools for archaeologists who see in this technology the alliance between the huge amounts of information collected in the field and graphical representation which supports the analysis. The GIS graphical representations most often originate from cartography, that is to say merging vectors, images, and symbology in 2D visualization tools. The old culture of chart reading is very useful in the use of GIS and probably one of the obstacles in the way of a truly 3D GIS. As a matter of fact, even without the realistic representation, the strength of the GIS is linked to the symbolic cartographic representation of the data offering a synthetic expression of the data analysis. If the 2D representation is sufficient to demonstrate the archaeological work concerning an urban scale or larger, applied to a period for which traces of the elevations do not exist, it is far from being the same when one is studying a building, or in this present case, a ship. The need for a 3D representation is then of first importance and the global understanding of the study revolved around that kind of representation. For instance, Karma VI was an interface for ESRI’s spatial Database Engine (which produced the arcGIS more recently) that supports powerful visualization, manipulation, and editing of standard GIS data in a VR environment (Germs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Germs R, Van Maren G, Verbree E, Jansen FW (1999) A multi-view VR interface for 3D GIS. Comput Graph 23(4):497–506" href="/article/10.1007/s10055-010-0176-4#ref-CR22" id="ref-link-section-d39760e384">1999</a>), However, as mentioned by Vote et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vote E, Feliz DA, Laidlaw DH, Joukowsky MS (2002) Discovering petra: archaeological analysis in VR. IEEE Comput Graph Appl 22(5):38–50" href="/article/10.1007/s10055-010-0176-4#ref-CR46" id="ref-link-section-d39760e387">2002</a>): “these Immersive Virtual Reality applications weren’t developed for archaeological inquiry and therefore don’t consider the specific research tasks archaeologists need to perform”. Therefore, specially tailored multimedia databases have been developed to connect field measurements and findings to VR visualisation tools such as the one developed in the 3D Murale project (Cosmas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Cosmas, John, Take I, Damian G, Edward G, Fred W, Luc VG, Alexy Z, Desi V, Franz L, Markus G, Konrad S, Konrad K, Michael G, Stefan H, Marc W, Marc P, Roland D, Robert S, Martin K (2001) 3D MURALE: a multimedia system for archaeology. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 297–306" href="/article/10.1007/s10055-010-0176-4#ref-CR14" id="ref-link-section-d39760e390">2001</a>). Although efforts have been provided in underwater archaeology to turn underwater photogrammetric surveys into interactive virtual environments as mentioned in Drap and Long (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Drap P, Long L (2001) Towards a digital excavation data management system: the “Grand Ribaud F” Estruscan deep-water wreck. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 17–26" href="/article/10.1007/s10055-010-0176-4#ref-CR18" id="ref-link-section-d39760e393">2001</a>) which used VRML output for 3D visualisation purposes, one the goals of the archaeological demonstrator within VENUS project is to transport archaeologists within such a reconstructed archaeological site but most of all allow them to interact with the gathered data by connecting tools in the VE to an underlying archaeological database.</p><h3 class="c-article__sub-heading" id="Sec3">Immersive virtual reality</h3><p>First of all, we need to establish a definition of immersive virtual reality. A commonly accepted definition of virtual reality has been provided by Rheingold (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Rheingold H (1991) Virtual reality. Summit Books, London" href="/article/10.1007/s10055-010-0176-4#ref-CR38" id="ref-link-section-d39760e403">1991</a>) as an experience in which a person is “surrounded by a three dimensional computer generated representation, and is able to move around in the virtual world and see it from different angles, to reach into it, grab it, and reshape it”. Cruz-Neira et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the CAVE. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, ACM, New York, pp 135–142" href="/article/10.1007/s10055-010-0176-4#ref-CR15" id="ref-link-section-d39760e406">1993</a>) have proposed a definition more confined to the visual domain: “a VR system is one which provides real time viewer centered head tracking perspective with a large angle of view, interactive control, and binocular display”. However, all definitions of VR agree on three distinctive features: (1) immersion, (2) interaction and (3) real time. Billinghurst et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: VRAIS ’98: proceedings of the virtual reality annual international symposium. IEEE Computer Society, Washington, DC, pages 20" href="/article/10.1007/s10055-010-0176-4#ref-CR7" id="ref-link-section-d39760e409">1998</a>) defined three kinds of information presentation paradigms requiring increasingly complex head tracking technologies:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>In “Head stabilised” information is fixed to the user’s viewpoint and does not change as the user changes viewpoint orientation or position. Any VR system that does not provide tracking is then considered as “Head stabilised”. In the VENUS project, the low-end demonstrator running on a laptop with no tracking devices could be considered as a “Head stabilised” system.</p>
                    </li>
                    <li>
                      <p>In “Body stabilised” information is fixed relative to the user’s body position and varies as the user changes viewpoint orientation, but not as they change position. The semi-immersive and immersive demonstrators are “Body stabilised” systems since head’s orientation changes viewpoint, but motion within the virtual environment is controlled with 3D joysticks rather than head’s position.</p>
                    </li>
                    <li>
                      <p>And finally in “ World stabilised” information is fixed to real world locations and varies as the user changes viewpoint orientation and position. The augmented reality demonstrator is typically a “world stabilised” system, as position and orientation of the viewpoints needs to be computed in order to register the virtual environment over the real map. However, the immersive demonstrator could also use body motion in a “World stabilised” way for small motion within the tracking space, and whereas “ Body stabilised” is considered when travelling through the environment with the 3D joysticks.</p>
                    </li>
                  </ul>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec4">Immersion with head-mounted displays</h4><p>The main difference between the semi-immersive and the immersive demonstrator is the use of a head-mounted display (HMD) as the display device in the immersive demonstrator. We call it “fully” immersive as the HMD is tracked by an optical ART (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="ART (2009) Advanced realtime tracking" href="/article/10.1007/s10055-010-0176-4#ref-CR1" id="ref-link-section-d39760e437">2009</a>) system, so that the user can look around. Ruddle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Ruddle RA, Payne SJ, Jones DM (1999) Navigating large-scale virtual environments: what differences occur between helmet-mounted and desk-top displays? Presence: Teleoperators Virtual Environ 8(2):157–168" href="/article/10.1007/s10055-010-0176-4#ref-CR40" id="ref-link-section-d39760e440">1999</a>) found that the ability to look around with HMD allowed users to be less static in the environment, as they do not have to stop travelling, take a look around and choose a new travel direction since looking around is allowed during travel: On average, participants who were immersed in the virtual environment using the HMD navigated the environment twelve per cent faster. The decreased time was attributed to the participants utilising the ability to “look around” while they were moving when immersed, as the participants spent eight per cent more time stationary when using a desktop workstation. Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance with different display devices for virtual environments. In: Proceedings of the human factors and ergonomics society annual meeting (HFES’ 02). Human Factors and Ergonomics Society, pp 2134–2138" href="/article/10.1007/s10055-010-0176-4#ref-CR10" id="ref-link-section-d39760e443">2002</a>) also studied human behaviour and performance between an HMD and a four-sided spatially immersive display (SID or CAVE). In particular, he studied users’ preferences for natural orientation (user turns his head or entire body naturally while making a turn) versus manual orientation (user uses the joystick to rotate the world about its vertical axis) in the virtual environment. The results indicated that participants have a significant preference for the natural rotation with the HMD and for manual rotation in the SID. This suggests that HMDs are an appropriate choice when users perform frequent turns and require spatial orientation. Even though HMD’s field of view and resolution have drastically increased lately, one can not consider HMD’s field of view as larger than standing in front of large screen or several screens (in the case of a CAVE); however, this drawback is easily compensated by the “look around” features provided by tracked HMDs: By tracking head orientation, the user experiences a hemispherical information surround—in effect a “hundred million pixel display” and nowadays even more as (Reichlen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Reichlen BA (1993) Sparcchair: a one hundred million pixel display. In: 1993 IEEE virtual reality annual international symposium, pp 300–307" href="/article/10.1007/s10055-010-0176-4#ref-CR37" id="ref-link-section-d39760e446">1993</a>) coined this term.</p><h3 class="c-article__sub-heading" id="Sec5">Virtual reality applied to archaeology</h3><p>Even though archaeology benefits from information technology for many years through web publications of research and findings Hall et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hall Rebecca A, Hall Andrew W, Barto Arnold J III (1997) Presenting archaeology on the Web: The La Salle Shipwreck Project. Int J Nautical Archaeol 26(3):247–251" href="/article/10.1007/s10055-010-0176-4#ref-CR24" id="ref-link-section-d39760e458">1997</a>) and Watts and Knoerl (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Watts Gordon P, Kurt Knoerl T (2007) Out of the blue—public interpretation of maritime cultural resources, chapter entering the virtual world of underwater archaeology. Springer, pp 223–239" href="/article/10.1007/s10055-010-0176-4#ref-CR48" id="ref-link-section-d39760e461">2007</a>), very few attempts have been proposed to immerse users within an archaeological virtual reality such as the one defined above. More specifically, virtual reality has been considered in this field as a visualisation tool until now. For instance, VR display of ancient Greece for the broad public by the Foundation of the Hellenic World in a museum has received an enthusiastic welcome (Gaitatzes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Gaitatzes A, Christopoulos D, Roussou M (2001) Reviving the past: cultural heritage meets virtual reality. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 103–110" href="/article/10.1007/s10055-010-0176-4#ref-CR21" id="ref-link-section-d39760e464">2001</a>); however, (Barceló <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Barceló JA (2000) Visualizing what might be: an introduction to virtual reality techniques in archaeology. Virtual reality in archaeology. British Archaeol Rep S843:9–36" href="/article/10.1007/s10055-010-0176-4#ref-CR4" id="ref-link-section-d39760e467">2000</a>) stressed that virtual reality in the archaeology field should not be used only as a visualisation tool but also as a way to manipulate the archaeological interpretation. The ARCHAVE system created by Vote et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vote E, Feliz DA, Laidlaw DH, Joukowsky MS (2002) Discovering petra: archaeological analysis in VR. IEEE Comput Graph Appl 22(5):38–50" href="/article/10.1007/s10055-010-0176-4#ref-CR46" id="ref-link-section-d39760e470">2002</a>) and dedicated to the archaeological analysis in VR of the excavated finds from the Great Temple site at Petra, Jordan, provided some interesting outcomes on the use of immersive VR for archaeology: The system used a CAVE for display and was interfaced with an artefact database containing over 250,000 catalogued finds. Concerning visualisation, using an immersive CAVE allowed to examine the data in the context of a “life size” representation; the immersive VR visualization gave the archaeologists the opportunity to explore a site in a new and dynamic way and, in several cases enabled them to make discoveries that opened new lines of investigation about the excavation. However (as mentioned by Acevedo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Acevedo D, Vote E, Laidlaw DH, Joukowsky MS (2001) Archaeological data visualization in VR: analysis of lamp finds at the great temple of Petra, a case study. In: Proceedings of the 12th IEEE conference on visualization (VIS ’01). IEEE Computer Society, Washington, DC, pp 493–496" href="/article/10.1007/s10055-010-0176-4#ref-CR2" id="ref-link-section-d39760e474">2001</a>), archaeologists “consistently needed an easily accessible overview of the model, much like the experience they obtain by flying high up over the virtual model, so they could study how the different artefacts were distributed over the entire site”. This problem has been addressed by accessing a “Miniature model for a site wide analysis” at any time during exploration.</p><p>Van Dam et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization. Comput Grap 26(4):535–555" href="/article/10.1007/s10055-010-0176-4#ref-CR44" id="ref-link-section-d39760e480">2002</a>) propose a review of experiments in immersive VR for scientific visualisation including the above-mentioned ARCHAVE system. Clearly, visualisation by itself will not solve the problem of understanding truly large datasets that would overwhelm both display and human vision system. They advocate a human computer partnership that uses algorithmic culling and feature detection used to identify small fraction of the data that should be visually examined in detail by the human. Immersive VR could then be a potent tool to let humans “see” patterns, trends and anomalies in their data well beyond what they can do with conventional 3D desktop displays. The immersive surrounding context provides a kinesthetic depth perception that helps users to better apprehend 3D structures and spatial relationships. It makes size, distance and angle judgments easier since it is more like in being in the real world than looking through the screen of a desktop monitor to the world behind it; the advantages arise from the difference between “looking at” a 2D image of a 3D environment on a conventional display screen and “being in” that 3D environment and basing spatial judgments relative to one’s own moving body. Concerning 3D interactions, (Hinckley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassell N (1997) Usability analysis of 3D rotation techniques. In: Proceedings of the 10th annual ACM symposium on user interface software and technology. ACM, New York, pp 1–10" href="/article/10.1007/s10055-010-0176-4#ref-CR26" id="ref-link-section-d39760e483">1997</a>) presented a formal user study of interactive 3D virtual sphere with multidimensional input devices and found out that multidimensional input tasks presented a clear advantage over conventional devices. The study provides clear evidence that test users were able to take advantage of the integrated control of 3D to perform a task more quickly than with 2D input techniques.</p><h3 class="c-article__sub-heading" id="Sec6">Augmented reality and tangible interfaces</h3><p>Azuma et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyr B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47" href="/article/10.1007/s10055-010-0176-4#ref-CR3" id="ref-link-section-d39760e494">2001</a>) define an AR system as a system that “supplements the real world with virtual (computer generated) objects that appear to coexist in the same space as the real world”. Moreover, the system should feature characteristics quite similar to VR features mentioned above such as:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>combines real and virtual objects in a real environment;</p>
                    </li>
                    <li>
                      <p>runs interactively (2), and in real time(3); and</p>
                    </li>
                    <li>
                      <p>registers (aligns) real and virtual objects with each other.</p>
                    </li>
                  </ul>
                        <p>On the one hand, virtual reality technologies immerse the user in a synthetic environment in which, he cannot see the real world around him. On the other hand, augmented reality allows the user to see the real environment with superimposed virtual objects. Rather than replacing the real world, the user is immersed in an environment where the virtual and real objects coexist in the same space.</p><p>Interaction with the augmented environment is performed with tangible user interfaces (TUIs) that use physical objects as tools to establish an interface with the virtual environment. The TUI provides a natural and intuitive interface, a user can manipulate virtual 3D objects by simply handling physical objects. A considerable amount of research has been done in the domain of TUIs, and new human computer interaction approaches were proposed to improve the physical interaction with computational media. Kato et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: ISAR’ 00 : proceedings of the international symposium on augmented reality, Munich, pp 111–119" href="/article/10.1007/s10055-010-0176-4#ref-CR29" id="ref-link-section-d39760e520">2000</a>) implemented table top AR environments with conventional markers and paddles for object manipulation. They advocate designing the form of physical objects in the interface using established tangible user interface design methods. Some of the tangible design principles include:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Object affordances should match the physical constraints of the object to the requirements of the task.</p>
                    </li>
                    <li>
                      <p>The ability to support parallel activity where multiple objects or interface elements are being manipulated at once.</p>
                    </li>
                    <li>
                      <p>Support for physically based interaction techniques (such as using object proximity or spatial relations).</p>
                    </li>
                    <li>
                      <p>The form of objects should encourage and support spatial manipulation</p>
                    </li>
                    <li>
                      <p>Support for multihanded interaction.</p>
                    </li>
                  </ul>
                        <p>So that in an AR interface, the physical objects can further be enhanced in ways not normally possible such as providing dynamic information overlay, context sensitive visual appearance and physically based interactions.</p><p>One of the most obvious benefits of tangible user interface pointed out by Kato et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: ISAR’ 00 : proceedings of the international symposium on augmented reality, Munich, pp 111–119" href="/article/10.1007/s10055-010-0176-4#ref-CR29" id="ref-link-section-d39760e557">2000</a>) is that users do not need to learn any complicated computer interface or command set to use tangible interfaces.</p><h3 class="c-article__sub-heading" id="Sec7">Augmented reality applied to archaeology</h3><p>Augmented or Mixed Reality could be applied to archaeological field in several ways: The first and most obvious way is an “on site” or outdoor augmentation performed on mobile devices as an “augmented walkthrough”. “On site” augmentations generally focus on the correct registration of virtual elements over real ones and interactions with virtual content depends on the mobile device used as there is no workplace covered by sensors to support tangible interactions for instance. In this context, the Archeoguide project (Vlahakis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Vlahakis V, Ioannidis N, Karigiannis J, Tsotros M, Gounaris M, Stricker D, Gleue T, Daehne P, Almeida L (2002) Archeoguide: an augmented reality guide for archaeological sites. IEEE Comput Grap Appl 22(5):52–60" href="/article/10.1007/s10055-010-0176-4#ref-CR45" id="ref-link-section-d39760e569">2002</a>) is a good example of such a paradigm by enhancing the archaeological site of Olympia in Greece with augmented views of reconstructed ruins. The second way to apply AR to the archaeological field is an “off-site” or indoor augmentation. “Off-site” augmentation generally relies on a tracked workspace where users can then focus on interactions with the virtual content such as in the VITA project presented (Benko et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Benko H, Ishak EW, Feiner S (2004) Collaborative mixed reality visualization of an archaeological excavation. In: ISMAR ’04: proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality. IEEE Computer Society, Washington, DC, pp 132–140" href="/article/10.1007/s10055-010-0176-4#ref-CR6" id="ref-link-section-d39760e572">2004</a>) where archaeologists can review the findings and layers of an excavation of the acropolis at Monte Polizzo, Western Sicily. Users can then cooperate by combining speech, touch and 3D hand gestures over either a “world in miniature” (WIM) representation or a life-size model of a portion of the excavation. Since Archaeologists are used to report all findings on a map for off-site review, the “World in Miniature” paradigm can then be used to augment such a map as presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0176-4#Sec18">3.3</a>.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Virtual and augmented reality for underwater archaeology</h2><div class="c-article-section__content" id="Sec8-content"><p>Before starting to build virtual environments reflecting the surveys performed on wreck sites, a case study has been performed on a typical underwater archaeological survey, and archaeologists were interviewed concerning the requirements of such virtual environments. Archaeologists are mainly interested in the cargo that leads to determine the period of the wreck but also in the environment which could explain the artefacts’ layout. A full list of requirements was built concerning visualisation (full view and close range), navigation (free navigation, artefact’s-based navigation or diver’s navigation) and interaction (artefact’s individual data facts, inventory and artefacts statistics in terms of types, dimensions, locations and fragment status for broken artefacts). These requirements were later transposed into a list of features implemented in various ways within the different demonstrators.</p><p>Although all proposed technologies have been already used in other fields, and even though computer graphics is a common tool to represent cultural heritage findings and results, our goal is to introduce VR and AR technology as a working tool for archaeologists allowing them to perform actual archaeological tasks rather than just a presentation tool as there is no previous such systems in underwater archaeology.</p><p>Within the framework of the VENUS project, we propose two distinct forms for the archaeological demonstrator featuring both VR semi-immersive and VR immersive technologies. The immersive surrounding context of VR provides a kinesthetic depth perception that lets users better apprehend 3D structures and spatial relationships. It makes size, distance and angle judgments easier since it is more like being in the real world than looking through the screen of a desktop monitor to the world behind it; the advantages arise from the difference between “looking at” a 2D image of a 3D environment on a conventional display screen and “being in” that 3D environment and basing spatial judgments relative to one’s own moving body. On the other hand, by combining augmented reality techniques with tangible user interface elements, we can create interfaces in which users can interact with spatial data as easy as real objects. Tangible AR interfaces remove the separation between the real and virtual worlds and so enhance natural interactions. The semi-immersive demonstrator is based on a large stereo display, and 3D navigation and interaction are based on 3D wireless joysticks (also called “flysticks”). The immersive demonstrator is based on the same navigation and interaction devices but uses a tracked HMD to provide complete surroundings to the user. And finally the augmented reality demonstrator is based on a camera tracking system associated with a see through HMD registering the users viewpoint and allowing interaction with tangible interfaces over a map of the site.</p><h3 class="c-article__sub-heading" id="Sec9">Tracking technologies</h3><p>The tracking technology is an optical tracking from ART (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="ART (2009) Advanced realtime tracking" href="/article/10.1007/s10055-010-0176-4#ref-CR1" id="ref-link-section-d39760e597">2009</a>) using two infrared cameras (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig6">6</a>) for tracking unique patterns composed of retroreflective balls that could be used as input devices such as flysticks and handles or head-tracked devices such as LCD glasses or HMD. Optical tracking allows users to wear or handle any wireless devices as long as they can be equipped with unique retroreflective balls pattern (as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Tracking devices:<b> a</b> ART tracking camera,<b> b</b> flystick,<b> c</b> tracked LCD glasses and<b> d</b> tracked HMD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The ART system is capable of measuring position and orientation of several patterns at the same time (up to 64 balls) at a 60 Hz measure rate (54 Hz if LCD shutter glasses synchronisation is required). The achievable accuracy of the tracking system is 0.4 mm for position and 0.12° for orientation within a 3 × 3 × 3 m volume. However, the average measured accuracy precision of the system is actually between 0.5 and 2 mm for position and 0.3° for orientation due to slow decalibration of the system over several months, which could be easily corrected by a system recalibration from time to time.</p><h3 class="c-article__sub-heading" id="Sec10">Virtual reality demonstrators</h3><p>We developed two versions of the VR application that uses different devices technology. The first version works with simple input/output devices (mouse, keyboard and monitor) in order to easily run the demonstrator without needing any specific devices that can be difficult to transport.</p><p>In the second version, we employed more advanced visualisation and tracking devices to offer a semi or complete immersive navigation and more natural interaction with the environment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Virtual reality system description</h4><p>This section presents the structure and the construction of virtual environment and the corresponding virtual reality system.</p><h5 class="c-article__sub-heading" id="Sec12">Virtual environment structure</h5><p>All virtual environments for the VENUS project are developed around the “OpenScenegraph” (OSG) open source high performance 3D graphics toolkit for VE modelling and visualization (Burns and Osfield <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Burns D, Osfield R (2004) Open scene graph a: introduction, b: examples and applications. In: VR ’04: proceedings of the IEEE virtual reality 2004. IEEE Computer Society, Washington, DC, page 265" href="/article/10.1007/s10055-010-0176-4#ref-CR12" id="ref-link-section-d39760e662">2004</a>). The choice of OSG was motivated by the need of a high-level API abstracting rendering features for the 3D objects, scene control and cameras views management, which is also flexible enough to develop specially tailored visualisations and interactions techniques wherever they are necessary. The main structure of the VE developed for archaeologists contains the various seabeds (large bathymetric seabed and photogrammetric seabed with textures) and the various artefacts (in our case amphorae) lying on the seabed and recorded in the database.</p><p>The construction of the VE is divided into 3 principal steps:
</p><ul class="u-list-style-bullet">
                        <li>
                          <p><i>Seabed:</i> Seabed meshes are loaded from an XML file containing 3D vertices and texture information.</p>
                        </li>
                        <li>
                          <p><i>Artefacts:</i> An initial request to the database is performed to retrieve artefacts parameters such as location, orientation, status and artefacts models. Then, registered artefacts and markers 3D models are loaded.</p>
                        </li>
                        <li>
                          <p><i>Virtual Environment:</i> These elements are placed in the virtual environment, and navigation and interaction managers are started. When 3D interaction devices are available, a connection to input devices is opened by using a VRPN server (Taylor II et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Taylor II, Russell M, Hudson Thomas C, Seeger A, Weber H, Juliano J, Helser Aron T (2001) VRPN: a device-independent, network-transparent VR peripheral system. In: VRST ’01: proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 55–61" href="/article/10.1007/s10055-010-0176-4#ref-CR42" id="ref-link-section-d39760e692">2001</a>). The interaction manager handles inputs and eventually sends queries to the database.</p>
                        </li>
                      </ul>
                              <h5 class="c-article__sub-heading" id="Sec13">Virtual reality system architecture</h5><p>The architecture of the VR system is composed of a database containing all required data such as photos, artefacts parameters, 2D/3D objects location, etc (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig2">2</a>). The archaeological database contains the pictures taken during the survey and the 2D and 3D points of artefacts lying on the seabed measured during the photogrammetry process. When these points are labelled to belong to a recognised artefact type, an actual artefact could then be reconstructed in terms of location, orientation and size, and all these artefacts’ parameters are stored in the database. Therefore, such a database could be shared between the photogrammetric reconstruction process and the virtual environments designed to immersively explore the site. In order for VE users to extract and study properties of the cargo (registered artefacts), users interaction with artefacts are translated into SQL queries sent to the database, and results are displayed through selections or numeric data display depending on the nature of the results. Queries to the database can concern partial or complete inventory, metrology statistics (average size, similar sets, etc.) or spatial relationships between artefacts.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>VR system architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <h5 class="c-article__sub-heading" id="Sec14">Virtual reality interface and interactions</h5><p>The VE interface is composed of many classical tools: menu bar, information panel and popup message. The menu bar contains multiple submenus, the first menu is a selection menu that proposes visualization/selection of any type of artefacts registered in the site, the second menu allows to switch between analysis and exploring navigation mode, the third one allows to hide some parts (terrain) of the VE to highlight others parts (artefacts). Otherwise, several classical menus are provided by the interface, like, font and colour manager, help and exit menu. The information panel displayed on the bottom of the VE (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig3">3</a>) shows information about objects loading progress, user location or interaction result (e.g. amphora Id 21 was selected). A 3D popup message is displayed when the mouse passes over an object (or when the flystick selection ray casts an objects) showing the type of the objects or other information on selected objects.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Tools in the virtual environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>3D interactions with a virtual environment can be divided into three principal tasks: navigation, selection and manipulation. The navigation or the control of the user’s viewpoint is the most important task and most used when using the virtual environment. Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman AD, Kruijff E, Laviola I, Poupyrev J (2005) 3D user interfaces: theory and practice. Addison Wesley Longman Publishing Co., Inc., Redwood City" href="/article/10.1007/s10055-010-0176-4#ref-CR9" id="ref-link-section-d39760e759">2005</a>) recognized this task as the most common to all virtual environments. It allows users to explore, investigate and/or operate in a virtual space. They identified two main components for navigation: travel and way finding (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: VRAIS ’97: proceedings of the 1997 virtual reality annual international symposium (VRAIS ’97). IEEE Computer Society, Washington, DC, page 45" href="/article/10.1007/s10055-010-0176-4#ref-CR11" id="ref-link-section-d39760e762">1997</a>), where they classified the different navigation techniques into three basic motion tasks: the choice of direction or target, the choice of motion speed/acceleration and choice of entry conditions (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman AD, Kruijff E, Laviola I, Poupyrev J (2005) 3D user interfaces: theory and practice. Addison Wesley Longman Publishing Co., Inc., Redwood City" href="/article/10.1007/s10055-010-0176-4#ref-CR9" id="ref-link-section-d39760e765">2005</a>). For 3D interaction, we used 2 flysticks tracked by an ART camera system that allows motion control and hence navigation, and each flystick has 8 buttons and offers important number of choice to accomplish multiple tasks simultaneously. Display can be performed by a large screen with active stereo visualization or by a tracked head-mounted display (HMD) to increase immersion (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig4">4</a> for tracked devices details).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>VE devices technology</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>A new navigation technique has been developed (Haydar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Haydar M, Maïdi M, Roussel D, Mallem M (2009) A new navigation method for 3D virtual environment exploration. In: AIP (ed) The 2nd Mediterranean conference on intelligent systems and automation (CISA 2009), vol 1107. AIP, Zarzis (Tunisia), pp 190–195" href="/article/10.1007/s10055-010-0176-4#ref-CR25" id="ref-link-section-d39760e792">2009</a>) using both hands to determine motion direction and control speed. A similar technique has been proposed by Mine et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Mine Mark R Jr., Brooks Frederick P, Sequin Carlo H (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: SIGGRAPH ’97: Proceedings of the 24th annual conference on computer graphics and interactive techniques, vol 31. ACM Press/Addison-Wesley Publishing Co, pp 19–26" href="/article/10.1007/s10055-010-0176-4#ref-CR33" id="ref-link-section-d39760e795">1997</a>) and is based on the knowledge of both hands position where speed is computed according to the distance between the two hands. Such a technique is cognitively difficult because the user may have difficulty in controlling the motion speed through the gap between his two hands. We used the angle between the hands rather than the distance that is easier to control. The motion direction is then given by the orthogonal axis to the segment joining hands positions. The viewpoint of the user is defined by two 3D points, the <i>eye</i> point and the <i>at</i> point. Changing user’s viewpoint can be a translation of the viewpoint, or a rotation, or both translation and rotation. A viewpoint translation can be done by translating both <i>eye</i> and <i>at</i> points while a rotation is done by a rotation of the <i>at</i> point around the <i>eye</i> point. The motion speed is defined by the value of the translation step and the value of the rotation angle in each frame update. Motion speed is computed according to angle α between the hands. The rotation direction and speed is given by the angle β between the segment joining hands positions and the horizontal axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Speed and new position computing</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                                 <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Semi-immersive demonstrator setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>Considering <i>C</i>(<i>x</i>
                                 <sub>
                        <i>c</i>
                      </sub>, <i>y</i>
                                 <sub>
                        <i>c</i>
                      </sub>, <i>z</i>
                                 <sub>
                        <i>c</i>
                      </sub>) the centre of <span class="mathjax-tex">\([\overrightarrow{P_1P_2}]\)</span> as the origin of the coordinate system. All coordinates can be computed in the new coordinate system by a simple translation <i>T</i>(<i>x</i>
                                 <sub>
                        <i>c</i>
                      </sub>, <i>y</i>
                                 <sub>
                        <i>c</i>
                      </sub>, <i>z</i>
                                 <sub>
                        <i>c</i>
                      </sub>). Having the position of the two points <i>eye</i>(<i>x</i>
                                 <sub>
                        <i>eye</i>
                      </sub>, <i>y</i>
                                 <sub>
                        <i>eye</i>
                      </sub>, <i>z</i>
                                 <sub>
                        <i>eye</i>
                      </sub>) and <i>at</i>(<i>x</i>
                                 <sub>
                        <i>at</i>
                      </sub>, <i>y</i>
                                 <sub>
                        <i>at</i>
                      </sub>, <i>z</i>
                                 <sub>
                        <i>at</i>
                      </sub>) of the current frame view <i>f</i> (Camera position), and having the positions <span class="mathjax-tex">\(P_1(x_{p_1},y_{p_1},z_{p_1})\)</span> and <span class="mathjax-tex">\(P_2(x_{p_2}\)</span>, <span class="mathjax-tex">\(y_{p_2}\)</span>, <span class="mathjax-tex">\(z_{p_2})\)</span> of the hands (flysticks’ positions) and the angles <span class="mathjax-tex">\(r_{p_1}\)</span> and <span class="mathjax-tex">\(r_{p_2}\)</span> between the hands and the <span class="mathjax-tex">\(\overrightarrow{XX'}\)</span> axis, we need to compute the two new positions <i>eye</i>′(<i>x</i>
                                 <sub><i>eye</i>'</sub>, <i>y</i>
                                 <sub><i>eye</i>'</sub>, <i>z</i>
                                 <sub><i>eye</i>'</sub>) and <i>at</i>′(<i>x</i>
                                 <sub><i>at</i>'</sub>, <i>y</i>
                                 <sub><i>at</i>'</sub>, <i>z</i>
                                 <sub><i>at</i>'</sub>) of the new frame view <i>f</i>′. We define the coefficients, <i>S</i>
                                 <sub>
                        <i>t</i>
                      </sub>, <i>S</i>
                                 <sub>
                        <i>r</i>
                      </sub>, <i>S</i>
                                 <sub><i>t</i>.<i>max</i></sub>, <i>S</i>
                                 <sub><i>r</i>.<i>max</i></sub> as the translation speed, rotation speed, max motion speed and max rotation speed, respectively. <i>S</i>
                                 <sub><i>t</i>.<i>max</i></sub> and <i>S</i>
                                 <sub><i>r</i>.<i>max</i></sub> are predefined according to application needs. The values of α and β are given by the equations:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left\{ \begin{array}{l} \alpha=r_{p_2}-r_{p_1}\\ \beta=\arcsin \left(\frac{\Updelta_y}{|P_1P_2|}\right) \quad where\; \Updelta_y=y_{p_2}-y_{p_1} \end{array}\right. $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                              <p>Then, <i>S</i>
                                 <sub>
                        <i>t</i>
                      </sub> and <i>S</i>
                                 <sub>
                        <i>r</i>
                      </sub> are defined as follows:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{array}{lll} if &amp;|\alpha|\leq 10^{\circ}&amp;\Rightarrow S_t=S_{t.max}\cr if &amp;|\alpha|\geq 140^{\circ}&amp;\Rightarrow S_t=0 \cr if &amp;|\beta|\leq 10^{\circ}&amp;\Rightarrow S_r=0 \cr if &amp;|\beta|\geq 60^{\circ}&amp;\Rightarrow S_t=0 \\ \end{array} $$</span></div></div>
                                 <div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} \hbox{Otherwise,}&amp;\\ &amp; \left\{ \begin{array}{l} S_t=\left(1-\frac{\alpha}{140}\right)*S_{t.max}\\ S_r=\frac{\beta}{90}*S_{r.max}\\ \end{array}\right. \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                              <p>The β value is limited between −90° and 90°, and the rotation direction is defined by the sign of β. The rotation is clockwise when β is negative and anticlockwise when β is positive. To avoid the motion noise, due to user hands shaking, we define a noise angle value for rotation and translation. When β is between −10° and 10°, we consider that motion is a pure translation and the rotation speed is null, whenever α is between −10° and 10° the translation speed is considered as maximal. The values of the new two points positions <i>eye</i>′(<i>x</i>
                                 <sub><i>eye</i>′</sub>, <i>y</i>
                                 <sub><i>eye</i>′</sub>, <i>z</i>
                                 <sub><i>eye</i>′</sub>) and <i>at</i>′(<i>x</i>
                                 <sub><i>at</i>′</sub>, <i>y</i>
                                 <sub><i>at</i>′</sub>, <i>z</i>
                                 <sub><i>at</i>′</sub>) are given by the equations:</p><p>first, we apply the rotation <i>S</i>
                                 <sub>
                        <i>r</i>
                      </sub> around the point <i>eye</i>:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left\{ \begin{array}{l} x_{at'}=x_{eye}+( \left( x_{at}-x_{eye}\right)* \cos S_r + \left( y_{at}-y_{eye}\right)* \sin S_r )\\ y_{at'}=y_{eye}+( \left( y_{at}-y_{eye}\right)* \cos S_r - \left( x_{at}-x_{eye}\right)* \sin S_r )\\ z_{at'}=z_{at}\\ \end{array}\right. $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                              <p>Then, we apply the translation:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left\{ \begin{array}{l} x_{eye'}=x_{eye}+ S_t*\sin \theta \\ y_{eye'}=y_{eye}+S_t*\cos \theta \\ z_{eye'}=z_{eye}\\ x_{at'}=x_{at_1}+ S_t*\sin \theta\\ y_{at'}=y_{at_1}+ S_t*\cos \theta\\ z_{at'}=z_{at_1} \end{array}\right. $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Where θ is the camera rotation around the <span class="mathjax-tex">\(\overrightarrow{ZZ'}\)</span> axis of the viewer’s coordinate system. We multiply by θ to overlay the camera and the viewer coordinate system.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Virtual reality demonstrators setup</h4><h5 class="c-article__sub-heading" id="Sec16">Semi-immersive demonstrator setup</h5><p>The semi-immersive demonstrator (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig6">6</a>) allows a human scale representation of the virtual environment with a simultaneous navigation and interaction. It mimics the divers’ paradigm and hence recreates but also enhances the diving process by allowing user interaction with the data collected on the cargo. Several archaeologists can easily share the same immersion level to collaborate in front of the large screen and benefit from the stereo view. However, only one stereo viewpoint could be modified on the display according to a tracked head position.</p><p>The semi-immersive demonstrator uses the Evr@ platform at UEVE featuring a large screen (3.2 × 2.4 m) allowing a user standing 1.5 m away from the screen to experience a 94° horizontal field of view and a 77° vertical field of view. Navigation and interaction through the virtual environment are controlled by one or two flysticks.</p><h5 class="c-article__sub-heading" id="Sec17">Immersive demonstrator setup</h5><p>The immersive demonstrator takes up most of the semi-immersive demonstrator’s features such as life-size stereo viewing, simultaneous navigation and interaction but adds the look around capability that has proved to enhance users’ orientation and mobility within the virtual environment (see Ruddle et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Ruddle RA, Payne SJ, Jones DM (1999) Navigating large-scale virtual environments: what differences occur between helmet-mounted and desk-top displays? Presence: Teleoperators Virtual Environ 8(2):157–168" href="/article/10.1007/s10055-010-0176-4#ref-CR40" id="ref-link-section-d39760e1395">1999</a> and Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance with different display devices for virtual environments. In: Proceedings of the human factors and ergonomics society annual meeting (HFES’ 02). Human Factors and Ergonomics Society, pp 2134–2138" href="/article/10.1007/s10055-010-0176-4#ref-CR10" id="ref-link-section-d39760e1398">2002</a>).</p><p>The same tracking technologies as in the semi-immersive demonstrator are used, but a head-mounted display (HMD) is used for viewing the virtual environment. The chosen NVisor helmet offers a 50° field of view with a 100% overlap between each eye display, hence ensuring a complete stereo display with a double 1,280 × 1,024 resolution (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Immersive demonstrator setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>From a larger point of view, such a HMD with “see through” capabilities could also be used in outdoor environments such as terrestrial archaeological site as the one reported by Drap et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Drap P, Nedir M, Seinturier J, Papini O, Chapman P, Boucault F, Viant W, Vannini G, Nuccioti M (2006) Toward a photogrammetry and virtual reality based heritage information system: a case study of shawbak castle in jordan. In: Joint event conference of the37th CIPA international workshop dedicated on e-documentation and standardisation in cultural heritage, 7th VAST international symposium on virtual reality, archaeology and cultural heritage, 4th eurographics workshop on graphics and cultural heritage and 1st Euro-med conference on IT in cultural heritage" href="/article/10.1007/s10055-010-0176-4#ref-CR19" id="ref-link-section-d39760e1428">2006</a>). However, in this case, another kind of localisation sensors are required, such as the ones developed for the RAXENV
<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> Project (Zendjebil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zendjebil IM, Ababsa F, Didier J, Vairon J, Frauciel L, Hachet M, Guitton P, Delmont R (2008) Outdoor augmented reality: state of the art and issues. In: Virtual reality international conference, pp 177–187" href="/article/10.1007/s10055-010-0176-4#ref-CR49" id="ref-link-section-d39760e1443">2008</a>).</p><h3 class="c-article__sub-heading" id="Sec18">Augmented reality demonstrator</h3><p>Since archaeologists interest is mainly focused on the nature of the cargo one of the first feedbacks from archaeologists concerning VR demonstrators was that immersive navigation did not provide much help to archaeological tasks in opposition to general public concerns where immersive navigation provides a deeper experience of a site. This observation leads us to propose an augmented map-based navigation paradigm such as the “World in Miniature” proposed by Stoakley et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stoakley R, Conway Mathew J, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: CHI ’95: Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 265–272" href="/article/10.1007/s10055-010-0176-4#ref-CR41" id="ref-link-section-d39760e1456">1995</a>) and later applied to augmented reality (Bell et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bell B, Höllerer T, Feiner S (2002) An annotated situation-awareness aid for augmented reality. In: UIST ’02: proceedings of the 15th annual ACM symposium on user interface software and technology. ACM, New York, pp 213–216" href="/article/10.1007/s10055-010-0176-4#ref-CR5" id="ref-link-section-d39760e1459">2002</a>) that provides a much more familiar interface to archaeologists. Indeed, archaeologists have more ease working with maps where they can see the real world rather than a totally immersive environment in which it is difficult to be localised. Moreover, the augmented reality paradigm offers the opportunity to introduce a tangible interface (Ishii and Ullmer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ’97: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 234–241" href="/article/10.1007/s10055-010-0176-4#ref-CR27" id="ref-link-section-d39760e1462">1997</a>; Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Poupyrev, Ivan, Tan Desney S, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2001) Tiles: a mixed reality authoring interface. In: INTERACT 2001 conference on human computer interaction, Tokyo, pp 334–341" href="/article/10.1007/s10055-010-0176-4#ref-CR35" id="ref-link-section-d39760e1465">2001</a>) to the tools developed in the VR demonstrator for archaeologists. These elements lead to the definition of a new demonstrator for archaeologists: AR VENUS.</p><p>In AR VENUS, archaeologists use a real map representing the deep underwater site. AR VENUS proposes to enrich this environment and complete the real world perception by adding synthetic elements to it rather than to immerse the archaeologist in a completely simulated artificial world. AR VENUS provides an easy tool to interact with the real world using tangible interface (in our case physical objects equipped with visual targets) to select and manipulate virtual objects by using a pose estimation algorithms to display artefacts models at the right location on the 2D map. Users need to wear special equipment, such as “see through” head-mounted display, to see the map, augmented in real time with computer-generated features (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig8">8</a>a).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The AR VENUS system and pose estimation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">3D map overlay</h4><p>The first step in AR VENUS is to project the 3D models of the seabed on the real 2D map using a system of visual markers identification and a pose estimation algorithm. For this visual tracking module, we used a simple webcam for tracking visual markers made up with printed 60 × 60 mm black and white fiducials. The tracking algorithm computes the real camera position and orientation relative to the physical markers in real time and also identify the content of the fiducial as a unique identifier (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig8">8</a>b). Some fiducials are stuck on the real map in order to compute the pose of the virtual environment over the real map, whereas others are used to interact.</p><p>We used OSGART library to identify targets and overlay the 3D models on the real scene. OSGART has been designed to provide an easy bidirectional transition from VR to AR (Looser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Looser J, Grasset R, Seichter H, Billinghurst M (2006) OSGART—a pragmatic approach to MR. In: International symposium of mixed and augmented reality (ISMAR 2006), Santa Barbara" href="/article/10.1007/s10055-010-0176-4#ref-CR31" id="ref-link-section-d39760e1505">2006</a>) by integrating ARToolkit (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M, Blanding B, May R (1999) ARToolKit (Technical Report). Technical report, Hiroshima City University" href="/article/10.1007/s10055-010-0176-4#ref-CR28" id="ref-link-section-d39760e1508">1999</a>) within OpenSceneGraph. The tracking library finds all squares in the binary image. For each square, the pattern inside the square is captured and matched to some pretrained pattern templates. The square size and pattern orientation are used to compute the position of the camera relative to the physical marker; hence, the pose accuracy mostly depends on the marker size. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig8">8</a>b shows the different steps of pose estimation algorithm (also called registration).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Virtual objects registration</h4><p>We used single and multiple targets with different scale to improve the tracking stability and accuracy. We started our tests using a single marker. The obtained results with a single marker were not accurate, and we noticed a large shift between the virtual model and the real one represented on the 2D map. The size ratio between the small target and the large map did not provide a correct registration, which led us after trying a larger target to consider a multitarget tracking approach since these targets are lying on the same map plane.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Tangible interface</h4><p>We saw in the previous section that static fiducials are used to register the virtual environment and artefacts; however, other targets can also be moved around the map and associated with virtual tools allowing the users to interact with the augmented environment using tangible user interfaces (TUI). Many research work has been done in this field over the last years (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: ISAR’ 00 : proceedings of the international symposium on augmented reality, Munich, pp 111–119" href="/article/10.1007/s10055-010-0176-4#ref-CR29" id="ref-link-section-d39760e1530">2000</a>), and several interfaces have been developed for manipulation and exploration of digital information (Gorbet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gorbet MG, Orth M, Ishii H (1998) Triangles: tangible interface for manipulation and exploration of digital information topography. In: CHI ’98: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 49–56" href="/article/10.1007/s10055-010-0176-4#ref-CR23" id="ref-link-section-d39760e1533">1998</a>) using physical objects as interaction tools in virtual environments (Ishii and Ullmer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ’97: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 234–241" href="/article/10.1007/s10055-010-0176-4#ref-CR27" id="ref-link-section-d39760e1536">1997</a>). Several moving targets have been associated with virtual tools. These tools are activated whenever the camera identifies their corresponding patterns and discarded when they are not visible anymore such as:
</p><ul class="u-list-style-bullet">
                      <li>
                        <p><i>selection tool:</i> The selection tool represents two sides of the same tangible “pallet” or “tile” as denoted by Billinghurst et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) Collaboration with tangible augmented reality interfaces. In: HCI ’2001: international conference on human computer interaction, New Orleans" href="/article/10.1007/s10055-010-0176-4#ref-CR8" id="ref-link-section-d39760e1547">2001</a>). The first side is used to trigger nearby object search and when an object is selected we can then flip to the other side for a closer inspection of the selected object (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig9">9</a>a). Tracking of the selection tile is important in this case since objects are selected when the tile’s target is close to the object of interest. The selection of amphorae is performed using a probe target, and the selected object stays attached to the selection probe for closer investigation. The developed technique consists in computing distance between the marker centre attached to the selection tool and the centre of amphorae got from the archaeological database. For unselecting, another marker is attached in the other side of the selection probe, when this marker is visible, the amphorae is deselected and placed into its original position on the map.</p>
                      </li>
                      <li>
                        <p><i>measuring tool:</i> the measuring tool allows to measure distances (expressed in the VE dimensions) between two specific targets moved around on the real map (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig9">9</a>b).</p>
                      </li>
                      <li>
                        <p><i>inventory tool:</i> Whenever the inventory tool target appears to the tracking camera, a virtual site inventory (in terms of artefacts’ type and occurrences) is attached above the target and can be placed on the map or handled for a closer inspection (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig9">9</a>c).</p>
                      </li>
                      <li>
                        <p><i>grid tool:</i> The grid tool allows to display a north south oriented regular grid on the site (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig9">9</a>d). The grid target uses the same principle of a two-sided tile used for selection tool in order to provide two different grid steps. Tracking the targets on the tile is not important in this case, as only target recognition is used to trigger the two possible grids.</p>
                      </li>
                    </ul>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>AR VENUS tools</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Evaluation</h2><div class="c-article-section__content" id="Sec22-content"><p>Some recent work (Dünser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dünser A, Grasset R, Billinghurst M (2008) A survey of evaluation techniques used in augmented reality studies. In: international conference on computer graphics and interactive techniques. ACM, New York" href="/article/10.1007/s10055-010-0176-4#ref-CR20" id="ref-link-section-d39760e1617">2008</a>; Domingues et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Domingues C, Otmane S, Davesne F, Mallem M (2008) Creating 3D interaction technique empirical evaluation with the use of a knowledge database of interaction experiments. In: Human system interactions, 2008 Conference on, pp 170–175" href="/article/10.1007/s10055-010-0176-4#ref-CR16" id="ref-link-section-d39760e1620">2008</a>) addressing the evaluation problems in VR and AR systems confirms the domination of objective and subjective measurement. Objective and subjective measurements are measurements methods used in empirical evaluations. Usually, we found these measurements in many evaluations (McMahan and Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="McMahan RP, Bowman DA (2007) An empirical comparison of task sequences for immersive virtual environments. In: IEEE symposium on 3D user interfaces" href="/article/10.1007/s10055-010-0176-4#ref-CR32" id="ref-link-section-d39760e1623">2007</a>; Looser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Looser J, Billinghurst M, Grasset R, Cockburn A (2007) An evaluation of virtual lenses for object selection in augmented reality. In: Proceedings of the 5th international conference on computer graphics and interactive techniques in Australia and Southeast Asia. ACM, New York, pp 203–210" href="/article/10.1007/s10055-010-0176-4#ref-CR30" id="ref-link-section-d39760e1626">2007</a>; Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance with different display devices for virtual environments. In: Proceedings of the human factors and ergonomics society annual meeting (HFES’ 02). Human Factors and Ergonomics Society, pp 2134–2138" href="/article/10.1007/s10055-010-0176-4#ref-CR10" id="ref-link-section-d39760e1629">2002</a>). Objective measurements are studies that include objective measurements such as completion times, accuracy/error rates and generally, statistical analyses are made on the measured variables. Subjective measurement studies users using questionnaires. They also employ statistical analysis of the results or a descriptive analysis.</p><h3 class="c-article__sub-heading" id="Sec23">Subjective evaluation</h3><p>The functionalities offered by the various demonstrators have been evaluated by archaeologists during a workshop. The goal of the workshop was to present the already developed features to archaeologists but also to trigger feedbacks by having the archaeologists actually experiment the demonstrator by themselves. After a short presentation of the goals and means of the archaeologists demonstrators and a quick demo of the main features, each of the archaeologists was asked to perform the following tasks:
</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Artefacts group selection.</p>
                    </li>
                    <li>
                      <p>Individual artefact selection.</p>
                    </li>
                    <li>
                      <p>Circular area selection.</p>
                    </li>
                    <li>
                      <p>Distance measuring between two artefacts.</p>
                    </li>
                    <li>
                      <p>Add an annotation on the site, modify it then remove the annotation.</p>
                    </li>
                    <li>
                      <p>Switch between seabed display mode (regular → grid → topographic)</p>
                    </li>
                    <li>
                      <p>Display snapshot locations and images related to an artefact.</p>
                    </li>
                  </ul>
                        <p>The survey was conducted individually with 11 archaeologists on two laptops running the low-end demonstrator. By the end of the test, they were asked to fill in a questionnaire dedicated to help us pointing what is good and bad in the VR demonstrator for archaeologists.</p><p>The first questionnaire was about participant skills and habits concerning computer usage and more specifically the use of 3D and VR software as well as their current state by the end of the test. The second questionnaire was about the usability of the archaeological demonstrator in terms of navigation, interaction, accessing the tools and displayed data. The third questionnaire referred to users’ satisfaction and allowed users to suggest improvements. The exploitation of these evaluation results is part of the validation of developed software as well as part of an iterative cycle methodology. This workshop was a unique opportunity to get feedback from a large panel of (well known) archaeologists.</p><p>The survey has been performed by 7 women and 4 men, aged between 31 and 60 with an average of 43 ± 10 years old, hence providing a rather senior researchers panel. Most of them are using computers on a daily basis. More than half of the users have already used 3D software, and a bit less than a half of them already used virtual reality software.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Usability</h4><p>Users were asked to rank the difficulty (on a 1–5 scale) of various aspects of the demonstrator such as navigation, interaction, accessing tools and also rank whether data display associated with various selection methods were satisfying. Individual data facts refer to the data sheet associated with a single selected object (as shown on Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig11">11</a>), type data facts refer to the information displayed by selecting all instances of a specific type of artefact (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig12">12</a>) and area data facts refer to the information displayed during area-based selection (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig13">13</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig10">10</a> presents the assessments of these various topics.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Usability summary</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Individual data facts</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Type data facts, selected artefacts and fracture lines</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Area data facts</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The following analysis on the most frequent actions is based on the results of the questionnaires as well as archaeologists’ comments during the test written down by the two experimenters.</p><h5 class="c-article__sub-heading" id="Sec25">Navigatio</h5><p>Navigation (whether free or guided by the artefacts) was considered as easy (or very easy) by 64% of the users; however, 36% gave marks between 2 and 3 that tends to indicate they encountered some difficulty in navigating. This might explain the suggestions for new navigation modes (such as diver’s mode) we have found in the possible improvements.</p><h5 class="c-article__sub-heading" id="Sec26">Interaction</h5><p>Interaction marks features the same distribution as navigation, 64 % found it easy and 36 % encountered some difficulties. Once again, this might explain the number of improvements requests concerning the “Interaction with artefacts” feature (14 improvements requests out of 27).</p><h5 class="c-article__sub-heading" id="Sec27">Tools access</h5><p>Tools are mainly situated in the menu and were considered as easy by a majority of 73% of the users; however, 27% of them gave a mark of 3 and would have appreciated a clear separation of tools and various options in the menu.</p><h5 class="c-article__sub-heading" id="Sec28">Individual data facts</h5><p>Individual data facts presenting the data sheet of a selected artefact were considered as satisfactory by 55% of the users; however, this also means that 45% of them were not satisfied or at least had not a good opinion of it. This also shows through the improvements request as 5 of them directly concerned “Individual data facts”. The comments made on this point focused on the numerical aspect of the information presented to the user (location and statistics), whereas they would have appreciated pieces of information like orientation and tilt or object visual reference (as displayed in classical artefacts catalogues such as the Dressel catalogue), which has been partially fulfilled by a new information panel (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig11">11</a>).</p><h5 class="c-article__sub-heading" id="Sec29">Type data facts</h5><p>Type data facts present the exact opposite repartition, as 45% of the users only were satisfied by the information displayed by selecting types. Once again, the numerical nature of the information panel (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig12">12</a>) could have been enhanced by showing an instance (the 3D model) of the selected type in the information panel. Besides, selecting types lead to a viewpoint change encompassing all instances of the selected type as well as a highlight of the selected artefacts. As a matter of fact, this highlighting could have easily been confused with fracture lines on artefacts (see also Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig12">12</a>) representing broken artefacts at the moment, and this might also explain why the improvement request concerning a “better enhancement of selected artefacts” occurred 4 times in the survey.</p><h5 class="c-article__sub-heading" id="Sec30">Area data facts</h5><p>55% of the users were satisfied by information panel displayed during circular area selection featuring the population of selected artefacts showing artefacts’ type and occurrences (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig13">13</a>). But 45% had no opinion of were dissatisfied once again by the numerical nature of the information panel that could have been enhanced with artefacts type silhouettes.</p><p>This subjective evaluation performed with archaeologists allowed us to improve the core functionalities of the demonstrators and also add some new features to the virtual environment such as “diver’s navigation” mode that allows archaeologists to review the site and measured artefacts just like divers would during the photogrammetric survey.</p><h3 class="c-article__sub-heading" id="Sec31">Objective evaluation</h3><p>In addition to the subjective evaluation mentioned above, we also evaluated the benefits of immersive VR versus nonimmersive by comparing users performances with three distinct forms of the VR demonstrators: in our case, the low-end demonstrator, the semi-immersive demonstrator and the immersive demonstrator.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec32">Evaluation setup</h4><p>The evaluated platforms are:
</p><ul class="u-list-style-bullet">
                      <li>
                        <p>D1: The low-end demonstrator (nonimmersive), featuring only standard input/output devices such as keyboard, mouse and 19” monitor.</p>
                      </li>
                      <li>
                        <p>D2: The semi-immersive demonstrator, featuring large stereo screen display and two flysticks for navigation and selection tasks. Snowplough is used as a navigation metaphor with the flysticks (as exposed in 3.2.1.3)</p>
                      </li>
                      <li>
                        <p>D3: The immersive demonstrator, featuring a tracked VR helmet allowing to look around and the same two flysticks.</p>
                      </li>
                    </ul>
                           <p>Navigation on demonstrators (D2) and (D3) uses the same technique and same interface.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec33">Evaluation protocol and experiment</h4><p>In this experiment, a navigation task was carried out on all three platforms. Fifteen volunteers (six women and nine men) performed this experiment. All of them were right handed. Each subject was given pretest along with a short briefing. The subjects were divided into 3 groups. Each group performed the experiment using demonstrators in the following order: D1, D2 and D3 for group 1, D2, D3 and D1 for group 2 and finally D3, D1 and D2 for group 3.</p><p>Each group carried out four tests of each task and for each demonstrator. The evaluation is based on task completion time for the navigation.</p><h5 class="c-article__sub-heading" id="Sec34">Task completion time</h5><p>Navigation task consisted in reaching a sequence of hotspots within the virtual site (progressively appearing and disappearing once they have been reached).</p><p>The average completion time for this task was 99 ± 26 s with the immersive demonstrator, 120 ± 43 s with the semi-immersive demonstrator and 187 ± 81 s with the nonimmersive demonstrator with a significant ANOVA (<i>P</i> = 0.000185 ≪ 0.01). These results (presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig14">14</a>) show that immersive conditions have an influence on navigation task performance, and the immersive demonstrator provided the best navigation task performances.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Navigation task completion time under various conditions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <h5 class="c-article__sub-heading" id="Sec35">User learning</h5><p>Learning is defined here by the improvement of group performance during task repetitions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0176-4#Fig15">15</a> shows user learning during the navigation task for various conditions (immersive, semi-immersive and nonimmersive demonstrators). The results show that using immersive condition, the average completion time was 123 ± 35 s during the first test and 81 ± 20 s during the fourth test. The average completion time under semi-immersive condition was 147 ± 71 s during the first test and 101 ± 32 s during the fourth test. Similarly, the average completion time under nonimmersive condition was 242 ± 115 s for the first test and 125 ± 54 s for the last test. These results show a navigation performance improvement of 34.22, 31.09 and 48.24% for immersive, semi-immersive and nonimmersive conditions, respectively.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0176-4/MediaObjects/10055_2010_176_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Illustration of user learning for various conditions in navigation task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0176-4/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <p>In this section, we introduced the first results of evaluation of the three demonstrators (Immersive, semi-immersive and nonimmersive). Two types of evaluation have been accomplished: objective and subjective evaluations. We compared principally the influence of the immersion type with the navigation and selection tasks performances. Results show that the performances for the navigation task are clearly better with the immersive demonstrator. However, the performances for the selection task are better with the nonimmersive. It is necessary to note that the preliminary objective evaluation introduced here is based on task completion time performance measurements. Other measurements (selection and/or navigation errors) can be considered in the future. In the same way, the use of the virtual guides (Rosenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In: Proceedings of IEEE virtual reality international symposium, pp 76–82" href="/article/10.1007/s10055-010-0176-4#ref-CR39" id="ref-link-section-d39760e1948">1993</a>; Otmane et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guide as an apparatus for augmented reality based telemanipulation system on the Internet. In: Proceedings of the 33rd annual simulation symposium, (SS 2000). IEEE Computer Society" href="/article/10.1007/s10055-010-0176-4#ref-CR34" id="ref-link-section-d39760e1951">2000</a>; Prada and Payandeh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real J 13(2):117–129" href="/article/10.1007/s10055-010-0176-4#ref-CR36" id="ref-link-section-d39760e1954">2009</a>) as assistance tools for navigation and in selection could also contribute to improve 3D interaction tasks performances. Some other evaluations concerning the augmented reality (AR) demonstrator still need to be performed. We will attempt to study the influence of this type of immersion on navigation and selection task performances and to compare it with the three previous demonstrators.</p></div></div></section><section aria-labelledby="Sec36"><div class="c-article-section" id="Sec36-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec36">Conclusion and perspectives</h2><div class="c-article-section__content" id="Sec36-content"><p>We have described an attempt to introduce VR and AR technologies in the field of underwater archaeology as a working tool rather than the usual presentation tool. Pursuing this goal, we tried to apply various levels of immersion and interaction facilities to a virtual environment designed to allow archaeologists to review and study underwater wreck sites possibly unreachable to divers reconstructed in terms of environment (seabed) and content (cargo) only through bathymetric and photogrammetric surveys. Several demonstrators using VR and AR technologies were built around this environment allowing us to start evaluating the benefits of these technologies with archaeologists.</p><p>A first subjective evaluation with archaeologists allowed us to review and amend the features of the virtual environment and also introduce new features resulting from this first “handover” and enter the iterative cycle of refining the features. In a similar way, preliminary evaluations have been performed to compare immersive and nonimmersive VR demonstrator in terms of performance gain in order to assess the benefits of immersive VR. However, deeper evaluation needs to be performed as well as an evaluation of the AR demonstrator against the other ones.</p><p>Using these innovative methods of research and dissemination can capture the imagination of the general public and generate interest not only in the historical aspect of archaeology but also in the work and expertise that goes into supporting these archaeological surveys.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>“Arpenteur” photogrammetric tool is developed by Pierre Drap’s team at LSIS, in Marseille, France (Drap and Grussenmeyer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Drap P, Grussenmeyer P (2000) A digital photogrammetric workstation on the WEB. J Photogramm Remote Sensing 55(1):48–58" href="/article/10.1007/s10055-010-0176-4#ref-CR17" id="ref-link-section-d39760e365">2000</a>).</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p><a href="http://raxenv.brgm.fr/">http://raxenv.brgm.fr/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Acevedo D, Vote E, Laidlaw DH, Joukowsky MS (2001) Archaeological data visualization in VR: analysis of lamp f" /><p class="c-article-references__text" id="ref-CR2">Acevedo D, Vote E, Laidlaw DH, Joukowsky MS (2001) Archaeological data visualization in VR: analysis of lamp finds at the great temple of Petra, a case study. In: Proceedings of the 12th IEEE conference on visualization (VIS ’01). IEEE Computer Society, Washington, DC, pp 493–496</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ART (2009) Advanced realtime tracking" /><p class="c-article-references__text" id="ref-CR1">ART (2009) Advanced realtime tracking</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyr, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyr B (2001) Recent advances in augmented reality. I" /><p class="c-article-references__text" id="ref-CR3">Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyr B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=6&amp;pages=34-47&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY&amp;author=Behringer%2CR&amp;author=Feiner%2CS&amp;author=Julier%2CS&amp;author=MacIntyr%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JA. Barceló, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Barceló JA (2000) Visualizing what might be: an introduction to virtual reality techniques in archaeology. Vir" /><p class="c-article-references__text" id="ref-CR4">Barceló JA (2000) Visualizing what might be: an introduction to virtual reality techniques in archaeology. Virtual reality in archaeology. British Archaeol Rep S843:9–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visualizing%20what%20might%20be%3A%20an%20introduction%20to%20virtual%20reality%20techniques%20in%20archaeology.%20Virtual%20reality%20in%20archaeology&amp;journal=British%20Archaeol%20Rep&amp;volume=S843&amp;pages=9-36&amp;publication_year=2000&amp;author=Barcel%C3%B3%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bell B, Höllerer T, Feiner S (2002) An annotated situation-awareness aid for augmented reality. In: UIST ’02: " /><p class="c-article-references__text" id="ref-CR5">Bell B, Höllerer T, Feiner S (2002) An annotated situation-awareness aid for augmented reality. In: UIST ’02: proceedings of the 15th annual ACM symposium on user interface software and technology. ACM, New York, pp 213–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benko H, Ishak EW, Feiner S (2004) Collaborative mixed reality visualization of an archaeological excavation. " /><p class="c-article-references__text" id="ref-CR6">Benko H, Ishak EW, Feiner S (2004) Collaborative mixed reality visualization of an archaeological excavation. In: ISMAR ’04: proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality. IEEE Computer Society, Washington, DC, pp 132–140</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: VRAIS " /><p class="c-article-references__text" id="ref-CR7">Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: VRAIS ’98: proceedings of the virtual reality annual international symposium. IEEE Computer Society, Washington, DC, pages 20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I (2001) Collaboration with tangible augmented reality interfaces. In: HCI ’2" /><p class="c-article-references__text" id="ref-CR8">Billinghurst M, Kato H, Poupyrev I (2001) Collaboration with tangible augmented reality interfaces. In: HCI ’2001: international conference on human computer interaction, New Orleans</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AD. Bowman, E. Kruijff, I. Laviola, J. Poupyrev, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bowman AD, Kruijff E, Laviola I, Poupyrev J (2005) 3D user interfaces: theory and practice. Addison Wesley Lon" /><p class="c-article-references__text" id="ref-CR9">Bowman AD, Kruijff E, Laviola I, Poupyrev J (2005) 3D user interfaces: theory and practice. Addison Wesley Longman Publishing Co., Inc., Redwood City</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20theory%20and%20practice&amp;publication_year=2005&amp;author=Bowman%2CAD&amp;author=Kruijff%2CE&amp;author=Laviola%2CI&amp;author=Poupyrev%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance " /><p class="c-article-references__text" id="ref-CR10">Bowman DA, Datey A, Ryu YS, Farooq U, Vasnaik O (2002) Empirical comparison of human behavior and performance with different display devices for virtual environments. In: Proceedings of the human factors and ergonomics society annual meeting (HFES’ 02). Human Factors and Ergonomics Society, pp 2134–2138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint mot" /><p class="c-article-references__text" id="ref-CR11">Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: VRAIS ’97: proceedings of the 1997 virtual reality annual international symposium (VRAIS ’97). IEEE Computer Society, Washington, DC, page 45</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Burns D, Osfield R (2004) Open scene graph a: introduction, b: examples and applications. In: VR ’04: proceedi" /><p class="c-article-references__text" id="ref-CR12">Burns D, Osfield R (2004) Open scene graph a: introduction, b: examples and applications. In: VR ’04: proceedings of the IEEE virtual reality 2004. IEEE Computer Society, Washington, DC, page 265</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="CC (2009) The cultural computing program" /><p class="c-article-references__text" id="ref-CR13">CC (2009) The cultural computing program</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cosmas, John, Take I, Damian G, Edward G, Fred W, Luc VG, Alexy Z, Desi V, Franz L, Markus G, Konrad S, Konrad" /><p class="c-article-references__text" id="ref-CR14">Cosmas, John, Take I, Damian G, Edward G, Fred W, Luc VG, Alexy Z, Desi V, Franz L, Markus G, Konrad S, Konrad K, Michael G, Stefan H, Marc W, Marc P, Roland D, Robert S, Martin K (2001) 3D MURALE: a multimedia system for archaeology. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 297–306</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and im" /><p class="c-article-references__text" id="ref-CR15">Cruz-Neira C, Sandin DJ, DeFanti TA (1993) Surround-screen projection-based virtual reality: the design and implementation of the CAVE. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, ACM, New York, pp 135–142</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Domingues C, Otmane S, Davesne F, Mallem M (2008) Creating 3D interaction technique empirical evaluation with " /><p class="c-article-references__text" id="ref-CR16">Domingues C, Otmane S, Davesne F, Mallem M (2008) Creating 3D interaction technique empirical evaluation with the use of a knowledge database of interaction experiments. In: Human system interactions, 2008 Conference on, pp 170–175</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Drap, P. Grussenmeyer, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Drap P, Grussenmeyer P (2000) A digital photogrammetric workstation on the WEB. J Photogramm Remote Sensing 55" /><p class="c-article-references__text" id="ref-CR17">Drap P, Grussenmeyer P (2000) A digital photogrammetric workstation on the WEB. J Photogramm Remote Sensing 55(1):48–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0924-2716%2899%2900038-6" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20digital%20photogrammetric%20workstation%20on%20the%20WEB&amp;journal=J%20Photogramm%20Remote%20Sensing&amp;volume=55&amp;issue=1&amp;pages=48-58&amp;publication_year=2000&amp;author=Drap%2CP&amp;author=Grussenmeyer%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Drap P, Long L (2001) Towards a digital excavation data management system: the “Grand Ribaud F” Estruscan deep" /><p class="c-article-references__text" id="ref-CR18">Drap P, Long L (2001) Towards a digital excavation data management system: the “Grand Ribaud F” Estruscan deep-water wreck. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 17–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Drap P, Nedir M, Seinturier J, Papini O, Chapman P, Boucault F, Viant W, Vannini G, Nuccioti M (2006) Toward a" /><p class="c-article-references__text" id="ref-CR19">Drap P, Nedir M, Seinturier J, Papini O, Chapman P, Boucault F, Viant W, Vannini G, Nuccioti M (2006) Toward a photogrammetry and virtual reality based heritage information system: a case study of shawbak castle in jordan. In: Joint event conference of the37th CIPA international workshop dedicated on e-documentation and standardisation in cultural heritage, 7th VAST international symposium on virtual reality, archaeology and cultural heritage, 4th eurographics workshop on graphics and cultural heritage and 1st Euro-med conference on IT in cultural heritage</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dünser A, Grasset R, Billinghurst M (2008) A survey of evaluation techniques used in augmented reality studies" /><p class="c-article-references__text" id="ref-CR20">Dünser A, Grasset R, Billinghurst M (2008) A survey of evaluation techniques used in augmented reality studies. In: international conference on computer graphics and interactive techniques. ACM, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gaitatzes A, Christopoulos D, Roussou M (2001) Reviving the past: cultural heritage meets virtual reality. In:" /><p class="c-article-references__text" id="ref-CR21">Gaitatzes A, Christopoulos D, Roussou M (2001) Reviving the past: cultural heritage meets virtual reality. In: VAST ’01: proceedings of the 2001 conference on virtual reality, archeology, and cultural heritage. ACM, New York, pp 103–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Germs, G. Van Maren, E. Verbree, FW. Jansen, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Germs R, Van Maren G, Verbree E, Jansen FW (1999) A multi-view VR interface for 3D GIS. Comput Graph 23(4):497" /><p class="c-article-references__text" id="ref-CR22">Germs R, Van Maren G, Verbree E, Jansen FW (1999) A multi-view VR interface for 3D GIS. Comput Graph 23(4):497–506</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2899%2900069-2" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20multi-view%20VR%20interface%20for%203D%20GIS&amp;journal=Comput%20Graph&amp;volume=23&amp;issue=4&amp;pages=497-506&amp;publication_year=1999&amp;author=Germs%2CR&amp;author=Van%20Maren%2CG&amp;author=Verbree%2CE&amp;author=Jansen%2CFW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gorbet MG, Orth M, Ishii H (1998) Triangles: tangible interface for manipulation and exploration of digital in" /><p class="c-article-references__text" id="ref-CR23">Gorbet MG, Orth M, Ishii H (1998) Triangles: tangible interface for manipulation and exploration of digital information topography. In: CHI ’98: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 49–56</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hall Rebecca, W. Hall Andrew, J. Barto Arnold, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hall Rebecca A, Hall Andrew W, Barto Arnold J III (1997) Presenting archaeology on the Web: The La Salle Shipw" /><p class="c-article-references__text" id="ref-CR24">Hall Rebecca A, Hall Andrew W, Barto Arnold J III (1997) Presenting archaeology on the Web: The La Salle Shipwreck Project. Int J Nautical Archaeol 26(3):247–251</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Presenting%20archaeology%20on%20the%20Web%3A%20The%20La%20Salle%20Shipwreck%20Project&amp;journal=Int%20J%20Nautical%20Archaeol&amp;volume=26&amp;issue=3&amp;pages=247-251&amp;publication_year=1997&amp;author=Hall%20Rebecca%2CA&amp;author=Hall%20Andrew%2CW&amp;author=Barto%20Arnold%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haydar M, Maïdi M, Roussel D, Mallem M (2009) A new navigation method for 3D virtual environment exploration. " /><p class="c-article-references__text" id="ref-CR25">Haydar M, Maïdi M, Roussel D, Mallem M (2009) A new navigation method for 3D virtual environment exploration. In: AIP (ed) The 2nd Mediterranean conference on intelligent systems and automation (CISA 2009), vol 1107. AIP, Zarzis (Tunisia), pp 190–195</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassell N (1997) Usability analysis of 3D rotation techniques. In:" /><p class="c-article-references__text" id="ref-CR26">Hinckley K, Tullio J, Pausch R, Proffitt D, Kassell N (1997) Usability analysis of 3D rotation techniques. In: Proceedings of the 10th annual ACM symposium on user interface software and technology. ACM, New York, pp 1–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ’9" /><p class="c-article-references__text" id="ref-CR27">Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: CHI ’97: proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Blanding B, May R (1999) ARToolKit (Technical Report). Technical report, Hiroshima Cit" /><p class="c-article-references__text" id="ref-CR28">Kato H, Billinghurst M, Blanding B, May R (1999) ARToolKit (Technical Report). Technical report, Hiroshima City University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top A" /><p class="c-article-references__text" id="ref-CR29">Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: ISAR’ 00 : proceedings of the international symposium on augmented reality, Munich, pp 111–119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Looser J, Billinghurst M, Grasset R, Cockburn A (2007) An evaluation of virtual lenses for object selection in" /><p class="c-article-references__text" id="ref-CR30">Looser J, Billinghurst M, Grasset R, Cockburn A (2007) An evaluation of virtual lenses for object selection in augmented reality. In: Proceedings of the 5th international conference on computer graphics and interactive techniques in Australia and Southeast Asia. ACM, New York, pp 203–210</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Looser J, Grasset R, Seichter H, Billinghurst M (2006) OSGART—a pragmatic approach to MR. In: International sy" /><p class="c-article-references__text" id="ref-CR31">Looser J, Grasset R, Seichter H, Billinghurst M (2006) OSGART—a pragmatic approach to MR. In: International symposium of mixed and augmented reality (ISMAR 2006), Santa Barbara</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McMahan RP, Bowman DA (2007) An empirical comparison of task sequences for immersive virtual environments. In:" /><p class="c-article-references__text" id="ref-CR32">McMahan RP, Bowman DA (2007) An empirical comparison of task sequences for immersive virtual environments. In: IEEE symposium on 3D user interfaces</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mine Mark R Jr., Brooks Frederick P, Sequin Carlo H (1997) Moving objects in space: exploiting proprioception " /><p class="c-article-references__text" id="ref-CR33">Mine Mark R Jr., Brooks Frederick P, Sequin Carlo H (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. In: SIGGRAPH ’97: Proceedings of the 24th annual conference on computer graphics and interactive techniques, vol 31. ACM Press/Addison-Wesley Publishing Co, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guide as an apparatus for augmented reality bas" /><p class="c-article-references__text" id="ref-CR34">Otmane S, Mallem M, Kheddar A, Chavand F (2000) Active virtual guide as an apparatus for augmented reality based telemanipulation system on the Internet. In: Proceedings of the 33rd annual simulation symposium, (SS 2000). IEEE Computer Society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev, Ivan, Tan Desney S, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2001) Tiles: a mixed reality" /><p class="c-article-references__text" id="ref-CR35">Poupyrev, Ivan, Tan Desney S, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2001) Tiles: a mixed reality authoring interface. In: INTERACT 2001 conference on human computer interaction, Tokyo, pp 334–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Prada, S. Payandeh, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real J 13(2):117" /><p class="c-article-references__text" id="ref-CR36">Prada R, Payandeh S (2009) On study of design and implementation of virtual fixtures. Virtual Real J 13(2):117–129</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0115-4" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20study%20of%20design%20and%20implementation%20of%20virtual%20fixtures&amp;journal=Virtual%20Real%20J&amp;volume=13&amp;issue=2&amp;pages=117-129&amp;publication_year=2009&amp;author=Prada%2CR&amp;author=Payandeh%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reichlen BA (1993) Sparcchair: a one hundred million pixel display. In: 1993 IEEE virtual reality annual inter" /><p class="c-article-references__text" id="ref-CR37">Reichlen BA (1993) Sparcchair: a one hundred million pixel display. In: 1993 IEEE virtual reality annual international symposium, pp 300–307</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Rheingold, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Rheingold H (1991) Virtual reality. Summit Books, London" /><p class="c-article-references__text" id="ref-CR38">Rheingold H (1991) Virtual reality. Summit Books, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality&amp;publication_year=1991&amp;author=Rheingold%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In: Proceedings of IEEE vi" /><p class="c-article-references__text" id="ref-CR39">Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In: Proceedings of IEEE virtual reality international symposium, pp 76–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Ruddle, SJ. Payne, DM. Jones, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Ruddle RA, Payne SJ, Jones DM (1999) Navigating large-scale virtual environments: what differences occur betwe" /><p class="c-article-references__text" id="ref-CR40">Ruddle RA, Payne SJ, Jones DM (1999) Navigating large-scale virtual environments: what differences occur between helmet-mounted and desk-top displays? Presence: Teleoperators Virtual Environ 8(2):157–168</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474699566143" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigating%20large-scale%20virtual%20environments%3A%20what%20differences%20occur%20between%20helmet-mounted%20and%20desk-top%20displays%3F&amp;journal=Presence%3A%20Teleoperators%20Virtual%20Environ&amp;volume=8&amp;issue=2&amp;pages=157-168&amp;publication_year=1999&amp;author=Ruddle%2CRA&amp;author=Payne%2CSJ&amp;author=Jones%2CDM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoakley R, Conway Mathew J, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: CH" /><p class="c-article-references__text" id="ref-CR41">Stoakley R, Conway Mathew J, Pausch R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: CHI ’95: Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press/Addison-Wesley Publishing Co, New York, pp 265–272</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor II, Russell M, Hudson Thomas C, Seeger A, Weber H, Juliano J, Helser Aron T (2001) VRPN: a device-indep" /><p class="c-article-references__text" id="ref-CR42">Taylor II, Russell M, Hudson Thomas C, Seeger A, Weber H, Juliano J, Helser Aron T (2001) VRPN: a device-independent, network-transparent VR peripheral system. In: VRST ’01: proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 55–61</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Tosa, S. Matsuoka, B. Ellis, H. Ueda, R. Nakatsu, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2009) Cultural computing with context-aware application: ZENet" /><p class="c-article-references__text" id="ref-CR43">Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2009) Cultural computing with context-aware application: ZENetic computer. Lect Notes Comput Sci 3711:13–23</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F11558651_2" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cultural%20computing%20with%20context-aware%20application%3A%20ZENetic%20computer&amp;journal=Lect%20Notes%20Comput%20Sci&amp;volume=3711&amp;pages=13-23&amp;publication_year=2009&amp;author=Tosa%2CN&amp;author=Matsuoka%2CS&amp;author=Ellis%2CB&amp;author=Ueda%2CH&amp;author=Nakatsu%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Van Dam, DH. Laidlaw, RM. Simpson, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization" /><p class="c-article-references__text" id="ref-CR44">Van Dam A, Laidlaw DH, Simpson RM (2002) Experiments in immersive virtual reality for scientific visualization. Comput Grap 26(4):535–555</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2802%2900113-9" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Experiments%20in%20immersive%20virtual%20reality%20for%20scientific%20visualization&amp;journal=Comput%20Grap&amp;volume=26&amp;issue=4&amp;pages=535-555&amp;publication_year=2002&amp;author=Van%20Dam%2CA&amp;author=Laidlaw%2CDH&amp;author=Simpson%2CRM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Vlahakis, N. Ioannidis, J. Karigiannis, M. Tsotros, M. Gounaris, D. Stricker, T. Gleue, P. Daehne, L. Almeida, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Vlahakis V, Ioannidis N, Karigiannis J, Tsotros M, Gounaris M, Stricker D, Gleue T, Daehne P, Almeida L (2002)" /><p class="c-article-references__text" id="ref-CR45">Vlahakis V, Ioannidis N, Karigiannis J, Tsotros M, Gounaris M, Stricker D, Gleue T, Daehne P, Almeida L (2002) Archeoguide: an augmented reality guide for archaeological sites. IEEE Comput Grap Appl 22(5):52–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2002.1028726" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Archeoguide%3A%20an%20augmented%20reality%20guide%20for%20archaeological%20sites&amp;journal=IEEE%20Comput%20Grap%20Appl&amp;volume=22&amp;issue=5&amp;pages=52-60&amp;publication_year=2002&amp;author=Vlahakis%2CV&amp;author=Ioannidis%2CN&amp;author=Karigiannis%2CJ&amp;author=Tsotros%2CM&amp;author=Gounaris%2CM&amp;author=Stricker%2CD&amp;author=Gleue%2CT&amp;author=Daehne%2CP&amp;author=Almeida%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Vote, DA. Feliz, DH. Laidlaw, MS. Joukowsky, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Vote E, Feliz DA, Laidlaw DH, Joukowsky MS (2002) Discovering petra: archaeological analysis in VR. IEEE Compu" /><p class="c-article-references__text" id="ref-CR46">Vote E, Feliz DA, Laidlaw DH, Joukowsky MS (2002) Discovering petra: archaeological analysis in VR. IEEE Comput Graph Appl 22(5):38–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2002.1028725" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Discovering%20petra%3A%20archaeological%20analysis%20in%20VR&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=22&amp;issue=5&amp;pages=38-50&amp;publication_year=2002&amp;author=Vote%2CE&amp;author=Feliz%2CDA&amp;author=Laidlaw%2CDH&amp;author=Joukowsky%2CMS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F-Y. Wang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Wang F-Y (2009) Is culture computable?. IEEE Intell Syst 24(2):2–3" /><p class="c-article-references__text" id="ref-CR47">Wang F-Y (2009) Is culture computable?. IEEE Intell Syst 24(2):2–3</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMIS.2009.31" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Is%20culture%20computable%3F&amp;journal=IEEE%20Intell%20Syst&amp;volume=24&amp;issue=2&amp;pages=2-3&amp;publication_year=2009&amp;author=Wang%2CF-Y">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Watts Gordon P, Kurt Knoerl T (2007) Out of the blue—public interpretation of maritime cultural resources, cha" /><p class="c-article-references__text" id="ref-CR48">Watts Gordon P, Kurt Knoerl T (2007) Out of the blue—public interpretation of maritime cultural resources, chapter entering the virtual world of underwater archaeology. Springer, pp 223–239</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zendjebil IM, Ababsa F, Didier J, Vairon J, Frauciel L, Hachet M, Guitton P, Delmont R (2008) Outdoor augmente" /><p class="c-article-references__text" id="ref-CR49">Zendjebil IM, Ababsa F, Didier J, Vairon J, Frauciel L, Hachet M, Guitton P, Delmont R (2008) Outdoor augmented reality: state of the art and issues. In: Virtual reality international conference, pp 177–187</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0176-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>VENUS is partially supported by the European Community under project VENUS (Contract IST-034924) of the “Information Society Technologies (IST) programme of the 6th FP for RTD”. The authors are solely responsible for the content of this paper. It does not represent the opinion of the European Community, and the European Community is not responsible for any use that might be made of data appearing therein.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Laboratoire IBISC, Université d’Evry, 40 rue du Pelvoux, 91020, Evry, France</p><p class="c-article-author-affiliation__authors-list">Mahmoud Haydar, David Roussel, Madjid Maïdi, Samir Otmane &amp; Malik Mallem</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mahmoud-Haydar"><span class="c-article-authors-search__title u-h3 js-search-name">Mahmoud Haydar</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mahmoud+Haydar&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mahmoud+Haydar" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mahmoud+Haydar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-David-Roussel"><span class="c-article-authors-search__title u-h3 js-search-name">David Roussel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;David+Roussel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=David+Roussel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22David+Roussel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Madjid-Ma_di"><span class="c-article-authors-search__title u-h3 js-search-name">Madjid Maïdi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Madjid+Ma%C3%AFdi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Madjid+Ma%C3%AFdi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Madjid+Ma%C3%AFdi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Samir-Otmane"><span class="c-article-authors-search__title u-h3 js-search-name">Samir Otmane</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Samir+Otmane&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samir+Otmane" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samir+Otmane%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Malik-Mallem"><span class="c-article-authors-search__title u-h3 js-search-name">Malik Mallem</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Malik+Mallem&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Malik+Mallem" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Malik+Mallem%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0176-4/email/correspondent/c1/new">Mahmoud Haydar</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Virtual%20and%20augmented%20reality%20for%20cultural%20computing%20and%20heritage%3A%20a%20case%20study%20of%20virtual%20exploration%20of%20underwater%20archaeological%20sites%20%28preprint%29&amp;author=Mahmoud%20Haydar%20et%20al&amp;contentID=10.1007%2Fs10055-010-0176-4&amp;publication=1359-4338&amp;publicationDate=2010-10-29&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Haydar, M., Roussel, D., Maïdi, M. <i>et al.</i> Virtual and augmented reality for cultural computing and heritage: a case study of virtual exploration of underwater archaeological sites (preprint).
                    <i>Virtual Reality</i> <b>15, </b>311–327 (2011). https://doi.org/10.1007/s10055-010-0176-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0176-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-04-24">24 April 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-10-12">12 October 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-10-29">29 October 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11">November 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0176-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0176-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Underwater archaeology</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cultural heritage</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cultural computing</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0176-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=176;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

