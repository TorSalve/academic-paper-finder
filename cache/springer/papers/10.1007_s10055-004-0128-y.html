<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A system for desktop conceptual 3D design"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configuration until they are satisfied with the..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/7/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A system for desktop conceptual 3D design"/>

    <meta name="dc.source" content="Virtual Reality 2004 7:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2004-05-27"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configuration until they are satisfied with the general layout. In this activity, one of the main operations is the rearrangement of single and composite parts of a scene. With current desktop virtual reality (VR) systems, the selection and manipulation of arbitrary objects in 3D is still difficult. In this work, we present new and efficient techniques that allow even novice users to perform meaningful rearrangement tasks with traditional input devices. The results of our work show that the presented techniques can be mastered quickly and enable users to perform complex tasks on composite objects. Moreover, the system is easy to learn, supports creativity, and is fun to use."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2004-05-27"/>

    <meta name="prism.volume" content="7"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="198"/>

    <meta name="prism.endingPage" content="211"/>

    <meta name="prism.copyright" content="2004 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0128-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0128-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0128-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0128-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A system for desktop conceptual 3D design"/>

    <meta name="citation_volume" content="7"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2004/06"/>

    <meta name="citation_online_date" content="2004/05/27"/>

    <meta name="citation_firstpage" content="198"/>

    <meta name="citation_lastpage" content="211"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0128-y"/>

    <meta name="DOI" content="10.1007/s10055-004-0128-y"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0128-y"/>

    <meta name="description" content="In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configura"/>

    <meta name="dc.creator" content="Ji-Young Oh"/>

    <meta name="dc.creator" content="Wolfgang Stuerzlinger"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Aish R, Frankel J, Frazer J, Patera A, Marks J (2001) Computational construction kits for geometric modeling and design (Panel). In: Proceedings of the ACM symposium on interactive 3D graphics (I3D&#8217;01), North Carolina, March 2001, pp 125&#8211;128"/>

    <meta name="citation_reference" content="Anderson D et al (2000) Tangible interactions and graphical interpretation: a new approach to 3D modeling. In: Proceedings of ACM SIGGRAPH, New York, July 2000, pp 393&#8211;402"/>

    <meta name="citation_reference" content="Balakrishnan R, Hinckley K (1999) The role of kinesthetic reference frames in two-handed input performance. In: Proceedings of UIST&#8217;99, North Carolina, pp 171&#8211;178"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Rev; citation_author=null Biederman; citation_volume=94; citation_publication_date=1987; citation_pages=115; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGGRAPH Comput Graph; citation_author=null Bier; citation_volume=24; citation_publication_date=1990; citation_pages=193; citation_id=CR5"/>

    <meta name="citation_reference" content="Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D&#8217;95), Monterey, California, April 1995, pp 131&#8211;138"/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_author=null Clowes; citation_volume=2; citation_publication_date=1971; citation_pages=79; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Engineering design; citation_author=null Cross; citation_volume=methods; citation_publication_date=2000; citation_pages=strategies; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Aided Design; citation_author=null Eggli; citation_volume=29; citation_publication_date=1997; citation_pages=101; citation_id=CR9"/>

    <meta name="citation_reference" content="Fjeld M et al (2002) Physical and virtual tools: activity theory applied to the design of groupware. A special issue of CSCW: activity theory and the practice of design 11(1&#8211;2):153&#8211;180"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_author=null Gross; citation_volume=24; citation_publication_date=2000; citation_pages=835; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=J Motor Behav; citation_author=null Guiard; citation_volume=19; citation_publication_date=1987; citation_pages=486; citation_id=CR12"/>

    <meta name="citation_reference" content="Hinckley K, Pausch R, Proffitt D (1997) Attention and visual feedback: the bimanual frame of reference. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D&#8217;97), Providence, Rhode Island, April 1997, pp 121&#8211;126"/>

    <meta name="citation_reference" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM CHI&#8217;97 conference, Los Angeles, April 1997, pp 234&#8211;241"/>

    <meta name="citation_reference" content="Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course notes from the SIGGRAPH 2002 conference, San Antonio, Texas, July 2002"/>

    <meta name="citation_reference" content="citation_journal_title=PRESENCE; citation_author=null Kitamura; citation_volume=7; citation_publication_date=1998; citation_pages=460; citation_doi=10.1162/105474698565857; citation_id=CR16"/>

    <meta name="citation_reference" content="Kitamura Y, Itoh Y, Kishino F (2001) Real-time 3D interaction with ActiveCube. In: Proceedings of the ACM CHI2001 conference, Seattle, Washington, extended abstract, pp 355&#8211;556"/>

    <meta name="citation_reference" content="Lawson B (1990) How designers think, 2nd edn. Butterworth Architecture, London"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Aided Design; citation_author=null Lipson; citation_volume=28; citation_publication_date=1996; citation_pages=651; citation_id=CR19"/>

    <meta name="citation_reference" content="Marr D (1982) Chapter 5: representing shapes for recognition. In: Vision. Freeman, New York"/>

    <meta name="citation_reference" content="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-D tangible interface for landscape analysis. In: Proceedings of the ACM CHI2002 conference, Minneapolis, Minnesota, April 2002"/>

    <meta name="citation_reference" content="citation_journal_title=Design Studies; citation_author=null Purcell; citation_volume=19; citation_publication_date=1998; citation_pages=389; citation_doi=10.1016/S0142-694X(98)00015-5; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Aided Design; citation_author=null Qin; citation_volume=32; citation_publication_date=2000; citation_pages=851; citation_id=CR23"/>

    <meta name="citation_reference" content="Schumann J, Strothotte T, Laser S, Raab A (1996) Assessing the effect of non-photorealistic rendered images in CAD. In: Proceedings of the CHI&#8217;96 conference, Vancouver, Canada, April 1996, pp 35&#8211;41"/>

    <meta name="citation_reference" content="Schweikardt E, Gross M (1998) Digital Clay: deriving digital models from freehand sketches. In: Proceedings of ACADIA 1998, Quebec City, Canada, October 1998, pp 202&#8211;211"/>

    <meta name="citation_reference" content="Smith G, Stuerzlinger W (2001) Integration of constraints into a VR environment. In: Proceedings of VRIC 2001, Laval, France, May 2001, pp 103&#8211;110"/>

    <meta name="citation_reference" content="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. In: Proceedings of the IEEE VR conference, Orlando, Florida, March 2002, pp 251&#8211;258"/>

    <meta name="citation_reference" content="citation_journal_title=Design Studies; citation_author=null Verstijnen; citation_volume=19; citation_publication_date=1998; citation_pages=519; citation_doi=10.1016/S0142-694X(98)00017-9; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=Design Studies; citation_author=null de; citation_volume=23; citation_publication_date=2002; citation_pages=515; citation_doi=10.1016/S0142-694X(02)00006-6; citation_id=CR29"/>

    <meta name="citation_reference" content="Wickins CD, Hollands JG (1999) Chapter 4: spatial displays. In: Engineering psychology and human performance, 3rd edn. Prentice-Hall,"/>

    <meta name="citation_reference" content="Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGGRAPH&#8217;96, New Orleans, Los Angeles, August 1996, pp 163&#8211;170"/>

    <meta name="citation_author" content="Ji-Young Oh"/>

    <meta name="citation_author_email" content="jyoh@cs.yorku.ca"/>

    <meta name="citation_author_institution" content="Department of Computer Science, York University, Toronto, Canada"/>

    <meta name="citation_author" content="Wolfgang Stuerzlinger"/>

    <meta name="citation_author_institution" content="Department of Computer Science, York University, Toronto, Canada"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0128-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2004/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0128-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A system for desktop conceptual 3D design"/>
        <meta property="og:description" content="In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configuration until they are satisfied with the general layout. In this activity, one of the main operations is the rearrangement of single and composite parts of a scene. With current desktop virtual reality (VR) systems, the selection and manipulation of arbitrary objects in 3D is still difficult. In this work, we present new and efficient techniques that allow even novice users to perform meaningful rearrangement tasks with traditional input devices. The results of our work show that the presented techniques can be mastered quickly and enable users to perform complex tasks on composite objects. Moreover, the system is easy to learn, supports creativity, and is fun to use."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A system for desktop conceptual 3D design | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0128-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Conceptual 3D design, Interactive 3D environment, 3D manipulation","kwrd":["Conceptual_3D_design","Interactive_3D_environment","3D_manipulation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0128-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0128-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=128;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0128-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A system for desktop conceptual 3D design
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0128-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0128-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2004-05-27" itemprop="datePublished">27 May 2004</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A system for desktop conceptual 3D design</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ji_Young-Oh" data-author-popup="auth-Ji_Young-Oh" data-corresp-id="c1">Ji-Young Oh<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="York University" /><meta itemprop="address" content="grid.21100.32, 0000000419369430, Department of Computer Science, York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Wolfgang-Stuerzlinger" data-author-popup="auth-Wolfgang-Stuerzlinger">Wolfgang Stuerzlinger</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="York University" /><meta itemprop="address" content="grid.21100.32, 0000000419369430, Department of Computer Science, York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 7</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">198</span>–<span itemprop="pageEnd">211</span>(<span data-test="article-publication-year">2004</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">132 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0128-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In the traditional design process for a 3D environment, people usually depict a rough prototype to verify their ideas, and iteratively modify its configuration until they are satisfied with the general layout. In this activity, one of the main operations is the rearrangement of single and composite parts of a scene. With current desktop virtual reality (VR) systems, the selection and manipulation of arbitrary objects in 3D is still difficult. In this work, we present new and efficient techniques that allow even novice users to perform meaningful rearrangement tasks with traditional input devices. The results of our work show that the presented techniques can be mastered quickly and enable users to perform complex tasks on composite objects. Moreover, the system is easy to learn, supports creativity, and is fun to use.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Introduction</h2><div class="c-article-section__content" id="Sec2-content"><p>We present a system that targets the support of 3D scene construction, particularly during the conceptual design session, an early stage in the design process. In such a session, a designer explores the problem/solution space to produce an optimum solution that satisfies design constraints. This is normally done by externalizing various solutions (e.g., via drawing), evaluating them (e.g., visually), and then modifying the solution according to the evaluation. In this stage, many parameters of the model are still undecided and/or frequently changed. Consequently, modifications are frequently drastic and structural, and the visualization of the model is done in a rapid way without much emphasis on precision. In addition, design researchers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Purcell AT, Gero JS (1998) Drawings and the design process: a review of protocol studies in design and other disciplines and related research in cognitive psychology. Design Studies 19(4):389–430" href="/article/10.1007/s10055-004-0128-y#ref-CR22" id="ref-link-section-d66812e281">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Verstijnen I, van Leeuwen C, Goldschmidt G, Hamel R, Hennessey J (1998) Sketching and creative discovery. Design Studies 19(4):519–546" href="/article/10.1007/s10055-004-0128-y#ref-CR28" id="ref-link-section-d66812e284">28</a>] argue that the main operations in this phase of the process are recognition of the parts of a model by function or structure, and manipulation of these recognized parts.</p><p>Therefore, one of the requirements for computer-aided design (CAD) systems for the conceptual session is the ability to rapidly build a model without much commitment to detail, and to be able to modify even the overall structure efficiently. Another goal is to provide a scene construction system that supports an iterative (and progressive) modeling process. In this sense, desktop environments are a good solution, as they are the normal work platform for designers, and many other applications are available to aid the users’ decision process.</p><p>However, mapping from a 2D mouse input to a 3D position is a difficult problem in desktop VR [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course notes from the SIGGRAPH 2002 conference, San Antonio, Texas, July 2002" href="/article/10.1007/s10055-004-0128-y#ref-CR15" id="ref-link-section-d66812e292">15</a>]. This poses a challenge in that the system needs to provide for 3D motion of the objects, as well as afford object group selection and manipulation. The system presented in this paper, Virtual Lego, was designed with new techniques to solve these problems.</p><p>Our system is based on the idea of Lego. It uses blocks as primitives that behave similar to real Lego, but there is additional functionality. From a user’s standpoint, Lego is very simple and versatile, in that various models can be rapidly built using just simple blocks. From a system developer’s standpoint, it is easy to test ideas for interaction techniques by narrowing the application domain, while still keeping relatively close to the context of real situations.</p><p>Judging from the way designers work, the (photo-)realistic display of an early solution is not desirable as it narrows the solution space too quickly [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Lawson B (1990) How designers think, 2nd edn. Butterworth Architecture, London" href="/article/10.1007/s10055-004-0128-y#ref-CR18" id="ref-link-section-d66812e301">18</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Schumann J, Strothotte T, Laser S, Raab A (1996) Assessing the effect of non-photorealistic rendered images in CAD. In: Proceedings of the CHI’96 conference, Vancouver, Canada, April 1996, pp 35–41" href="/article/10.1007/s10055-004-0128-y#ref-CR24" id="ref-link-section-d66812e304">24</a>]. Consequently, our work focuses on the user interface and not on the realistic display of geometric shapes.</p><p>In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec3">2</a> of this work, the motivation and our technical contributions are presented. Furthermore, we discuss some characteristics of the conceptual design session as well as the limitations of current CAD systems. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec4">3</a>, we review related work. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec10">4</a> introduces our new conceptual design system. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec12">5</a>, <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec16">6</a>, and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec19">7</a> discuss the new techniques in detail, and present a comparison with other existing solutions. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec20">8</a>, the results of our user studies are presented. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec32">9</a> discusses the generalization of our new techniques to systems that support arbitrary geometries. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec33">10</a> summarizes our works and provides an outlook to future work.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Motivation and contribution</h2><div class="c-article-section__content" id="Sec3-content"><p>In general, the goal of any design activity is to produce a description of an artifact that satisfies a set of design problems. The main concern of designers is that design problems are typically<i> ill-defined</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Cross N (2000) Engineering design methods: strategies for product design, 3rd edn. Wiley, New York" href="/article/10.1007/s10055-004-0128-y#ref-CR8" id="ref-link-section-d66812e349">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Lawson B (1990) How designers think, 2nd edn. Butterworth Architecture, London" href="/article/10.1007/s10055-004-0128-y#ref-CR18" id="ref-link-section-d66812e352">18</a>], as opposed to well defined problems (e.g., the Towers of Hanoi problem). Ill-defined problems poorly describe the starting state, the constraints that must be fulfilled in the solution, and the form of the final solution. Therefore, they require extensive time to structure the problem itself, and new constraints can arise as preliminary solutions are created. Furthermore, because design constraints are not well defined, solutions require subjective judgment to decide if one is better than the other.</p><p>Therefore, designers generally go iteratively through several distinctive stages, to clarify the problem and, eventually, to come up with a new design. These stages are the analysis of problems, the conceptual design, the embodiment of schemes, and the detailing stage [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Cross N (2000) Engineering design methods: strategies for product design, 3rd edn. Wiley, New York" href="/article/10.1007/s10055-004-0128-y#ref-CR8" id="ref-link-section-d66812e358">8</a>]. In particular, there are frequent iterations between the analysis of problems and the conceptual design session. In these stages, designers try out different solutions and clarify design constraints via the visualization of solutions.</p><p>Sketching is the conventional way to conduct the visual evaluation during the conceptual session. Recent research into the use of sketching in the design session argues that recognition of composite objects is one of the main mental operations involved in this stage [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Purcell AT, Gero JS (1998) Drawings and the design process: a review of protocol studies in design and other disciplines and related research in cognitive psychology. Design Studies 19(4):389–430" href="/article/10.1007/s10055-004-0128-y#ref-CR22" id="ref-link-section-d66812e364">22</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Verstijnen I, van Leeuwen C, Goldschmidt G, Hamel R, Hennessey J (1998) Sketching and creative discovery. Design Studies 19(4):519–546" href="/article/10.1007/s10055-004-0128-y#ref-CR28" id="ref-link-section-d66812e367">28</a>]. However, it is important to note that, while sketching facilitates the recognition of composite objects, it does not afford the manipulation of such composites. Phrased differently, it is extremely important that conceptual design systems provide simple ways to modify and rearrange arbitrarily complex parts of a scene. In addition, the above-cited research showed that current CAD systems do not support these operations very well, and that this is the one of bottlenecks for CAD systems to support conceptual design.</p><p>There are three main technical contributions in our work. Each one of them addresses a particular sub-problem of the task of manipulating arbitrary parts of a model:</p><ul class="u-list-style-dash">
                  <li>
                    <p>Group separation. We present three new techniques that allow the user to separate (almost) arbitrary parts from a composite object.</p>
                  </li>
                  <li>
                    <p>Group movement. We present a new technique for the movement of arbitrary objects. It provides predictable results and visually smooth motion.</p>
                  </li>
                  <li>
                    <p>Group placement. To support the re-attachment of a complex composite part to the existing scene, we present an adaptation of previous work to our context.</p>
                  </li>
                </ul>
</div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Related work</h2><div class="c-article-section__content" id="Sec4-content"><p>In this section, we first review previous work on techniques that are targeted at group selection and movement, as well as placement. Then, we review several classes of systems that are targeted at the early design process, namely tangible user interfaces and electronic 3D construction kits, sketching systems, as well as Lego CAD systems.</p><h3 class="c-article__sub-heading" id="Sec5">Group selection</h3><p>The selection of arbitrary groups of primitive objects has been largely neglected in the VR/CAD research community, since few people realized this to be a significant problem. However, as is evident from the research into the design process, group manipulation is very important early on in the 3D design process.</p><p>Group selection is fairly easy in 2D drawing applications and is usually implemented by a rectangle selection process or via shift-click selection. However, in 3D, these solutions are not applicable due to the perspective distortion, as well as the occlusion between objects. A few systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’95), Monterey, California, April 1995, pp 131–138" href="/article/10.1007/s10055-004-0128-y#ref-CR6" id="ref-link-section-d66812e407">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGGRAPH’96, New Orleans, Los Angeles, August 1996, pp 163–170" href="/article/10.1007/s10055-004-0128-y#ref-CR31" id="ref-link-section-d66812e410">31</a>] support hierarchical grouping or lassoing. A hierarchical relationship is usually established when one object is placed on another, like a cup on a tabletop. Lassoing allows for more flexibility and can select groups of arbitrary objects, but the accuracy of a lassoing gesture, a quick circling motion, is usually very limited. Furthermore, the problem of perspective and occlusion still exists.</p><p>Another approach for grouping can be found in DDDoolz [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="de Vries B, Achten HH (2002) DDDoolz: Designing with modular masses. Design Studies 23(6):515–531" href="/article/10.1007/s10055-004-0128-y#ref-CR29" id="ref-link-section-d66812e416">29</a>], an architectural conceptual design tool. In this system, a user fills a space with blocks, and these blocks are grouped together by assigning them the same color. The authors motivate their choice with the fact that different colors are used to visualize different architectural elements. However, in other application domains, this may not be appropriate and is unnecessarily restrictive.</p><p>Stuerzlinger and Smith [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. In: Proceedings of the IEEE VR conference, Orlando, Florida, March 2002, pp 251–258" href="/article/10.1007/s10055-004-0128-y#ref-CR27" id="ref-link-section-d66812e422">27</a>] investigate the use of a set of constraints to rearrange predefined objects efficiently. This system supports a form of bi-directional grouping, named dual constraints, to align objects of the same kind (e.g., cabinets on a wall, or chairs in a classroom). Objects satisfying a dual constraint snap together and can be manipulated as a group. However, dual constraints must be pre-defined for each object and the grouping process still supports only 2D relationships.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Object motion and placement</h2><div class="c-article-section__content" id="Sec6-content"><p>Providing an intuitive mapping from 2D mouse motion to 3D object motion is a common problem in desktop 3D environments. In a perspective view, there are an infinite number of possible positions along each ray defined by the eye and the mouse position in the image plane. The simplest solution is not to attempt to interpret this, but to provide handles/widgets for three-axis manipulation, as many conventional CAD systems do. While this solution allows no room for the system to fail or to present unexpected results, the task of movement becomes tedious and especially difficult when a user wants to place an object in relation to other objects.</p><p>Another way to solve this problem is to provide a snap-to grid in space or to snap to surfaces of objects. For example, Bier [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Bier E (1990) Snap-dragging in three dimensions. ACM SIGGRAPH Comput Graph 24(2):193–204" href="/article/10.1007/s10055-004-0128-y#ref-CR5" id="ref-link-section-d66812e436">5</a>] presented snap-dragging, a technique where the cursor snaps to interesting features in the scene, such as intersection of grids, edges, or vertices of other objects. This technique works well only when the scene is not cluttered with many objects. As the complexity of the scene increases, it will interrupt the user more frequently with unnecessary snapping and the usability may suffer.</p><p>Apart from snapping to any object or grid in a scene, another solution is to snap to certain types of surfaces, usually defined by the real-world behavior of an object. Such systems, e.g., [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’95), Monterey, California, April 1995, pp 131–138" href="/article/10.1007/s10055-004-0128-y#ref-CR6" id="ref-link-section-d66812e442">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Smith G, Stuerzlinger W (2001) Integration of constraints into a VR environment. In: Proceedings of VRIC 2001, Laval, France, May 2001, pp 103–110" href="/article/10.1007/s10055-004-0128-y#ref-CR26" id="ref-link-section-d66812e445">26</a>], use pre-defined constraints for each object in a scene. There, these constraints specify how an object can move on horizontal or vertical surfaces, and to which constraining surfaces it can attach. By doing this, a user can quickly populate a building with furniture, books, etc. The constraints are also used to define a hierarchical scene graph, which affords composite object manipulation. Last, but not least, several systems that use 6 degrees-of-freedom (6-DOF) trackers as input devices also use constraints to identify surfaces for object movement. Kitamura et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kitamura Y, Yee A, Kishino F (1998) A sophisticated manipulation aid in a virtual environment using dynamic constraints among object faces. PRESENCE 7(5):460–477" href="/article/10.1007/s10055-004-0128-y#ref-CR16" id="ref-link-section-d66812e448">16</a>] presented a scheme that uses a collision with an existing surface to constrain the movement of an object in 3D. This allows for natural object movement, except when many other objects are present in a region.</p><h3 class="c-article__sub-heading" id="Sec7">Scene construction in tangible interfaces</h3><p>Ishii and Ullmer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM CHI’97 conference, Los Angeles, April 1997, pp 234–241" href="/article/10.1007/s10055-004-0128-y#ref-CR14" id="ref-link-section-d66812e458">14</a>] were among the first to present the idea of tangible interfaces. The motivation for tangible interfaces is to provide rich affordances and haptic feedback by enabling users to<i> grasp and manipulate</i> a physical representation of a virtual object. Most tangible interfaces support only 2D (or at most 2.5D) by using a large table with the image projected on top [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Fjeld M et al (2002) Physical and virtual tools: activity theory applied to the design of groupware. A special issue of CSCW: activity theory and the practice of design 11(1–2):153–180" href="/article/10.1007/s10055-004-0128-y#ref-CR10" id="ref-link-section-d66812e464">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM CHI’97 conference, Los Angeles, April 1997, pp 234–241" href="/article/10.1007/s10055-004-0128-y#ref-CR14" id="ref-link-section-d66812e467">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-D tangible interface for landscape analysis. In: Proceedings of the ACM CHI2002 conference, Minneapolis, Minnesota, April 2002" href="/article/10.1007/s10055-004-0128-y#ref-CR21" id="ref-link-section-d66812e470">21</a>]. One fundamental limitation of such systems is that it is practically impossible to support full 3D scene construction, as everything is bound to the tabletop. This includes the input devices as well as the focal plane of the projection. Furthermore, these systems lack an undo/redo capability and replay option. Also, it is not clear how tangible objects (e.g., “phicons”) can attach together to support grouping operations in such systems.</p><p>One way to support 3D modeling with tangible user interfaces is to use Lego-like construction kits or clay models; see, for example, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Aish R, Frankel J, Frazer J, Patera A, Marks J (2001) Computational construction kits for geometric modeling and design (Panel). In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’01), North Carolina, March 2001, pp 125–128" href="/article/10.1007/s10055-004-0128-y#ref-CR1" id="ref-link-section-d66812e476">1</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Anderson D et al (2000) Tangible interactions and graphical interpretation: a new approach to 3D modeling. In: Proceedings of ACM SIGGRAPH, New York, July 2000, pp 393–402" href="/article/10.1007/s10055-004-0128-y#ref-CR2" id="ref-link-section-d66812e479">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Kitamura Y, Itoh Y, Kishino F (2001) Real-time 3D interaction with ActiveCube. In: Proceedings of the ACM CHI2001 conference, Seattle, Washington, extended abstract, pp 355–556" href="/article/10.1007/s10055-004-0128-y#ref-CR17" id="ref-link-section-d66812e482">17</a>]. A computer acquires the composition of the physical shapes, and then converts them into graphical representations. Typically, blocks are equipped with computational chips and communication ports so that the connectivity between blocks can be detected and transmitted to the computer. This can also be thought of as a specialized input device to enter arbitrary compositions of such blocks. In these systems, users can build scenes with almost no need of training. However, as the tight coupling between physical and graphical representation is important, the set of possible scenes is limited by the available shapes and the number of physical construction units. To work around this limitation, Anderson et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Anderson D et al (2000) Tangible interactions and graphical interpretation: a new approach to 3D modeling. In: Proceedings of ACM SIGGRAPH, New York, July 2000, pp 393–402" href="/article/10.1007/s10055-004-0128-y#ref-CR2" id="ref-link-section-d66812e485">2</a>] proposed to implement semantic rules based on the composition of the blocks. Their system uses semantic rules to recognize architectural elements, such as a door or a roof, and<i> automatically</i> decorates corresponding parts. This is not a general approach, however, as the semantic rules constrain the type of structures one can build. Furthermore, there is, again, no undo/redo capability and no replay option.</p><h3 class="c-article__sub-heading" id="Sec8">Sketching systems</h3><p>Several research projects use sketching as the user interface for computer-aided modeling. Most of the systems interpret each sketching stroke as a gesture to create a primitive shape [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Eggli L, Hsu C, Bruderlin B, Elber G (1997) Inferring 3D models from freehand sketches and constraints. Comput Aided Design 29(2):101-112" href="/article/10.1007/s10055-004-0128-y#ref-CR9" id="ref-link-section-d66812e500">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Qin SF, Wright DK, Jordanov IN (2000) From on-line sketching to 2D and 3D geometry: a system based on fuzzy knowledge. Comput Aided Design 32(14):851–866" href="/article/10.1007/s10055-004-0128-y#ref-CR23" id="ref-link-section-d66812e503">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGGRAPH’96, New Orleans, Los Angeles, August 1996, pp 163–170" href="/article/10.1007/s10055-004-0128-y#ref-CR31" id="ref-link-section-d66812e506">31</a>]. For example, in Sketch [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGGRAPH’96, New Orleans, Los Angeles, August 1996, pp 163–170" href="/article/10.1007/s10055-004-0128-y#ref-CR31" id="ref-link-section-d66812e509">31</a>], a user enters a gesture that represents a salient feature of a primitive shape to produce a corresponding shape. Such gesture interfaces improve the efficiency of shape creations. However, the systems constrain users to a limited set of strokes to create a scene, thereby, limiting the expressivity of the system. This contravenes a desirable property of the early sketching process where designers repeatedly and unconsciously doodle before they produce a definite shape. In addition, the gestures require the users to memorize and to recall them for each primitive shape, which can interfere with the designers’ thought process during the design sessions.</p><p>In contrast to gesture-based interfaces, the research group led by Gross [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Schweikardt E, Gross M (1998) Digital Clay: deriving digital models from freehand sketches. In: Proceedings of ACADIA 1998, Quebec City, Canada, October 1998, pp 202–211" href="/article/10.1007/s10055-004-0128-y#ref-CR25" id="ref-link-section-d66812e515">25</a>] proposed to reconstruct a 3D model from sketching only when on the basis of an explicit request by the user. However, their focus is on 2D sketching applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Gross MD, Do EY- L (2000) Drawing on the back of envelope: a framework for interacting with application programs by freehand drawing. Comput Graph 24:835–849" href="/article/10.1007/s10055-004-0128-y#ref-CR11" id="ref-link-section-d66812e518">11</a>], and not directly on a 3D system. In particular, they assumed all the creation activities occur in 2D so the 3D scene must be reconstructed from the 2D drawing. However, there is no complete algorithm that can successfully perform this, especially using only one sketch from one (unknown) viewpoint. To do this, reconstruction algorithms use a set of constraints. However, depending on the constraints, there is a limit to the variety of shapes that can be handled by such a system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Clowes MB (1971) On seeing things. Artif Intell 2:79–116" href="/article/10.1007/s10055-004-0128-y#ref-CR7" id="ref-link-section-d66812e521">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Lipson H, Shpitalni M (1996) Optimization based reconstruction of a 3D object from a single freehand line drawing. Comput Aided Design 28(8):651–663" href="/article/10.1007/s10055-004-0128-y#ref-CR19" id="ref-link-section-d66812e524">19</a>].</p><h3 class="c-article__sub-heading" id="Sec9">Lego CAD systems</h3><p>Lego CAD systems (e.g., Ldraw <a href="http://www.ldraw.org">http://www.ldraw.org</a>, Lego CAD <a href="http://www.workshop3d.com/cybertoys/legocad.htm">http://www.workshop3d.com/cybertoys/legocad.htm</a>) are usually developed for people who want to build computer Lego models. For this reason, they stick to the rules of Lego blocks, and the user can only build models that are possible in real Lego. Such systems provide usually virtual replicas of many real Lego parts, so that the visualization of the computer model looks exactly like the real one.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">A new conceptual design tool</h2><div class="c-article-section__content" id="Sec10-content"><p>The new system presented in this work implements a new solution for 3D conceptual design. At first glance, our system resembles a Virtual Lego system (or Lego CAD system). However, our system does not strictly follow the rules for Lego blocks. For example, blocks attach side-by-side, as well as on top of each other, and blocks can be resized without limits. Consequently, the resulting models are sometimes not feasible with real Lego. Given that our goal is the conceptual design session, we explicitly do not want to create a Virtual Lego system. However, in order to create an easy-to-use conceptual design tool, we strive to leverage skills that a user has learned from previous experience with real Lego.</p><p>The fundamental primitives in our system are basic Lego block shapes; rectangular blocks with two different heights and two different wedge shapes. When an object, be it a single block or a composite, is selected, it follows the mouse cursor and slides over the (horizontal and vertical) surfaces of other blocks. Details of this process are described in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec16">6</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec19">7</a>. In many virtual reality systems, objects can interpenetrate. This is something that almost all naïve users find hard to understand. Consequently, our system uses collision detection to prevent objects from interpenetrating each other. The main innovative feature of the system is the operations that affect more than a single primitive block. These are described in detail in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec12">5</a>, <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec16">6</a>, and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec19">7</a>.</p><p>If the object is moved over the background, it remains floating in the air. This is frequently used as a temporary storage facility during complex operations. In this case, the floating object does not have a surface of movement. We choose to let the object slide on the axis-aligned plane that is most orthogonal to the viewing direction.</p><h3 class="c-article__sub-heading" id="Sec11">The user interface</h3><p>The user interface of the Virtual Lego system consists of the main 3D scene view and a menu on the right-hand side. The menu offers a color and object selection palette, an undo button, and a recycle bin (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig1">1</a>). A user can place a shape into the 3D view by selecting one object from the palette and then clicking on any surface in the scene, or by dragging the shape from the palette to the view.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb1.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb1.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p> The Virtual Lego interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Also, the user can select a group of objects in the scene using techniques that are described in the Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-004-0128-y#Sec12">5</a>. After selecting a group, he/she can perform several manipulation actions on that selected group, such as resizing, recolor, rotation, and cloning.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0128-y#Tab1">1</a> shows an overview of the commands in the system. Different actions are assigned to the different mouse buttons’ click and mouse drag actions, which makes (time-consuming) menu selection unnecessary. A threshold on the movement distance is used to distinguish between click and drag operations.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1  Mouse and keyboard commands</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0128-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Discrete actions, such as adding a new shape, coloring, and 90° rotation, are assigned to button clicks, whereas continuous actions, such as movement and resizing, are assigned to mouse drag actions. The new term “anchor” in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0128-y#Tab1">1</a> will be explained in the later section. For now, it is important to mention that the action is discrete and performed by a middle button click. In contrast, a middle button<i> drag</i> results in camera rotation. Also, to make this frequently used operation even more easily accessible, any mouse button drag started on the background also results in camera rotation.</p><p>In addition, cloning is considered as a continuous action, rather than a discrete one. If an object is cloned at a mouse click position, then the cloned objects will overlap with the source objects. This violates the assumption that all objects are solid. Therefore, the system waits with the instantiation of the objects until the mouse is dragged far enough away from the original position so that the cloned objects do not overlap with the source object. Also, different auditory feedback is provided when an operation succeeds or fails, such as snapping to a surface or deletion to the recycle bin.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Group separation</h2><div class="c-article-section__content" id="Sec12-content"><p>To afford structural modification of 3D models, efficient group manipulation is important. As an analogy, to draw a large 2D diagram, users usually group shapes to designate them as a component, and then rearrange these components by copying, pasting, resizing, etc. To select a group of objects, users utilize either a rectangular drag gesture, or accumulate objects via shift-click operations. In 2D, this is a simple task, as objects are aligned in the plane. However, in 3D, selecting a rectangular region on the screen is not an appropriate mechanism for object selection due to the perspective distortion and the occlusion between objects.</p><p>In this section, we present two new techniques for grouping of objects:<i> intelligent separation</i> and<i> separation with anchoring</i>. Separation with anchoring optionally allows for two-handed operation and this will be described as well.</p><h3 class="c-article__sub-heading" id="Sec13">Intelligent separation and grouping</h3><p>In intelligent separation, a user clicks with the left mouse button on a block and drags it<i> away</i> to separate a part. The initial direction of the mouse drag is used to determine which part of the object is being manipulated. Whenever the direction of the movement “pushes” another block away, that block is also considered to be a part of the new group. This relatively simple mechanism affords the formation and separation of arbitrary parts.</p><p>To make the separation visually predictable, the group of objects that is going to be separated is highlighted according to the current mouse direction. For simplicity, we show first a 2D example and then a 3D example. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig2">2</a>a, the user starts the selection by clicking on a block and keeping the mouse button pressed. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig2">2</a>b–e, the mouse is moved in a small circle to visualize all possible combinations of connected components. Finally, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig2">2</a>f, the user separates the component chosen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig2">2</a>e by moving the mouse further than a certain threshold. Now, the user can release the mouse button to place the selected component at a new location. However, to select a part that is interlocked with other parts, a user has to repeatedly decompose the model until he/she obtains the desired part.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2a–f</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb2.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb2.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p> Intelligent separation technique. (<b>a</b>) Start of separation by clicking on a block. (<b>b</b>)–(<b>e</b>) Different drag directions form different groups. (<b>f</b>) Dragging further separates a group of blocks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Note that, if the user does not move further than a certain threshold, he/she has the option to change the selection. The visual highlighting helps to prevent the user from making mistakes. In addition, if the user stops dragging before separating the highlighted group (i.e., before Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig2">2</a>f), the group remains selected. This group can then be manipulated with other actions, for example, 90° rotation, resizing, or recoloring.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig3">3</a> visualizes the behavior of this technique in a 3D example, where different parts of a plane model are selected. Starting from position A, different connected parts of the plane model are selected according to the drag direction. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig3">3</a>c, the whole plane is selected, as all the blocks are on the left side of the right wing (the starting position A).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3a–c</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb3.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb3.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p> 3D example for intelligent separation technique. Starting from position A, different drag directions select different parts. See text for detail</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec14">Separation with anchoring</h3><p>In initial evaluations of the intelligent separation technique, we found that many first-time users did not find this technique easy to learn as they frequently activated it unintentionally. Consequently, we implemented a new separation technique that makes the process more explicit. Here, the user must first specify the part that has to remain in place. This step is called<i> anchoring</i> (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig4">4</a>a). Then, in a second step, the user selects and drags the part that is to be broken off. If no anchor is placed, all objects connected to the selected one simply move together, regardless of the dragging direction. In other words, if no anchor is placed, the whole object moves. While this technique requires an additional manipulation step compared to the intelligent separation technique, it makes the separation process more explicit for the user, but still maintains the benefits of a directionally dependent method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4a–d</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb4.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb4.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p> Anchoring with one mouse. (<b>a</b>) Dragging into any direction without placing an anchor selects the whole (connected) object. (<b>b</b>) The user places an anchor visualized by the white rectangle, (<b>c</b>) clicks and drags rightwards, and, (<b>d</b>) once a certain threshold is reached, the group is separated</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In our system, anchoring is activated via the middle mouse button, and the separation is activated with the left button (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig4">4</a>). As this two-step approach can be easily mapped to a two-handed user interface, we also implemented a second version where the user holds two mice. An anchor is placed with the left button on either device and separation is done with the other mouse (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5a–c</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p> Anchoring with two mice. The second mouse cursor is visualized with a black circle. See text for description</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>For this technique, we first test if there is a plane between the two mouse positions that allows for a simple cut through the model. If such a plane exists, it is used to separate the new part. Otherwise, the drag direction from the second click is used as the input to the algorithm presented with the intelligent separation technique.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig6">6</a> shows a 3D example of this technique. For simplicity, only the one mouse option is visualized, the two-mice option works analogously. Again, as with intelligent grouping, group manipulation is possible after highlighting a group (e.g., at the step depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig6">6</a>b), and then starting other actions on that group, such as recoloring or resizing.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6a–d</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb6.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb6.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p> 3D example for anchoring with one mouse. To select the wing, a user places an anchor (<b>a</b>) and then selects the upper part (<b>b</b>). The relative position between the anchor and the selected object position is used for group selection. The same rule applies to steps (<b>c</b>) and (<b>d</b>). See text for details</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec15">2D to 3D mapping of selection gesture</h3><p>In the techniques described above, we need to identify the drag direction to map the 2D input to a 3D position. For this to happen, we assume that mouse movements take place on the axis-aligned plane that is most orthogonal to the view direction. Consequently, different movement directions can be obtained by changing the view direction.</p><p>However, it is important to point out that this view-dependency affects mostly the results of the intelligent separation technique. In the separation with anchoring technique, the objects to be separated are primarily determined by the spatial relationship between the anchored object and the dragged object. The drag direction is only used when the relationship is ambiguous.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Group movement</h2><div class="c-article-section__content" id="Sec16-content"><p>Moving objects in 3D with a 2D input device is a typical problem in desktop CAD/VR systems. Clearly, the goal for this is to provide movement of objects that conforms best to the intuition of the user. The SIGGRAPH 2002 course notes on this topic [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course notes from the SIGGRAPH 2002 conference, San Antonio, Texas, July 2002" href="/article/10.1007/s10055-004-0128-y#ref-CR15" id="ref-link-section-d66812e1042">15</a>] summarize typical solutions for this problem as follows:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Let a user select a movement axis by providing handles for all three axes. In this method, the movement task becomes tedious and sometimes non-intuitive, especially when a user needs to place an object relative to others. For example, IronCAD (<a href="http://www.ironcad.com">http://www.ironcad.com</a>)provides a plane handle and an axis handle for movement. A user has to repeatedly click different handles to select the desired manipulation and then move the object until he/she can reach the target position.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Move an object on a plane parallel to the viewing plane. Although this is simple to implement, this technique does not work for users. The resulting movement is not intuitive and frequently misleading.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Use obvious structures in the scene to determine the plane of motion. When there is a drawing plane, then move the object parallel to it. When a user positions something on a surface, then snap to that surface. This is generally achieved by defining the behavior of an object, for example, whether it moves on a horizontal or vertical surface, e.g., [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’95), Monterey, California, April 1995, pp 131–138" href="/article/10.1007/s10055-004-0128-y#ref-CR6" id="ref-link-section-d66812e1083">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. In: Proceedings of the IEEE VR conference, Orlando, Florida, March 2002, pp 251–258" href="/article/10.1007/s10055-004-0128-y#ref-CR27" id="ref-link-section-d66812e1086">27</a>]. The definition is generally hard-wired into the internal description of the object, and, thus, changing its behavior is difficult for the user.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Finally, the system can try to guess the movement direction based on the initial cursor movement. Usually, heuristics are employed for this. Although the heuristics may work correctly most of the time, one wrong guess can frustrate the user and deteriorate the overall usability of the system significantly. Furthermore, it is important that appropriate thresholds are employed, otherwise, involuntary activation is a real problem.</p>
                    
                  </li>
                </ol>
<p>In addition to the issues mentioned above, all of these techniques strongly rely on the exact mouse cursor position. For example, in Cosmo3D (discussed in the SIGGRAPH 2002 course notes on 3D manipulation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course notes from the SIGGRAPH 2002 conference, San Antonio, Texas, July 2002" href="/article/10.1007/s10055-004-0128-y#ref-CR15" id="ref-link-section-d66812e1106">15</a>]), the<i> follow-the-cursor</i> technique is used to find and to snap to the closest geometric feature (e.g., vertex, edge, or surface) behind the cursor. This approach works reasonably well when small single objects are manipulated. However, when moving bigger composite objects, this method becomes problematic. In our initial evaluations, we observed that users seem to concentrate more on the position of the moving object in relation to the scene, rather than the position of the cursor. When only the cursor position is taken into account, the same kind of mouse motions may then also result in different movements of the object depending on where the cursor position is on the visible surface of the moving object. This is clearly undesirable. Also, due to the lack of visual depth information, the movement task, thus, becomes strenuous since users cannot predict how the mouse movement will be mapped to object movement.</p><h3 class="c-article__sub-heading" id="Sec17">Movement on a surface</h3><p>In our technique, we try to provide visually smooth and predictable object motion without handles or predefined object behavior. Instead, we use occlusion, which is known to be the most powerful depth cue [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Wickins CD, Hollands JG (1999) Chapter 4: spatial displays. In: Engineering psychology and human performance, 3rd edn. Prentice-Hall," href="/article/10.1007/s10055-004-0128-y#ref-CR30" id="ref-link-section-d66812e1119">30</a>].</p><p>In preliminary experiments during our development process, we noticed that users consider the entire area of the visual overlap of a foreground object and a complex background. The users seem to expect that the object moves on the foremost (hidden) surface behind the moving object. Consequently, we select the movement plane according to the object surface that is closest to the viewpoint in the region that is<i> occluded</i> by the moving object. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig7">7</a> demonstrates the concept. When the mouse cursor is in position (1), surface A is the surface closest to the viewer among all hidden surfaces. Therefore, the object slides on surface A. When moving to position (2), the closest surface is B and, consequently, the object slides on top of the block. For efficiency, we perform the computation of the foremost occluded surface with the aid of graphics hardware.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb7.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb7.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p> Objects slide on the surface that is both closest to the viewer and occluded by the object</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec18">Movement in free space</h3><p>In general, objects slide on the foremost surface, except when there is no surface behind the moving object. In this case, the object moves in free space on an axis-aligned plane. The choice of this plane is dictated by the viewing direction. That is, the system chooses the axis-aligned plane that is most orthogonal to the view-direction, as this provides the most predictable mapping between 2D input and object motion.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Group placement</h2><div class="c-article-section__content" id="Sec19-content"><p>Once an object is released, it is snapped to the nearest surface. If an object is released above the background, it continues to float in free space, which provides a convenient temporary storage space.</p><p>Placing an arbitrary composite object onto an arbitrary surface requires that many possibilities are considered, as any face may potentially snap to any face. To solve this, we adapted an algorithm first described by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kitamura Y, Yee A, Kishino F (1998) A sophisticated manipulation aid in a virtual environment using dynamic constraints among object faces. PRESENCE 7(5):460–477" href="/article/10.1007/s10055-004-0128-y#ref-CR16" id="ref-link-section-d66812e1167">16</a>]. In this work, several criteria, such as angles between faces, movement direction, overlap ratio, and face distances, are used to reduce the number of candidate faces that can match each other. Scores are calculated for each pair of candidates, and the face pair with the highest score is selected as the constraining pair.</p><p>In our implementation, we use only the overlap ratio and the face distance. The reason for this simplifying choice is that we deal, basically, with only axis-aligned, rectangular blocks and, consequently, all other criteria are not appropriate. The overlap ratio computes the relative overlap of two surfaces that face each other. For simplicity, we use the distance along the normal of the current movement plane. We then construct a candidate set of all blocks whose overlap ratio is greater than some threshold. The set is sorted by descending order of overlap ratio and ascending order of distance as the secondary criterion. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig8">8</a> illustrates this concept. According to the measure of face distance, block 4 is closest to block 5, 6, 7, and 8. Among these pairs, the faces of block 8 overlap most with the faces of block 4, and, thus, block 4 and 8 snap together. The first entry of this set is chosen as it minimizes object movement and, thus, conforms best to user expectations. The snap is accomplished by translating the moving object so that the pair of best matching faces coincides.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb8.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb8.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p> Object placement is determined by finding the pair with minimum distance among the ones with maximum overlap ratio</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">User studies</h2><div class="c-article-section__content" id="Sec20-content"><p>To verify the usability of the system, we conducted a pilot test and a formal test. The pilot test was mainly used to verify the movement algorithm by asking the users to perform a simple task (moving one block at a time). In this test, we also compared the task completion time with real Lego.</p><p>In the formal test, we evaluated mainly the different group manipulation techniques, as these are the most innovative parts of the system. We used three different tasks for the user test, which require different 3D manipulations and, consequently, exercise different parts of the functionality of the new techniques.</p><h3 class="c-article__sub-heading" id="Sec21">Pilot test</h3><p>We performed a pilot test with a preliminary version of the system. Nine subjects (three females and three males, age range 21–30) from the pool of graduate students in our computer science department volunteered. We asked subjects to build a simple duck model composed of ten Lego blocks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig9">9</a>). In this pilot test, we compared the performance of a 6-DOF Polhemus tracker and a 2D mouse as input device for our system, as well as user performance with real Lego. To provide a fair comparison between our system and real Lego, we disabled the option to add a block in place via a simple mouse click in the Virtual Lego system. Users had to<i> drag</i> the shape from the menu and place it in the target position in the working area.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9a, b</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p> Screen capture of the system for the pilot test. (<b>a</b>) Screen capture of the target duck model. (<b>b</b>) Menu window for the pilot test</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In the 6-DOF tracker condition, collision detection and snapping to the closest surface were used to make the comparison fair. We also disregarded tracker rotations, which made the tracking system effectively a 3-DOF input device. We emphasize that both of these choices were made to increase user performance with the 6-DOF tracker.</p><p>The results of the pilot test show that, on average, the construction time with the 2D mouse was approximately 1.5 times slower than the performance with real Lego. However, the performance of the Polhemus tracker was 3.6 times slower compared to real Lego. Apparently, this solution did not work well. We hypothesize that the reasons for this are hand fatigue due to the lack of a resting surface, insufficient depth cues (non-stereo display), and, most importantly, no haptic feedback. As we currently do not have access to a 6-DOF haptics system, we did not follow this line of research.</p><p>Overall, we were content with the results for the Virtual Lego system, given that it is hard to provide the same visual perception and the same tactile feedback as real Lego does, especially as the Virtual Lego system uses a desktop environment with a 2D mouse. Therefore, we selected more complex tasks to investigate the grouping features of the system. The experiment and the results are presented in the next section.</p><h3 class="c-article__sub-heading" id="Sec22">Formal test</h3><p>Twelve paid participants (six females, six males, age range 18–39, average 25.08 years) out of a pool of graduate and undergraduate university students were recruited. All of the subjects were right-handed. In addition, all of them had experience with either 2D authoring tools or 3D games, or both. None of the participants had previous experience with our system. We consider this population to be a reasonable choice, given that many designers use computers on an everyday basis.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Test procedure</h4><p>The test consisted of a practice session, two evaluation sessions, and a subjective evaluation session. First, subjects learned the general operations of the system and practiced simple 3D object movement with instruction provided by the experimenter. Each of the three interaction techniques was practiced until participants felt comfortable with them. The average time period for the whole practice session was 20 minutes.</p><p>In the first evaluation session, participants were asked to perform two different experimental tasks that evaluate the grouping techniques. The first task was to merge two 4-layer towers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig10">10</a>), and the second, a rearrangement of a 3D floor plan (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig11">11</a>). Each participant was asked to perform each task twice under all three conditions; intelligent separation, separation with anchoring with one mouse, and with two mice. In the two-mice condition, the left-hand mouse has the same functionality as the right one. A user can select an anchoring position with either hand and move other objects with the other hand. Users were informed about this feature. To combat learning effects, the order of conditions was counterbalanced across subjects. As the two tasks are non-trivial and require some mental reflection, we considered the first set of three trials as practice, and analyzed only the last three trials for each task.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10a, b</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p> Merging two 4-layer towers. (<b>a</b>) Initial scene with two towers. (<b>b</b>) Target scene</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11a, b</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p> Rearrangement of 3D floor plan. (<b>a</b>) Initial 3D floor plan. (<b>b</b>) Target 3D floor plan</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In the second evaluation session, participants constructed the Lego duck (as in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig9">9</a>) with real Lego blocks and the Virtual Lego system. This was done to check the consistency of the results with the previous test.</p><p>For all tasks and participants, time-stamped mouse movements and all actions that the user performed were logged into a file. After finishing all the evaluation sessions, participants answered a questionnaire on the usability of the presented interaction techniques.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Tasks</h4><p>In the first task, subjects had to merge two 4-layer towers into one (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig10">10</a>). The individual parts of the tower are stacked on top of each other in a repeated pattern. We hypothesized that a first-time user could easily do this task since there was only a simple relationship between objects (one on top of the other) and the task was repetitive. The minimum number of movements to complete the task was fifteen.</p><p>In the second task, participants had to change the arrangement of a floor plan by moving walls and wall assemblies around (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig11">11</a>). The task is close to how a conceptual 3D design system would be used in the real world. This task is relatively difficult because a user must reflect on the connectivity of walls in order to select the correct wall fragments. The minimum number of movements was ten.</p><p>Only actions that are related to rearranging the scene are allowed, such as object separation, movement, and placement, as well as navigation. Other actions, such as adding new blocks, rotation, or resizing were disabled during the tests.</p><p>In the following sections, we refer to the intelligent separation technique as<i> intelligent</i>, to the separation with anchoring technique with one mouse as<i> anchor</i>, and the two-mice variant as<i> 2-mice</i>, for brevity. The task of merging the 4-layer towers is called<i> 4-layer</i>, and the rearrangement of the 3D floor plan is referred to as<i> floor plan</i>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Task completion times</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig12">12</a> shows the completion times for the two tasks using the various interaction techniques. In the 4-layer task, 2-mice is significantly slower than intelligent and anchor (F<sub>11,2</sub>=7.3,<i> p</i>&lt;0.01). In the floor plan task, there is no significant difference between the techniques (F<sub>11,2</sub>=1.86,<i> p</i>&gt;0.18). The high variation in completion time of intelligent and 2-mice seems to be the main cause of this. The reasons for this are investigated later in this section. Nevertheless, it should be noted that, with a few exceptions, most participants could complete the tasks in a reasonable amount of time. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12a, b</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yfhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p> Completion time for (<b>a</b>) 4-layer task. (<b>b</b>) Floor plan task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Decomposition of actions</h4><p>To gain an insight into how participants spent their time on each task, we decomposed time into the different action categories. The actions are move, anchoring, navigate, undo, and<i> visualize group</i>. Visualize group signifies that the user highlights a group via directional dragging, but cancels the operations before the actual separation occurs.</p><p>One noticeable result for the 2-mice condition is that users practically assigned exclusive roles to each hand. Usually, the left hand is used only for anchoring while the right performs all other actions (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0128-y#Tab2">2</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2  Average number of actions by each hand in 2-mice condition</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0128-y/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>To simplify further analysis, we merged the data for left-hand and right-hand actions. Most of actions performed are move, anchoring, and navigation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig13">13</a>). If we compare intelligent and anchor, it is clear that users made significantly more errors with intelligent and also utilized the visualize group action significantly more often. In the floor plan task, there is a significant correlation between the completion time and the number of navigation actions in both the intelligent separation (a correlation coefficient of 0.96) and 2-mice condition (0.73), but not in the anchor condition (0.2). It seems that the users navigated more often in these conditions to check the result of their object movement. We hypothesize that this is because they didn’t always understand the result immediately and needed to verify it. While this may be interpreted that they did not master the interaction techniques completely, it also points out that intelligent separation is harder to understand than the anchor technique.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb13.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb13.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p> (<b>a</b>) Average number of actions in 4-layer task. (<b>b</b>) Average number of actions in floor plan task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Number of move operations</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig14">14</a> shows the number of operations required to complete the two tasks. In the 4-layer task, the number of move operations is not different (F<sub>11,2</sub>=1.75,<i> p</i>&gt;0.1) between the conditions. However, in the floor plan task, intelligent required significantly more operations than anchor (F<sub>11,2</sub>=3.56,<i> p</i>&lt;0.05).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14a, b</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb14.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0128-y/MediaObjects/s10055-004-0128-yflb14.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p> Number of move operations in (<b>a</b>) 4-layer task and (<b>b</b>) floor plan task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0128-y/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>In many instances, participants took longer with intelligent than with anchor. Usually, this is due to an erroneous activation of the intelligent technique, which then resulted in a sequence of corrective actions. This did not occur with anchor. However, in the floor plan task under the intelligent condition, subjects #2 and #11 were able to finish the task with an (almost) optimal number of move operations, which shows that some participants were able to utilize this technique fully, as their task times were also minimal (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig12">12</a>b).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Select time before a movement</h4><p>To investigate how long it took users to select a group to separate, we decomposed the sequence of actions for a move operation (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0128-y#Tab3">3</a>). In particular, we focus on the time before, and up to, the first mouse click. We call this period<i> select time</i> and show it in italics in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-004-0128-y#Tab3">3</a>. We hypothesize that this select time would allow us to gain an insight into the overall complexity of selecting the right object for an action, which is a mixture of cognitive processing as well as motor action.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3  Sequence of actions to start move operation (select time in italic)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-004-0128-y/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We computed the average select time by identifying the time before a move action from the log files and divided the sum of all the select times with the number of select actions for each subject. For both tasks, the difference in the average select time is strongly significant (4-layer: F<sub>11,2</sub>=28.69,<i> p</i>&lt;&lt;0.01; floor plan: F<sub>11,2</sub>=28.31,<i> p</i>&lt;&lt;0.01). For the 4 layer task, the average select time for intelligent is 0.8 s, for anchor 1.21 s, and for 2-mice 1.55 s. In the floor plan task, the average select time for intelligent is 1.52 s, for anchor 1.86 s, for 2-mice 2.55 s.</p><p>The select time for intelligent is significantly lower than for anchor and 2-mice. This is surprising, given that the motor action for these is practically identical (moving the cursor over an object). It seems that participants took less time to reflect on their choice of drag direction for intelligent. The higher select time for 2-mice is most probably due to the fact that not many people are trained to use a mouse with the left hand.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec29">Results of the duck construction task</h4><p>In the formal user test, the duck construction task with Virtual Lego was measured to be 2.72 times slower compared to real Lego. We hypothesize that this is because all the participants in this test were relatively inexperienced with computers compared to the group in the pilot test (graduate students in computer science department), which resulted in 1.5 times slower performance of Virtual Lego than real Lego. A more detailed analysis of the data revealed that subjects spent most of their time on selecting objects from the menu window, which only contains the seven different shapes required for the duck construction (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0128-y#Fig9">9</a>b); this seems to support our hypothesis. If we subtract the time spent selecting objects in the menu window, the overall slowdown factor reduces to 1.5 times compared to real Lego. This coincides with the result of the previous study.</p><p>Overall, our analysis of the results for the duck construction task shows that the manipulation of Virtual Lego blocks with a 2D input device seems to be approximately 1.5 slower compared to the manipulation of real Lego. There are some fundamental differences between reality and the virtual system that cause this lower performance. For instance, real Lego provides better 3D perception as well as haptic feedback. Also, the relative movement distance was shorter in the real task, as subjects showed a tendency to move their model as close as possible toward the appropriate blocks with real Lego. However, in the virtual system they had to repeatedly move their cursor over the shape menu, which is positioned at a relatively larger distance from the working area.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec30">Qualitative results and observations</h4><p>After finishing all the tasks, participants rated each technique according to a Likert scale (1=worst, 7=excellent). The average preference for intelligent is 5.33, for anchor 5.5, and for 2-mice 4.08. The difference is not significant (F<sub>11,2</sub>=2.92,<i> p</i>&gt;0.05).</p><p>Many subjects commented that anchor is more explicit and predictable for the first-time user, but intelligent is more natural to use. There was also a noticeable change of opinion after each task. Most of the users showed strong preference for intelligent after finishing the 4-layer task, but after the more challenging floor plan task, many withdrew their preference towards the technique. In the 4-layer task, there is only one object relationship—one on top of another. Consequently, the task did not require much attention on how to separate a particular group. However, in the floor plan task, the connectivity relationship between objects is more complex and participants had to experiment with different drag directions. This is reflected in the larger number of mistakes with this condition. Additionally, for the 2-mice option with anchoring, most of the subjects commented that the mouse in the left hand feels unnatural.</p><p>Some of the participants said explicitly that they liked that objects moved on the (closest) visible surfaces. None of the participants commented on the placement technique, so we can assume that this method works well. However, some participants found it non-intuitive that the viewpoint has to be changed to place an object onto a surface that is not visible from the current viewpoint. Some participants also used the free space outside of the base plane (the “air”) as a temporary storage place when the working plane got too crowded or when they couldn’t immediately find the right place for an object.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec31">Discussion</h4><p>One finding of this user study is that even first-time users can successfully and quickly complete conceptual 3D design tasks, such as rearrangement, with the presented manipulation techniques. In particular, the intelligent separation technique performs well when the task is easy. For more difficult tasks, such as the floor plan scenario, separation with anchoring seems to be a better alternative.</p><p>The select time for intelligent separation is the lowest, regardless of the task. This contradicts our expectations. We expected that the select time for intelligent separation would be longer than that for other techniques, since a user would have to reflect on the drag direction before the action. Participants seemed to use intelligent separation with a<i> trial and error</i> approach. Conversely, in the separation with anchoring condition, participants spent more time reflecting on how to separate a group of objects. Note that, in this condition, users<i> cannot</i> separate connected objects if they do not place an anchor. The two explicit steps needed to perform separation with anchoring (first selecting the object to remain and then the actual movement of the “moving” group of objects) seemed to force the users to plan more ahead.</p><p>Also, intelligent separation is clearly more susceptible to mistakes. In the floor plan task, only two users finished the task without serious errors. Some participants went through lengthy corrections that were triggered by a small mistake in a separation action. This can happen, as intelligent separation will break off parts for every selection action.</p><p>Overall, the results presented above suggest that intelligent separation requires a longer learning period than separation with anchoring. Although the quantitative results are preferable for separation with anchoring, we do not see evidence to abandon the intelligent separation technique completely. Since this technique does not require the extra (anchoring) step, we hypothesize that the performance and ease-of-use will improve once users master this technique. As indirect evidence, we infer that one of authors has used the system extensively and clearly prefers intelligent separation because of the reduced effort. In addition, participants clearly showed a preference for intelligent separation during the 4-layer task. To clarify this issue, a long-term user test needs to be performed.</p><p>For the 2-mice condition, all the (right-handed) participants used their other (left) hand exclusively for anchoring. It is important to note that both mice actually had the same functionality, and participants were informed about this during the practice session. However, all users had difficulty with moving the cursor with the left hand, and complained that a mouse in the left hand feels awkward. To a certain degree, this result contradicts most previous studies on bi-manual tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Balakrishnan R, Hinckley K (1999) The role of kinesthetic reference frames in two-handed input performance. In: Proceedings of UIST’99, North Carolina, pp 171–178" href="/article/10.1007/s10055-004-0128-y#ref-CR3" id="ref-link-section-d66812e1859">3</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Motor Behav 19:486–517" href="/article/10.1007/s10055-004-0128-y#ref-CR12" id="ref-link-section-d66812e1862">12</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Hinckley K, Pausch R, Proffitt D (1997) Attention and visual feedback: the bimanual frame of reference. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’97), Providence, Rhode Island, April 1997, pp 121–126" href="/article/10.1007/s10055-004-0128-y#ref-CR13" id="ref-link-section-d66812e1865">13</a>]. Users frequently looked at the left mouse and its cursor on screen to coordinate between their relative positions. We initially speculated that the 2-mice condition should work well, since the technique uses the relative positions of two cursors. However, the 2-mice condition did not provide enough cues for the left hand’s physical position in relation to the right hand.</p><p>Two possible ways to improve the 2-mice technique can be considered. The first option is to use a device where display and input coincide (e.g., tablet display). Users will see the positions of their hands, and will not have to consciously coordinate between the input device and its cursor. The second option is to map the left-hand mouse position relative to the right-hand mouse position. As the left mouse moves farther away from the right mouse, the left-hand cursor moves away from the other. At first, this suggestion seems to contradict common conceptions about bi-manual tasks, namely that the left hand provides a frame of reference for the right hand. However, in our anchor technique, users are always aware of the position of the right hand, and their mental focus seems to be always on the right hand cursor. Therefore, using the right hand as a reference frame seems to be appropriate.</p><p>Last, but not least, several participants asked if they could download this system to perform creative work! This is very encouraging, as the current system is only a prototype that is limited in several aspects.</p></div></div></section><section aria-labelledby="Sec32"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Discussion for generalization</h2><div class="c-article-section__content" id="Sec32-content"><p>In this section, we speculate on generalizing our techniques to more conventional systems based on general polygonal models.</p><p>For the separation techniques, the fundamental question is whether the average user will easily understand how objects can be separated for composite objects that are connected at different (non-axis-aligned) angles. In the current system, the user composes a scene with blocks of the same size, which indirectly implies a bi-directional relationship. Therefore, grouping without any concept of hierarchy works naturally. If the scene is composed of different kinds of primitives with different size, such as blocks, wedges, and cylinders, then the relative position becomes an important property. For example, if a small object is attached to the vertical face of a big object, it “belongs” to it. This can be implemented via an appropriate hierarchy. To generalize the techniques presented here, it is then necessary to add hierarchical information according to relative positions. Even in this case, objects at the same level of the hierarchy should be grouped by a bi-directional relationship.</p><p>When a scene contains concave objects, we have to consider how a user will visually decompose the scene. Psychologists [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Biederman I (1987) Recognition-by-components: a theory of human image understanding. Psychol Rev 94(2):115–147" href="/article/10.1007/s10055-004-0128-y#ref-CR4" id="ref-link-section-d66812e1886">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Marr D (1982) Chapter 5: representing shapes for recognition. In: Vision. Freeman, New York" href="/article/10.1007/s10055-004-0128-y#ref-CR20" id="ref-link-section-d66812e1889">20</a>] argue that people recognize the 3D structure of an object by decomposing the object into convex volume primitives. If this is the case, the system must use some form of decomposition of concave objects. However, this is non-trivial and it is not clear how to decompose general concave objects into parts that coincide with the user’s expectations.</p><p>The group movement technique will generalize to arbitrary scenes, since it only depends on occlusion and depth information. The group placement algorithm is based on Kitamura’s algorithm [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Kitamura Y, Yee A, Kishino F (1998) A sophisticated manipulation aid in a virtual environment using dynamic constraints among object faces. PRESENCE 7(5):460–477" href="/article/10.1007/s10055-004-0128-y#ref-CR16" id="ref-link-section-d66812e1895">16</a>], which was originally designed to find a constraining surface between two arbitrary polygonal shapes. As only blocks are represented in our system, only two criteria, face distance and overlap ratio, are used. To snap arbitrary objects at arbitrary angles, more criteria must be added, such as the angle between two surfaces.</p></div></div></section><section aria-labelledby="Sec33"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33">Conclusion and future work</h2><div class="c-article-section__content" id="Sec33-content"><p>In this paper, we presented a conceptual 3D design system that allows even novice users to rapidly build a structurally complex scene. We discussed the problems of manipulating 3D objects in a desktop VR system with a 2D mouse, mentioned the design rationale behind our system, and presented the user interface. We also described details of the new techniques for selection, manipulation, and placement of arbitrary groups of 3D objects. To verify the effectiveness of the proposed system, a series of user tests were performed. The results showed that even novice users could rapidly perform meaningful tasks in 3D using our proposed techniques.</p><p>In the future, more user tests will be conducted once we improve the solutions for conditions that turned out to be inferior. For example, the 6-DOF tracker was very difficult to use due to hand fatigue, a lack of depth cues, and tactile feedback. We plan to use a 6-DOF haptics device to address this issue. The group separation task with two mice was difficult for most users. As previously discussed, this might be because our interface design did not provide enough awareness of the position of left hand. We plan to investigate different techniques that could improve the two-handed performance. Furthermore, we wish to extend our system to support at least rudimentary sketching capabilities. Finally, we also plan to investigate grouping features with arbitrary geometric shapes.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aish R, Frankel J, Frazer J, Patera A, Marks J (2001) Computational construction kits for geometric modeling a" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Aish R, Frankel J, Frazer J, Patera A, Marks J (2001) Computational construction kits for geometric modeling and design (Panel). In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’01), North Carolina, March 2001, pp 125–128</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Anderson D et al (2000) Tangible interactions and graphical interpretation: a new approach to 3D modeling. In:" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Anderson D et al (2000) Tangible interactions and graphical interpretation: a new approach to 3D modeling. In: Proceedings of ACM SIGGRAPH, New York, July 2000, pp 393–402</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Balakrishnan R, Hinckley K (1999) The role of kinesthetic reference frames in two-handed input performance. In" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Balakrishnan R, Hinckley K (1999) The role of kinesthetic reference frames in two-handed input performance. In: Proceedings of UIST’99, North Carolina, pp 171–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Biederman, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Biederman I (1987) Recognition-by-components: a theory of human image understanding. Psychol Rev 94(2):115–147" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Biederman I (1987) Recognition-by-components: a theory of human image understanding. Psychol Rev 94(2):115–147</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3ABiiB3cvotVw%253D" aria-label="View reference 4 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3575582" aria-label="View reference 4 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Psychol%20Rev&amp;volume=94&amp;publication_year=1987&amp;author=Biederman%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Bier, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Bier E (1990) Snap-dragging in three dimensions. ACM SIGGRAPH Comput Graph 24(2):193–204" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Bier E (1990) Snap-dragging in three dimensions. ACM SIGGRAPH Comput Graph 24(2):193–204</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=ACM%20SIGGRAPH%20Comput%20Graph&amp;volume=24&amp;publication_year=1990&amp;author=Bier%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. I" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Bukowski R, Sequin C (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’95), Monterey, California, April 1995, pp 131–138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Clowes, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="Clowes MB (1971) On seeing things. Artif Intell 2:79–116" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Clowes MB (1971) On seeing things. Artif Intell 2:79–116</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Artif%20Intell&amp;volume=2&amp;publication_year=1971&amp;author=Clowes%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Cross, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Cross N (2000) Engineering design methods: strategies for product design, 3rd edn. Wiley, New York" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Cross N (2000) Engineering design methods: strategies for product design, 3rd edn. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Engineering%20design&amp;volume=methods&amp;publication_year=2000&amp;author=Cross%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Eggli, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Eggli L, Hsu C, Bruderlin B, Elber G (1997) Inferring 3D models from freehand sketches and constraints. Comput" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Eggli L, Hsu C, Bruderlin B, Elber G (1997) Inferring 3D models from freehand sketches and constraints. Comput Aided Design 29(2):101-112</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Comput%20Aided%20Design&amp;volume=29&amp;publication_year=1997&amp;author=Eggli%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fjeld M et al (2002) Physical and virtual tools: activity theory applied to the design of groupware. A special" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Fjeld M et al (2002) Physical and virtual tools: activity theory applied to the design of groupware. A special issue of CSCW: activity theory and the practice of design 11(1–2):153–180</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Gross, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Gross MD, Do EY- L (2000) Drawing on the back of envelope: a framework for interacting with application progra" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Gross MD, Do EY- L (2000) Drawing on the back of envelope: a framework for interacting with application programs by freehand drawing. Comput Graph 24:835–849</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Comput%20Graph&amp;volume=24&amp;publication_year=2000&amp;author=Gross%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Guiard, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model." /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Guiard Y (1987) Asymmetric division of labor in human skilled bimanual action: the kinematic chain as a model. J Motor Behav 19:486–517</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=J%20Motor%20Behav&amp;volume=19&amp;publication_year=1987&amp;author=Guiard%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Pausch R, Proffitt D (1997) Attention and visual feedback: the bimanual frame of reference. In: Pr" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Hinckley K, Pausch R, Proffitt D (1997) Attention and visual feedback: the bimanual frame of reference. In: Proceedings of the ACM symposium on interactive 3D graphics (I3D’97), Providence, Rhode Island, April 1997, pp 121–126</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Procee" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the ACM CHI’97 conference, Los Angeles, April 1997, pp 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course no" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Issacs P, Shrag J, Strauss PS (2002) The design and implementation of direct manipulation in 3D. In: Course notes from the SIGGRAPH 2002 conference, San Antonio, Texas, July 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Kitamura, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Kitamura Y, Yee A, Kishino F (1998) A sophisticated manipulation aid in a virtual environment using dynamic co" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Kitamura Y, Yee A, Kishino F (1998) A sophisticated manipulation aid in a virtual environment using dynamic constraints among object faces. PRESENCE 7(5):460–477</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565857" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=PRESENCE&amp;volume=7&amp;publication_year=1998&amp;author=Kitamura%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kitamura Y, Itoh Y, Kishino F (2001) Real-time 3D interaction with ActiveCube. In: Proceedings of the ACM CHI2" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Kitamura Y, Itoh Y, Kishino F (2001) Real-time 3D interaction with ActiveCube. In: Proceedings of the ACM CHI2001 conference, Seattle, Washington, extended abstract, pp 355–556</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lawson B (1990) How designers think, 2nd edn. Butterworth Architecture, London" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Lawson B (1990) How designers think, 2nd edn. Butterworth Architecture, London</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Lipson, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Lipson H, Shpitalni M (1996) Optimization based reconstruction of a 3D object from a single freehand line draw" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Lipson H, Shpitalni M (1996) Optimization based reconstruction of a 3D object from a single freehand line drawing. Comput Aided Design 28(8):651–663</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Comput%20Aided%20Design&amp;volume=28&amp;publication_year=1996&amp;author=Lipson%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marr D (1982) Chapter 5: representing shapes for recognition. In: Vision. Freeman, New York" /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Marr D (1982) Chapter 5: representing shapes for recognition. In: Vision. Freeman, New York</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-D tangible interface for landscape analysis. In: Proce" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-D tangible interface for landscape analysis. In: Proceedings of the ACM CHI2002 conference, Minneapolis, Minnesota, April 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Purcell, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Purcell AT, Gero JS (1998) Drawings and the design process: a review of protocol studies in design and other d" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Purcell AT, Gero JS (1998) Drawings and the design process: a review of protocol studies in design and other disciplines and related research in cognitive psychology. Design Studies 19(4):389–430</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0142-694X%2898%2900015-5" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Design%20Studies&amp;volume=19&amp;publication_year=1998&amp;author=Purcell%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Qin, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Qin SF, Wright DK, Jordanov IN (2000) From on-line sketching to 2D and 3D geometry: a system based on fuzzy kn" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Qin SF, Wright DK, Jordanov IN (2000) From on-line sketching to 2D and 3D geometry: a system based on fuzzy knowledge. Comput Aided Design 32(14):851–866</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Comput%20Aided%20Design&amp;volume=32&amp;publication_year=2000&amp;author=Qin%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schumann J, Strothotte T, Laser S, Raab A (1996) Assessing the effect of non-photorealistic rendered images in" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Schumann J, Strothotte T, Laser S, Raab A (1996) Assessing the effect of non-photorealistic rendered images in CAD. In: Proceedings of the CHI’96 conference, Vancouver, Canada, April 1996, pp 35–41</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schweikardt E, Gross M (1998) Digital Clay: deriving digital models from freehand sketches. In: Proceedings of" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Schweikardt E, Gross M (1998) Digital Clay: deriving digital models from freehand sketches. In: Proceedings of ACADIA 1998, Quebec City, Canada, October 1998, pp 202–211</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smith G, Stuerzlinger W (2001) Integration of constraints into a VR environment. In: Proceedings of VRIC 2001," /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">Smith G, Stuerzlinger W (2001) Integration of constraints into a VR environment. In: Proceedings of VRIC 2001, Laval, France, May 2001, pp 103–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. In: Proceeding" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. In: Proceedings of the IEEE VR conference, Orlando, Florida, March 2002, pp 251–258</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". Verstijnen, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Verstijnen I, van Leeuwen C, Goldschmidt G, Hamel R, Hennessey J (1998) Sketching and creative discovery. Desi" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Verstijnen I, van Leeuwen C, Goldschmidt G, Hamel R, Hennessey J (1998) Sketching and creative discovery. Design Studies 19(4):519–546</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0142-694X%2898%2900017-9" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Design%20Studies&amp;volume=19&amp;publication_year=1998&amp;author=Verstijnen%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content=". de, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="de Vries B, Achten HH (2002) DDDoolz: Designing with modular masses. Design Studies 23(6):515–531" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">de Vries B, Achten HH (2002) DDDoolz: Designing with modular masses. Design Studies 23(6):515–531</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0142-694X%2802%2900006-6" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Design%20Studies&amp;volume=23&amp;publication_year=2002&amp;author=de%2C">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wickins CD, Hollands JG (1999) Chapter 4: spatial displays. In: Engineering psychology and human performance, " /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Wickins CD, Hollands JG (1999) Chapter 4: spatial displays. In: Engineering psychology and human performance, 3rd edn. Prentice-Hall,</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGG" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Zeleznik RC, Herndon K, Hughes JF (1996) SKETCH: an interface for sketching 3D scenes. In: Proceedings of SIGGRAPH’96, New Orleans, Los Angeles, August 1996, pp 163–170</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0128-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada</p><p class="c-article-author-affiliation__authors-list">Ji-Young Oh &amp; Wolfgang Stuerzlinger</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Ji_Young-Oh"><span class="c-article-authors-search__title u-h3 js-search-name">Ji-Young Oh</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ji-Young+Oh&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ji-Young+Oh" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ji-Young+Oh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Wolfgang-Stuerzlinger"><span class="c-article-authors-search__title u-h3 js-search-name">Wolfgang Stuerzlinger</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Wolfgang+Stuerzlinger&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wolfgang+Stuerzlinger" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wolfgang+Stuerzlinger%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0128-y/email/correspondent/c1/new">Ji-Young Oh</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20system%20for%20desktop%20conceptual%203D%20design&amp;author=Ji-Young%20Oh%20et%20al&amp;contentID=10.1007%2Fs10055-004-0128-y&amp;publication=1359-4338&amp;publicationDate=2004-05-27&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Oh, J., Stuerzlinger, W. A system for desktop conceptual 3D design.
                    <i>Virtual Reality</i> <b>7, </b>198–211 (2004). https://doi.org/10.1007/s10055-004-0128-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0128-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-12">12 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-03-30">30 March 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-05-27">27 May 2004</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2004-06">June 2004</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0128-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0128-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Conceptual 3D design</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Interactive 3D environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D manipulation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0128-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=128;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

