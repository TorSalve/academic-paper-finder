<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Intermodal audio&#8211;haptic intermodal display: improvement of commu"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="
This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Intermodal audio&#8211;haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-09-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="
This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The sensorial approach was investigated in the context of the search and identification of targets in a simplified 2D environment. It consists in finding a spatial configuration corresponding to a defined criterion such as a maximum, minimum, or defined score. During the collaborative search, users need to be aware of their results and also the results of their partners. In addition, they need to compare examined scores (e.g., docking score or physical value) with other results to make decisions. To support these features, an audio&#8211;haptic display was developed employing binaural audio with an intermodal stimuli synthesis design to improve the collaborative search. This rendering tool allows for simultaneous use of the audio and haptic channels which enables an efficient individual search and comparison of results. In addition, it improves the communication and activity coordination between the partners. An experiment was carried out to evaluate the contribution of the tool to improve the collaborative search of targets in a 2D non-visual environment. The results clearly show a significant improvement in performance and working efficiency with the audio&#8211;haptic display as compared to a traditional haptic-only condition. Moreover, we observed a reduction in the need for verbal communication during some steps of the search process. However, this approach introduces some communication conflicts during the steps presenting high-level interactions between partners which reduce the working efficiency of some groups."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-09-12"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="235"/>

    <meta name="prism.endingPage" content="252"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0273-5"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0273-5"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0273-5.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0273-5"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Intermodal audio&#8211;haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2015/11"/>

    <meta name="citation_online_date" content="2015/09/12"/>

    <meta name="citation_firstpage" content="235"/>

    <meta name="citation_lastpage" content="252"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0273-5"/>

    <meta name="DOI" content="10.1007/s10055-015-0273-5"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0273-5"/>

    <meta name="description" content="
This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual"/>

    <meta name="dc.creator" content="Mehdi Ammi"/>

    <meta name="dc.creator" content="Brian F. G. Katz"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=An experimental study on the effects of network delay in cooperative shared haptic virtual environment; citation_author=M Alhalabi; citation_volume=27; citation_issue=2; citation_publication_date=2003; citation_pages=205-213; citation_doi=10.1016/S0097-8493(02)00277-7; citation_id=CR1"/>

    <meta name="citation_reference" content="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. In: IEEE ICRA, pp 454&#8211;460"/>

    <meta name="citation_reference" content="Ammi M, Katz BF (2011) Design of haptic stimuli for audio-haptic concurrent coupling. In: IEEE international symposium haptic audio-visual environments and games (IEEE HAVE), Hebei, pp 74&#8211;80"/>

    <meta name="citation_reference" content="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307&#8211;311"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=Intermodal audio-haptic metaphor: improvement of target search in abstract environments; citation_author=M Ammi, BF Katz; citation_volume=30; citation_issue=11; citation_publication_date=2014; citation_pages=921-933; citation_doi=10.1080/10447318.2014.941277; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Multimed Tools Appl; citation_title=Virtual interpersonal touch: haptic interaction and copresence in collaborative virtual environments; citation_author=JN Bailenson, N Yee; citation_volume=37; citation_publication_date=2008; citation_pages=5-14; citation_doi=10.1007/s11042-007-0171-2; citation_id=CR6"/>

    <meta name="citation_reference" content="Bandura A (1986) Social foundations of thought and action: a social cognitive theory. Prentice-Hall series in social learning theory. Prentice-Hall"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=An experimental study on the role of touch in shared virtual environments; citation_author=C Basdogan, CH Ho, MA Srinivasan, M Slater; citation_volume=7; citation_publication_date=2000; citation_pages=443-460; citation_doi=10.1145/365058.365082; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_title=3-D sound for virtual reality and multimedia; citation_publication_date=1994; citation_id=CR9; citation_author=DR Begault; citation_publisher=Academic Press"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Media spaces: bringing people together in a video, audio, and computing environment; citation_author=SA Bly, SR Harrison, S Irwin; citation_volume=36; citation_publication_date=1993; citation_pages=28-46; citation_doi=10.1145/151233.151235; citation_id=CR10"/>

    <meta name="citation_reference" content="Bruseberg A, Johnson P (2001) Collaboration in the flightdeck: opportunities for interaction design. In: Technical report, Department of Computer Science, University of Bath. 
                    http://www.cs.bath.ac.uk/~anneb/collwn.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="Carroll JM, Neale DC, Isenhour PL, Rosson MB, McCrickard DS (2003) Notification and awareness: synchronizing task-oriented collaborative activity. Int J Hum Comput Stud 8:605&#8211;632"/>

    <meta name="citation_reference" content="Catlin T, Bush P, Yankelovich N (1989) Internote: extending a hypermedia framework to support annotative collaboration. In: Proceedings of ACM conference hypertext, ACM, New York, HYPERTEXT &#8217;89, pp 365&#8211;378. doi:
                    10.1145/74224.74252
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Designing haptic icons to support collaborative turn-taking; citation_author=A Chan, K MacLean, J McGrenere; citation_volume=66; citation_publication_date=2008; citation_pages=333-355; citation_doi=10.1016/j.ijhcs.2007.11.002; citation_id=CR14"/>

    <meta name="citation_reference" content="Chastine JW, Zhu Y (2008) The cost of supporting references in collaborative augmented reality. In: Proceedings of Canadian information processing society, Windsor. Graphics Interface, pp 275&#8211;282"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=An investigation of groupware support for collaborative awareness through distortion-oriented views; citation_author=A Cockburn, P Weir; citation_volume=3; citation_issue=11; citation_publication_date=1999; citation_pages=231-255; citation_doi=10.1207/S15327590IJHC1103_3; citation_id=CR16"/>

    <meta name="citation_reference" content="Codella C, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: Proceedings of SIGCHI conference human factors in computing system, ACM, New York, CHI &#8217;92, pp 329&#8211;334. doi:
                    10.1145/142750.142825
                    
                  
                        "/>

    <meta name="citation_reference" content="Cornuet N (2009) Study of new audio-haptic coupling strategies. Master&#8217;s thesis, University of Paris-Sud 11&#8212;CNRS/LIMSI"/>

    <meta name="citation_reference" content="Cycling &#8217;74 (2010) MAX-MSP. 
                    http://cycling74.com/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Supported Coop Work; citation_title=Challenges for cooperative work on the web: an analytical approach; citation_author=A Dix; citation_volume=6; citation_publication_date=1997; citation_pages=135-156; citation_doi=10.1023/A:1008635907287; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_title=Situation awareness analysis and measurement; citation_publication_date=2000; citation_id=CR21; citation_author=M Endsley; citation_author=D Garland; citation_publisher=LEA"/>

    <meta name="citation_reference" content="citation_journal_title=Environ Model Softw; citation_title=The EM algorithm in a distributed computing environment for modelling environmental space-time data; citation_author=A Fass, M Cameletti; citation_volume=24; citation_issue=9; citation_publication_date=2009; citation_pages=1027-1035; citation_doi=10.1016/j.envsoft.2009.02.009; citation_id=CR22"/>

    <meta name="citation_reference" content="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of ACM symposium user interface software andtechnology, vol 1, pp 27&#8211;36. doi:
                    10.1145/320719.322580
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_title=The senses considered as perceptual systems; citation_publication_date=1966; citation_id=CR24; citation_author=JJ Gibson; citation_publisher=Houghton-Mifflin"/>

    <meta name="citation_reference" content="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of meeting human factors and ergonomics society. Minneapolis/St. Paul, pp 1911&#8211;1915"/>

    <meta name="citation_reference" content="Gutwin C, Greenberg S (1996) Workspace Awareness for Groupware. In: Conference companion human factor computing system common ground, ACM, Vancouver, pp 208&#8211;209"/>

    <meta name="citation_reference" content="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. In: ACM transactions on computer&#8211;human interaction, vol 6, pp 243&#8211;281"/>

    <meta name="citation_reference" content="Hertenstein MJ, Keltner D, App B, Bulleit B, Jaskolka A (2006) Touch communicates distinct emotions. In: Association AP (ed) Emotion, vol 6, pp 528&#8211;533"/>

    <meta name="citation_reference" content="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: Proceedings of international ACM SIGGROUP conference on supporting. Group work, ACM, New York, pp 258&#8211;267. doi:
                    10.1145/958160.958201
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_title=Cognition in the wild; citation_publication_date=1996; citation_id=CR30; citation_author=E Hutchins; citation_publisher=The MIT Press"/>

    <meta name="citation_reference" content="Idrus Z, Abidin SZZ, Hashim R, Omar N (2010) Social awareness: the power of digital elements in collaborative environment. WSEAS Trans Comput 9:644&#8211;653. 
                    http://portal.acm.org/citation.cfm?id=1852437.1852447
                    
                  
                        "/>

    <meta name="citation_reference" content="Jensen N, Olbrich S, Nejdl W (2007) Building a collaborative virtual environment for scientific visualization. In: Technical report Learning Lab Lower Saxony"/>

    <meta name="citation_reference" content="Katz B, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference on auditory display, Paris, pp 1&#8211;7. 
                    http://www.icad.org/node/2355
                    
                  
                        "/>

    <meta name="citation_reference" content="Katz B, Rio E, Picinali L (2010) LIMSI spatialisation engine. International Deposit Digital Number IDDN.FR.001.340014.000.S.P.2010.000.31235"/>

    <meta name="citation_reference" content="Kiyokawa K, Iwasa H, Takemura H, Yokoya N (1998) Collaborative immersive workspace through a shared augmented environment. In: Proceedings of SPIE international symposium on intelligent system and advantage manufacturing, vol 3517, pp 2&#8211;13"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=A decade of increased oil recovery in virtual reality; citation_author=EM Lidal, T Langeland, C Giertsen, J Grimsgaard, R Helland; citation_volume=27; citation_publication_date=2007; citation_pages=94-97; citation_doi=10.1109/MCG.2007.141; citation_id=CR36"/>

    <meta name="citation_reference" content="McGookin DK, Brewster SA (2006) Contextual audio in haptic graph browsing. In: Proceedings of international conference on auditory display, London, pp 91&#8211;94"/>

    <meta name="citation_reference" content="Menelas BAJ, Bourdot P, Picinali L, Katz BF (2014) Non-visual identification, localization, and selection of entities of interest in a 3D environment. J Multimod User Interfaces, pp 1&#8211;14. doi:
                    10.1007/s12193-014-0148-1
                    
                  
                        "/>

    <meta name="citation_reference" content="Negron APP, d A Jimunez A (2009) Using avatar&#8217;s nonverbal communication to monitor collaboration in a task-oriented learning situation in a cve. In: Workshop on intelligent and innovative support for collaborative learning activities, Rhodes, Greece, pp 19&#8211;26"/>

    <meta name="citation_reference" content="Oliveira JC, Shen X, Georganas ND (2000) Collaborative virtual environment for industrial training and e-commerce"/>

    <meta name="citation_reference" content="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc 60(6):409&#8211;418. 
                    http://www.aes.org/e-lib/browse.cfm?elib=16355
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Front Neuro; citation_title=Reaching nearby sources: comparison between real and virtual sound and visual targets; citation_author=G Parseihian, C Jouffrais, BF Katz; citation_volume=8; citation_issue=269; citation_publication_date=2014; citation_pages=1-13; citation_id=CR42"/>

    <meta name="citation_reference" content="Pellerin R, Bouillot N, Pietkiewicz T, Wozniewski M, Settel Z, Gressier-Soudan E, Cooperstock J (2009) SoundPark: exploring ubiquitous computing through a mixed reality multi-player game experiment. In: Conference international Nouvelles Technologiesde la Repartition, p a paraitre"/>

    <meta name="citation_reference" content="Picinali L, Feakes C, Mauro D, Katz BF (2012a) Tone-2 tones discrimination task comparing audio and haptics. In: IEEE international symposium on haptic audio-visual environments and games, Munich, pp 19&#8211;24, doi:
                    10.1109/HAVE.2012.6374432
                    
                  
                        "/>

    <meta name="citation_reference" content="Picinali L, Feakes C, Mauro DA, Katz BF (2012b) Spectral discrimination thresholds comparing audio and haptics for complex stimuli. In: Magnusson C, Szymczak D, BrewsterInt S (eds)  Workshop on haptic and audio interaction design, Lund, Sweden, HAID 2012, pp 131&#8211;140,doi:
                    10.1007/978-3-642-32796-4_14
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Supporting presence in collaborative environments by haptic force feedback; citation_author=EL Salln&#228;s, K Rassmus-Gr&#246;hn, C Sj&#246;str&#246;m; citation_volume=7; citation_publication_date=2000; citation_pages=461-476; citation_doi=10.1145/365058.365086; citation_id=CR46"/>

    <meta name="citation_reference" content="Sanchez JH (2003) Audiobattleship: blind learners collaboration through sound. In: Proceedings of ACM CHI, pp 798&#8211;799"/>

    <meta name="citation_reference" content="Sch&#246;nstein D, Ferr&#233; L, Katz B (2008) Comparison of headphones and equalization for virtual auditory source localization. In: Acoustics&#8217;08. 9e Congres Franais d&#8217;Acoustique of the SFA. Paru dans: JASA, vol 123, n5, Paris, pp 1&#8211;5"/>

    <meta name="citation_reference" content="Simard J, Ammi M (2010) Gesture coordination in collaborative tasks through augmented feedthrough. In: EGVE/EuroVR/VEC, pp 43&#8211;50"/>

    <meta name="citation_reference" content="Simard J, Ammi M, Auvray M (2010) Study of synchronous and colocated collaboration for search tasks. In: Proceedings of joint virtual reality conference"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Interaction with co-located haptic feedback in virtual reality; citation_author=D Swapp, V Pawar, C Loscos; citation_volume=10; citation_issue=1; citation_publication_date=2006; citation_pages=24-30; citation_doi=10.1007/s10055-006-0027-5; citation_id=CR51"/>

    <meta name="citation_reference" content="Victor B (2004) Structuring shared-collaborative interaction using layered and localised auditory feedback. In: Sound in mobile and ubiquitous human computer interaction WS, Glasgow, pp 1&#8211;6"/>

    <meta name="citation_reference" content="Ying HY, Jonas M, Eva-Lotta S, Yngve S (2007) Auditory feedback in haptic collaborative interfaces. Int J Hum Comput Stud QC20100701"/>

    <meta name="citation_author" content="Mehdi Ammi"/>

    <meta name="citation_author_email" content="mehdi.ammi@limsi.fr"/>

    <meta name="citation_author_institution" content="University of  Paris-Sud - CNRS/LIMSI, Orsay, France"/>

    <meta name="citation_author" content="Brian F. G. Katz"/>

    <meta name="citation_author_email" content="brian.katz@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, Orsay, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0273-5&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0273-5"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks"/>
        <meta property="og:description" content="This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The sensorial approach was investigated in the context of the search and identification of targets in a simplified 2D environment. It consists in finding a spatial configuration corresponding to a defined criterion such as a maximum, minimum, or defined score. During the collaborative search, users need to be aware of their results and also the results of their partners. In addition, they need to compare examined scores (e.g., docking score or physical value) with other results to make decisions. To support these features, an audio–haptic display was developed employing binaural audio with an intermodal stimuli synthesis design to improve the collaborative search. This rendering tool allows for simultaneous use of the audio and haptic channels which enables an efficient individual search and comparison of results. In addition, it improves the communication and activity coordination between the partners. An experiment was carried out to evaluate the contribution of the tool to improve the collaborative search of targets in a 2D non-visual environment. The results clearly show a significant improvement in performance and working efficiency with the audio–haptic display as compared to a traditional haptic-only condition. Moreover, we observed a reduction in the need for verbal communication during some steps of the search process. However, this approach introduces some communication conflicts during the steps presenting high-level interactions between partners which reduce the working efficiency of some groups."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0273-5","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Audio–haptic interaction, Collaborative virtual environment, Sonification, Search of targets","kwrd":["Audio–haptic_interaction","Collaborative_virtual_environment","Sonification","Search_of_targets"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0273-5","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0273-5","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=273;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0273-5">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0273-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0273-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-09-12" itemprop="datePublished">12 September 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mehdi-Ammi" data-author-popup="auth-Mehdi-Ammi" data-corresp-id="c1">Mehdi Ammi<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of  Paris-Sud - CNRS/LIMSI" /><meta itemprop="address" content="grid.4444.0, 0000000121129282, University of  Paris-Sud - CNRS/LIMSI, Orsay, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brian_F__G_-Katz" data-author-popup="auth-Brian_F__G_-Katz">Brian F. G. Katz</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="LIMSI-CNRS" /><meta itemprop="address" content="grid.420043.1, 0000000119596666, LIMSI-CNRS, Orsay, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">235</span>–<span itemprop="pageEnd">252</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">362 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0273-5/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>
This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The sensorial approach was investigated in the context of the search and identification of targets in a simplified 2D environment. It consists in finding a spatial configuration corresponding to a defined criterion such as a maximum, minimum, or defined score. During the collaborative search, users need to be aware of their results and also the results of their partners. In addition, they need to compare examined scores (e.g., docking score or physical value) with other results to make decisions. To support these features, an audio–haptic display was developed employing binaural audio with an intermodal stimuli synthesis design to improve the collaborative search. This rendering tool allows for simultaneous use of the audio and haptic channels which enables an efficient individual search and comparison of results. In addition, it improves the communication and activity coordination between the partners. An experiment was carried out to evaluate the contribution of the tool to improve the collaborative search of targets in a 2D non-visual environment. The results clearly show a significant improvement in performance and working efficiency with the audio–haptic display as compared to a traditional haptic-only condition. Moreover, we observed a reduction in the need for verbal communication during some steps of the search process. However, this approach introduces some communication conflicts during the steps presenting high-level interactions between partners which reduce the working efficiency of some groups.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>This study examines the benefits of using a developed spatial audio–haptic display, compared to traditional haptic feedback, for a collaborative task.</p><p>In the domain of virtual reality, collaborative virtual environments (CVEs) provide an efficient way to work on problems requiring simultaneous presence of several human operators in the same task environment. This working configuration enables the combination of complementary skills to process multiexpert problems. Several works have investigated the potential of these environments in different fields. For instance, Jensen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Jensen N, Olbrich S, Nejdl W (2007) Building a collaborative virtual environment for scientific visualization. In: Technical report Learning Lab Lower Saxony" href="/article/10.1007/s10055-015-0273-5#ref-CR32" id="ref-link-section-d35661e371">2007</a>) explored CVE-based solutions for the scientific visualization of complex and abstract data; Lidal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lidal EM, Langeland T, Giertsen C, Grimsgaard J, Helland R (2007) A decade of increased oil recovery in virtual reality. IEEE Comput Graph Appl 27:94–97. doi:&#xA;                    10.1109/MCG.2007.141&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR36" id="ref-link-section-d35661e374">2007</a>) used a collaborative environment to review projects of geological resources prospection; and Oliveira et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Oliveira JC, Shen X, Georganas ND (2000) Collaborative virtual environment for industrial training and e-commerce" href="/article/10.1007/s10055-015-0273-5#ref-CR40" id="ref-link-section-d35661e377">2000</a>) exploited CVEs as a medium for Industrial Training and Electronic Commerce.</p><p>In order to improve the processing of complex problems, Bruseberg and Johnson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bruseberg A, Johnson P (2001) Collaboration in the flightdeck: opportunities for interaction design. In: Technical report, Department of Computer Science, University of Bath. &#xA;                    http://www.cs.bath.ac.uk/~anneb/collwn.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR11" id="ref-link-section-d35661e383">2001</a>), inspired from <i>Distributed Cognition</i> theory (Hutchins <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Hutchins E (1996) Cognition in the wild. The MIT Press, Cambridge" href="/article/10.1007/s10055-015-0273-5#ref-CR30" id="ref-link-section-d35661e389">1996</a>), proposed to share important workloads resulting from the manipulation of complex environments across a group of individuals with the same or complementary skills or knowledge. The <i>Distributed Cognition</i> theory suggests that cognitive systems including more than one individual have different cognitive features from individuals involved in those systems. The results of this research showed an improvement in reliability, robustness, and working efficiency (Bandura <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Bandura A (1986) Social foundations of thought and action: a social cognitive theory. Prentice-Hall series in social learning theory. Prentice-Hall" href="/article/10.1007/s10055-015-0273-5#ref-CR7" id="ref-link-section-d35661e395">1986</a>; Simard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simard J, Ammi M, Auvray M (2010) Study of synchronous and colocated collaboration for search tasks. In: Proceedings of joint virtual reality conference" href="/article/10.1007/s10055-015-0273-5#ref-CR50" id="ref-link-section-d35661e399">2010</a>).</p><p>Despite these apparent of obvious advantages, CVEs introduce new types of problems and constraints. The most critical is probably the limits on communication between partners in some specific situations. Communication plays strategic roles at different levels. For instance, to designate a region of interest, it supports explicit exchanges between partners with the verbal and gestural channels. At a higher level, it supports the conscious presence of other participants and the understanding of their activities (i.e., <i>awareness</i>) (Cockburn and Weir <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion-oriented views. Int J Hum Comput Interact 3(11):231–255" href="/article/10.1007/s10055-015-0273-5#ref-CR16" id="ref-link-section-d35661e408">1999</a>). These communication mechanisms allow the establishment and maintenance of a shared background of understanding called <i>common ground</i> (Dix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dix A (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6:135–156. doi:&#xA;                    10.1023/A:1008635907287&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR20" id="ref-link-section-d35661e414">1997</a>). Moreover, they support the coordination of actions between partners during closely coupled collaborations such as the manipulation of shared objects, or during overlapping collaborative tasks. The use of CVEs and VEs inhibits some of those verbal and non-verbal communication mechanisms which are important for the coordination of activities. This has a significant impact on the effectiveness of collaboration and thus on the relevance of CVEs.</p><p>This paper studies a new sensorial communication strategy based on the development of a spatial binaural audio–haptic intermodal display which aims at improving communication and working efficiency during synchronous collaborative tasks. Using the intermodal display, designed initially to improve <i>Situation Awareness</i> for single-user applications (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307–311" href="/article/10.1007/s10055-015-0273-5#ref-CR4" id="ref-link-section-d35661e424">2012</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments. Int J Hum Comput Interact 30(11):921–933. doi:&#xA;                    10.1080/10447318.2014.941277&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR5" id="ref-link-section-d35661e427">2014</a>), we explore communicative features of this tool for collaborative tasks. The tool combines audio and haptic channels through complementary renderings. First, the audio channel (through a suitable binaural rendering and sonification synthesis function) enables the registration of different features of relevant configurations, such as the position and associated score of a given target, which become available for all users. Second, a complementary haptic feedback provides multiple users with continuous access to all values of the examined function through a spatial exploration procedure as well as a flexible comparison of newly obtained results with the registered value. Beyond the registration of relevant configurations, the set of audio components plays the role of a communication tool by allowing the designation of relevant configurations to partners who can participate in the decision-making process without the need for direct verbal communication.</p><p>The intermodal display was evaluated for the collaborative search and identification of targets in a 2D non-visual environment. This task was inspired by applications presenting complex environments where visual feedback is overloaded, for instance in molecular docking or computational fluid dynamics (CFD) simulations, and requiring the search and identification of relevant targets, such as the docking sites for molecular docking or the centers of vortexes in CFD simulation, according to complex defined criterion. This study focused on a 2D environment in order to bypass some issues related to 3D environments such as the problem of depth perception.</p><p>The paper is structured as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec2">2</a> introduces the various concepts regarding interpersonal communication as related to CVEs, while specifically focusing on the concept of situation awareness. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec5">3</a> presents a review of existing audio–haptic coupling and situation awareness improvement strategies. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec8">4</a> details the developed audio–haptic comparison display introduced here for improving situation awareness in CVEs. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec15">5</a> presents the evaluation experiment using the search of a target value in a 2D space inside a continuous abstract function as a test case. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec25">6</a> presents a global synthesis of this study.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Communication in synchronous collaborative environments</h2><div class="c-article-section__content" id="Sec2-content"><p>Before presenting the design of the spatialized audio–haptic intermodal display, it is necessary to understand the interpersonal communication mechanisms during collaborative tasks and their limits in virtual and collaborative environments.</p><h3 class="c-article__sub-heading" id="Sec3">Communication mechanisms</h3><p>Synchronous and collocated collaboration involves different levels of communication (Dix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dix A (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6:135–156. doi:&#xA;                    10.1023/A:1008635907287&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR20" id="ref-link-section-d35661e466">1997</a>; Hill et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: Proceedings of international ACM SIGGROUP conference on supporting. Group work, ACM, New York, pp 258–267. doi:&#xA;                    10.1145/958160.958201&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR29" id="ref-link-section-d35661e469">2003</a>). First is <i>explicit</i> communication, which supports conscious exchanges between partners. This is mainly based on the verbal channel. Second is the <i>back-channel feedback</i> communication which supports unconscious exchanges such as emotions. This is based on the gestural channel, facial expressions, and vocal activities. Third is the <i>feed-through</i> communication channel, which conveys physical information between participants through the shared object with the haptic channel.</p><p>Based on these standard communication mechanisms, the collaborative work involves an abstract and non-observable dimension: <i>awareness</i>. This awareness is the ability to be conscious of the presence of other participants and to understand their activities. With this consciousness, each participant can adjust and plan their behavior based on what they know of each other. The awareness process exploits standard communication mechanisms through different sensorial channels (visual, haptic, audio). It fulfills three main functions: (1) the collective economy of movement, (2) the need for a non-intrusive communication, and (3) the need to avoid conflicting actions.</p><p>To support these functions, awareness can convey static information such as the partner’s workspace (<b>workspace awareness</b>) (see Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. In: ACM transactions on computer–human interaction, vol 6, pp 243–281" href="/article/10.1007/s10055-015-0273-5#ref-CR27" id="ref-link-section-d35661e493">1999</a>), the structures of the group (<b>group structural awareness</b>), information related to the social sphere (<b>social awareness</b>), and information related to the nature of tasks realized by partners (<b>informal awareness</b>). Beyond these static information, awareness also concerns dynamic knowledge such as the frequency of the partner’s interactions with a shared resource (<b>action awareness</b>) (see Carroll et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Carroll JM, Neale DC, Isenhour PL, Rosson MB, McCrickard DS (2003) Notification and awareness: synchronizing task-oriented collaborative activity. Int J Hum Comput Stud 8:605–632" href="/article/10.1007/s10055-015-0273-5#ref-CR12" id="ref-link-section-d35661e509">2003</a>), progress on the accomplishment of tasks by different partners (<b>activity awareness</b>), and progress of the overall work-flow (<b>process awareness</b>).</p><h3 class="c-article__sub-heading" id="Sec4">Communication limits</h3><p>If collaboration in real environments effectively exploits these communication mechanisms, the use of CVEs and VEs inhibits some conscious and unconscious exchange mechanisms such as the gestural channel and affective communication. We identify two main categories of constraints related to CVEs and VEs (Simard and Ammi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simard J, Ammi M (2010) Gesture coordination in collaborative tasks through augmented feedthrough. In: EGVE/EuroVR/VEC, pp 43–50" href="/article/10.1007/s10055-015-0273-5#ref-CR49" id="ref-link-section-d35661e526">2010</a>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <b>Distance between partners</b>: Natural collaboration, which requires the simultaneous presence of partners in the same environment, is based on collocated face-to-face communication. However, CVEs introduce physical and/or virtual distances between partners. The real distance between partners comes from the non-collocated collaborations. Partners working from remote sites and cannot exploit natural communication mechanisms such as gestures to designate targets. The virtual distance between partners stems from collaborations in large and/or complex virtual environments, such as molecular environments and CFD simulations. These environments introduce a spatial distance between partners and limit the visibility between them (Simard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simard J, Ammi M, Auvray M (2010) Study of synchronous and colocated collaboration for search tasks. In: Proceedings of joint virtual reality conference" href="/article/10.1007/s10055-015-0273-5#ref-CR50" id="ref-link-section-d35661e538">2010</a>).</p>
                    </li>
                    <li>
                      <p>
                                    <b>Distance to virtual environments</b>: This class of constraints is caused by the distance between the user and the virtual environment (Swapp et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Swapp D, Pawar V, Loscos C (2006) Interaction with co-located haptic feedback in virtual reality. Virtual Reality 10(1):24–30" href="/article/10.1007/s10055-015-0273-5#ref-CR51" id="ref-link-section-d35661e550">2006</a>). In fact, natural interaction with real objects involves a geometrical alignment between the gestural interaction and its corresponding visual feedback. However, usual VR technologies introduce a physical distance between the users who typically use haptic arms or tracking systems to interact with the virtual scene, and corresponding representations in the virtual environment, such as the virtual hand or end-effector. This class of constrains limits the interreferential communication and makes elementary tasks such as the designation of targets to a partner very difficult.</p>
                      <p>Beyond these restrictions, other constraints such as latencies between actions and feedbacks, and limitations in the 3D rendering environment (non-existent stereo-vision for depth perception), reduce users’ interaction and subsequently their potential communication through the virtual environment (Alhalabi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Alhalabi M (2003) An experimental study on the effects of network delay in cooperative shared haptic virtual environment. Comput Graph 27(2):205–213. doi:&#xA;                    10.1016/S0097-8493(02)00277-7&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR1" id="ref-link-section-d35661e556">2003</a>).</p>
                    </li>
                  </ul>
                        <p>These different levels of constraints on communication have a direct impact on not only the implicit and explicit communication processes, but also the various levels of awareness. The consciousness of the presence of other partners and the understanding of their activities become very difficult, making elementary tasks like the generation of effective references or interreferential communication, or the designation or the collaborative selection of targets, very difficult.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">State of the art</h2><div class="c-article-section__content" id="Sec5-content"><p>Several solutions aimed at improving the different levels of communication in collaborative environments have been explored. First collaborative platforms enabled several users to edit and manipulate text and graphical documents through the Internet and local networks (Catlin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Catlin T, Bush P, Yankelovich N (1989) Internote: extending a hypermedia framework to support annotative collaboration. In: Proceedings of ACM conference hypertext, ACM, New York, HYPERTEXT ’89, pp 365–378. doi:&#xA;                    10.1145/74224.74252&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR13" id="ref-link-section-d35661e574">1989</a>). Thereafter, these platforms introduced videoconferencing systems to allow direct communication between collaborators (Bly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing environment. Commun ACM 36:28–46. doi:&#xA;                    10.1145/151233.151235&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR10" id="ref-link-section-d35661e577">1993</a>). More recently, virtual reality environments (VREs), through 3D immersive and interactive techniques, have been explored to establish new communication and exchange approaches. Initial platforms integrated elementary simulations with multisensory user interfaces such as hand motion and gestures, speech input and output, sound output, 3D stereoscopic graphics and head-motion parallax (Codella et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Codella C, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: Proceedings of SIGCHI conference human factors in computing system, ACM, New York, CHI ’92, pp 329–334. doi:&#xA;                    10.1145/142750.142825&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR17" id="ref-link-section-d35661e580">1992</a>). Several works relied on the integration of virtual avatars to improve communication between remote partners (Negron and Jimunez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Negron APP, d A Jimunez A (2009) Using avatar’s nonverbal communication to monitor collaboration in a task-oriented learning situation in a cve. In: Workshop on intelligent and innovative support for collaborative learning activities, Rhodes, Greece, pp 19–26" href="/article/10.1007/s10055-015-0273-5#ref-CR39" id="ref-link-section-d35661e583">2009</a>).</p><p>As developed above, face-to-face configuration supports the communication of an enormous amount of information between collaborators. It is not obvious that such information richness can be easily replicated through standard technical communication approaches such as videoconferencing systems or CVEs. Among alternative communication approaches, several researchers investigated non-verbal mediated communication approaches such as haptic and non-verbal audio strategies.</p><p>The following state-of-the-art review highlights the role of audio and haptic strategies to support <i>social awareness</i> and <i>presence</i> in collaborative environments, showing how they support different types of communication. Haptic strategies convey mainly physical information or the states of shared artifacts. However, this communication strategy usually requires physical interaction between partners. In contrast, audio strategies support spatial and temporal information and can be suitable for communication between distant partners without physical contact. Moreover, this communication strategy supports information with a high level of abstraction related to the partner and their activity.</p><h3 class="c-article__sub-heading" id="Sec6">Audio strategies</h3><p>Non-verbal audio communication has received important attention, particularly for applications related to the visually impaired. McGookin and Brewster investigated issues of communication between visually impaired people during collaborative browsing and manipulation of graphs. They specifically considered supporting awareness of the partners’ activities with audio feedback (McGookin and Brewster <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="McGookin DK, Brewster SA (2006) Contextual audio in haptic graph browsing. In: Proceedings of international conference on auditory display, London, pp 91–94" href="/article/10.1007/s10055-015-0273-5#ref-CR37" id="ref-link-section-d35661e605">2006</a>). The audio feedback was used to display the relative position of the partner with a spatialized audio feedback. Ying et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ying HY, Jonas M, Eva-Lotta S, Yngve S (2007) Auditory feedback in haptic collaborative interfaces. Int J Hum Comput Stud QC20100701" href="/article/10.1007/s10055-015-0273-5#ref-CR53" id="ref-link-section-d35661e608">2007</a>) investigated the contribution of audio cues during collaborative visual and haptic tasks where participants were asked to assemble 3D objects. The experiment involved both sighted and blind subjects. The audio cues display some events such as the gripping of objects and contact with the floor. Beyond the improvement in working efficiency, this study highlighted how two partners used the auditory and haptic feedback in order to understand each other’s working space and actions.</p><p>
Idrus (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Idrus Z, Abidin SZZ, Hashim R, Omar N (2010) Social awareness: the power of digital elements in collaborative environment. WSEAS Trans Comput 9:644–653. &#xA;                    http://portal.acm.org/citation.cfm?id=1852437.1852447&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR31" id="ref-link-section-d35661e614">2010</a>) displayed some awareness components, for instance awareness of presence or awareness of turn taking, with non-verbal audio feedback (e.g., sound of an explosion, creaking sounds from a door, or a sound of a car engines). This study showed that these renderings must be combined with other communication channels, for instance text, 2D graphics, or 3D avatars, to provide an effective communication. Victor (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Victor B (2004) Structuring shared-collaborative interaction using layered and localised auditory feedback. In: Sound in mobile and ubiquitous human computer interaction WS, Glasgow, pp 1–6" href="/article/10.1007/s10055-015-0273-5#ref-CR52" id="ref-link-section-d35661e617">2004</a>) used the Auditory Interfaces to improve attention of the working space and the activity of partners. This tool was used to display some events related to the activity of each partner during a collaborative design task in CVEs. The experiment showed an improvement of understanding of the structure and flow of interactions between partners. Moreover, partners used less visual feedback and relied more on Auditory Interfaces to be aware of each other.</p><p>Several studies investigated the use of audio feedback in multiuser games to improve social awareness. Pellerin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Pellerin R, Bouillot N, Pietkiewicz T, Wozniewski M, Settel Z, Gressier-Soudan E, Cooperstock J (2009) SoundPark: exploring ubiquitous computing through a mixed reality multi-player game experiment. In: Conference international Nouvelles Technologiesde la Repartition, p a paraitre" href="/article/10.1007/s10055-015-0273-5#ref-CR43" id="ref-link-section-d35661e623">2009</a>) used 3D audio feedback to provide a strong social feature by gathering people with varying disabilities in the same game environment. A spatialized audio feedback was used to display some spatial positions to facilitate navigation during the game. Sanchez (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sanchez JH (2003) Audiobattleship: blind learners collaboration through sound. In: Proceedings of ACM CHI, pp 798–799" href="/article/10.1007/s10055-015-0273-5#ref-CR47" id="ref-link-section-d35661e626">2003</a>) designed an audio-based battleship game for blind individuals. The interface, based on spatialized sounds which display the positions of detected ships, enhanced group interaction, developed collaborative skills, and helped in the creation of spatial mental images.</p><h3 class="c-article__sub-heading" id="Sec7">Haptic strategies</h3><p>Haptic interpersonal communication has received significant attention. Early works showed the role of kinesthetic feedback in the representation of a partner’s movements during 3D manipulation tasks (Basdogan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Basdogan C, Ho CH, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput Hum Interact 7:443–460. doi:&#xA;                    10.1145/365058.365082&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR8" id="ref-link-section-d35661e638">2000</a>). Moreover, some studies dealing with collaborative manipulations of 3D objects showed that the haptic channel brings social presence into virtual and remote environments (Sallnäs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sallnäs EL, Rassmus-Gröhn K, Sjöström C (2000) Supporting presence in collaborative environments by haptic force feedback. ACM Trans Comput Hum Interact 7:461–476. doi:&#xA;                    10.1145/365058.365086&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR46" id="ref-link-section-d35661e641">2000</a>). Glynn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of meeting human factors and ergonomics society. Minneapolis/St. Paul, pp 1911–1915" href="/article/10.1007/s10055-015-0273-5#ref-CR25" id="ref-link-section-d35661e644">2001</a>) carried out experiments to understand the type of information that can be communicated through the haptic channel. These experiments showed that physical interaction between partners enables the communication of force and position simultaneously without ambiguity, since positional information does not overlap the force information. To augment natural communication, Chan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chan A, MacLean K, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355. doi:&#xA;                    10.1016/j.ijhcs.2007.11.002&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR14" id="ref-link-section-d35661e647">2008</a>) investigated a set of vibrotactile perceptions to support turn-taking communication. This high-level communication approach indicated the several states of control of the shared object to the partners (who controls the object, releases the object, etc.). These perceptions concern (1) requests for control and release of control, and (2) gentle and urgent requests for control. These perceptions concern both users with current control of the object and users who have made requests. It consists of the combination of elementary haptic icons.</p><p>More recently, Simard and Ammi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simard J, Ammi M (2010) Gesture coordination in collaborative tasks through augmented feedthrough. In: EGVE/EuroVR/VEC, pp 43–50" href="/article/10.1007/s10055-015-0273-5#ref-CR49" id="ref-link-section-d35661e653">2010</a>) used a collaborative haptic metaphor to improve the coordination of actions during closely coupled tasks. The metaphor enables the communication of information such as relative positions and relative velocities between two partners in remote sites. This metaphor improved significantly the performance and movement accuracy during the collaborative manipulation of huge molecules. Different works have studied the role of the haptic channel to convey emotions between remote partners or with virtual humans. Hertenstein et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hertenstein MJ, Keltner D, App B, Bulleit B, Jaskolka A (2006) Touch communicates distinct emotions. In: Association AP (ed) Emotion, vol 6, pp 528–533" href="/article/10.1007/s10055-015-0273-5#ref-CR28" id="ref-link-section-d35661e656">2006</a>) were among the first who highlighted this feature. This work showed that human–human haptic interactions can convey a great variety of emotions. Bailenson and Yee (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bailenson JN, Yee N (2008) Virtual interpersonal touch: haptic interaction and copresence in collaborative virtual environments. Multimed Tools Appl 37:5–14. doi:&#xA;                    10.1007/s11042-007-0171-2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR6" id="ref-link-section-d35661e659">2008</a>) tried to introduce a haptic arm as a medium of communication. However, results showed the limits of such approaches to support affective communication.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Improvement of awareness and communication through the collaborative intermodal display</h2><div class="c-article-section__content" id="Sec8-content"><p>Based on some features of these different communication strategies, we designed a flexible collaborative display to improve communication between partners while taking into account search and identification task requirements.</p><p>First of all, we detail the specifications of the search and identification task. They concern, of course, not only the communication between partners (called collaborative specifications), but also information related to the progress of the task for each partner (called individual specifications).</p><h3 class="c-article__sub-heading" id="Sec9">Specifications</h3><p>The search task consists of identifying I) a <b>target</b> (<span class="mathjax-tex">\(x_\mathrm{ref}\)</span>), which can be defined with a reference value (e.g., predefined constant) or with a criterion (e.g., global maximum or minimum), which exists on II) a <b>function</b> to explore whose variable is of the same nature as the target (<span class="mathjax-tex">\(y = F(x)\)</span>). For instance, the <b>function</b> can be the temperature map for a given region, and the <b>target</b> can be the maximum temperature on this map. During the search process, the user explores the function in order to find the region corresponding to the maximum temperature.</p><p>To design the intermodal display for a collaborative search, we have to consider two levels of specifications.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Individual specifications</h4><p>The first level of specifications concerns individual requirements without taking into consideration the collaborative configuration of the work environment or task. We identify three individual specifications:</p><ul class="u-list-style-none">
                      <li>
                        <p>
                                       <b>S1</b> 
                                       <b>Continuous perception and discrimination of data</b>: The individual search requires the ability to explore freely and effectively the function (<span class="mathjax-tex">\(f = f(x,y)\)</span>). This involves free navigation within the function, and continuous perception and discrimination of corresponding data. This procedure enables the orientation of the search toward local features such as the local maximum and minimum. The main issues for this procedure are to define (1) a suitable mapping between the motion of the user and the function’s variables (<i>x</i>) and (2) a suitable sensorial feedback to display the output of the function (<i>y</i>).</p>
                      </li>
                      <li>
                        <p>
                                       <b>S2</b> 
                                       <b>Comparison of values</b>: The individual search involves the ability to compare the current tested value with previous results or with a reference value. This procedure requires numerous back-and-forth actions since the compared values necessarily belong to different spatial configurations. For instance, molecular docking can provide different distant potential docking sites which must be compared in order to find the optimal score. Moreover, the comparison procedure involves the memorization of one of the two values and the detection of differences with the second value which can be constrained by the limits of sensorial memory and sensorial acuity.</p>
                      </li>
                      <li>
                        <p>
                                       <b>S3</b> 
                                       <b>Support for situation awareness</b>: The individual search involves the ability to be aware of surrounding results detected in subsequent steps. More precisely, the user needs to know their spatial configurations and corresponding features, for instance, to make a decision about close results or to return to a relevant configuration observed previously. This level of specification refers to a more general concept called <b>“situation awareness”</b>. <i>Situation awareness is the perception of environmental elements within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future</i> (Endsley and Garland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Endsley M, Garland D (2000) Situation awareness analysis and measurement. LEA, Mahwak" href="/article/10.1007/s10055-015-0273-5#ref-CR21" id="ref-link-section-d35661e851">2000</a>). <i>Situation awareness</i> involves being aware of what is happening around you in order to understand how information, events, and your own actions will have an impact on your goals and objectives, both now and in the near future. <i>Situation awareness</i> becomes complex for tasks occurring in complex environments because of high sensorial load.</p>
                      </li>
                    </ul>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Collaborative specifications</h4><p>The second level of specifications concerns collaborative requirements. This concerns information exchanged explicitly and implicitly between participants and more precisely some components of the interpersonal awareness between partners. We identify two collaborative specifications:</p><ul class="u-list-style-none">
                      <li>
                        <p>
                                       <b>S4</b> 
                                       <b>Support for process awareness</b>: Collaborative search requires the ability to be aware of results of partners. In fact, users can detect individually some relevant configurations which can be useful to other partners such as the identification of more or less important scores. This level of specifications refers to <i>process awareness</i> and consists in displaying the newly detected configurations with corresponding features to all partners. Thus, other partners can include this new configuration in their own search process.</p>
                      </li>
                      <li>
                        <p>
                                       <b>S5</b> 
                                       <b>Support for workspace awareness</b>: The collaborative search requires the ability to be aware of the workspace of the partner. <i>Workspace awareness</i> plays a strategic role during closely coupled collaborations since it enables one to establish and maintain a common ground through different interreferential communication mechanisms (Chastine and Zhu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chastine JW, Zhu Y (2008) The cost of supporting references in collaborative augmented reality. In: Proceedings of Canadian information processing society, Windsor. Graphics Interface, pp 275–282" href="/article/10.1007/s10055-015-0273-5#ref-CR15" id="ref-link-section-d35661e902">2008</a>). With this awareness component, partners can designate remote Regions of Interest (RoI), or collaboratively select a given target. Gutwin defines workspace awareness as “<i>The collection of up-to-the-minute knowledge a person uses to capture another’s interaction with the workspace</i>” (Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gutwin C, Greenberg S (1996) Workspace Awareness for Groupware. In: Conference companion human factor computing system common ground, ACM, Vancouver, pp 208–209" href="/article/10.1007/s10055-015-0273-5#ref-CR26" id="ref-link-section-d35661e909">1996</a>). Several approaches were explored to support workspace awareness in 3D CVEs. Kiyokawa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Kiyokawa K, Iwasa H, Takemura H, Yokoya N (1998) Collaborative immersive workspace through a shared augmented environment. In: Proceedings of SPIE international symposium on intelligent system and advantage manufacturing, vol 3517, pp 2–13" href="/article/10.1007/s10055-015-0273-5#ref-CR35" id="ref-link-section-d35661e912">1998</a>) used a 3D virtual human to designate targets in augmented reality environment. Fraser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of ACM symposium user interface software andtechnology, vol 1, pp 27–36. doi:&#xA;                    10.1145/320719.322580&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR23" id="ref-link-section-d35661e915">1999</a>) used of Peripheral Lenses and Field-of-View approaches to highlight the RoI. Chastine and Zhu (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chastine JW, Zhu Y (2008) The cost of supporting references in collaborative augmented reality. In: Proceedings of Canadian information processing society, Windsor. Graphics Interface, pp 275–282" href="/article/10.1007/s10055-015-0273-5#ref-CR15" id="ref-link-section-d35661e918">2008</a>) proposed to visually blur the space around the current workspace of the partner, while the visual feedback in the working space is in focus.</p>
                      </li>
                    </ul>
                           <h3 class="c-article__sub-heading" id="Sec12">Designed intermodal display</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Integration of specifications</h4><p>To support these two levels of specifications, we designed a tool that simultaneously combines haptic and audio channels. The complementarity between these two stimuli enables the integration of the required specifications while offering an intuitive tool for collaborative search.</p><p>
                    <i>Individual specifications</i>
                  </p><p>
                              <i>Haptic feedback</i>: The haptic channel enables the continuous perception and discrimination of data (<b>S1</b>). The exploration procedure is supported by mapping the variable (<i>x</i>) on the user’s hand motion in 2D space. Then, a suitable mapping between the function output and the haptic feedback enables an efficient exploration, perception, and discrimination of the function values.</p><p>Among various haptic feedbacks, we use vibrotactile stimuli. This consists of generating quick low-amplitude oscillations without a kinesthetic effect. Access to the current value is available even if there is no movement, facilitating the comparison procedure (<b>S2</b>). The vibrotactile stimuli enable an efficient detection of local features, for instance, a local maximum or a local minimum, since the tactile perception is based on a temporal integration mechanism. The mapping function between the function output and the vibrotactile is presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec14">4.2.2</a>.</p><p>
                              <i>Audio feedback:</i> The audio channel simultaneously supports the comparison procedure (<b>S2</b>) and situation awareness (<b>S3</b>). Once a relevant value is detected, the user uses the audio channel to register its spatial configuration and corresponding features through a suitable 3D spatialized audio signal (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig3">3</a>a). During the search, the user is continuously aware of the registered result (<b>S3</b>) which enables him/her to compare it instantaneously with the current tested value, displayed on the haptic channel (<b>S2</b>). A mapping function is also used between the function output and the audio signal. It is also presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0273-5#Sec14">4.2.2</a>.</p><p>
                              <i>Comparison procedure</i>: In order to design an efficient and intuitive comparison procedure, we were inspired by a single-user configuration study where a user tries to simultaneously compare haptic and audio pulses presenting different frequencies (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig1">1</a>a). Within certain design constraints, the user can determine which signal presented over the two channels exhibits the higher/lower frequency or whether the two signals have equal frequencies (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments. Int J Hum Comput Interact 30(11):921–933. doi:&#xA;                    10.1080/10447318.2014.941277&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR5" id="ref-link-section-d35661e999">2014</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Proposed approach for intermodal comparison and the corresponding rendering signal. <b>a</b> The compared values are displayed simultaneously on audio and haptic channels. For a range of variations in frequency, the user can determine which signal frequency is higher or lower or if the two signals present the same frequencies, <b>b</b> rendering signal and corresponding control parameters: The function is composed of a high-frequency carrier signal (<i>f</i>, <i>A</i>) modulated by a low-frequency Gaussian pulse (<i>T</i>, <span class="mathjax-tex">\(\tau \)</span>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>
                              <i>Collaborative specifications</i> Beyond supporting individual specifications (<b>S1</b>, <b>S2</b>, and <b>S3</b>), the audio component takes into account the collaborative specifications (<b>S4</b> and <b>S5</b>). In fact, once a relevant result is registered by one partner, we make it available to the other partners (<b>S4</b>) to enable them to access and use this new result as a new reference for their search (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig3">3</a>b).</p><p>Finally, the audio component, through the 3D spatialized rendering, can be used as a designation tool to indicate to partners the spatial configuration of relevant configurations (<b>S5</b>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Audio and haptic mapping</h4><p>To support these specifications (<b>S1</b>, <b>S2</b>, <b>S3</b>, <b>S4</b>, and <b>S5</b>), we use the same mapping model for both audio and haptic feedbacks using modulated wavelets according to Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0273-5#Equ1">1</a> (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig1">1</a>b).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \left( \begin{array}{ll} A \times \exp \left( -\frac{t^2}{T}\right) \times \cos (f \times t) &amp;: \quad 0 \le t &lt; T \\ 0 &amp;: \quad T \le t &lt;\tau \\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where  <i>f</i>: carrier pulse frequency, <span class="mathjax-tex">\(\tau \)</span>: pulse period or repetition rate (tempo<span class="mathjax-tex">\(^{-1})\)</span>, <i>T</i>: pulse length, <i>t</i>
                              <i>A</i>: time variable</p><p>The mapping function combines a high-frequency carrier signal (<i>f</i>, <i>A</i>) modulated by a low-frequency Gaussian pulse (<i>T</i>, <span class="mathjax-tex">\(\tau \)</span>). The combination of high-frequency and low-frequency variations exploits the common perceptual frequency range between haptic and audio perceptions where the upper haptic frequency range overlaps with the lower end of the audible frequency range. The objective is twofold: on the one hand, we want to make the signal accessible to the haptic channel with the frequency parameter (<i>f</i>) and to the audio channel with the repetition rate parameter (<span class="mathjax-tex">\(\tau \)</span>); on the other hand, we want to create a bridge between the two channels to support comparison with the tempo of the signal (which equates to the reciprocal of the signal repetition rate parameter, <span class="mathjax-tex">\(\tau \)</span>) which is perceptible with both channels.</p><p>Previous studies have been carried out to identify the most suitable parameter for the audio and haptic rendering to support the required specifications. These experiments enabled us to investigate several parameters couplings (e.g., <span class="mathjax-tex">\(\tau/f\)</span>, <span class="mathjax-tex">\(\tau/A\)</span>, <i>T</i>/<i>f</i>, etc.) to find the most suitable parameters configuration. The details of these experiments are developed in (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ammi M, Katz BF (2011) Design of haptic stimuli for audio-haptic concurrent coupling. In: IEEE international symposium haptic audio-visual environments and games (IEEE HAVE), Hebei, pp 74–80" href="/article/10.1007/s10055-015-0273-5#ref-CR3" id="ref-link-section-d35661e1481">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307–311" href="/article/10.1007/s10055-015-0273-5#ref-CR4" id="ref-link-section-d35661e1485">2012</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments. Int J Hum Comput Interact 30(11):921–933. doi:&#xA;                    10.1080/10447318.2014.941277&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR5" id="ref-link-section-d35661e1488">2014</a>). The current study exploits these results to identify the most suitable parameters for the audio–haptic display. The following models show the adopted parameters from the previous work. Based on these results, we will study the communicative features of the audio–haptic display.</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                                       <b>Audio stimuli</b>: The audio signal presents the reference data and supports the <i>intermodal comparison</i>. For this purpose, we map the reference intensity to the tempo relation parameter (<span class="mathjax-tex">\(\tau \)</span>) which is common with the haptic stimuli. </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left( {\begin{array}{*{20}l}    {\tau  = 4.92 \times \exp ^{{\frac{{0.13}}{{I_{{{\text{ref}}}} }}}} } \hfill  \\    {f = 5\;{\text{kHz}}} \hfill  \\    {T = 150\;{\text{ms}}} \hfill  \\    {A = 50\;{\text{dB}}} \hfill  \\   \end{array} } \right. $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>
where <span class="mathjax-tex">\(I_{\rm ref}\)</span>: is the reference value.</p>
                      </li>
                      <li>
                        <p>
                                       <b>Haptic stimuli</b>: The most efficient haptic stimuli simultaneously combine variation of two parameters for the same perception (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig2">2</a>): (1) frequency parameter <i>f</i> and (2) tempo parameter <span class="mathjax-tex">\(\tau \)</span>. The frequency parameter supports the continuous perception and discrimination of information which enables the continuous haptic search of the target value. This parameter enables the user to quickly find the local maximum of the function. The use of tempo as a parameter allows intermodal comparison between the tested haptic value and the target audio feedback. The discrete nature of this parameter (pulses or taps), supported by both audio and haptic feedbacks, provides a bridge between the haptic and audio channels and enables an interactive and qualitative comparison of the two stimuli (higher, lower, equal). This feature is due to the modal invariance (i.e., stimulus properties that are equal across modalities) of tempo (Gibson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1966" title="Gibson JJ (1966) The senses considered as perceptual systems. Houghton-Mifflin, Boston" href="/article/10.1007/s10055-015-0273-5#ref-CR24" id="ref-link-section-d35661e1721">1966</a>). </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left( {\begin{array}{*{20}l}    {\tau  = 4.92 \times \exp ^{{\frac{{0.13}}{I}}} } \hfill  \\    {f = 168.25 \times \exp ^{{0.0681 \times I}} } \hfill  \\    {T = 150\;{\text{ms}}} \hfill  \\    {A = 2.5\;{\text{N}}} \hfill  \\   \end{array} } \right. $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <i>I</i>: is the current value for the varying function intensity.</p>
                      </li>
                    </ul>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Design of the haptic stimuli: the explored intensity function [here a linearly increasing function of type <span class="mathjax-tex">\(f(x,y)=k\times x\)</span>] is mapped simultaneously to the <i>tempo</i> (local period) and signal <i>frequency</i> parameters: the variation range of <i>tempo</i> (low-frequency pulses/taps) enables the intermodal comparison with the audio channel</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>To support the awareness specifications (<b>S3</b>, <b>S4</b>, and <b>S5</b>), we use a 3D spatialized audio feedback, through an ear-in-hand metaphor (Ammi and Ferreira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. In: IEEE ICRA, pp 454–460" href="/article/10.1007/s10055-015-0273-5#ref-CR2" id="ref-link-section-d35661e1959">2007</a>; Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments. Int J Hum Comput Interact 30(11):921–933. doi:&#xA;                    10.1080/10447318.2014.941277&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR5" id="ref-link-section-d35661e1962">2014</a>; Menelas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Menelas BAJ, Bourdot P, Picinali L, Katz BF (2014) Non-visual identification, localization, and selection of entities of interest in a 3D environment. J Multimod User Interfaces, pp 1–14. doi:&#xA;                    10.1007/s12193-014-0148-1&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR38" id="ref-link-section-d35661e1966">2014</a>), to register results. This approach uses the hand as a token of the head with respect to listening position on the explored topography. Audio stimuli positions are rendered around the users head relative to the current position of the virtual end-effector in the explored 2D dataspace. Only distance and azimuth were considered, with elevation constrained to the horizontal plane. The virtual end-effector position is controlled with the haptic interaction device handled by the user.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Experiment: Collaborative search of targets</h2><div class="c-article-section__content" id="Sec15-content"><h3 class="c-article__sub-heading" id="Sec16">Objectives and hypotheses</h3><p>A previous work studied the intermodal display for individual search tasks (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307–311" href="/article/10.1007/s10055-015-0273-5#ref-CR4" id="ref-link-section-d35661e1983">2012</a>). Results showed an improvement in performance and working efficiency. In addition, we observed a significant improvement of the comparison accuracy. Based on these results, we investigate in the current paper the communicative features of this tool for collaborative tasks. Based on a similar context—search of targets in a 2D non-visual environment—we study a scenario where two participants collaborate in a shared space for the search and identification of a given target. As developed above, this context was inspired by applications requiring the search and identification of targets in complex environments where the visual feedback is overloaded. We have simplified the task by providing an abstraction of the actual task employing a 2D environment to process instead of a 3D environment which can require complex manipulation procedures (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Katz B, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference on auditory display, Paris, pp 1–7. &#xA;                    http://www.icad.org/node/2355&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR33" id="ref-link-section-d35661e1986">2008</a>). Beyond providing efficient tools for the individual search, the intermodal display supports communication and coordination between the two partners which should lead to an improvement of the collaborative working efficiency.</p><p>The experiment consists of a comparative study, with and without use of the intermodal display, in which the two partners are asked to work together to find and agree on the maximum value of the explored function. There are no visual information and visual communication. In both conditions, the two partners can communicate only verbally without visual feedback. The explored environment corresponds to a set of Gaussian functions with different amplitudes. The overall function corresponds to a set of continuous scalar fields presenting different peak values and positions (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig3">3</a>). Based on these objectives, the following hypotheses were adopted:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <b>H1</b> The intermodal display improves overall performance of the pairs (i.e., two partners working together). This hypothesis is evaluated with the required travelled distance, completion time, number of clicks, and number of consultations to perform the task.</p>
                    </li>
                    <li>
                      <p>
                                    <b>H2</b> The intermodal display reduces the need for verbal communication between partners. This hypothesis is evaluated with the number of verbal exchanges and number simultaneous clicks during the collaborative task.</p>
                    </li>
                    <li>
                      <p>
                                    <b>H3</b> The use of the intermodal display is easy to understand. This hypothesis is evaluated with subjective feedbacks.</p>
                    </li>
                  </ul>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Proposed concept to improve the search and the communication between partners. <b>a</b> Use of the intermodal display for the comparison of two values: The target value is registered with an audio feedback, and the function is explored though a suitable haptic feedback. The user can determine which signal presents the higher/lower intensity or whether the two have equal intensities; <b>b</b> use of the intermodal display for the communication between the two participants: once the first participant registers a relevant configuration with the 3D audio feedback, the second participant can access the different features of the target</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>
                           <b>Hardware setup and apparatus</b>: The platform includes two client nodes as workstations and one server node as supervision station (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig4">4</a>). The client nodes support the audio and haptic modules, the interfacing components (i.e., 3D Systems Geomagic Touch and 3D audio headphone system). The audio (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz B, Rio E, Picinali L (2010) LIMSI spatialisation engine. International Deposit Digital Number IDDN.FR.001.340014.000.S.P.2010.000.31235" href="/article/10.1007/s10055-015-0273-5#ref-CR34" id="ref-link-section-d35661e2059">2010</a>) and haptic (Cornuet <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Cornuet N (2009) Study of new audio-haptic coupling strategies. Master’s thesis, University of Paris-Sud 11—CNRS/LIMSI" href="/article/10.1007/s10055-015-0273-5#ref-CR18" id="ref-link-section-d35661e2062">2009</a>) software modules were developed with MaxMSP (Cycling ’74 <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Cycling ’74 (2010) MAX-MSP. &#xA;                    http://cycling74.com/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR19" id="ref-link-section-d35661e2065">2010</a>). This graphical programming real-time environment provides a flexible architecture and enables rapid integration of the required software modules as external patches. The server node supports the data exchange and synchronization between the client nodes. Moreover, it enables control of the progress of the experiments. The two workstations were installed in an acoustically treated sound isolation booth <span class="mathjax-tex">\((\hbox {SPL}\,\le 30\,\text{dBA}\)</span>), with computer hardware externally located to minimize external noises and distractions (PC fan noise, supervisor activity, etc.). Participants were asked to take their place in front of the workspace according to a face-to-face configuration (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig4">4</a>). The two working spaces were separated with a small bulkhead (yellow bulkhead). For each participant, we provide a haptic arm and headphones (no computer screen).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Platform. <b>a</b> Supervision station: enables control of the experiment’s progress, <b>b</b> platform configuration: participants explore a shared 2D space with the haptic arm and register relevant configuration with a spatialized audio signal. The <i>yellow</i> bulkhead visually separates the two working spaces. A visual representation of the explored function is shown here, but was absent in the actual experiment (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To display the spatial position of the registered value, binaural audio synthesis was used (Begault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, New York" href="/article/10.1007/s10055-015-0273-5#ref-CR9" id="ref-link-section-d35661e2139">1994</a>). To account for the difference in scale between the audio rendering and the haptic devices’ operating range, audio distances were scaled by a factor of <span class="mathjax-tex">\(\times 10\)</span>. This was performed to improve externalization of the sources, as both distance attenuation and ILD corrections were applied as a function of distance. Using a factor of <span class="mathjax-tex">\(\times 10\)</span> with source gain adjusted to be comfortable when at the source position allowed sources at extreme positions within the dataspace to still remain audible.</p><p>The binaural audio rendering was performed using the LSE (Katz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Katz B, Rio E, Picinali L (2010) LIMSI spatialisation engine. International Deposit Digital Number IDDN.FR.001.340014.000.S.P.2010.000.31235" href="/article/10.1007/s10055-015-0273-5#ref-CR34" id="ref-link-section-d35661e2194">2010</a>), a real-time spatialization engine in MaxMSP based on full-phase head-related impulse response (HRIR) convolution. This approach is in contrast to the minimum phase HRIR approach often used (Begault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, New York" href="/article/10.1007/s10055-015-0273-5#ref-CR9" id="ref-link-section-d35661e2197">1994</a>), thereby providing improved spatial precision but at an additional computational cost. All subjects received binaural renderings using the KEMAR mannequin HRIR, 256 sample length (see Parseihian et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Parseihian G, Jouffrais C, Katz BF (2014) Reaching nearby sources: comparison between real and virtual sound and visual targets. Front Neuro 8(269):1–13. doi:&#xA;                    10.3389/fnins.2014.00269&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR42" id="ref-link-section-d35661e2200">2014</a>, for HRTF measurement and processing details). Dynamic interpolation between measured positions is accomplished through a high-resolution off-line interpolation of the HRTF database and a dynamic panning between adjacent four positions (quadrilateral interpolation). Interaural level difference (ILD) cues were modified to account for contralateral level differences for near distances using a spherical head model, and a parallax effect correction was implemented for distances less than 1 m, meaning that HRIRs were selected taking into account the angle of the source relative to the ear rather than the angle relative to the head center. The use of non-individual HRTFs was not deemed detrimental to this experiment due to the fact that precise spatial auditory source localization was not part of the task and since the binaural rendering was dynamic with user displacement in the virtual environment, providing a token version of head-tracking. The auditory markers served primarily as reference beacons of general position and function value.</p><p>No additional effects, such as reverberation, were included. Headphone equalization was not employed since naturalness of the stimuli was not a concern, and previous studies (see Schönstein et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Schönstein D, Ferré L, Katz B (2008) Comparison of headphones and equalization for virtual auditory source localization. In: Acoustics’08. 9e Congres Franais d’Acoustique of the SFA. Paru dans: JASA, vol 123, n5, Paris, pp 1–5" href="/article/10.1007/s10055-015-0273-5#ref-CR48" id="ref-link-section-d35661e2206">2008</a>) have shown that such equalization does not affect localization performance.</p><p>The maximum source level was adjusted prior to the experiment and remained fixed for the entire duration of the experiment for all participants. Individual spatialized audio was rendered for each participant over Sennheiser HD570 open circumaural headphones. No audio cross talk between participants’ headphones was observed. Verbalizations were recorded using an AKG414 microphone placed between the pair, in-line with the visual bulkhead barrier (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig4">4</a>).</p><p>
                           <b>Explored function</b>: The explored function corresponds to a combination of five Gaussian models with various amplitudes and positions but the same standard deviations (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig3">3</a>). The position and maximum of these functions are generated randomly and are uniformly distributed (no overlapping, exploration of all available space, etc.). The explored function is common between the two partners for each trial.</p><p>
                           <b>Tools and communication</b>: Two feedbacks were provided to participants. First, the haptic feedback which maps the function’s amplitude to vibrotactile stimuli according to the designed model (see Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0273-5#Equ1">1</a> and corresponding parameters in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0273-5#Equ3">3</a>). In addition to the vibrotactile stimuli, we generate a horizontal virtual haptic plan (<span class="mathjax-tex">\(X-Z\)</span>) on which participants can move freely while perceiving the vibrotactile stimuli. This virtual plan is used to generate a force applied on the end-effector with the penalty model to constrain the user’s movement on the plan. The vibrotactile stimulus is displayed with the haptic arm and is available when the participant touches the virtual plan.</p><p>Second, we allow participants to register the relevant values through a 3D spatialized audio stimulus. The mapping between the function’s amplitude and corresponding audio stimuli is supported by the audio model (see Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0273-5#Equ1">1</a> and corresponding parameters in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0273-5#Equ2">2</a>). The registration of values is made by clicking the dark button. Once registered, the spatial configuration and corresponding amplitude are available continuously. The registered values of both partners are rendered to both partners, at their respective spatial positions. No distinction was made regarding the sonification design for the two participants. Based on these two stimuli, participants are able to compare two regions, through the intermodal display, by associating one region with the audio stimulus and exploring the second region with the haptic stimulus (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig3">3</a>). Finally, to support the interpersonal awareness specifications (<b>S4</b> and <b>S5</b>), the 3D spatialized audio feedback is shared between the two partners. Beyond the intermodal display, the two partners can communicate verbally to indicate relevant regions or to request the help from each other.</p><h3 class="c-article__sub-heading" id="Sec17">Methods</h3><p>
                           <b>Participants</b>: 24 participants (16 male, 8 female; age range from 20 to 41 with median 30) took part in this study; most were university graduate students in Computer Science. Most participants had experience with haptic-3D audio displays, VR, and video games. All participants self-reported normal uncorrected hearing thresholds. The 24 participants were grouped into 12 pairs. There is no special relationship between partners of the same pair.</p><p>
                           <b>Conditions</b>: As developed above, two conditions were presented to the pairs during the collaborative search of targets. The order of presentation was alternated in order to limit the learning effect.</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <b>C1</b>: Collaborative search without the intermodal display. Only haptic feedback is provided to pairs to explore the function. Pairs can communicate verbally. This is the control condition.</p>
                    </li>
                    <li>
                      <p>
                                    <b>C2</b>: Collaborative search with the intermodal display. Both haptic and 3D audio feedbacks are provided to the pairs. Pairs can communicate verbally but also with 3D audio feedback by registering some regions in the explored function.</p>
                    </li>
                  </ul>
                        <p>
                           <b>Procedure</b>: Two repetitions of the experiment were carried out according to the investigated conditions in question (<b>C1</b> or <b>C2</b>). For each experiment, the subject pair was presented a series of 10 different topographical maps (in 2D space), each containing 5 peaks of various amplitudes. For each trial consisting of a single map, two main steps were carried out:</p><p>
                           <b>Identification step</b>: Pairs were asked (1) to explore the function, (2) to find peaks, and then (3) to register them. They had the possibility to register several times the same peaks (in case of memorization problems). During this step, participants could work individually, but can collaborate and communicate with their partner (give indications, request help, etc.). However, this step concludes when both partners have found and registered all five peaks. The registration of peaks was performed by clicking the white button (haptic arms).</p><p>
                           <b>Search step</b>: For the same map, and based on registered peaks, pairs were asked: (1) to explore again the map, and (2) to find and confirm the global maximum. During this step, participants could work individually or collaborate and communicate to find the target (advice, confirmation, etc.). The experiment finishes when both partners select the same global maximum. The final validation of the global maximum was done with the black button (haptic arms). For both steps, we asked participants to perform the task as fast as possible.</p><p>The end of each step is indicated with vocal messages (i.e., “step one is finished”, “step two is finished”). Once the maximum is registered by the two partners (search step), the system automatically generates a new map to explore and process. During the two steps, participants are free to discuss and communicate verbally with their partner. Moreover, we asked them to determine before the experiment a communication strategy to improve the communication during the operational steps (e.g., left region, right region). Before the beginning of the experiment, participants were given a document describing the context, overall objectives, steps to achieve, and available perceptions (haptic, intermodal display). After reading the document, participants had the possibility to perform five training search tests (1st and 2nd steps under <b>C1</b> and <b>C2</b>) to enable a better understanding of the search process, involved perceptions, and proposed tools. Once the training was finished, the experiment began.</p><p>
                           <b>Measures</b>: Based on the objectives and hypotheses developed above, the following dependent variables (V) were analyzed:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <b>V1</b> Travelled distance:</p><ul class="u-list-style-none">
                          <li>
                            <p>
                                            <b>V1.1</b> Travelled distance to identify all peaks during the identification step.</p>
                          </li>
                          <li>
                            <p>
                                            <b>V1.2</b> Travelled distance to find the global maximum during the search step.</p>
                          </li>
                        </ul>
                                 
                    </li>
                    <li>
                      <p>
                                    <b>V2</b> Execution time:</p><ul class="u-list-style-bullet">
                          <li>
                            <p>
                                            <b>V2.1</b> Time to identify all peaks during the identification step.</p>
                          </li>
                          <li>
                            <p>
                                            <b>V2.2</b> Time to find the global maximum during the search step.</p>
                          </li>
                        </ul>
                                 
                    </li>
                    <li>
                      <p>
                                    <b>V3</b> Number of clicks: expresses the level of awareness about previous results. We consider two types of clicks:</p><ul class="u-list-style-bullet">
                          <li>
                            <p>
                                            <b>V3.1</b> Number of registrations: number of times participants registered peaks during the identification step. This measure expresses the difficulties in finding all the peaks and the level of memorization of detected peaks.</p>
                          </li>
                          <li>
                            <p>
                                            <b>V3.2</b> Number of final confirmations: number of times when the participant validated the maximum during the search step. This measure expresses the level of error for the designation of the global maximum.</p>
                          </li>
                        </ul>
                                 
                    </li>
                    <li>
                      <p>
                                    <b>V4</b> Consultations</p><ul class="u-list-style-bullet">
                          <li>
                            <p>
                                            <b>V4.1</b> Number of consultations: number of times when participants consulted haptically the Gaussians, without registering for comparison. This measure expresses the influence of the intermodal display on the number of consultations and comparisons.</p>
                          </li>
                          <li>
                            <p>
                                            <b>V4.2</b> Duration of consultations: time spent for the consultation of peaks before validation.</p>
                          </li>
                        </ul>
                                 
                    </li>
                    <li>
                      <p>
                                    <b>V5</b> Verbal communication between the two participants: number of verbal exchanges between the two partners. A verbal exchange is an expression, including one or more sentences, from one user to his partner which can correspond to an order, a question, a request for help, an indication, etc.</p>
                    </li>
                    <li>
                      <p>
                                    <b>V6</b> Number of simultaneous clicks: number of simultaneous clicks, presenting a delay less than 1 s, between the two partners. This threshold was defined experimentally during pretests. This measure could highlight conflicts of use of the intermodal between the two partners.</p>
                    </li>
                  </ul>
                        <p>The travelled distance and time measures were calculated from the vector of recorded positions of the end-effector. The number of registrations and confirmations corresponds to the number of clicks made with the button. The number of consultations was calculated from the recorded positions of the end-effector. This variable is incremented each time the end-effector approached a peak under a given distance. Finally, the number of verbal exchanges was used to annotate the recorder conversation between partners.</p><p>Finally, we consider feedback from participants using a short questionnaire. Participants answered on a five-point Likert scale (from 1 to 5):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                                    <b>Q1</b>: Level of understanding of the use of the intermodal display.</p>
                    </li>
                    <li>
                      <p>
                                    <b>Q2</b>: Perceived efficiency during the two steps.</p>
                    </li>
                    <li>
                      <p>
                                    <b>Q3</b>: Degree of intentional use of the intermodal display during the two steps.</p>
                    </li>
                    <li>
                      <p>
                                    <b>Q4</b>: Perceived level of verbal communication during the two steps.</p>
                    </li>
                  </ul>
                        <h3 class="c-article__sub-heading" id="Sec18">Results</h3><p>Repeated measures analysis of variance (ANOVA) was used for the statistical analysis of data. A <i>p</i> value<span class="mathjax-tex">\(&lt;0.05\)</span> was considered statistically significant.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Evolution of efficiency during the identification and search steps</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig5">5</a> shows, according to the two conditions (haptic condition: <b>C1</b> and intermodal condition: <b>C2</b>), the evolution of the travelled distance (identification of peaks: <b>V1.1</b>, and finding the maximum: <b>V1.2</b>) and the completion time (identification of peaks: <b>V2.1</b>, and finding the maximum: <b>V2.2</b>) for the identification and search steps. These results show a significant reduction in the travelled distance and completion time with the intermodal display (<b>C2</b>) during the identification step (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig5">5</a>a, c). The mean travelled distance decreased from 1738 to 1401 mm [<span class="mathjax-tex">\(F(1, 11)=6.8712\)</span>, <span class="mathjax-tex">\(p=.03059\)</span>], and the mean completion time decreased from 415 to 337 s (<span class="mathjax-tex">\(F(1, 11)=7.1334\)</span>, <span class="mathjax-tex">\(p=.02176\)</span>). However, the intermodal display has a negative outcome, without a significant effect, on these two variables for the search of the global maximum during the search step (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig5">5</a>b, c). The mean crossed distance increases from 695 to 873 mm (<span class="mathjax-tex">\(p &gt; .05\)</span>), and the mean execution time increased from 242 to 302 s (<span class="mathjax-tex">\(p &gt; .05\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Evolution of the crossed distances and required time of the identification of peaks and the global maximum with the intermodal display (<i>vertical bars</i> denote 0.95 confidence intervals). H is the haptic condition <b>C1</b>, and AH is the intermodal condition <b>C2</b>. <b>a</b> Total travelled distance for the identification of peaks during the identification step. <b>b</b> Total travelled distance for the search of the global maximum during the search step. <b>c</b> Required time for identification of peaks during the identification step. <b>d</b> Required time for the search of the global maximum during the search step</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig6">6</a> shows the evolution of the number of registrations (<b>V3.1</b>) and confirmations (<b>V3.2</b>), respectively, for the two conditions (<b>C1</b> and <b>C2</b>). The result showed that the intermodal display (<b>C2</b>) reduces significantly the number of registrations during the identification step (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig6">6</a>a), from 6.75 to 6.04 [<span class="mathjax-tex">\(F(1, 11)=15.544\)</span>, <span class="mathjax-tex">\(p=.00558\)</span>]. Moreover, we observed a significant reduction (with a borderline <i>p</i> value) of the number of validation errors during the search step (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig6">6</a>b), from 1.45 to 1.18 (<span class="mathjax-tex">\(F(1, 11)=4.7579\)</span>, <span class="mathjax-tex">\(p=.06552\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Evolution of the number of registrations and final confirmations with the intermodal display (<i>vertical bars</i> denote 0.95 confidence intervals). <i>H</i> is the haptic condition <b>C1</b>, and AH is the intermodal condition <b>C2</b>, <b>a</b> number of registrations of peaks during the identification step, <b>b</b> number of final confirmations of the global maximum during the search step</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig7">7</a> shows the evolution of the number (<b>V4.1</b>) and duration (<b>V4.2</b>) of consultations of peaks during the two steps. The result showed that the number and the duration of consultations during the identification step are improved with the intermodal display (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig7">7</a>a, c). The number of consultations decreased significantly from 10.73 to 8.74 [<span class="mathjax-tex">\(F(1, 11)=6.6381\)</span>, <span class="mathjax-tex">\(p=.04197\)</span>]. The consultation durations decreases significantly from 194 to 164 s (<span class="mathjax-tex">\(F(1, 11)=5.2008\)</span>, <span class="mathjax-tex">\(p=.04350\)</span>). The number and duration of consultations during the search step are not improved with the intermodal display (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig7">7</a>b, d). The number of consultations increased from 7.00 to 8.09 s (<span class="mathjax-tex">\(p &gt; .05\)</span>), and the consultation durations increased slightly from 175 to 185 s (<span class="mathjax-tex">\(p &gt; .05\)</span>). These different results enable us to validate hypothesis <b>H1</b>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Evolution of the number of consultations and consultation time for the identification of peaks and the global maximum with the intermodal display (<i>vertical bars</i> denote 0.95 confidence intervals). H is the haptic condition <b>C1</b>, and AH is the intermodal condition <b>C2</b>, <b>a</b> number of consultations for the identification of peaks during the identification step, <b>b</b> number of consultations for the search of the global maximum during the search step, <b>c</b> consultation time during the identification of peaks during the identification step, <b>d</b> consultation time during the identification of the global maximum during the search step</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Verbal communication</h4><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig9">9</a> show the analysis of the verbal exchanges (<b>V5</b>) between partners during the identification and search steps, respectively. An unsupervised clustering procedure (using the WEKA software) was applied to verbal communication results coupled with the completion time results (Fass and Cameletti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Fass A, Cameletti M (2009) The EM algorithm in a distributed computing environment for modelling environmental space-time data. Environ Model Softw 24(9):1027–1035" href="/article/10.1007/s10055-015-0273-5#ref-CR22" id="ref-link-section-d35661e3325">2009</a>). This analysis highlights two groups (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig9">9</a>).</p><ul class="u-list-style-bullet">
                      <li>
                        <p>Group 1: corresponding to <span class="mathjax-tex">\(\frac{2}{3}\)</span> of pairs.</p>
                      </li>
                      <li>
                        <p>Group 2: corresponding to <span class="mathjax-tex">\(\frac{1}{3}\)</span> of pairs.</p>
                      </li>
                    </ul>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Analysis of the number of interventions during the identification step (<i>vertical bars</i> denote 0.95 confidence intervals). H is the haptic condition <b>C1</b>, and AH is the intermodal condition <b>C2</b>, <b>a</b> number of interventions during the identification step, <b>b</b> number of interventions during the identification step according to the identified groups (haptic condition <b>C1</b>), <b>c</b> number of interventions during the identification step according to the identified groups (intermodal condition <b>C2</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Analysis of the number of interventions during the second step (<i>vertical bars</i> denote 0.95 confidence intervals). H is the haptic condition <b>C1</b>, and AH is the intermodal condition <b>C2</b>, <b>a</b> number of verbal exchanges during the search, <b>b</b> number of verbal exchanges during the search step according to the identified groups (haptic condition <b>C1</b>), <b>c</b> number of verbal exchanges during the search step according to the identified groups (intermodal condition <b>C2</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>We develop in the following sections these two groups according to the identification and search steps.</p><p>
                              <b>Identification step</b>: Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a>a highlights a significant reduction in the overall number of interventions of <span class="mathjax-tex">\(37\%\)</span> from haptic condition (<b>C1</b>) to intermodal condition (<b>C2</b>) during the identification step. This decreased from 59 interventions for <b>C1</b> to 43 interventions for <b>C2</b> [<span class="mathjax-tex">\(F(1, 11)=6.7870\)</span>, <span class="mathjax-tex">\(p=.02447\)</span>]. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a>b, c shows that the decrease in the number of verbal interventions mainly concerns Group 1 (reduction in <span class="mathjax-tex">\(41\,\%\)</span>). Group 2 presents a similar number of verbal interventions for the two conditions (<span class="mathjax-tex">\(\sim\)</span>11 interventions). These results explain partially the improvement in performance during the identification step (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig5">5</a>).
</p><p>
                              <b>Search step:</b> Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig9">9</a>a highlights an increase in the overall number of interventions of <span class="mathjax-tex">\(27\,\%\)</span> from haptic condition (<b>C1</b>) to intermodal condition (<b>C2</b>) during the search step. This increased from 7.91 interventions for <b>C1</b> to 11 for <b>C2</b> [<span class="mathjax-tex">\(F(1, 11)=40.590\)</span>, <span class="mathjax-tex">\(p=.05567\)</span>]. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig9">9</a>b, c shows that the increase in the number of interventions concerns all pairs (Group 1: <span class="mathjax-tex">\(22\%\)</span>, Group 2: <span class="mathjax-tex">\(30\,\%\)</span>). These results partially explain the reduction in performance during the search step (Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig5">5</a>).</p><p>Based on these results, we can consider hypothesis <b>H2</b> validated for the identification step but not for the search step.</p><p>In addition to the communication results, we investigated the evolution, for the two identified groups, of some components of completion time and the simultaneous uses of the intermodal display. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig10">10</a>a shows the difference in completion times to identify all peaks (<b>V2.1</b>) for the identification step. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig10">10</a>b shows the difference in time to find the global maximum (<b>V2.2</b>) for the search step. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig10">10</a>c shows the simultaneous uses of the intermodal display for the search step the number.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0273-5/MediaObjects/10055_2015_273_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Difference in completion times between audio–haptic condition (<b>C2</b>) and haptic condition (<b>C1</b>) for the two steps and number of simultaneous clicks during the search step, <b>a</b> difference in completion time between the haptic condition (<b>C1</b>) and the intermodal condition (<b>C2</b>) for identification step, <b>b</b> difference in completion time between the haptic condition (<b>C1</b>) and the intermodal condition (<b>C2</b>) for search step, <b>c</b> number of simultaneous clicks during the search step</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0273-5/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>We present these results for the two examined steps:</p><p>
                    <b>Identification step:</b>
                    </p><ul class="u-list-style-bullet">
                      <li>
                        <p>G1.I: The result showed that the first group effectively uses the intermodal display (<b>C2</b>) and presents a significant difference in completion time of 113 s with the haptic condition (<b>C1</b>).</p>
                      </li>
                      <li>
                        <p>G2.I: The result showed that the second group presents similar results (difference of 5 s) for the two conditions.</p>
                      </li>
                    </ul>
                  <p>
                    <b>Search step:</b>
                    </p><ul class="u-list-style-bullet">
                      <li>
                        <p>G1.S: The result showed that the first group effectively used the intermodal display (<b>C2</b>) and presented a significant difference of 48 s with the haptic condition (<b>C1</b>). However, the improvement is less significant than for the identification step. Moreover, we observed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig10">10</a>c a significant increase in simultaneous clicks, of <span class="mathjax-tex">\(41\,\%\)</span> from <b>C1</b> to <b>C2</b> (<span class="mathjax-tex">\(F(1, 11)=30.88\)</span>, <span class="mathjax-tex">\(p=.0189\)</span>), which highlights the emergence of some conflicts in the use of the intermodal display.</p>
                      </li>
                      <li>
                        <p>G2.S: This group was significantly less effective with the intermodal display (<b>C2</b>). The completion time decreased 78 s. Moreover, we observed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig10">10</a>c a significant and greater increase in the number of simultaneous clicks than for Group 1, of <span class="mathjax-tex">\(75\,\%\)</span> from haptic condition <b>C1</b> to intermodal condition <b>C2</b> [<span class="mathjax-tex">\(F(1, 11)=50.21\)</span>, <span class="mathjax-tex">\(p=.00587\)</span>], which highlights a greater level of conflicts with the intermodal display than for Group 1.</p>
                      </li>
                    </ul>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Subjective feedback</h4><p>The results from the questionnaire show that the majority of the participants easily understood the use of the intermodal display for the collaborative configuration (<b>Q1</b>, mean score on the five-point Likert scale of <span class="mathjax-tex">\(4.1 \pm 0.4\)</span>). This result allows us to validate <b>H3</b>. The questionnaire highlights a high level of perceived efficiency with the intermodal display during the identification step (<b>Q2</b>, mean of <span class="mathjax-tex">\(3.8 \pm 0.8\)</span>) and a low level of perceived efficiency with the intermodal display during the search step (<b>Q2</b>, mean of <span class="mathjax-tex">\(1.4 \pm 1.1\)</span>). The use of the intermodal display was variable during the identification step (<b>Q3</b>, mean of <span class="mathjax-tex">\(3.1 \pm 0.9\)</span>) and low during the search step (<b>Q3</b>, mean of <span class="mathjax-tex">\(1.2 \pm 1.1\)</span>). Finally, the pairs perceived a slight improvement in communication with the intermodal display during the identification step (<b>Q4</b>, mean of <span class="mathjax-tex">\(2.8 \pm 0.8\)</span>), but had difficulty evaluating their communications during the search step (<b>Q4</b>, mean of <span class="mathjax-tex">\(1.7 \pm 1.5\)</span>).</p><h3 class="c-article__sub-heading" id="Sec22">Discussion</h3><p>This section presents the global analysis of results according to the previously identified groups.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Identification step</h4>
                              <ul class="u-list-style-bullet">
                      <li>
                        <p>G1.I: The result showed that the first group effectively uses the intermodal display. From verbal communication results (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a>), we observed that this group greatly used the verbal communication with the haptic condition (<b>C1</b>) which is the main communication channel between the two partners. With the audio–haptic intermodal display (<b>C2</b>), the duration of verbal communication between subjects significantly decreased. In fact, the spatialized binaural audio feedback enabled an efficient communication and designation of spatial configurations of targets. Thus, a simple peak registration with the 3D audio signal enabled partners to quickly find the designated target. This led to an improvement in the simplification of communication between partners. Based on these results, we can consider that the audio–haptic intermodal display had a significant effect on the performance of this group during the identification steps relative to the haptic-only condition.</p>
                      </li>
                      <li>
                        <p>G2.I: The result showed that the second group presents similar performances for the two conditions. The verbal communication results (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig8">8</a>) show a lower level of communication for both conditions. This group of participant pairs did not choose to communicate verbally. Moreover, they did not exploit the audio components of the intermodal display to improve their communication during this step. Thus, the intermodal display had no effect on the performance of this group.</p>
                      </li>
                    </ul><p>In conclusion, during the identification step, the audio–haptic intermodal display played the role of a communication tool which improved exchanges between partners and their performance. However, some participant pairs, presenting a low level of verbal communication in conditions both with and without audio feedback, did not exploit the intermodal display and presented similar performances with and without the intermodal display.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">Search step</h4>
                    <ul class="u-list-style-bullet">
                      <li>
                        <p>G1.S: The result showed that the first group effectively used the audio–haptic intermodal display for the search step. From the verbal communication results (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0273-5#Fig9">9</a>), we observed an increase in verbal communication from the haptic condition (<b>C1</b>) to the intermodal condition (<b>C2</b>) for this group. In fact, this step required closely coupled collaboration between partners to find the target, which lead to more communication to coordinate their activities. In addition, we observed an emergence of some conflicts in the use of the intermodal display for this group. We suppose that the intermodal display introduced some conflicts in communication and conflicts of use since it is shared and used by the two partners (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307–311" href="/article/10.1007/s10055-015-0273-5#ref-CR4" id="ref-link-section-d35661e4447">2012</a>). In fact, the intermodal display is used by both users for communication, for instance for the designation of targets, but also individually for the comparison of values. This working configuration could lead to conflicts in use between these two functions. These results explain, for this group, the reduction in performance during the search step compared to the identification step.</p>
                      </li>
                      <li>
                        <p>G2.S: This group was significantly less effective with the intermodal display for the search step. From the verbal communication results, the result showed that the level of communication was greater in both conditions than for the identification step with larger values for the intermodal display (<b>C2</b>). In fact, as mentioned above, this step required closely coupled collaboration between the two partners to find the target which involved more communication than for the identification step. Moreover, we observed a greater level of conflicts with the intermodal display than for Group 1. As developed above, the intermodal display was shared between the two users and simultaneously served two different functions: a communication function and comparison function.</p>
                      </li>
                    </ul>
                  <p>In conclusion, during the search step, the intermodal display introduced some conflicts because of the closely coupled collaboration required by the search task. Thus, the level of improvement was less than for the identification step. However, for some pairs, the level of conflicts introduced by the intermodal display was too strong, making this approach not useful for this group.</p></div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusion</h2><div class="c-article-section__content" id="Sec25-content"><p>This paper presented and evaluated an audio–haptic intermodal display for the improvement of collaborative search of targets in a non-visual abstract environment. Based on efficient audio–haptic rendering functions, including a tempo parameter in both signals and coupled with a 3D spatialized audio feedback, users could effectively search the targets while communicating relevant configurations to their partner. The experiment, consisting of finding the maximum score in a 2D abstract non-visual environment, clearly shows improvement of the search performance and efficiency. Moreover, the result showed that the intermodal display plays the role of a communication tool which improves the verbal communication between partners. However, the approach introduces some communication conflicts during steps presenting high coupling between partners what reduces the efficiency of the group.</p><p>Based on these results, we propose the simultaneous use of individualized spatial audio feedback (unique to each user) to reduce the level of conflicts between partners related to identifying and separating one’s own audio feedback from the shared audio feedback of other collaborators. Such an approach could employ the addition of low-amplitude harmonic components which can be varied in order to provide both audio and haptic variations while maintaining the same fundamental frequencies (Picinali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012a" title="Picinali L, Feakes C, Mauro D, Katz BF (2012a) Tone-2 tones discrimination task comparing audio and haptics. In: IEEE international symposium on haptic audio-visual environments and games, Munich, pp 19–24, doi:&#xA;                    10.1109/HAVE.2012.6374432&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR44" id="ref-link-section-d35661e4479">2012a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Picinali L, Feakes C, Mauro DA, Katz BF (2012b) Spectral discrimination thresholds comparing audio and haptics for complex stimuli. In: Magnusson C, Szymczak D, BrewsterInt S (eds)  Workshop on haptic and audio interaction design, Lund, Sweden, HAID 2012, pp 131–140,doi:&#xA;                    10.1007/978-3-642-32796-4_14&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR45" id="ref-link-section-d35661e4482">b</a>). An alternative approach could consider the use of temporal-frequency acoustic signal morphologies which are adaptable to various timber palettes allowing for consistent conceptual identities across palettes while being customizable and distinctly identifiable (Parseihian and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc 60(6):409–418. &#xA;                    http://www.aes.org/e-lib/browse.cfm?elib=16355&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0273-5#ref-CR41" id="ref-link-section-d35661e4485">2012</a>). Finally, we plan to investigate the role of relationships between the partners and their personalities, in interpersonal communication and collaboration with this type of tools.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Alhalabi, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Alhalabi M (2003) An experimental study on the effects of network delay in cooperative shared haptic virtual e" /><p class="c-article-references__text" id="ref-CR1">Alhalabi M (2003) An experimental study on the effects of network delay in cooperative shared haptic virtual environment. Comput Graph 27(2):205–213. doi:<a href="https://doi.org/10.1016/S0097-8493(02)00277-7">10.1016/S0097-8493(02)00277-7</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2802%2900277-7" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20study%20on%20the%20effects%20of%20network%20delay%20in%20cooperative%20shared%20haptic%20virtual%20environment&amp;journal=Comput%20Graph&amp;doi=10.1016%2FS0097-8493%2802%2900277-7&amp;volume=27&amp;issue=2&amp;pages=205-213&amp;publication_year=2003&amp;author=Alhalabi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. In: " /><p class="c-article-references__text" id="ref-CR2">Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. In: IEEE ICRA, pp 454–460</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ammi M, Katz BF (2011) Design of haptic stimuli for audio-haptic concurrent coupling. In: IEEE international s" /><p class="c-article-references__text" id="ref-CR3">Ammi M, Katz BF (2011) Design of haptic stimuli for audio-haptic concurrent coupling. In: IEEE international symposium haptic audio-visual environments and games (IEEE HAVE), Hebei, pp 74–80</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposi" /><p class="c-article-references__text" id="ref-CR4">Ammi M, Katz BF (2012) Audio-haptic intermodal coupling for comparative search tasks. In: IEEE haptics symposium, Vancover, pp 307–311</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ammi, BF. Katz, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments" /><p class="c-article-references__text" id="ref-CR5">Ammi M, Katz BF (2014) Intermodal audio-haptic metaphor: improvement of target search in abstract environments. Int J Hum Comput Interact 30(11):921–933. doi:<a href="https://doi.org/10.1080/10447318.2014.941277">10.1080/10447318.2014.941277</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10447318.2014.941277" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intermodal%20audio-haptic%20metaphor%3A%20improvement%20of%20target%20search%20in%20abstract%20environments&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;doi=10.1080%2F10447318.2014.941277&amp;volume=30&amp;issue=11&amp;pages=921-933&amp;publication_year=2014&amp;author=Ammi%2CM&amp;author=Katz%2CBF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JN. Bailenson, N. Yee, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bailenson JN, Yee N (2008) Virtual interpersonal touch: haptic interaction and copresence in collaborative vir" /><p class="c-article-references__text" id="ref-CR6">Bailenson JN, Yee N (2008) Virtual interpersonal touch: haptic interaction and copresence in collaborative virtual environments. Multimed Tools Appl 37:5–14. doi:<a href="https://doi.org/10.1007/s11042-007-0171-2">10.1007/s11042-007-0171-2</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11042-007-0171-2" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20interpersonal%20touch%3A%20haptic%20interaction%20and%20copresence%20in%20collaborative%20virtual%20environments&amp;journal=Multimed%20Tools%20Appl&amp;doi=10.1007%2Fs11042-007-0171-2&amp;volume=37&amp;pages=5-14&amp;publication_year=2008&amp;author=Bailenson%2CJN&amp;author=Yee%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bandura A (1986) Social foundations of thought and action: a social cognitive theory. Prentice-Hall series in " /><p class="c-article-references__text" id="ref-CR7">Bandura A (1986) Social foundations of thought and action: a social cognitive theory. Prentice-Hall series in social learning theory. Prentice-Hall</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Basdogan, CH. Ho, MA. Srinivasan, M. Slater, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Basdogan C, Ho CH, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtual" /><p class="c-article-references__text" id="ref-CR8">Basdogan C, Ho CH, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput Hum Interact 7:443–460. doi:<a href="https://doi.org/10.1145/365058.365082">10.1145/365058.365082</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365082" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20study%20on%20the%20role%20of%20touch%20in%20shared%20virtual%20environments&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;doi=10.1145%2F365058.365082&amp;volume=7&amp;pages=443-460&amp;publication_year=2000&amp;author=Basdogan%2CC&amp;author=Ho%2CCH&amp;author=Srinivasan%2CMA&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DR. Begault, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, New York" /><p class="c-article-references__text" id="ref-CR9">Begault DR (1994) 3-D sound for virtual reality and multimedia. Academic Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3-D%20sound%20for%20virtual%20reality%20and%20multimedia&amp;publication_year=1994&amp;author=Begault%2CDR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SA. Bly, SR. Harrison, S. Irwin, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing en" /><p class="c-article-references__text" id="ref-CR10">Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing environment. Commun ACM 36:28–46. doi:<a href="https://doi.org/10.1145/151233.151235">10.1145/151233.151235</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F151233.151235" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Media%20spaces%3A%20bringing%20people%20together%20in%20a%20video%2C%20audio%2C%20and%20computing%20environment&amp;journal=Commun%20ACM&amp;doi=10.1145%2F151233.151235&amp;volume=36&amp;pages=28-46&amp;publication_year=1993&amp;author=Bly%2CSA&amp;author=Harrison%2CSR&amp;author=Irwin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bruseberg A, Johnson P (2001) Collaboration in the flightdeck: opportunities for interaction design. In: Techn" /><p class="c-article-references__text" id="ref-CR11">Bruseberg A, Johnson P (2001) Collaboration in the flightdeck: opportunities for interaction design. In: Technical report, Department of Computer Science, University of Bath. <a href="http://www.cs.bath.ac.uk/~anneb/collwn.pdf">http://www.cs.bath.ac.uk/~anneb/collwn.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Carroll JM, Neale DC, Isenhour PL, Rosson MB, McCrickard DS (2003) Notification and awareness: synchronizing t" /><p class="c-article-references__text" id="ref-CR12">Carroll JM, Neale DC, Isenhour PL, Rosson MB, McCrickard DS (2003) Notification and awareness: synchronizing task-oriented collaborative activity. Int J Hum Comput Stud 8:605–632</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Catlin T, Bush P, Yankelovich N (1989) Internote: extending a hypermedia framework to support annotative colla" /><p class="c-article-references__text" id="ref-CR13">Catlin T, Bush P, Yankelovich N (1989) Internote: extending a hypermedia framework to support annotative collaboration. In: Proceedings of ACM conference hypertext, ACM, New York, HYPERTEXT ’89, pp 365–378. doi:<a href="https://doi.org/10.1145/74224.74252">10.1145/74224.74252</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Chan, K. MacLean, J. McGrenere, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chan A, MacLean K, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum C" /><p class="c-article-references__text" id="ref-CR14">Chan A, MacLean K, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355. doi:<a href="https://doi.org/10.1016/j.ijhcs.2007.11.002">10.1016/j.ijhcs.2007.11.002</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2007.11.002" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Designing%20haptic%20icons%20to%20support%20collaborative%20turn-taking&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;doi=10.1016%2Fj.ijhcs.2007.11.002&amp;volume=66&amp;pages=333-355&amp;publication_year=2008&amp;author=Chan%2CA&amp;author=MacLean%2CK&amp;author=McGrenere%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chastine JW, Zhu Y (2008) The cost of supporting references in collaborative augmented reality. In: Proceeding" /><p class="c-article-references__text" id="ref-CR15">Chastine JW, Zhu Y (2008) The cost of supporting references in collaborative augmented reality. In: Proceedings of Canadian information processing society, Windsor. Graphics Interface, pp 275–282</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Cockburn, P. Weir, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion" /><p class="c-article-references__text" id="ref-CR16">Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion-oriented views. Int J Hum Comput Interact 3(11):231–255</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15327590IJHC1103_3" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20investigation%20of%20groupware%20support%20for%20collaborative%20awareness%20through%20distortion-oriented%20views&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;volume=3&amp;issue=11&amp;pages=231-255&amp;publication_year=1999&amp;author=Cockburn%2CA&amp;author=Weir%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Codella C, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Tur" /><p class="c-article-references__text" id="ref-CR17">Codella C, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: Proceedings of SIGCHI conference human factors in computing system, ACM, New York, CHI ’92, pp 329–334. doi:<a href="https://doi.org/10.1145/142750.142825">10.1145/142750.142825</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cornuet N (2009) Study of new audio-haptic coupling strategies. Master’s thesis, University of Paris-Sud 11—CN" /><p class="c-article-references__text" id="ref-CR18">Cornuet N (2009) Study of new audio-haptic coupling strategies. Master’s thesis, University of Paris-Sud 11—CNRS/LIMSI</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cycling ’74 (2010) MAX-MSP. http://cycling74.com/&#xA;                        " /><p class="c-article-references__text" id="ref-CR19">Cycling ’74 (2010) MAX-MSP. <a href="http://cycling74.com/">http://cycling74.com/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Dix, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Dix A (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6:" /><p class="c-article-references__text" id="ref-CR20">Dix A (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6:135–156. doi:<a href="https://doi.org/10.1023/A:1008635907287">10.1023/A:1008635907287</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1489284" aria-label="View reference 20 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1008635907287" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Challenges%20for%20cooperative%20work%20on%20the%20web%3A%20an%20analytical%20approach&amp;journal=Comput%20Supported%20Coop%20Work&amp;doi=10.1023%2FA%3A1008635907287&amp;volume=6&amp;pages=135-156&amp;publication_year=1997&amp;author=Dix%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Endsley, D. Garland, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Endsley M, Garland D (2000) Situation awareness analysis and measurement. LEA, Mahwak" /><p class="c-article-references__text" id="ref-CR21">Endsley M, Garland D (2000) Situation awareness analysis and measurement. LEA, Mahwak</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Situation%20awareness%20analysis%20and%20measurement&amp;publication_year=2000&amp;author=Endsley%2CM&amp;author=Garland%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Fass, M. Cameletti, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Fass A, Cameletti M (2009) The EM algorithm in a distributed computing environment for modelling environmental" /><p class="c-article-references__text" id="ref-CR22">Fass A, Cameletti M (2009) The EM algorithm in a distributed computing environment for modelling environmental space-time data. Environ Model Softw 24(9):1027–1035</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.envsoft.2009.02.009" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20EM%20algorithm%20in%20a%20distributed%20computing%20environment%20for%20modelling%20environmental%20space-time%20data&amp;journal=Environ%20Model%20Softw&amp;volume=24&amp;issue=9&amp;pages=1027-1035&amp;publication_year=2009&amp;author=Fass%2CA&amp;author=Cameletti%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative vi" /><p class="c-article-references__text" id="ref-CR23">Fraser M, Benford S, Hindmarsh J, Heath C (1999) Supporting awareness and interaction through collaborative virtual interfaces. In: Proceedings of ACM symposium user interface software andtechnology, vol 1, pp 27–36. doi:<a href="https://doi.org/10.1145/320719.322580">10.1145/320719.322580</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JJ. Gibson, " /><meta itemprop="datePublished" content="1966" /><meta itemprop="headline" content="Gibson JJ (1966) The senses considered as perceptual systems. Houghton-Mifflin, Boston" /><p class="c-article-references__text" id="ref-CR24">Gibson JJ (1966) The senses considered as perceptual systems. Houghton-Mifflin, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20senses%20considered%20as%20perceptual%20systems&amp;publication_year=1966&amp;author=Gibson%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleopera" /><p class="c-article-references__text" id="ref-CR25">Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of meeting human factors and ergonomics society. Minneapolis/St. Paul, pp 1911–1915</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutwin C, Greenberg S (1996) Workspace Awareness for Groupware. In: Conference companion human factor computin" /><p class="c-article-references__text" id="ref-CR26">Gutwin C, Greenberg S (1996) Workspace Awareness for Groupware. In: Conference companion human factor computing system common ground, ACM, Vancouver, pp 208–209</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distribu" /><p class="c-article-references__text" id="ref-CR27">Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. In: ACM transactions on computer–human interaction, vol 6, pp 243–281</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hertenstein MJ, Keltner D, App B, Bulleit B, Jaskolka A (2006) Touch communicates distinct emotions. In: Assoc" /><p class="c-article-references__text" id="ref-CR28">Hertenstein MJ, Keltner D, App B, Bulleit B, Jaskolka A (2006) Touch communicates distinct emotions. In: Association AP (ed) Emotion, vol 6, pp 528–533</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: Proceedings of international ACM " /><p class="c-article-references__text" id="ref-CR29">Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: Proceedings of international ACM SIGGROUP conference on supporting. Group work, ACM, New York, pp 258–267. doi:<a href="https://doi.org/10.1145/958160.958201">10.1145/958160.958201</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. Hutchins, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Hutchins E (1996) Cognition in the wild. The MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR30">Hutchins E (1996) Cognition in the wild. The MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognition%20in%20the%20wild&amp;publication_year=1996&amp;author=Hutchins%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Idrus Z, Abidin SZZ, Hashim R, Omar N (2010) Social awareness: the power of digital elements in collaborative " /><p class="c-article-references__text" id="ref-CR31">Idrus Z, Abidin SZZ, Hashim R, Omar N (2010) Social awareness: the power of digital elements in collaborative environment. WSEAS Trans Comput 9:644–653. <a href="http://portal.acm.org/citation.cfm?id=1852437.1852447">http://portal.acm.org/citation.cfm?id=1852437.1852447</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jensen N, Olbrich S, Nejdl W (2007) Building a collaborative virtual environment for scientific visualization." /><p class="c-article-references__text" id="ref-CR32">Jensen N, Olbrich S, Nejdl W (2007) Building a collaborative virtual environment for scientific visualization. In: Technical report Learning Lab Lower Saxony</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz B, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration t" /><p class="c-article-references__text" id="ref-CR33">Katz B, Rio E, Picinali L, Warusfel O (2008) The effect of spatialization in a data sonification exploration task. In: International conference on auditory display, Paris, pp 1–7. <a href="http://www.icad.org/node/2355">http://www.icad.org/node/2355</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Katz B, Rio E, Picinali L (2010) LIMSI spatialisation engine. International Deposit Digital Number IDDN.FR.001" /><p class="c-article-references__text" id="ref-CR34">Katz B, Rio E, Picinali L (2010) LIMSI spatialisation engine. International Deposit Digital Number IDDN.FR.001.340014.000.S.P.2010.000.31235</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kiyokawa K, Iwasa H, Takemura H, Yokoya N (1998) Collaborative immersive workspace through a shared augmented " /><p class="c-article-references__text" id="ref-CR35">Kiyokawa K, Iwasa H, Takemura H, Yokoya N (1998) Collaborative immersive workspace through a shared augmented environment. In: Proceedings of SPIE international symposium on intelligent system and advantage manufacturing, vol 3517, pp 2–13</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EM. Lidal, T. Langeland, C. Giertsen, J. Grimsgaard, R. Helland, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Lidal EM, Langeland T, Giertsen C, Grimsgaard J, Helland R (2007) A decade of increased oil recovery in virtua" /><p class="c-article-references__text" id="ref-CR36">Lidal EM, Langeland T, Giertsen C, Grimsgaard J, Helland R (2007) A decade of increased oil recovery in virtual reality. IEEE Comput Graph Appl 27:94–97. doi:<a href="https://doi.org/10.1109/MCG.2007.141">10.1109/MCG.2007.141</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2007.141" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20decade%20of%20increased%20oil%20recovery%20in%20virtual%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;doi=10.1109%2FMCG.2007.141&amp;volume=27&amp;pages=94-97&amp;publication_year=2007&amp;author=Lidal%2CEM&amp;author=Langeland%2CT&amp;author=Giertsen%2CC&amp;author=Grimsgaard%2CJ&amp;author=Helland%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McGookin DK, Brewster SA (2006) Contextual audio in haptic graph browsing. In: Proceedings of international co" /><p class="c-article-references__text" id="ref-CR37">McGookin DK, Brewster SA (2006) Contextual audio in haptic graph browsing. In: Proceedings of international conference on auditory display, London, pp 91–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Menelas BAJ, Bourdot P, Picinali L, Katz BF (2014) Non-visual identification, localization, and selection of e" /><p class="c-article-references__text" id="ref-CR38">Menelas BAJ, Bourdot P, Picinali L, Katz BF (2014) Non-visual identification, localization, and selection of entities of interest in a 3D environment. J Multimod User Interfaces, pp 1–14. doi:<a href="https://doi.org/10.1007/s12193-014-0148-1">10.1007/s12193-014-0148-1</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Negron APP, d A Jimunez A (2009) Using avatar’s nonverbal communication to monitor collaboration in a task-ori" /><p class="c-article-references__text" id="ref-CR39">Negron APP, d A Jimunez A (2009) Using avatar’s nonverbal communication to monitor collaboration in a task-oriented learning situation in a cve. In: Workshop on intelligent and innovative support for collaborative learning activities, Rhodes, Greece, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oliveira JC, Shen X, Georganas ND (2000) Collaborative virtual environment for industrial training and e-comme" /><p class="c-article-references__text" id="ref-CR40">Oliveira JC, Shen X, Georganas ND (2000) Collaborative virtual environment for industrial training and e-commerce</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio E" /><p class="c-article-references__text" id="ref-CR41">Parseihian G, Katz BFG (2012) Morphocons: a new sonification concept based on morphological earcons. J Audio Eng Soc 60(6):409–418. <a href="http://www.aes.org/e-lib/browse.cfm?elib=16355">http://www.aes.org/e-lib/browse.cfm?elib=16355</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Parseihian, C. Jouffrais, BF. Katz, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Parseihian G, Jouffrais C, Katz BF (2014) Reaching nearby sources: comparison between real and virtual sound a" /><p class="c-article-references__text" id="ref-CR42">Parseihian G, Jouffrais C, Katz BF (2014) Reaching nearby sources: comparison between real and virtual sound and visual targets. Front Neuro 8(269):1–13. doi:<a href="https://doi.org/10.3389/fnins.2014.00269">10.3389/fnins.2014.00269</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reaching%20nearby%20sources%3A%20comparison%20between%20real%20and%20virtual%20sound%20and%20visual%20targets&amp;journal=Front%20Neuro&amp;doi=10.3389%2Ffnins.2014.00269&amp;volume=8&amp;issue=269&amp;pages=1-13&amp;publication_year=2014&amp;author=Parseihian%2CG&amp;author=Jouffrais%2CC&amp;author=Katz%2CBF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pellerin R, Bouillot N, Pietkiewicz T, Wozniewski M, Settel Z, Gressier-Soudan E, Cooperstock J (2009) SoundPa" /><p class="c-article-references__text" id="ref-CR43">Pellerin R, Bouillot N, Pietkiewicz T, Wozniewski M, Settel Z, Gressier-Soudan E, Cooperstock J (2009) SoundPark: exploring ubiquitous computing through a mixed reality multi-player game experiment. In: Conference international Nouvelles Technologiesde la Repartition, p a paraitre</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Picinali L, Feakes C, Mauro D, Katz BF (2012a) Tone-2 tones discrimination task comparing audio and haptics. I" /><p class="c-article-references__text" id="ref-CR44">Picinali L, Feakes C, Mauro D, Katz BF (2012a) Tone-2 tones discrimination task comparing audio and haptics. In: IEEE international symposium on haptic audio-visual environments and games, Munich, pp 19–24, doi:<a href="https://doi.org/10.1109/HAVE.2012.6374432">10.1109/HAVE.2012.6374432</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Picinali L, Feakes C, Mauro DA, Katz BF (2012b) Spectral discrimination thresholds comparing audio and haptics" /><p class="c-article-references__text" id="ref-CR45">Picinali L, Feakes C, Mauro DA, Katz BF (2012b) Spectral discrimination thresholds comparing audio and haptics for complex stimuli. In: Magnusson C, Szymczak D, BrewsterInt S (eds)  Workshop on haptic and audio interaction design, Lund, Sweden, HAID 2012, pp 131–140,doi:<a href="https://doi.org/10.1007/978-3-642-32796-4_14">10.1007/978-3-642-32796-4_14</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EL. Sallnäs, K. Rassmus-Gröhn, C. Sjöström, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sallnäs EL, Rassmus-Gröhn K, Sjöström C (2000) Supporting presence in collaborative environments by haptic for" /><p class="c-article-references__text" id="ref-CR46">Sallnäs EL, Rassmus-Gröhn K, Sjöström C (2000) Supporting presence in collaborative environments by haptic force feedback. ACM Trans Comput Hum Interact 7:461–476. doi:<a href="https://doi.org/10.1145/365058.365086">10.1145/365058.365086</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365086" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20presence%20in%20collaborative%20environments%20by%20haptic%20force%20feedback&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;doi=10.1145%2F365058.365086&amp;volume=7&amp;pages=461-476&amp;publication_year=2000&amp;author=Salln%C3%A4s%2CEL&amp;author=Rassmus-Gr%C3%B6hn%2CK&amp;author=Sj%C3%B6str%C3%B6m%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sanchez JH (2003) Audiobattleship: blind learners collaboration through sound. In: Proceedings of ACM CHI, pp " /><p class="c-article-references__text" id="ref-CR47">Sanchez JH (2003) Audiobattleship: blind learners collaboration through sound. In: Proceedings of ACM CHI, pp 798–799</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schönstein D, Ferré L, Katz B (2008) Comparison of headphones and equalization for virtual auditory source loc" /><p class="c-article-references__text" id="ref-CR48">Schönstein D, Ferré L, Katz B (2008) Comparison of headphones and equalization for virtual auditory source localization. In: Acoustics’08. 9e Congres Franais d’Acoustique of the SFA. Paru dans: JASA, vol 123, n5, Paris, pp 1–5</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simard J, Ammi M (2010) Gesture coordination in collaborative tasks through augmented feedthrough. In: EGVE/Eu" /><p class="c-article-references__text" id="ref-CR49">Simard J, Ammi M (2010) Gesture coordination in collaborative tasks through augmented feedthrough. In: EGVE/EuroVR/VEC, pp 43–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simard J, Ammi M, Auvray M (2010) Study of synchronous and colocated collaboration for search tasks. In: Proce" /><p class="c-article-references__text" id="ref-CR50">Simard J, Ammi M, Auvray M (2010) Study of synchronous and colocated collaboration for search tasks. In: Proceedings of joint virtual reality conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Swapp, V. Pawar, C. Loscos, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Swapp D, Pawar V, Loscos C (2006) Interaction with co-located haptic feedback in virtual reality. Virtual Real" /><p class="c-article-references__text" id="ref-CR51">Swapp D, Pawar V, Loscos C (2006) Interaction with co-located haptic feedback in virtual reality. Virtual Reality 10(1):24–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-006-0027-5" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interaction%20with%20co-located%20haptic%20feedback%20in%20virtual%20reality&amp;journal=Virtual%20Reality&amp;volume=10&amp;issue=1&amp;pages=24-30&amp;publication_year=2006&amp;author=Swapp%2CD&amp;author=Pawar%2CV&amp;author=Loscos%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Victor B (2004) Structuring shared-collaborative interaction using layered and localised auditory feedback. In" /><p class="c-article-references__text" id="ref-CR52">Victor B (2004) Structuring shared-collaborative interaction using layered and localised auditory feedback. In: Sound in mobile and ubiquitous human computer interaction WS, Glasgow, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ying HY, Jonas M, Eva-Lotta S, Yngve S (2007) Auditory feedback in haptic collaborative interfaces. Int J Hum " /><p class="c-article-references__text" id="ref-CR53">Ying HY, Jonas M, Eva-Lotta S, Yngve S (2007) Auditory feedback in haptic collaborative interfaces. Int J Hum Comput Stud QC20100701</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0273-5-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">University of  Paris-Sud - CNRS/LIMSI, Orsay, France</p><p class="c-article-author-affiliation__authors-list">Mehdi Ammi</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">LIMSI-CNRS, Orsay, France</p><p class="c-article-author-affiliation__authors-list">Brian F. G. Katz</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Mehdi-Ammi"><span class="c-article-authors-search__title u-h3 js-search-name">Mehdi Ammi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mehdi+Ammi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mehdi+Ammi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mehdi+Ammi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Brian_F__G_-Katz"><span class="c-article-authors-search__title u-h3 js-search-name">Brian F. G. Katz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Brian F. G.+Katz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Brian F. G.+Katz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brian F. G.+Katz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0273-5/email/correspondent/c1/new">Mehdi Ammi</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Intermodal%20audio%E2%80%93haptic%20intermodal%20display%3A%20improvement%20of%20communication%20and%20interpersonal%20awareness%20for%20collaborative%20search%20tasks&amp;author=Mehdi%20Ammi%20et%20al&amp;contentID=10.1007%2Fs10055-015-0273-5&amp;publication=1359-4338&amp;publicationDate=2015-09-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0273-5" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0273-5" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ammi, M., Katz, B.F.G. Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks.
                    <i>Virtual Reality</i> <b>19, </b>235–252 (2015). https://doi.org/10.1007/s10055-015-0273-5</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0273-5.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-02-20">20 February 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-27">27 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-09-12">12 September 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11">November 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0273-5" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0273-5</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Audio–haptic interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative virtual environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sonification</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Search of targets</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0273-5.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=273;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

