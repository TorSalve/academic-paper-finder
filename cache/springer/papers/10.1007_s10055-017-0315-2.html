<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Augmented reality-based remote coaching for fast-paced physical task"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks,..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Augmented reality-based remote coaching for fast-paced physical task"/>

    <meta name="dc.source" content="Virtual Reality 2017 22:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2017-05-10"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2017 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks, presentation of such guidance in a way that the recipient can make immediate recognition and quick understanding can be an especially challenging problem. In this paper, we present an AR-based tele-coaching system applied to the game of tennis, called the AR coach, and explore for interface design guidelines through a user study. We have evaluated the player&#8217;s performance for instruction understanding when the coaching instruction was presented in four different modalities: (1) Visual&#8212;visual only, (2) Sound&#8212;aural only/mono, (3) 3D Sound&#8212;aural only/3D and (4) Multimodal&#8212;both visual and aural/mono. Results from the experiment suggested that, among the three, the visual-only augmentation was the most effective and least distracting for the given pace of information transfer (e.g., under every 3 s). We attribute such a result to the characteristic of the visual modality to encode and present a lot of information at once and the human&#8217;s limited capability in handling and fusing multimodal information at a relatively fast rate."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2017-05-10"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="25"/>

    <meta name="prism.endingPage" content="36"/>

    <meta name="prism.copyright" content="2017 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-017-0315-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-017-0315-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-017-0315-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-017-0315-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Augmented reality-based remote coaching for fast-paced physical task"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2018/03"/>

    <meta name="citation_online_date" content="2017/05/10"/>

    <meta name="citation_firstpage" content="25"/>

    <meta name="citation_lastpage" content="36"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-017-0315-2"/>

    <meta name="DOI" content="10.1007/s10055-017-0315-2"/>

    <meta name="citation_doi" content="10.1007/s10055-017-0315-2"/>

    <meta name="description" content="One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote exper"/>

    <meta name="dc.creator" content="Youngsun Kim"/>

    <meta name="dc.creator" content="Seokjun Hong"/>

    <meta name="dc.creator" content="Gerard Jounghyun Kim"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Avery B, Sandor C, Thomas BH (2012) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality. IEEE, pp 79&#8211;82"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Appl IEEE; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot, R Behringer, S Feiner, S Julier, B MacIntyre; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-47; citation_doi=10.1109/38.963459; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Put-that-there, voice and gesture at the graphics interface; citation_author=RA Bolt; citation_volume=14; citation_issue=3; citation_publication_date=1980; citation_pages=262-270; citation_doi=10.1145/965105.807503; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_title=The problem state: a cognitive bottleneck in multitasking; citation_author=J Borst, N Taatgen, H Rijn; citation_volume=36; citation_issue=2; citation_publication_date=2010; citation_pages=363-382; citation_doi=10.1037/a0018106; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Rev; citation_title=Serial modules in parallel: the psychological refractory period and perfect time sharing; citation_author=M Byrne, J Anderson; citation_volume=108; citation_issue=4; citation_publication_date=2001; citation_pages=847-869; citation_doi=10.1037/0033-295X.108.4.847; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_title=Dual-task methodology: some common problems; citation_inbook_title=Multiple-task performance; citation_publication_date=1991; citation_pages=101-119; citation_id=CR6; citation_author=D Damos; citation_publisher=CRC Press"/>

    <meta name="citation_reference" content="Fuchs H, Livingston MA, Raskar R, State A, Crawford JR, Rademacher R, Drake SH, Meyer AA (1998) Augmented reality visualization for laparoscopic surgery. In: Proceedings of the MICCAI, pp 934&#8211;943"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans CHI; citation_title=The integrality of speech in multimodal interfaces; citation_author=MA Grasso, DS Ebert, TW Finin; citation_volume=5; citation_issue=4; citation_publication_date=1998; citation_pages=303-325; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Proc SPIE; citation_title=Some aspects of role of audio in immersive visualization; citation_author=M Grohn, T Lokki, L Savioja, T Takala; citation_volume=4302; citation_publication_date=2001; citation_pages=13-22; citation_doi=10.1117/12.424939; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Man Mach Stud; citation_title=Gestures with speech for graphic manipulation; citation_author=A Hauptmann, P McAvinney; citation_volume=38; citation_issue=2; citation_publication_date=1993; citation_pages=231-249; citation_doi=10.1006/imms.1993.1011; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Multimodal virtual environments: response times, attention, and presence; citation_author=D Hecht, M Reiner, G Halevy; citation_volume=15; citation_issue=5; citation_publication_date=2006; citation_pages=515-523; citation_doi=10.1162/pres.15.5.515; citation_id=CR11"/>

    <meta name="citation_reference" content="Jaimes A, Sebe N (2005) Multimodal human computer interaction: a survey. In: IEEE international workshop on human computer interaction"/>

    <meta name="citation_reference" content="Kajastila R, Holsti L, H&#228;m&#228;l&#228;inen P (2016) The augmented climbing wall: high-exertion proximity interaction on a wall-sized interactive surface. In: Proceedings of the SIGCHI, pp 758&#8211;769"/>

    <meta name="citation_reference" content="Lecuyer A, Mobuchon P, Megard C, Perret J, Andriot C, Colinot JP (2003) HOMERE: a multimodal system for visually impaired people to explore virtual environments. In: Proceedings of IEEE international conference on virtual reality. IEEE, pp 251&#8211;257"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comput Interact; citation_title=Earcons and icons: their structure and common design principles; citation_author=MB Meera, AS Denise, MG Robert; citation_volume=4; citation_issue=1; citation_publication_date=1989; citation_pages=11-44; citation_doi=10.1207/s15327051hci0401_1; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=J Univers Access Inf Soc; citation_title=Augmented reality navigation systems; citation_author=W Narzt, G Pomberger, A Ferscha, D Kolb, R M&#252;ller, J Wieghardt, H H&#246;rtner, C Lindinger; citation_volume=4; citation_issue=3; citation_publication_date=2006; citation_pages=177-187; citation_doi=10.1007/s10209-005-0017-5; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Appl IEEE; citation_title=Action-and workflow-driven augmented reality for computer-aided medical procedures; citation_author=N Navab, J Traub, T Sielhorst, M Feuerstein, C Bichlmeier; citation_volume=27; citation_issue=5; citation_publication_date=2007; citation_pages=10-14; citation_doi=10.1109/MCG.2007.117; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_title=Multimodal interfaces; citation_inbook_title=Handbook of human&#8211;computer interaction; citation_publication_date=2002; citation_id=CR18; citation_author=S Oviatt; citation_publisher=Lawrence Erlbaum"/>

    <meta name="citation_reference" content="Richard P, Burdea G, Gomez D, Coiffet P (1994) A comparison of haptic, visual and auditive force feedback for deformable virtual objects. In: Proceedings of ICAT. pp 49&#8211;62"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Executive control of cognitive processes in task switching; citation_author=J Rubinstein, D Meyer, J Evans; citation_volume=27; citation_issue=4; citation_publication_date=2001; citation_pages=763-797; citation_doi=10.1037/0096-1523.27.4.763; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans CHI; citation_title=Supporting presence in collaborative environment by haptic force feedback; citation_author=E Sallnas, K Grohn, C Sjostrom; citation_volume=7; citation_issue=4; citation_publication_date=2000; citation_pages=461-476; citation_id=CR21"/>

    <meta name="citation_reference" content="Salvucci D, Tattgen N, Borst J (2009) Toward a unified theory of the multitasking continuum: from concurrent performance to task switching, interruption, and resumption. In: Proceedings of the SIGCHI, pp 1819&#8211;1828"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Comput Sci Int J; citation_title=A survey on applications of augmented reality; citation_author=A Sanna, F Manuri; citation_volume=5; citation_issue=1; citation_publication_date=2016; citation_pages=18-27; citation_id=CR23"/>

    <meta name="citation_reference" content="Schwald B, Laval BD (2003) An augmented system for training and assistance to maintenance in the industrial context. In: Proceedings of WSCG 2003"/>

    <meta name="citation_reference" content="citation_title=Multitasking in the automobile; citation_inbook_title=Applied attention; citation_publication_date=2007; citation_pages=121-133; citation_id=CR25; citation_author=D Strayer; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013. IEEE, pp 3&#8211;6"/>

    <meta name="citation_reference" content="Tonnis M, Klinker G, Plavsic M (2009) Survey and classification of head-up display presentation principles. In: Proceedings of the international ergonomics association"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factor; citation_title=Spearcons (speech-based earcons) improve navigation performance in advanced auditory menus; citation_author=BN Walker, J Lindsay, A Nance, Y Nakano, DK Palladino, T Dingler, M Jeon; citation_volume=55; citation_issue=1; citation_publication_date=2013; citation_pages=157-182; citation_doi=10.1177/0018720812450587; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_title=Information visualization: perception for design; citation_publication_date=2012; citation_id=CR29; citation_author=C Ware; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="Zheng XS, Foucault C, da Silva PM, Dasari S, Yang T, Goose S (2015) Eye-wearable technology for machine maintenance: effects of display position and hands-free operation. In: Proceedings of the SIGCHI, pp 2125&#8211;2134"/>

    <meta name="citation_author" content="Youngsun Kim"/>

    <meta name="citation_author_email" content="zyoko85@korea.ac.kr"/>

    <meta name="citation_author_institution" content="Digital Experience Laboratory, Korea University, Seoul, Korea"/>

    <meta name="citation_author" content="Seokjun Hong"/>

    <meta name="citation_author_email" content="hong921122@korea.ac.kr"/>

    <meta name="citation_author_institution" content="Digital Experience Laboratory, Korea University, Seoul, Korea"/>

    <meta name="citation_author" content="Gerard Jounghyun Kim"/>

    <meta name="citation_author_email" content="gjkim@korea.ac.kr"/>

    <meta name="citation_author_institution" content="Digital Experience Laboratory, Korea University, Seoul, Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-017-0315-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-017-0315-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Augmented reality-based remote coaching for fast-paced physical task"/>
        <meta property="og:description" content="One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks, presentation of such guidance in a way that the recipient can make immediate recognition and quick understanding can be an especially challenging problem. In this paper, we present an AR-based tele-coaching system applied to the game of tennis, called the AR coach, and explore for interface design guidelines through a user study. We have evaluated the player’s performance for instruction understanding when the coaching instruction was presented in four different modalities: (1) Visual—visual only, (2) Sound—aural only/mono, (3) 3D Sound—aural only/3D and (4) Multimodal—both visual and aural/mono. Results from the experiment suggested that, among the three, the visual-only augmentation was the most effective and least distracting for the given pace of information transfer (e.g., under every 3 s). We attribute such a result to the characteristic of the visual modality to encode and present a lot of information at once and the human’s limited capability in handling and fusing multimodal information at a relatively fast rate."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Augmented reality-based remote coaching for fast-paced physical task | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-017-0315-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Tele-coaching, Multimodal feedback, Pre-attentive recognition","kwrd":["Augmented_reality","Tele-coaching","Multimodal_feedback","Pre-attentive_recognition"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-017-0315-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-017-0315-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=315;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-017-0315-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Augmented reality-based remote coaching for fast-paced physical task
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0315-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0315-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2017-05-10" itemprop="datePublished">10 May 2017</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Augmented reality-based remote coaching for fast-paced physical task</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Youngsun-Kim" data-author-popup="auth-Youngsun-Kim">Youngsun Kim</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="0000 0001 0840 2678, grid.222754.4, Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Seokjun-Hong" data-author-popup="auth-Seokjun-Hong">Seokjun Hong</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="0000 0001 0840 2678, grid.222754.4, Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, Korea" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gerard_Jounghyun-Kim" data-author-popup="auth-Gerard_Jounghyun-Kim" data-corresp-id="c1">Gerard Jounghyun Kim<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="0000 0001 0840 2678, grid.222754.4, Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">25</span>–<span itemprop="pageEnd">36</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">640 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-017-0315-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks, presentation of such guidance in a way that the recipient can make immediate recognition and quick understanding can be an especially challenging problem. In this paper, we present an AR-based tele-coaching system applied to the game of tennis, called the AR coach, and explore for interface design guidelines through a user study. We have evaluated the player’s performance for instruction understanding when the coaching instruction was presented in four different modalities: (1) <i>Visual</i>—visual only, (2) <i>Sound</i>—aural only/mono, (3) <i>3D Sound</i>—aural only/3D and (4) <i>Multimodal</i>—both visual and aural/mono. Results from the experiment suggested that, among the three, the visual-only augmentation was the most effective and least distracting for the given pace of information transfer (e.g., under every 3 s). We attribute such a result to the characteristic of the visual modality to encode and present a lot of information at once and the human’s limited capability in handling and fusing multimodal information at a relatively fast rate.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>With the recent advances in head-mounted displays, see-through glasses and wearable computing, augmented reality (AR) is re-emerging as one of the promising interface technologies for the next generation (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. Comput Graph Appl IEEE 21(6):34–47" href="/article/10.1007/s10055-017-0315-2#ref-CR2" id="ref-link-section-d13216e349">2001</a>; Sanna and Manuri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Sanna A, Manuri F (2016) A survey on applications of augmented reality. Adv Comput Sci Int J 5(1):18–27" href="/article/10.1007/s10055-017-0315-2#ref-CR23" id="ref-link-section-d13216e352">2016</a>). One important application of augmented reality is the real-time user guidance and training by an expert remotely conveying useful information to the user, referred to as “AR-based coaching.” For example, AR-based coaching can help doctors with the remote specialists (or even intelligent agents) providing guidance and useful information during difficult medical operations (Fuchs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Fuchs H, Livingston MA, Raskar R, State A, Crawford JR, Rademacher R, Drake SH, Meyer AA (1998) Augmented reality visualization for laparoscopic surgery. In: Proceedings of the MICCAI, pp 934–943" href="/article/10.1007/s10055-017-0315-2#ref-CR7" id="ref-link-section-d13216e355">1998</a>). Such a coaching system can be used for not only an in situ guidance, but also non-live educational and training systems.</p><p>However, if the task at hand must proceed with important events occurring at a relatively fast pace, it becomes unclear whether it is even possible for the coach to compose and send sufficiently detailed and useful information, and for the recipient of the information to recognize and understand the incoming information and apply them in a timely manner. Although it is difficult to precisely define what constitutes a “fast”-paced task, examples might include many of the sporting tasks (e.g., hitting, passing and shooting the ball), musical instrument playing (hitting the keys/chords) and stock trading (buying and selling according to changing prices). In these tasks, user could typically encounter important events to respond to and require certain guidance information up to as quickly as every 1–3 s. Training for fast-paced tasks often involves a high number of trial and errors due to the inability to provide in situ (on-the-spot and at-the-moment) feedback. Thus, the interesting problem is of how to design the AR interface to support coaching of such fast-paced tasks as much as possible.</p><p>In this paper, we present an AR-based tele-coaching system for the game of tennis, called the ARCO (AR tennis coach), and explore for interface design guidelines through a usability experiment comparing three different ways of augmenting the coaching information to the player.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>AR has been applied to real-time guidance for many different tasks that are mostly relatively slow-/medium-paced, e.g., medical operations (Fuchs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Fuchs H, Livingston MA, Raskar R, State A, Crawford JR, Rademacher R, Drake SH, Meyer AA (1998) Augmented reality visualization for laparoscopic surgery. In: Proceedings of the MICCAI, pp 934–943" href="/article/10.1007/s10055-017-0315-2#ref-CR7" id="ref-link-section-d13216e372">1998</a>), geographical navigation (Narzt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented reality navigation systems. J Univers Access Inf Soc 4(3):177–187" href="/article/10.1007/s10055-017-0315-2#ref-CR16" id="ref-link-section-d13216e375">2006</a>) and machine maintenance (Schwald and Laval <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Schwald B, Laval BD (2003) An augmented system for training and assistance to maintenance in the industrial context. In: Proceedings of WSCG 2003" href="/article/10.1007/s10055-017-0315-2#ref-CR24" id="ref-link-section-d13216e378">2003</a>; Zheng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zheng XS, Foucault C, da Silva PM, Dasari S, Yang T, Goose S (2015) Eye-wearable technology for machine maintenance: effects of display position and hands-free operation. In: Proceedings of the SIGCHI, pp 2125–2134" href="/article/10.1007/s10055-017-0315-2#ref-CR30" id="ref-link-section-d13216e381">2015</a>). However, rarely has it been applied to a constantly fast-paced activity. For instance, while medical operations can be broadly categorized as a time critical task, medical AR has been used mostly to guide the overall general process rather than the minute and detailed tasks needed at the second-to-second moment. Raine et al. developed an augmented training system for indoor wall climbing (Kajastila et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Kajastila R, Holsti L, Hämäläinen P (2016) The augmented climbing wall: high-exertion proximity interaction on a wall-sized interactive surface. In: Proceedings of the SIGCHI, pp 758–769" href="/article/10.1007/s10055-017-0315-2#ref-CR13" id="ref-link-section-d13216e384">2016</a>) in which the application setting is similar to ours as a live sporting event, but actually quite slow-paced.</p><p>For fast-paced tasks, effective information presentation will be important so that the user can immediately perceive and understand them between short time intervals. For this, relying on the “pre-attentive” processing capability is a natural approach. Pre-attentive process refers to the humans’ ability to recognize visual/aural objects before the conscious attention takes over, at a rate of 1040 ms (Ware <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ware C (2012) Information visualization: perception for design, 3rd edn. Morgan Kaufmann, Burlington" href="/article/10.1007/s10055-017-0315-2#ref-CR29" id="ref-link-section-d13216e390">2012</a>). Typical pre-attentive visual features include orientation, shape, size, convexity, motion, contrast, patterns, connectedness and group. They can be effectively combined for further distinction and been applied widely for effective AR visualization, e.g., wireframe boundaries or emphasized edges (Avery et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Avery B, Sandor C, Thomas BH (2012) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality. IEEE, pp 79–82" href="/article/10.1007/s10055-017-0315-2#ref-CR1" id="ref-link-section-d13216e393">2012</a>), arrows and balloons (Tatzgern et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013. IEEE, pp 3–6" href="/article/10.1007/s10055-017-0315-2#ref-CR26" id="ref-link-section-d13216e396">2013</a>), motion profiles (Navab et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality for computer-aided medical procedures. Comput Graph Appl IEEE 27(5):10–14" href="/article/10.1007/s10055-017-0315-2#ref-CR17" id="ref-link-section-d13216e399">2007</a>). In particular, when the information is presented on a see-through glass (or similarly on a head-up display), one needs to consider the external environment with regard to visibility, occlusion (of or with important interaction objects) and multi-focus (Tonnis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Tonnis M, Klinker G, Plavsic M (2009) Survey and classification of head-up display presentation principles. In: Proceedings of the international ergonomics association" href="/article/10.1007/s10055-017-0315-2#ref-CR27" id="ref-link-section-d13216e402">2009</a>).</p><p>Another approach to effective interaction for time critical tasks is the use of and combining with non-visual modalities, such as sound and haptics/tactility. Multimodal feedback is generally known to be beneficial to task performance (Hecht et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hecht D, Reiner M, Halevy G (2006) Multimodal virtual environments: response times, attention, and presence. Presence Teleoper Virtual Environ 15(5):515–523" href="/article/10.1007/s10055-017-0315-2#ref-CR11" id="ref-link-section-d13216e408">2006</a>; Jaimes and Sebe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Jaimes A, Sebe N (2005) Multimodal human computer interaction: a survey. In: IEEE international workshop on human computer interaction" href="/article/10.1007/s10055-017-0315-2#ref-CR12" id="ref-link-section-d13216e411">2005</a>) and usability in general (Oviatt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Oviatt S (2002) Multimodal interfaces. In: Jacko J, Sears A (eds) Handbook of human–computer interaction. Lawrence Erlbaum, Mahwah" href="/article/10.1007/s10055-017-0315-2#ref-CR18" id="ref-link-section-d13216e414">2002</a>). Many synergistic multimodal interaction systems have been devised and studied (Bolt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Bolt RA (1980) Put-that-there, voice and gesture at the graphics interface. Comput Graph 14(3):262–270" href="/article/10.1007/s10055-017-0315-2#ref-CR3" id="ref-link-section-d13216e417">1980</a>; Grasso et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Grasso MA, Ebert DS, Finin TW (1998) The integrality of speech in multimodal interfaces. ACM Trans CHI 5(4):303–325" href="/article/10.1007/s10055-017-0315-2#ref-CR8" id="ref-link-section-d13216e420">1998</a>; Grohn et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Grohn M, Lokki T, Savioja L, Takala T (2001) Some aspects of role of audio in immersive visualization. Proc SPIE 4302:13–22" href="/article/10.1007/s10055-017-0315-2#ref-CR9" id="ref-link-section-d13216e424">2001</a>; Hauptmann and McAvinney <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Hauptmann A, McAvinney P (1993) Gestures with speech for graphic manipulation. Int J Man Mach Stud 38(2):231–249" href="/article/10.1007/s10055-017-0315-2#ref-CR10" id="ref-link-section-d13216e427">1993</a>; Lecuyer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Lecuyer A, Mobuchon P, Megard C, Perret J, Andriot C, Colinot JP (2003) HOMERE: a multimodal system for visually impaired people to explore virtual environments. In: Proceedings of IEEE international conference on virtual reality. IEEE, pp 251–257" href="/article/10.1007/s10055-017-0315-2#ref-CR14" id="ref-link-section-d13216e430">2003</a>; Richard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Richard P, Burdea G, Gomez D, Coiffet P (1994) A comparison of haptic, visual and auditive force feedback for deformable virtual objects. In: Proceedings of ICAT. pp 49–62" href="/article/10.1007/s10055-017-0315-2#ref-CR19" id="ref-link-section-d13216e433">1994</a>; Sallnas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sallnas E, Grohn K, Sjostrom C (2000) Supporting presence in collaborative environment by haptic force feedback. ACM Trans CHI 7(4):461–476" href="/article/10.1007/s10055-017-0315-2#ref-CR21" id="ref-link-section-d13216e436">2000</a>). However, it is not immediately clear what type of multimodal combination and presentation would be the most effective for the task of hitting the tennis ball or more generally supporting a fast interactive task at the rate of under 1–2 s.</p><p>The situation for the student (or player), who has to carry out the main task and at the same time understand the moment by moment coach instructions, can be regarded as a case of multitasking, albeit between closely related tasks. The task switch cost and interrupt overheads in multitasking have been studied (Salvucci et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Salvucci D, Tattgen N, Borst J (2009) Toward a unified theory of the multitasking continuum: from concurrent performance to task switching, interruption, and resumption. In: Proceedings of the SIGCHI, pp 1819–1828" href="/article/10.1007/s10055-017-0315-2#ref-CR22" id="ref-link-section-d13216e442">2009</a>) in the frames of memory and attention models. The switch cost is directly related to the way the multiple tasks are represented and encoded and the amount of information for the tasks. For example, Borst et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Borst J, Taatgen N, Rijn H (2010) The problem state: a cognitive bottleneck in multitasking. J Exp Psychol Learn Mem Cogn 36(2):363–382" href="/article/10.1007/s10055-017-0315-2#ref-CR4" id="ref-link-section-d13216e445">2010</a>) have shown that only one problem representation can be maintained concurrently and Rubinstein et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Rubinstein J, Meyer D, Evans J (2001) Executive control of cognitive processes in task switching. J Exp Psychol Hum Percept Perform 27(4):763–797" href="/article/10.1007/s10055-017-0315-2#ref-CR20" id="ref-link-section-d13216e448">2001</a>) have shown that the task switching overhead was reduced when task cueing was used and when switching to a familiar task. We can posit that at least the physical task of playing tennis and receiving and interpreting the coaching instruction are closely related, if not handled under a single cognitive problem space.</p><p>While the degree is debatable, the existence of parallelism in task processing is generally accepted (Byrne and Anderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Byrne M, Anderson J (2001) Serial modules in parallel: the psychological refractory period and perfect time sharing. Psychol Rev 108(4):847–869" href="/article/10.1007/s10055-017-0315-2#ref-CR5" id="ref-link-section-d13216e455">2001</a>), especially for handling multiple simple independent tasks. The ability, effect and degree of the parallel execution drops as the tasks become more difficult or when the number of tasks increases (Borst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Borst J, Taatgen N, Rijn H (2010) The problem state: a cognitive bottleneck in multitasking. J Exp Psychol Learn Mem Cogn 36(2):363–382" href="/article/10.1007/s10055-017-0315-2#ref-CR4" id="ref-link-section-d13216e458">2010</a>; Strayer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Strayer D (2007) Multitasking in the automobile. In: Kramer A, Wiegmann D, Kirlik A (eds) Applied attention. Oxford University Press, Oxford, pp 121–133" href="/article/10.1007/s10055-017-0315-2#ref-CR25" id="ref-link-section-d13216e461">2007</a>). Simply, humans are limited in their degree of parallel information processing, attentional capacity and sequential multitasking (Damos <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Damos D (1991) Dual-task methodology: some common problems. In: Damos D (ed) Multiple-task performance. CRC Press, Boca Raton, pp 101–119" href="/article/10.1007/s10055-017-0315-2#ref-CR6" id="ref-link-section-d13216e464">1991</a>). In the case of the tennis, note that the hitting activity is mostly physical (using the motor commands and ones’ limbs) understanding of the coaching instructions is mostly cognitive, and the visual channel must be shared. Thus, one possible solution is to redirect the coaching information to the aural channel.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">ARCO: AR-based (tennis) coach</h2><div class="c-article-section__content" id="Sec3-content"><p>ARCO is composed of two mobile devices, one for the coach, which sends tennis instructions (using a touch-based interface on an <i>iPad mini</i>) to the other for the student (player) to which instructions are visually or aurally augmented (using an Android-based mobile device with the <i>Epson Moverio</i> see-through glass) and an ear phone.</p><h3 class="c-article__sub-heading" id="Sec4">Coaching interface</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig1">1</a> shows the “touch”-based coaching interface. The coach would sit by the court with the <i>iPad mini</i> and send the playing instructions (e.g., forehand cross, backhand lob) by predicting the opponent’s possible move based on one’s expertise. Ideally, the instruction should be sent as early as possible (e.g., when the opponent is about to hit the ball) so that the student has sufficient time to understand the incoming instruction and make the proper response. Note that even the coach could mis-predict and send an incorrect instruction (e.g., telling the student to hit forehand when the ball actually came to the left side), but by experimental design, we eliminate such possibilities.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Touch-based interface on the <i>iPad mini</i> for the coach to send play information to the student. The <i>buttons</i> on the <i>top</i> are used only by the experiment administrator to connect the device (to the recipient) and to set and configure the instruction type (e.g., the modality type of the augmentation)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The instruction buttons were differently colored and shaped according to the target direction (e.g., lean yellow curves for lobs and stout short blue arrows for straight strokes). The background scene of the tennis court is taken from the student’s point of view by default (court side perspective was also possible as an option). Accordingly, the buttons were also laid out metaphorically, i.e., base line strokes in the bottom and lobs above. The interfaces shown on the top of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig1">1</a> were used only by the experiment administrator for system control and option configuration purpose (e.g., selecting the modality type of the augmented instruction). The selected instruction is wirelessly communicated (via Bluetooth connection) to the student’s hand-held device and either visually overlaid on the see-through glass, spoken to the student using recorded TTS voice or both. According to our preliminary test, the coaching instructions could be delivered to the student’s interface with the latency of under 16 ms, deemed sufficiently fast for the given coaching situation.</p><h3 class="c-article__sub-heading" id="Sec5">Student interface</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig2">2</a> shows what the student wore while playing tennis to receive the real-time coaching instructions. The <i>Epson Moverio</i> see-through glass (BT-200) was worn (strapped to the user’s head for a secure fit) while its Android-based control unit (124 g) was attached on the user’s preferred right or left upper arm. The see-through glass can overlay augmented information of 2 m (diagonal) at a virtual distance of 5 m with the field of view of 23° in the center of user’s visual field. An ear phone was used to hear the voice instructions. While wearing the glass was not most comfortable, it was still sufficiently light (88 g)—for playing tennis and carrying out the experimental task for 30 min.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Devices (see-through glass, mobile computer, ear phone and the button device for data collection)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec6">Instruction design</h3><p>While there are many more other possible playing instructions (such as for poach, volley, slice and smash), for our initial trial, we only used eight different instructions, as a combination of three two-level keywords: (1) “forehand” or “backhand,” (2) “left” or “right,” and (3) “stroke” or “lob.” The third keyword is used for when the instruction is for lobs (i.e., the keyword of “stroke” is omitted as the default). Thus, among the eight possible instructions, those for regular strokes (e.g., forehand (to) left) constituted the “short” instruction and the others the “long” (e.g., backhand (to) right lob).</p><p>The visual instructions were made of differently designed “arrows.” Textual instruction was not considered due to the expected time needed for comprehension, especially during heavy physical motion, and the amount of space it would occupy in the screen space. In contrast, one arrow could carry a lot of information intuitively at once by varying its length, thickness, shape of path and direction and color. For example, the diagonal white arrow shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig3">3</a> stems from the bottom right directed to the left, indicating a use of a cross forehand. The straightness of the arrow indicates a stroke shot, whereas a curved arrow a lob. To make the different arrows understood as quickly as possible, color codes were also used in a redundant fashion (e.g., white for straight stroke shots and yellow for lobs, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig3">3</a>). It seems quite possible to continue and extend the use of the arrow-based instructions for other additional play instructions (e.g., poach, smash and volley).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Examples of visual instructions used in the ARCO system. A <i>single arrow</i> varied in its shape, direction and <i>color</i> was shown to convey three qualities of the suggested tennis shot. At an outdoor tennis court (<i>top</i>) and an indoor screen tennis facility for the experiment (<i>bottom</i>) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The aural feedback (converted from the coaching touch commands) spoke of two or three simple words according to the instruction categories, e.g., “forehand-right-lob” and “backhand-left” (in the actual experiment, the words were spoken in the user’s native language). Aural feedback condition (spoken in natural language) was designed as a reference and a replication of the real-world coaching environment since in the real field, the coaches always convey their instruction via their own voice to trainees. For the same reason, auditory icons such as earcons (Meera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Meera MB, Denise AS, Robert MG (1989) Earcons and icons: their structure and common design principles. Hum Comput Interact 4(1):11–44" href="/article/10.1007/s10055-017-0315-2#ref-CR15" id="ref-link-section-d13216e611">1989</a>) or spearcons (Walker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Walker BN, Lindsay J, Nance A, Nakano Y, Palladino DK, Dingler T, Jeon M (2013) Spearcons (speech-based earcons) improve navigation performance in advanced auditory menus. Hum Factor 55(1):157–182" href="/article/10.1007/s10055-017-0315-2#ref-CR28" id="ref-link-section-d13216e614">2013</a>) are not considered in our current study.</p><p>The voice instructions were pre-recorded off-line using the Google TTS system and replayed (set at fast speed to last under one second) rather than using the TTS online (for performance reasons). Aural feedback lasted 500–600 ms for the short instructions (two words) and 600–700 ms for the long instructions (three words) to play. Shortening the play time further made the instructions difficult for the players to comprehend. If auditory icons were used, their duration would be shorter; however, users would require memorization and training to get themselves familiarized to understanding and using them.</p><p>The voice instruction could be conveyed either in mono or in 3D. In the case of 3D, the directionality of the sound “added” to the voice instruction to indicate the hitting direction (encoded by the direction of the sound) and redundantly whether a backhand or forehand was to be used (encoded by where the sound originated). Since the sound directionality encodes the hitting direction (e.g., to right or to left), the second key word is omitted from the voice feedback. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab2">2</a> shows how the 3D voice instruction was encoded, and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig4">4</a> illustrates two examples: For backhand strokes to the right (instructions 7 and 8 in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab1">1</a>), the sound feedback originated from the left rear and moved to the right front, and for forehand to the right (instructions 3 and 4), the sound traveled from the right rear to right front. Note that such traveling direction of the sound maps quite well to the actual motion of the arm to carry out the instruction (e.g., the arm actually moves from the left rear to the right front direction for the “backhand to the right”). The directionality of the sound itself did not carry the information of the third keyword (i.e., stroke vs. lob).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Voice instructions given in 3D. For example, for backhand to the right (instructions 7 and 8 in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab1">1</a>), the sound feedback originated from the left rear and moved to the right front (<i>left</i> of the figure), and for forehand to the right (instructions 3 and 4), the sound traveled from the right rear to right front (<i>right</i> of the figure)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Eight possible tennis instructions composed of two or three keywords</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0315-2/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 How 3D voice instructions were encoded</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0315-2/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>As for the case of multimodal feedback, the visual and monotype aural feedback were supplied simultaneously. As for the aural feedback, to minimize the distraction due to overly redundant feedback, the voice instruction only carried first keyword (forehand or backhand), which is considered as the most confusing instruction for tennis players. For the same reason, only aural/mono was tested as an added modality to the visual.</p><p>As indicated in the previous section, the incoming message could be incorrect due to the mistakes made by the coach. Students likewise time to time would not be able to follow the instruction for several reasons, e.g., because the instruction was neither well understood nor readable, the instruction arrived too late, etc.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">User study design</h2><div class="c-article-section__content" id="Sec7-content"><p>We put ARCO to a field test, evaluating and comparing the student performance (instruction understanding) and usability according to four different feedback conditions: (1) visual only (“<i>Visual</i>”), (2) aural only/mono (“<i>Sound</i>”), (3) aural only/3D (“<i>3D Sound</i>”) and visual + aura/mono (“<i>Multimodal</i>”). Thus, the experiment was designed as a one-factor four level within subject repeated measure. The dependent variables were the response time (to incoming instructions), hit ratio (how many instructions understood) and answers to the usability questions.</p><h3 class="c-article__sub-heading" id="Sec8">Participants</h3><p>Twenty-one paid players took part in the user study (mean age: 26.9; 12 males and 9 females). One-third of the participants had prior experience in tennis or squash and half using a see-through glass such as the Google Glass. The participants had neither vision problems nor color perception deficiencies. Eighteen participants were right-handed while three of them left-handed. Their expertise in tennis varied from novice (at least 2 years of experience) to the expert levels.</p><h3 class="c-article__sub-heading" id="Sec9">Experimental setup</h3><p>ARCO prototype as described in the previous section was used with an added feature: a button as a simple and easy way for the user to indicate whether the instruction was understood in time. The button was attached on the non-dominant hand (the unoccupied hand) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>A button, attached to the non-dominating hand, used to collect player data, e.g., understanding of the incoming instruction in time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The experiment was held at a so called, “screen tennis” facility, indoors (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig6">6</a>). We did conduct a pilot test outdoor at an actual clay tennis court, but the weather and lighting condition were not ideal for a bias-free experiment. Although the screen tennis facility would be able to send the balls to the player automatically, the balls were thrown manually (see the encircled coach A in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig6">6</a>) because the facility could not be reprogrammed for our experimental purpose.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Experimental setup (screen tennis). One coach A sends a ball to the player, and the other coach B on the sideline sends the play instruction (both synchronized). The player presses the button indicating whether the instruction was understood, and at the same time, hits back the ball</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec10">Procedure</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig6">6</a> illustrates the overall procedure of the experiment. The student player wore the ARCO system (with the added indicator button) and first practiced with the system for 5–6 min, making sure the setup was comfortable and not hindering, and the augmented instructions (in all the four modes, each for 1–2 min) were understood well during a practice play. Then in the main session, the players had to receive 12 tennis balls, for each feedback condition, in left or right at the base line of the screen tennis court. The tennis balls were given (thrown manually by coach A) such that the player would receive them comfortably (e.g., without having to run around to get to the ball) at the right and consistent timing (about every three seconds). Another coach B stood by the side line and sent out the play instruction, i.e., how to respond to the incoming shots. The sending of the instruction was synchronized with sending of the ball by mutual gestures and metronome signals between the two coaches.</p><p>The participant player was asked to press the button as soon as they fully understood the augmented information in sufficient time before or when having to hit back the ball according to the instruction. Otherwise, the player was instructed not to press the button (i.e., if not understood or if there was no time). By the use of the indicator button, it was possible to measure the response time to instruction understanding (i.e., time from when the instruction was given to the user button press) and the comprehension degree (percentage of button presses made out the whole). The players not only had to press the indicator button but also hit the ball back so that experiment reflected the actual playing situation more closely.</p><p>Each participant experienced the four different condition (<i>Visual</i>, <i>Sound</i>, <i>3D Sound</i> and <i>Multimodal</i>) in a balanced order, hitting 10 balls (coming in the left or right direction) for each treatment. The <i>Moverio</i> glass was still worn for the two <i>Sound conditions</i> to make the experiment unbiased by the operational procedure. After finishing each of the four treatments, participants filled out a general usability questionnaire answered in 7-point Likert scale (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab3">3</a>). The whole experiment took about 30 min. The experiment was carried out in indoors (screen tennis facility) with sufficient visibility of the augmented information on the see-through glass.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 General usability questionnaire</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0315-2/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">Hypotheses</h3><p>We have formulated the following initial hypotheses as for the outcome of the comparative experiment. It is projected that the AR feedback will be understood faster because the aural feedback takes at least 0.5 s to finish; then, there is an added comprehension time at the cognitive level. The <i>Visual</i> (visual-only) augmentation relies on pre-attentive features which are expected to be perceived much faster.</p><p>Despite the required length of time (0.5–0.8 s), the <i>Sounds</i> could still result in high comprehension degree because of the familiarity of the voice commands with the given pace of the incoming balls (every 3 s). As for the <i>Sound</i> versus <i>3D Sound</i>, the latter is expected to be not particularly effective because of the distraction caused by the redundancy within the same modality (even with the reduced voice instruction).</p><p>
                    <i>Multimodal</i> condition combines the advantages of the visual and familiarity of the sound and thus expected to have a synergistic effect (at least based on what has been generally reported in the related work). This hypothesis applied to both task performance and general usability.</p><p>The response time, which is measured from the time the instruction is given, will obviously vary for the both <i>Sound conditions</i> according to the length of the instruction and time to comprehend it (the longer the instruction is the longer it will take to understand). This will be less so for the <i>Visual</i> (or <i>Multimodal</i>) since the visual information is quicker to perceive and understand. However, as for the comprehension degree, since the indicator button has to be hit within the sufficient 3-s span, they will not be so different among four conditions. We summarize our hypotheses as follows.</p>
                  <h3 class="c-article__sub-heading">
                    <b>H1</b>
                  </h3>
                  <p>Players will exhibit faster response time and in the order of Multimodal (fastest) &gt; Visual &gt; Sound ≥ 3D Sound.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H2</b>
                  </h3>
                  <p>Players will exhibit different response time according to the length of the instructions (short or long) in the case of Sound and 3D Sound, but not for Visual and Multimodal.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H3</b>
                  </h3>
                  <p>Players will exhibit higher comprehension degree in the order of Multimodal (highest) &gt; Visual ≥ Sound ≥ 3D Sound.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H4</b>
                  </h3>
                  <p>Players will exhibit similar comprehension degree across the four feedback conditions regardless of the instruction lengths (short or long).</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>H5</b>
                  </h3>
                  <p>Players will exhibit higher usability in the order of Multimodal (highest) &gt; Visual ≥ Sound ≥ 3D Sound.</p>
                </div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Results</h2><div class="c-article-section__content" id="Sec12-content"><h3 class="c-article__sub-heading" id="Sec13">Response time</h3><p>A repeated measures ANOVA test was applied to analyze for any statistical differences in the response time. The analysis detected a statistically significant effect of the instruction modality conditions, <i>F</i>(3,60) = 100.816, <i>p</i> &lt; 0.05. The graphs in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig7">7</a> show the mean response times for all the instructions (short or long): <i>Visual</i> (1045 ms), <i>Sound</i> (1674 ms), <i>3D Sound</i> (1536 ms) and <i>Multimodal</i> (1116 ms). The post hoc using Bonferroni correction detected statistically significant differences in <i>Visual</i> &lt; <i>Sound</i> (<i>p</i> &lt; 0.001), <i>Visual</i> &lt; <i>3D Sound</i> (<i>p</i> &lt; 0.001) and <i>Multimodal</i> &lt; <i>Sound</i> (<i>p</i> &lt; 0.001). <i>Visual</i> generally showed the fastest response time with no different to the <i>Multimodal</i> and then followed by the <i>Sounds</i>. No statistically significant difference was found between <i>Sound</i> and <i>3D Sound</i>. Hence, H1 is only partially accepted.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Response times for all instructions across four feedback conditions. Statistically significant differences were marked <i>blue lines</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>We also evaluated the response time according to the instruction lengths (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig8">8</a>). The repeated measures ANOVA test revealed a statistically significant difference by the instruction modality condition and length, <i>F</i>(7, 140) = 107.72, <i>p</i> &lt; 0.001. The post hoc tests using the Bonferroni correction did not detect any statistically significant difference between the long and short instructions within the same modality conditions. Thus, H2 is rejected. The statistical differences exhibited among the different modality conditions obviously follow that of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig7">7</a>. Note that the sound feedback is given also in the Multimodal condition. In any case, the longer instruction (and projected longer time to comprehend the voice instruction) was not a significant factor because the addition of the third keyword was only marginal (only 100 ms longer). However, we still maintain and expect that if the instruction became much longer (e.g., total of five words or more), there will be a significant difference.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Response times by different instruction lengths across four feedback conditions. Statistically significant differences were marked by the <i>blue lines</i>. No significant differences are found within the same modality condition (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec14">Comprehension degree</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig9">9</a> shows results of the comprehension degree, number of button presses (indicating comprehension of the incoming instructions) out of the total, in percentages. The repeated measures ANOVA test revealed a statistically significant effect (<i>F</i>(3, 60) = 6.639, <i>p</i> &lt; 0.001). The post hoc using Bonferroni correction detected statistically significant differences in <i>Visual</i> &gt; <i>Sound</i> (<i>p</i> &lt; 0.001), <i>Multimodal</i> &gt; <i>Sound</i> (<i>p</i> &lt; 0.001) and <i>Multimodal</i> &gt; <i>3D Sound</i> (<i>p</i> &lt; 0.001). In general, the comprehension degree was higher in the order of <i>Multimodal</i> = <i>Visual</i>, <i>Visual</i> = <i>3D Sound</i> and <i>3D Sound</i> = <i>Sound</i>, partially supporting H3. Note that again, in terms of the comprehension degree, no differences were found between the Sound and 3D Sound.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Comprehension degree (number of button presses out of the total) by modality condition. Statistically significant differences are marked by the <i>blue lines</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Within the same modality conditions, no differences in the comprehension degree were found, similarly to the case of response time, supporting the H4 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig10">10</a>). The repeated measures ANOVA test revealed a statistically significant effect among the different modality conditions (<i>F</i>(7, 140) = 6.639, <i>p</i> &lt; 0.001), which obviously follows that of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Comprehension degree by different instruction lengths across four feedback conditions. Statistically significant differences were marked by the <i>blue lines</i>. No significant differences are found within the same modality condition (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec15">General usability</h3><p>Responses to the survey questionnaires by the players are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-017-0315-2#Fig11">11</a>. In general, although slightly different among the particular questions, the <i>Visual</i> and <i>Multimodal</i> showed higher usability than the <i>Sound</i> and <i>3D sound</i> with statistically significant differences according to the Kruskal–Wallis test and post hoc pairwise Mann–Whitney <i>U</i> test with the Bonferroni correction, but not between the two (<i>Visual</i> = <i>Multimodal</i>, <i>Sound</i> = <i>3D Sound</i>). Therefore, H5 is accepted. Again, we notice that even for the usability, the 3D spatialization was not particularly effective compared to the mono feedback. Eleven out of the twenty-one participants answered that they preferred the <i>Multimodal</i> condition the most (see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab4">4</a> for the detailed statistical figures).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Results of the usability survey (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-017-0315-2#Tab3">3</a>) for the four instruction modalities. Statistically significant differences were marked by the <i>blue lines</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-017-0315-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Detailed statistical indicator results of the Kruskal–Wallis test for the general UX survey</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-017-0315-2/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Discussion</h2><div class="c-article-section__content" id="Sec16-content"><p>We summarize the experimental results and some important observations and findings from the post-briefing interviews. It is quite clear that for both the response time and comprehension degree, there existed a marked difference between the <i>Visual</i>–<i>Multimodal</i> groups and <i>Sound</i>–<i>3D Sound</i>. As expected, the two Sound conditions showed worse performance in terms of the response time and expectedly so due to its sequential manner of conveying information (e.g., three key words). Voice as a secondary or an added modality to the visual turned out not to be of particular help, nor was the directional sound to the voice feedback.</p><p>As for the comprehension degree, many participants stated, regardless of the time taken, that the visual representation (such as the arrows) was much more intuitive and easier to understand (as also reflected in the survey responses). The post-analysis of the user behavior and player interviews confirmed that the spoken (and visual/multimodal) instructions themselves were actually well designed and easy to understand. The difference was apparent from the characteristic of the visual feedback that could provide almost instant momentary feedback and most appropriate as a feedback type in a relatively fast-paced augmented task.</p><h3 class="c-article__sub-heading" id="Sec17">Observation 1: pros and cons for visual</h3><p>Overall, the visual channel was considered the most effective because, as already suggested, it was possible to compactly encode and present multiple types of information at once (just using a variety of arrows) for instant perception on the part by the student. However, a commonly accepted, compact and intuitive visual representation may not always be possible and depend much on the given domain.</p><p>There were subjects who had complained of the classic “see-through glass problem,” i.e., the dizziness and fatigue due to having to switch the focus between looking into the real-world (e.g., the opponent, court and tennis ball) and augmented instructions. We also briefly tested a method, asking the students to focus onto the real world, playing tennis as usual, and try to perceive the visually augmented instructions using only peripheral vision. While this seemed to have reduced the aforementioned problem, students were still naturally drawn to focus on the augmented information, because the <i>Moverio</i> see-through glass displayed information in the center of the visual field with a limited field of view. Students also commented that some of the augmented instructions were too large in size obstructing the real-world view, differently from Zheng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zheng XS, Foucault C, da Silva PM, Dasari S, Yang T, Goose S (2015) Eye-wearable technology for machine maintenance: effects of display position and hands-free operation. In: Proceedings of the SIGCHI, pp 2125–2134" href="/article/10.1007/s10055-017-0315-2#ref-CR30" id="ref-link-section-d13216e2045">2015</a>) which looked at a much slower-paced machine maintenance task.</p><h3 class="c-article__sub-heading" id="Sec18">Observation 2: pros and cons for aural</h3><p>Initially, we expected that the students would favor the addition of the aural channel because the visual channel would be shared between the physical task of playing tennis and the cognitively understanding the instructions. However, the aural channel turned out to be distracting, least preferred and even tiring. Foremost, even with just three types of voiced instructions abridged to last little under a second, it was still too long to react sufficiently and unintuitive to understand clearly (compared to the visual). Yet many subject reported that the instruction was spoken too fast. The cognitive effort involved to cope with such a situation seems to have caused the observed level of distraction, fatigue and low satisfaction. We can expect, however, the aural channel would still be effective for tasks that require only short instructions (e.g., “fore (hand)” and “right”).</p><h3 class="c-article__sub-heading" id="Sec19">Observation 3: redundant feedback distracting</h3><p>The <i>Multimodal</i> feedback did not show any clear advantage over the <i>Visual</i> condition. In the fast-paced task situation, especially with demanding motor tasks such as in sports, multimodal feedback seems to have caused distraction to the user. A similar argument can be made for redundantly encoding instructions into the directionality of the voice feedback. In fact, the post-briefing revealed that many players tended to ignore the aural channel (due to distraction), being dominated by the visual.</p><h3 class="c-article__sub-heading" id="Sec20">Observation 4: coaching</h3><p>As for the coaching interface, since it was only casually designed, we did not make any in-depth usability evaluation. There is much to explore for the design of the coaching interface because it is equally important to send the instructions to the student as fast as possible. For example, our initial design had six buttons according to the types of the information for which the coach had to select three times, namely for forehand or backhand, straight or cross, and stroke or lob. This took too long to send information between the exchanged shots. Thus, we later opted for the eight button design with each button representing the specific combination so that the instruction could be sent with just one touch. However, if more instruction categories (spin, smash, approach) are added, such a design will require too many buttons. We may consider using an “arrow”-like touch gesture through which, similarly to the student’s visual augmentation, much information can be encoded at once (e.g., a “pressed” curved swipe could represent a top spin cross forward stroke).</p><p>In summary, our suggested guidelines are as follows: For fast-paced guidance tasks (e.g., with information transfer of under every three seconds),</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Design the visual feedback using intuitive pre-attentive features (e.g., animated arrows in our case) and in a compact way, encoding multiple types of information at once, for fast recognition and understanding.</p>
                    </li>
                    <li>
                      <p>Aural feedback is not ideal especially in fast-paced guidance tasks due to its sequential manner of conveying the information.</p>
                    </li>
                    <li>
                      <p>Message-based aural feedback requires conscious cognitive effort to understand its content. It can cause distraction and even fatigue.</p>
                    </li>
                    <li>
                      <p>If aural feedback has to be used, use short and well-known keywords to deliver aural feedback within the short period of time.</p>
                    </li>
                    <li>
                      <p>Avoid using the use of redundant multimodality due to the limitation of human’s capability to fuse the information in a relatively short amount of time. It can cause distraction.</p>
                    </li>
                  </ul>
</div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusion and future work</h2><div class="c-article-section__content" id="Sec21-content"><p>In this paper, we presented ARCO, an augmented reality-based coaching system for a fast-paced physical task such as tennis. Our user study showed that the compact visual augmentation would serve well, e.g., verses voice instructions that can get too long to convey in time. In the <i>Multimodal</i> condition, a number of participants stated that the sound feedback often caused distraction, resulting in no particular advantage in the user task performance. While there is a limit to how much information humans can digest and react to for different tasks with a given pace, a system like ARCO (and lessons learned) can help empower or train human users in this regard. The dual focus problem still remains and we plan to design visual augmentation methods on the visual periphery and conduct further tests. The presented study needs to be reinforced in terms of the number of subjects and better data collection. We will also continue to upgrade ARCO by adding more complex tennis instructions with other visual or multimodal feedback design. Finally, we would like to apply the principles learned in ARCO to augmenting other sports and fast-paced activities such as playing a musical instrument and stock trading.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Avery B, Sandor C, Thomas BH (2012) Improving spatial perception for augmented reality x-ray vision. In: Proce" /><p class="c-article-references__text" id="ref-CR1">Avery B, Sandor C, Thomas BH (2012) Improving spatial perception for augmented reality x-ray vision. In: Proceedings of the IEEE conference on virtual reality. IEEE, pp 79–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyre, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. " /><p class="c-article-references__text" id="ref-CR2">Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. Comput Graph Appl IEEE 21(6):34–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=Comput%20Graph%20Appl%20IEEE&amp;volume=21&amp;issue=6&amp;pages=34-47&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY&amp;author=Behringer%2CR&amp;author=Feiner%2CS&amp;author=Julier%2CS&amp;author=MacIntyre%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Bolt, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Bolt RA (1980) Put-that-there, voice and gesture at the graphics interface. Comput Graph 14(3):262–270" /><p class="c-article-references__text" id="ref-CR3">Bolt RA (1980) Put-that-there, voice and gesture at the graphics interface. Comput Graph 14(3):262–270</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F965105.807503" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Put-that-there%2C%20voice%20and%20gesture%20at%20the%20graphics%20interface&amp;journal=Comput%20Graph&amp;volume=14&amp;issue=3&amp;pages=262-270&amp;publication_year=1980&amp;author=Bolt%2CRA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Borst, N. Taatgen, H. Rijn, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Borst J, Taatgen N, Rijn H (2010) The problem state: a cognitive bottleneck in multitasking. J Exp Psychol Lea" /><p class="c-article-references__text" id="ref-CR4">Borst J, Taatgen N, Rijn H (2010) The problem state: a cognitive bottleneck in multitasking. J Exp Psychol Learn Mem Cogn 36(2):363–382</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fa0018106" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20problem%20state%3A%20a%20cognitive%20bottleneck%20in%20multitasking&amp;journal=J%20Exp%20Psychol%20Learn%20Mem%20Cogn&amp;volume=36&amp;issue=2&amp;pages=363-382&amp;publication_year=2010&amp;author=Borst%2CJ&amp;author=Taatgen%2CN&amp;author=Rijn%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Byrne, J. Anderson, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Byrne M, Anderson J (2001) Serial modules in parallel: the psychological refractory period and perfect time sh" /><p class="c-article-references__text" id="ref-CR5">Byrne M, Anderson J (2001) Serial modules in parallel: the psychological refractory period and perfect time sharing. Psychol Rev 108(4):847–869</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0033-295X.108.4.847" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Serial%20modules%20in%20parallel%3A%20the%20psychological%20refractory%20period%20and%20perfect%20time%20sharing&amp;journal=Psychol%20Rev&amp;volume=108&amp;issue=4&amp;pages=847-869&amp;publication_year=2001&amp;author=Byrne%2CM&amp;author=Anderson%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Damos, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Damos D (1991) Dual-task methodology: some common problems. In: Damos D (ed) Multiple-task performance. CRC Pr" /><p class="c-article-references__text" id="ref-CR6">Damos D (1991) Dual-task methodology: some common problems. In: Damos D (ed) Multiple-task performance. CRC Press, Boca Raton, pp 101–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple-task%20performance&amp;pages=101-119&amp;publication_year=1991&amp;author=Damos%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fuchs H, Livingston MA, Raskar R, State A, Crawford JR, Rademacher R, Drake SH, Meyer AA (1998) Augmented real" /><p class="c-article-references__text" id="ref-CR7">Fuchs H, Livingston MA, Raskar R, State A, Crawford JR, Rademacher R, Drake SH, Meyer AA (1998) Augmented reality visualization for laparoscopic surgery. In: Proceedings of the MICCAI, pp 934–943</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Grasso, DS. Ebert, TW. Finin, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Grasso MA, Ebert DS, Finin TW (1998) The integrality of speech in multimodal interfaces. ACM Trans CHI 5(4):30" /><p class="c-article-references__text" id="ref-CR8">Grasso MA, Ebert DS, Finin TW (1998) The integrality of speech in multimodal interfaces. ACM Trans CHI 5(4):303–325</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20integrality%20of%20speech%20in%20multimodal%20interfaces&amp;journal=ACM%20Trans%20CHI&amp;volume=5&amp;issue=4&amp;pages=303-325&amp;publication_year=1998&amp;author=Grasso%2CMA&amp;author=Ebert%2CDS&amp;author=Finin%2CTW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Grohn, T. Lokki, L. Savioja, T. Takala, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Grohn M, Lokki T, Savioja L, Takala T (2001) Some aspects of role of audio in immersive visualization. Proc SP" /><p class="c-article-references__text" id="ref-CR9">Grohn M, Lokki T, Savioja L, Takala T (2001) Some aspects of role of audio in immersive visualization. Proc SPIE 4302:13–22</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1117%2F12.424939" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Some%20aspects%20of%20role%20of%20audio%20in%20immersive%20visualization&amp;journal=Proc%20SPIE&amp;volume=4302&amp;pages=13-22&amp;publication_year=2001&amp;author=Grohn%2CM&amp;author=Lokki%2CT&amp;author=Savioja%2CL&amp;author=Takala%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hauptmann, P. McAvinney, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Hauptmann A, McAvinney P (1993) Gestures with speech for graphic manipulation. Int J Man Mach Stud 38(2):231–2" /><p class="c-article-references__text" id="ref-CR10">Hauptmann A, McAvinney P (1993) Gestures with speech for graphic manipulation. Int J Man Mach Stud 38(2):231–249</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fimms.1993.1011" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gestures%20with%20speech%20for%20graphic%20manipulation&amp;journal=Int%20J%20Man%20Mach%20Stud&amp;volume=38&amp;issue=2&amp;pages=231-249&amp;publication_year=1993&amp;author=Hauptmann%2CA&amp;author=McAvinney%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Hecht, M. Reiner, G. Halevy, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Hecht D, Reiner M, Halevy G (2006) Multimodal virtual environments: response times, attention, and presence. P" /><p class="c-article-references__text" id="ref-CR11">Hecht D, Reiner M, Halevy G (2006) Multimodal virtual environments: response times, attention, and presence. Presence Teleoper Virtual Environ 15(5):515–523</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.15.5.515" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20virtual%20environments%3A%20response%20times%2C%20attention%2C%20and%20presence&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=15&amp;issue=5&amp;pages=515-523&amp;publication_year=2006&amp;author=Hecht%2CD&amp;author=Reiner%2CM&amp;author=Halevy%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jaimes A, Sebe N (2005) Multimodal human computer interaction: a survey. In: IEEE international workshop on hu" /><p class="c-article-references__text" id="ref-CR12">Jaimes A, Sebe N (2005) Multimodal human computer interaction: a survey. In: IEEE international workshop on human computer interaction</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kajastila R, Holsti L, Hämäläinen P (2016) The augmented climbing wall: high-exertion proximity interaction on" /><p class="c-article-references__text" id="ref-CR13">Kajastila R, Holsti L, Hämäläinen P (2016) The augmented climbing wall: high-exertion proximity interaction on a wall-sized interactive surface. In: Proceedings of the SIGCHI, pp 758–769</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lecuyer A, Mobuchon P, Megard C, Perret J, Andriot C, Colinot JP (2003) HOMERE: a multimodal system for visual" /><p class="c-article-references__text" id="ref-CR14">Lecuyer A, Mobuchon P, Megard C, Perret J, Andriot C, Colinot JP (2003) HOMERE: a multimodal system for visually impaired people to explore virtual environments. In: Proceedings of IEEE international conference on virtual reality. IEEE, pp 251–257</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MB. Meera, AS. Denise, MG. Robert, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Meera MB, Denise AS, Robert MG (1989) Earcons and icons: their structure and common design principles. Hum Com" /><p class="c-article-references__text" id="ref-CR15">Meera MB, Denise AS, Robert MG (1989) Earcons and icons: their structure and common design principles. Hum Comput Interact 4(1):11–44</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327051hci0401_1" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Earcons%20and%20icons%3A%20their%20structure%20and%20common%20design%20principles&amp;journal=Hum%20Comput%20Interact&amp;volume=4&amp;issue=1&amp;pages=11-44&amp;publication_year=1989&amp;author=Meera%2CMB&amp;author=Denise%2CAS&amp;author=Robert%2CMG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Narzt, G. Pomberger, A. Ferscha, D. Kolb, R. Müller, J. Wieghardt, H. Hörtner, C. Lindinger, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented realit" /><p class="c-article-references__text" id="ref-CR16">Narzt W, Pomberger G, Ferscha A, Kolb D, Müller R, Wieghardt J, Hörtner H, Lindinger C (2006) Augmented reality navigation systems. J Univers Access Inf Soc 4(3):177–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10209-005-0017-5" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20navigation%20systems&amp;journal=J%20Univers%20Access%20Inf%20Soc&amp;volume=4&amp;issue=3&amp;pages=177-187&amp;publication_year=2006&amp;author=Narzt%2CW&amp;author=Pomberger%2CG&amp;author=Ferscha%2CA&amp;author=Kolb%2CD&amp;author=M%C3%BCller%2CR&amp;author=Wieghardt%2CJ&amp;author=H%C3%B6rtner%2CH&amp;author=Lindinger%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Navab, J. Traub, T. Sielhorst, M. Feuerstein, C. Bichlmeier, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality " /><p class="c-article-references__text" id="ref-CR17">Navab N, Traub J, Sielhorst T, Feuerstein M, Bichlmeier C (2007) Action-and workflow-driven augmented reality for computer-aided medical procedures. Comput Graph Appl IEEE 27(5):10–14</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2007.117" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Action-and%20workflow-driven%20augmented%20reality%20for%20computer-aided%20medical%20procedures&amp;journal=Comput%20Graph%20Appl%20IEEE&amp;volume=27&amp;issue=5&amp;pages=10-14&amp;publication_year=2007&amp;author=Navab%2CN&amp;author=Traub%2CJ&amp;author=Sielhorst%2CT&amp;author=Feuerstein%2CM&amp;author=Bichlmeier%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Oviatt, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Oviatt S (2002) Multimodal interfaces. In: Jacko J, Sears A (eds) Handbook of human–computer interaction. Lawr" /><p class="c-article-references__text" id="ref-CR18">Oviatt S (2002) Multimodal interfaces. In: Jacko J, Sears A (eds) Handbook of human–computer interaction. Lawrence Erlbaum, Mahwah</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20human%E2%80%93computer%20interaction&amp;publication_year=2002&amp;author=Oviatt%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richard P, Burdea G, Gomez D, Coiffet P (1994) A comparison of haptic, visual and auditive force feedback for " /><p class="c-article-references__text" id="ref-CR19">Richard P, Burdea G, Gomez D, Coiffet P (1994) A comparison of haptic, visual and auditive force feedback for deformable virtual objects. In: Proceedings of ICAT. pp 49–62</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Rubinstein, D. Meyer, J. Evans, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Rubinstein J, Meyer D, Evans J (2001) Executive control of cognitive processes in task switching. J Exp Psycho" /><p class="c-article-references__text" id="ref-CR20">Rubinstein J, Meyer D, Evans J (2001) Executive control of cognitive processes in task switching. J Exp Psychol Hum Percept Perform 27(4):763–797</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.27.4.763" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Executive%20control%20of%20cognitive%20processes%20in%20task%20switching&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=27&amp;issue=4&amp;pages=763-797&amp;publication_year=2001&amp;author=Rubinstein%2CJ&amp;author=Meyer%2CD&amp;author=Evans%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Sallnas, K. Grohn, C. Sjostrom, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sallnas E, Grohn K, Sjostrom C (2000) Supporting presence in collaborative environment by haptic force feedbac" /><p class="c-article-references__text" id="ref-CR21">Sallnas E, Grohn K, Sjostrom C (2000) Supporting presence in collaborative environment by haptic force feedback. ACM Trans CHI 7(4):461–476</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20presence%20in%20collaborative%20environment%20by%20haptic%20force%20feedback&amp;journal=ACM%20Trans%20CHI&amp;volume=7&amp;issue=4&amp;pages=461-476&amp;publication_year=2000&amp;author=Sallnas%2CE&amp;author=Grohn%2CK&amp;author=Sjostrom%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salvucci D, Tattgen N, Borst J (2009) Toward a unified theory of the multitasking continuum: from concurrent p" /><p class="c-article-references__text" id="ref-CR22">Salvucci D, Tattgen N, Borst J (2009) Toward a unified theory of the multitasking continuum: from concurrent performance to task switching, interruption, and resumption. In: Proceedings of the SIGCHI, pp 1819–1828</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Sanna, F. Manuri, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Sanna A, Manuri F (2016) A survey on applications of augmented reality. Adv Comput Sci Int J 5(1):18–27" /><p class="c-article-references__text" id="ref-CR23">Sanna A, Manuri F (2016) A survey on applications of augmented reality. Adv Comput Sci Int J 5(1):18–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20on%20applications%20of%20augmented%20reality&amp;journal=Adv%20Comput%20Sci%20Int%20J&amp;volume=5&amp;issue=1&amp;pages=18-27&amp;publication_year=2016&amp;author=Sanna%2CA&amp;author=Manuri%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schwald B, Laval BD (2003) An augmented system for training and assistance to maintenance in the industrial co" /><p class="c-article-references__text" id="ref-CR24">Schwald B, Laval BD (2003) An augmented system for training and assistance to maintenance in the industrial context. In: Proceedings of WSCG 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Strayer, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Strayer D (2007) Multitasking in the automobile. In: Kramer A, Wiegmann D, Kirlik A (eds) Applied attention. O" /><p class="c-article-references__text" id="ref-CR25">Strayer D (2007) Multitasking in the automobile. In: Kramer A, Wiegmann D, Kirlik A (eds) Applied attention. Oxford University Press, Oxford, pp 121–133</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Applied%20attention&amp;pages=121-133&amp;publication_year=2007&amp;author=Strayer%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceed" /><p class="c-article-references__text" id="ref-CR26">Tatzgern M, Kalkofen D, Schmalstieg D (2013) Dynamic compact visualizations for augmented reality. In: Proceedings of the IEEE conference on virtual reality 2013. IEEE, pp 3–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tonnis M, Klinker G, Plavsic M (2009) Survey and classification of head-up display presentation principles. In" /><p class="c-article-references__text" id="ref-CR27">Tonnis M, Klinker G, Plavsic M (2009) Survey and classification of head-up display presentation principles. In: Proceedings of the international ergonomics association</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BN. Walker, J. Lindsay, A. Nance, Y. Nakano, DK. Palladino, T. Dingler, M. Jeon, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Walker BN, Lindsay J, Nance A, Nakano Y, Palladino DK, Dingler T, Jeon M (2013) Spearcons (speech-based earcon" /><p class="c-article-references__text" id="ref-CR28">Walker BN, Lindsay J, Nance A, Nakano Y, Palladino DK, Dingler T, Jeon M (2013) Spearcons (speech-based earcons) improve navigation performance in advanced auditory menus. Hum Factor 55(1):157–182</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F0018720812450587" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spearcons%20%28speech-based%20earcons%29%20improve%20navigation%20performance%20in%20advanced%20auditory%20menus&amp;journal=Hum%20Factor&amp;volume=55&amp;issue=1&amp;pages=157-182&amp;publication_year=2013&amp;author=Walker%2CBN&amp;author=Lindsay%2CJ&amp;author=Nance%2CA&amp;author=Nakano%2CY&amp;author=Palladino%2CDK&amp;author=Dingler%2CT&amp;author=Jeon%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Ware, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Ware C (2012) Information visualization: perception for design, 3rd edn. Morgan Kaufmann, Burlington" /><p class="c-article-references__text" id="ref-CR29">Ware C (2012) Information visualization: perception for design, 3rd edn. Morgan Kaufmann, Burlington</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20visualization%3A%20perception%20for%20design&amp;publication_year=2012&amp;author=Ware%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zheng XS, Foucault C, da Silva PM, Dasari S, Yang T, Goose S (2015) Eye-wearable technology for machine mainte" /><p class="c-article-references__text" id="ref-CR30">Zheng XS, Foucault C, da Silva PM, Dasari S, Yang T, Goose S (2015) Eye-wearable technology for machine maintenance: effects of display position and hands-free operation. In: Proceedings of the SIGCHI, pp 2125–2134</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-017-0315-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported in part by Institute for Information &amp; communications Technology Promotion(IITP) grant funded by the Korea government(MSIP) (No. R0190-16-2011, Development of Vulnerability Discovery Technologies for IoT Software Security), and also in part by the National Research Foundation of Korea (NRF) Grant funded by the Korean Government(MSIP) (No.2011-0030079).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Digital Experience Laboratory, Korea University, Anam-dong, Seongbuk-gu, Seoul, Korea</p><p class="c-article-author-affiliation__authors-list">Youngsun Kim, Seokjun Hong &amp; Gerard Jounghyun Kim</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Youngsun-Kim"><span class="c-article-authors-search__title u-h3 js-search-name">Youngsun Kim</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Youngsun+Kim&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Youngsun+Kim" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Youngsun+Kim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Seokjun-Hong"><span class="c-article-authors-search__title u-h3 js-search-name">Seokjun Hong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Seokjun+Hong&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Seokjun+Hong" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Seokjun+Hong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gerard_Jounghyun-Kim"><span class="c-article-authors-search__title u-h3 js-search-name">Gerard Jounghyun Kim</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gerard Jounghyun+Kim&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gerard Jounghyun+Kim" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gerard Jounghyun+Kim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-017-0315-2/email/correspondent/c1/new">Gerard Jounghyun Kim</a>.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec22-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1007%2Fs10055-017-0315-2/MediaObjects/10055_2017_315_MOESM1_ESM.mp4" data-supp-info-image="">Supplementary material 1 (MP4 58803 kb)</a></h3></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Augmented%20reality-based%20remote%20coaching%20for%20fast-paced%20physical%20task&amp;author=Youngsun%20Kim%20et%20al&amp;contentID=10.1007%2Fs10055-017-0315-2&amp;publication=1359-4338&amp;publicationDate=2017-05-10&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-017-0315-2" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-017-0315-2" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kim, Y., Hong, S. &amp; Kim, G.J. Augmented reality-based remote coaching for fast-paced physical task.
                    <i>Virtual Reality</i> <b>22, </b>25–36 (2018). https://doi.org/10.1007/s10055-017-0315-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-017-0315-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-03-29">29 March 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-05-03">03 May 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-05-10">10 May 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03">March 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-017-0315-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-017-0315-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tele-coaching</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Pre-attentive recognition</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-017-0315-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=315;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

