<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Camera tracking by online learning of keypoint arrangements using LLAH"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We propose a camera-tracking method by on-line learning of keypoint arrangements in augmented reality applications. As target objects, we deal with intersection maps from GIS and text documents,..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-10-07"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We propose a camera-tracking method by on-line learning of keypoint arrangements in augmented reality applications. As target objects, we deal with intersection maps from GIS and text documents, which are not dealt with by the popular SIFT and SURF descriptors. For keypoint matching by keypoint arrangement, we use locally likely arrangement hashing (LLAH), in which the descriptors of the arrangement in a viewpoint are not invariant to the wide range of viewpoints because the arrangement is changeable with respect to viewpoints. In order to solve this problem, we propose online learning of descriptors using new configurations of keypoints at new viewpoints. The proposed method allows keypoint matching to proceed under new viewpoints. We evaluate the performance and robustness of our tracking method using view changes."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-10-07"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="109"/>

    <meta name="prism.endingPage" content="117"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0173-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0173-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0173-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0173-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2011/06"/>

    <meta name="citation_online_date" content="2010/10/07"/>

    <meta name="citation_firstpage" content="109"/>

    <meta name="citation_lastpage" content="117"/>

    <meta name="citation_article_type" content="SI: Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0173-7"/>

    <meta name="DOI" content="10.1007/s10055-010-0173-7"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0173-7"/>

    <meta name="description" content="We propose a camera-tracking method by on-line learning of keypoint arrangements in augmented reality applications. As target objects, we deal with interse"/>

    <meta name="dc.creator" content="Hideaki Uchiyama"/>

    <meta name="dc.creator" content="Hideo Saito"/>

    <meta name="dc.creator" content="Myriam Servi&#232;res"/>

    <meta name="dc.creator" content="Guillaume Moreau"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=J ACM; citation_title=An optimal algorithm for approximate nearest neighbor searching fixed dimensions; citation_author=S Arya, DM Mount, NS Netanyahu, R Silverman, A Wu; citation_volume=45; citation_publication_date=1998; citation_pages=891-923; citation_doi=10.1145/293347.293348; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=SURF: speeded up robust features; citation_author=H Bay, A Ess, T Tuytelaars, LV Gool; citation_volume=110; citation_publication_date=2008; citation_pages=346-359; citation_doi=10.1016/j.cviu.2007.09.014; citation_id=CR2"/>

    <meta name="citation_reference" content="Datar M, Indyk P, Immorlica N, Mirrokni VS (2004) Locality-sensitive hashing scheme based on p-stable distributions. In: Proceedings of SCG, pp 253&#8211;262"/>

    <meta name="citation_reference" content="Drummond T, Cipolla R (1999) Real-time tracking of complex structures with on-line camera calibration. In: Proceedings of BMVC, pp 574&#8211;583"/>

    <meta name="citation_reference" content="Fiala M (2005) Artag, a fiducial marker system using digital techniques. In: Proc. CVPR, pp 590&#8211;596"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography; citation_author=MA Fischler, RC Bolles; citation_volume=24; citation_publication_date=1981; citation_pages=381-395; citation_id=CR6"/>

    <meta name="citation_reference" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of AVC, pp 147&#8211;151"/>

    <meta name="citation_reference" content="Hull J, Erol B, Graham J, Ke Q, Kishi H, Moraleda J, Van Olst D (2007) Paper-based augmented reality. In: Proceedings of ICAT, pp 205&#8211;209"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M (1999) Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In: Proceedings of IWAR"/>

    <meta name="citation_reference" content="Klein G, Murray D (2008) Improving the agility of keyframe-based SLAM. In: Proceedings of ECCV, pp 802&#8211;815"/>

    <meta name="citation_reference" content="Kotake D, Satoh K, Uchiyama S, Yamamoto H (2007) A fast initialization method for edge-based registration using an inclination constraint. In: Proceedings of ISMAR, pp 239&#8211;248"/>

    <meta name="citation_reference" content="Lamdan Y, Wolfson H (1988) Geometric hashing: a general and efficient model-based recognition scheme. In: Proceedings of ICCV, pp 238&#8211;249"/>

    <meta name="citation_reference" content="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Proceedings of CVPR, pp 244&#8211;250"/>

    <meta name="citation_reference" content="citation_journal_title=IJCV; citation_title=Distinctive image features from scale-invariant keypoints; citation_author=DG Lowe; citation_volume=60; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=PAMI; citation_title=A performance evaluation of local descriptors; citation_author=K Mikolajczyk, C Schmid; citation_volume=27; citation_publication_date=2005; citation_pages=1615-1630; citation_doi=10.1109/TPAMI.2005.188; citation_id=CR15"/>

    <meta name="citation_reference" content="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87&#8211;94"/>

    <meta name="citation_reference" content="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541&#8211;552"/>

    <meta name="citation_reference" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Proceedings of CVPR, pp 2161&#8211;2168"/>

    <meta name="citation_reference" content="Pentenrieder K, Bade C, Doil F, Meier P (2007) Augmented reality-based factory planning&#8212;an application tailored to industrial needs. In: Proceedings of ISMAR, pp 1&#8211;9"/>

    <meta name="citation_reference" content="Rosten E, Drummond T (2006) Machine learning for high speed corner detection. In: Proceedings of ECCV, pp 430&#8211;443"/>

    <meta name="citation_reference" content="Sinha S, Frahm J, Pollefeys M, Genc Y (2006) GPU-based video feature tracking and matching. In: Proceedings of EDGE"/>

    <meta name="citation_reference" content="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: Proceedings of ISMAR, pp 95&#8211;98"/>

    <meta name="citation_reference" content="Uchiyama H, Saito H, Servi&#232;res M, Moreau G (2008) AR representation system for 3D GIS based on camera pose estimation using distribution of intersections. In: Proceedings of ICAT, pp 218&#8211;225"/>

    <meta name="citation_reference" content="Uchiyama H, Saito H, Servi&#232;res M, Moreau G (2009) AR GIS on a physical map based on map image retrieval using LLAH tracking. In: Proceedings of MVA, pp 382&#8211;385"/>

    <meta name="citation_reference" content="Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proceedings of ISMAR, pp 121&#8211;124"/>

    <meta name="citation_reference" content="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobile phones. In: Proceedings of ISMAR, pp 125&#8211;134"/>

    <meta name="citation_reference" content="Willowgarage (2010) OpenCV 2.0. 
                    http://opencv.willowgarage.com/wiki/
                    
                  
                        "/>

    <meta name="citation_author" content="Hideaki Uchiyama"/>

    <meta name="citation_author_email" content="uchiyama@hvrl.ics.keio.ac.jp"/>

    <meta name="citation_author_institution" content="Keio University, Kohoku-ku, Japan"/>

    <meta name="citation_author" content="Hideo Saito"/>

    <meta name="citation_author_email" content="saito@hvrl.ics.keio.ac.jp"/>

    <meta name="citation_author_institution" content="Keio University, Kohoku-ku, Japan"/>

    <meta name="citation_author" content="Myriam Servi&#232;res"/>

    <meta name="citation_author_email" content="myriam.servieres@ec-nantes.fr"/>

    <meta name="citation_author_institution" content="Ecole Centrale de Nantes - CERMA IRSTV, Nantes Cedex 3, France"/>

    <meta name="citation_author" content="Guillaume Moreau"/>

    <meta name="citation_author_email" content="guillaume.moreau@ec-nantes.fr"/>

    <meta name="citation_author_institution" content="Ecole Centrale de Nantes - CERMA IRSTV, Nantes Cedex 3, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0173-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0173-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications"/>
        <meta property="og:description" content="We propose a camera-tracking method by on-line learning of keypoint arrangements in augmented reality applications. As target objects, we deal with intersection maps from GIS and text documents, which are not dealt with by the popular SIFT and SURF descriptors. For keypoint matching by keypoint arrangement, we use locally likely arrangement hashing (LLAH), in which the descriptors of the arrangement in a viewpoint are not invariant to the wide range of viewpoints because the arrangement is changeable with respect to viewpoints. In order to solve this problem, we propose online learning of descriptors using new configurations of keypoints at new viewpoints. The proposed method allows keypoint matching to proceed under new viewpoints. We evaluate the performance and robustness of our tracking method using view changes."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0173-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"LLAH, Feature descriptor, Camera tracking, Augmented reality","kwrd":["LLAH","Feature_descriptor","Camera_tracking","Augmented_reality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0173-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0173-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=173;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0173-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0173-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0173-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-10-07" itemprop="datePublished">07 October 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hideaki-Uchiyama" data-author-popup="auth-Hideaki-Uchiyama" data-corresp-id="c1">Hideaki Uchiyama<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University" /><meta itemprop="address" content="grid.26091.3c, 0000000419369959, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, 223-8522, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hideo-Saito" data-author-popup="auth-Hideo-Saito">Hideo Saito</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University" /><meta itemprop="address" content="grid.26091.3c, 0000000419369959, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, 223-8522, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Myriam-Servi_res" data-author-popup="auth-Myriam-Servi_res">Myriam Servières</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ecole Centrale de Nantes - CERMA IRSTV" /><meta itemprop="address" content="grid.16068.39, 0000000122039289, Ecole Centrale de Nantes - CERMA IRSTV, Rue de la Noë, BP 92101, 44321, Nantes Cedex 3, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guillaume-Moreau" data-author-popup="auth-Guillaume-Moreau">Guillaume Moreau</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ecole Centrale de Nantes - CERMA IRSTV" /><meta itemprop="address" content="grid.16068.39, 0000000122039289, Ecole Centrale de Nantes - CERMA IRSTV, Rue de la Noë, BP 92101, 44321, Nantes Cedex 3, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">109</span>–<span itemprop="pageEnd">117</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">276 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0173-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We propose a camera-tracking method by on-line learning of keypoint arrangements in augmented reality applications. As target objects, we deal with intersection maps from GIS and text documents, which are not dealt with by the popular SIFT and SURF descriptors. For keypoint matching by keypoint arrangement, we use locally likely arrangement hashing (LLAH), in which the descriptors of the arrangement in a viewpoint are not invariant to the wide range of viewpoints because the arrangement is changeable with respect to viewpoints. In order to solve this problem, we propose online learning of descriptors using new configurations of keypoints at new viewpoints. The proposed method allows keypoint matching to proceed under new viewpoints. We evaluate the performance and robustness of our tracking method using view changes.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Camera tracking remains an open and active fundamental problem in the computer vision domain. In particular, augmented reality (AR) applications need real-time processing and robust camera tracking in order to place virtual objects in an actual scene. To meet the requirement, several approaches have been proposed.</p><p>Fiducial markers have been widely developed since a long time, and they have been used in many AR frameworks (Kato and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In: Proceedings of IWAR" href="/article/10.1007/s10055-010-0173-7#ref-CR9" id="ref-link-section-d99678e354">1999</a>; Fiala <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Fiala M (2005) Artag, a fiducial marker system using digital techniques. In: Proc. CVPR, pp 590–596" href="/article/10.1007/s10055-010-0173-7#ref-CR5" id="ref-link-section-d99678e357">2005</a>; Wagner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proceedings of ISMAR, pp 121–124" href="/article/10.1007/s10055-010-0173-7#ref-CR25" id="ref-link-section-d99678e360">2008a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobile phones. In: Proceedings of ISMAR, pp 125–134" href="/article/10.1007/s10055-010-0173-7#ref-CR26" id="ref-link-section-d99678e363">b</a>). In fact, these marker systems are used in practical applications in companies and industries (Pentenrieder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Pentenrieder K, Bade C, Doil F, Meier P (2007) Augmented reality-based factory planning—an application tailored to industrial needs. In: Proceedings of ISMAR, pp 1–9" href="/article/10.1007/s10055-010-0173-7#ref-CR19" id="ref-link-section-d99678e366">2007</a>).</p><p>Recently, the focus of research has been shifting toward the use of natural features from actual environments such as edges and feature points, because fiducial markers may not be available (for instance, because of installation permission in outdoor environments, size-based issues, and application constraints). For example, edge-based approaches can be found in model-based tracking (Drummond and Cipolla <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Drummond T, Cipolla R (1999) Real-time tracking of complex structures with on-line camera calibration. In: Proceedings of BMVC, pp 574–583" href="/article/10.1007/s10055-010-0173-7#ref-CR4" id="ref-link-section-d99678e372">1999</a>), initialization of tracking (Kotake et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kotake D, Satoh K, Uchiyama S, Yamamoto H (2007) A fast initialization method for edge-based registration using an inclination constraint. In: Proceedings of ISMAR, pp 239–248" href="/article/10.1007/s10055-010-0173-7#ref-CR11" id="ref-link-section-d99678e375">2007</a>) and visual SLAM (Klein and Murray <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Klein G, Murray D (2008) Improving the agility of keyframe-based SLAM. In: Proceedings of ECCV, pp 802–815" href="/article/10.1007/s10055-010-0173-7#ref-CR10" id="ref-link-section-d99678e378">2008</a>) using boundaries of a room and the rims of a nontextured object. The keypoint (feature point) matching-based approach is also becoming common, owing to the development of local descriptors such as SIFT (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60:91–110" href="/article/10.1007/s10055-010-0173-7#ref-CR14" id="ref-link-section-d99678e381">2004</a>). In addition, the computational complexity of this approach is drastically decreasing and allows for the implementation of the method on a mobile device with a low-speed CPU and less memory, as in Phony SIFT (Wagner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proceedings of ISMAR, pp 121–124" href="/article/10.1007/s10055-010-0173-7#ref-CR25" id="ref-link-section-d99678e384">2008a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobile phones. In: Proceedings of ISMAR, pp 125–134" href="/article/10.1007/s10055-010-0173-7#ref-CR26" id="ref-link-section-d99678e388">b</a>).</p><p>Even though these remarkable developments on local descriptors have already been achieved, these approaches cannot be applied to our target objects such as maps that include only intersections as simple circular dots (Uchiyama et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Uchiyama H, Saito H, Servières M, Moreau G (2008) AR representation system for 3D GIS based on camera pose estimation using distribution of intersections. In: Proceedings of ICAT, pp 218–225" href="/article/10.1007/s10055-010-0173-7#ref-CR23" id="ref-link-section-d99678e394">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: Proceedings of ISMAR, pp 95–98" href="/article/10.1007/s10055-010-0173-7#ref-CR22" id="ref-link-section-d99678e397">2009</a>) and text documents having locally repetitive patterns (Uchiyama and Saito <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: Proceedings of ISMAR, pp 95–98" href="/article/10.1007/s10055-010-0173-7#ref-CR22" id="ref-link-section-d99678e400">2009</a>), because rich textured objects are necessary for the descriptors. Instead of the local patch–based descriptors, the local arrangement of keypoints is shown as a distinctive descriptor in such cases.</p><p>In this study, we describe a camera-tracking method for our target objects, which cannot be tracked by traditional methods. We use LLAH (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e407">2005</a>) to describe the local arrangement of keypoints. Because the local arrangement might be modified when the viewpoint changes, we propose a method for learning the new configuration of keypoints at new viewpoints, in order to handle a large range of viewpoint changes.</p><p>The rest of this paper is arranged as follows. The next section describes related studies regarding keypoint matching. In particular, the details of LLAH are highlighted in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec3">3</a>, because LLAH is an important component in our method. Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec4">4</a> explains our main contribution to the learning process with LLAH. Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec12">5</a> demonstrates the performance and robustness of our method. Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec16">6</a> discusses our conclusions and future work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related works</h2><div class="c-article-section__content" id="Sec2-content"><p>The entire process of keypoint matching can be divided into three parts: extraction, description, and matching.</p><p>For keypoint extraction, Harris corner (Harris and Stephens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of AVC, pp 147–151" href="/article/10.1007/s10055-010-0173-7#ref-CR7" id="ref-link-section-d99678e435">1988</a>) and FAST corner (Rosten and Drummond <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rosten E, Drummond T (2006) Machine learning for high speed corner detection. In: Proceedings of ECCV, pp 430–443" href="/article/10.1007/s10055-010-0173-7#ref-CR20" id="ref-link-section-d99678e438">2006</a>) have been proposed for extracting keypoints that have a different appearance from their neighboring pixels. These methods can be applied to multiscale images to take into account scale changes. There have been several approaches for extracting scale-invariant feature points, such as the difference of Gaussians (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60:91–110" href="/article/10.1007/s10055-010-0173-7#ref-CR14" id="ref-link-section-d99678e441">2004</a>), gradient locations and orientation histograms (Mikolajczyk and Schmid <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Mikolajczyk K, Schmid C (2005) A performance evaluation of local descriptors. PAMI 27:1615–1630" href="/article/10.1007/s10055-010-0173-7#ref-CR15" id="ref-link-section-d99678e444">2005</a>), and basic Hessian-matrix approximation (Bay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bay H, Ess A, Tuytelaars T, Gool LV (2008) SURF: speeded up robust features. Comput Vis Image Underst 110:346–359" href="/article/10.1007/s10055-010-0173-7#ref-CR2" id="ref-link-section-d99678e447">2008</a>).</p><p>A keypoint descriptor is a high dimensional vector computed from the local neighbor region of the keypoint in order to construct a discriminative power. Descriptors such as SIFT (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60:91–110" href="/article/10.1007/s10055-010-0173-7#ref-CR14" id="ref-link-section-d99678e453">2004</a>) and SURF (Bay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bay H, Ess A, Tuytelaars T, Gool LV (2008) SURF: speeded up robust features. Comput Vis Image Underst 110:346–359" href="/article/10.1007/s10055-010-0173-7#ref-CR2" id="ref-link-section-d99678e456">2008</a>) have been designed to be invariant to illumination, scale, rotation, and translation changes. Because they require high computational power, several attempts have been made to accelerate the computation of such descriptors. In particular, it is important to run at interactive frame rates in order to provide real-time applications and interaction with the user in AR systems. Among them, Sinha et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sinha S, Frahm J, Pollefeys M, Genc Y (2006) GPU-based video feature tracking and matching. In: Proceedings of EDGE" href="/article/10.1007/s10055-010-0173-7#ref-CR21" id="ref-link-section-d99678e459">2006</a>) has implemented SIFT on a GPU in order to use parallel processing. Wagner et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proceedings of ISMAR, pp 121–124" href="/article/10.1007/s10055-010-0173-7#ref-CR25" id="ref-link-section-d99678e462">2008a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobile phones. In: Proceedings of ISMAR, pp 125–134" href="/article/10.1007/s10055-010-0173-7#ref-CR26" id="ref-link-section-d99678e465">b</a>) has proposed Phony SIFT, which is a mobile phone version of SIFT that removes some computational costs related to keypoint extraction and descriptor computation.</p><p>The matching of descriptors can be addressed as a nearest neighbor searching problem between high dimensional vectors. Approximate nearest neighbor is a searching method based on kd-trees and box-decomposition trees (Arya et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Arya S, Mount DM, Netanyahu NS, Silverman R, Wu A (1998) An optimal algorithm for approximate nearest neighbor searching fixed dimensions. J ACM 45:891–923" href="/article/10.1007/s10055-010-0173-7#ref-CR1" id="ref-link-section-d99678e471">1998</a>). Because a distance computation is performed for the comparison between two vectors, the retrieval cost depends on the dimension of the vector. Locality sensitive hashing (LSH) is another approximate searching method based on probabilistic dimension reduction with a hash scheme (Datar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Datar M, Indyk P, Immorlica N, Mirrokni VS (2004) Locality-sensitive hashing scheme based on p-stable distributions. In: Proceedings of SCG, pp 253–262" href="/article/10.1007/s10055-010-0173-7#ref-CR3" id="ref-link-section-d99678e474">2004</a>). The computational cost of LSH is always <i>O</i>(1), but the nearest neighbor points might not be found. The design of the hash function remains an important issue in order to efficiently store data, which requires having as few collisions as possible in the hash table. Nister and Stewenius have proposed a recursive k-means tree as a vocabulary tree for quick retrieval (Nister and Stewenius <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Proceedings of CVPR, pp 2161–2168" href="/article/10.1007/s10055-010-0173-7#ref-CR18" id="ref-link-section-d99678e480">2006</a>). Lepetit et al. have treated the matching of descriptors as a classification problem (Lepetit et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Proceedings of CVPR, pp 244–250" href="/article/10.1007/s10055-010-0173-7#ref-CR13" id="ref-link-section-d99678e483">2004</a>).</p><p>Local descriptors are well suited to match keypoints with rich texture patterns. In contrast, these descriptors cannot be applied to our targets, such as intersection maps and text documents. For example, the local textures in the intersection maps are the same because the texture is only composed of identical circular dots. In this case, descriptors such as SIFT and SURF do not work well because local areas do not have enough discriminative power to be distinct from other areas. In addition, in text documents, local textures are almost identical and cannot be described by SIFT and SURF. Instead of local patch–based descriptors, we promote the use of descriptors that consider the geometrical relationship between keypoints, which has already been proposed in studies of document image retrieval (Hull et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hull J, Erol B, Graham J, Ke Q, Kishi H, Moraleda J, Van Olst D (2007) Paper-based augmented reality. In: Proceedings of ICAT, pp 205–209" href="/article/10.1007/s10055-010-0173-7#ref-CR8" id="ref-link-section-d99678e490">2007</a>; Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e493">2005</a>).</p><p>Hull et al. have proposed to use the horizontal connectivity of word lengths as a descriptor (Hull et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Hull J, Erol B, Graham J, Ke Q, Kishi H, Moraleda J, Van Olst D (2007) Paper-based augmented reality. In: Proceedings of ICAT, pp 205–209" href="/article/10.1007/s10055-010-0173-7#ref-CR8" id="ref-link-section-d99678e499">2007</a>). Word length refers to the number of characters and is linked with the previous and next word lengths. Because word lengths are very sensitive to viewpoint changes, this descriptor is valid only when a user captures an image where the camera is orthogonal to the document and close enough to the paper. Text lines must also be parallel to the lower side of the image.</p><p>Nakai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e505">2005</a>) have proposed keypoint matching using the local arrangement of keypoints for document image retrieval, called LLAH. The objective is to quickly find a document relevant to a query image from a database containing numerous documents. LLAH is an improved method of geometric hashing (GH) (Lamdan and Wolfson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Lamdan Y, Wolfson H (1988) Geometric hashing: a general and efficient model-based recognition scheme. In: Proceedings of ICCV, pp 238–249" href="/article/10.1007/s10055-010-0173-7#ref-CR12" id="ref-link-section-d99678e508">1988</a>) in terms of memory use and computational cost. Because the computational cost of GH considering perspective distortion is <i>O</i>(<i>N</i>
                        <sup>5</sup>), where <i>N</i> is the number of keypoints in a query, it is difficult to apply it to real-time applications, such as AR systems. To solve the computational cost problem, LLAH focuses only on local geometry with neighbor keypoints. However, the descriptors of local arrangement in LLAH are invariant to the narrow range of viewpoints because the arrangement is changeable with respect to viewpoints. In order to handle a large range of viewpoints, we merge the online learning of the new configuration of keypoints into LLAH.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Descriptors in LLAH</h2><div class="c-article-section__content" id="Sec3-content"><p>In this section, we explain the descriptors in LLAH because our method is mainly based on these descriptors (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552" href="/article/10.1007/s10055-010-0173-7#ref-CR17" id="ref-link-section-d99678e531">2006</a>).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig1">1</a>, <b>x</b> is an example target keypoint. First, the <i>n</i> nearest neighbor points around <b>x</b> are selected as <b>abcdefg</b> (<i>n</i> = 7). The order to select the <i>n</i> points should be defined beforehand. For example, we select from <b>a</b> in a counterclockwise fashion based on the reference axes, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Descriptors in LLAH. The descriptors of a keypoint are computed from the combination of the ratio of two triangles</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Next, <i>m</i> points out of <i>n</i> points are selected as <b>abcde</b> (<i>m</i> = 5). From these <i>m</i> points, one descriptor is computed. Because a descriptor is computed on each combination, a keypoint has <span class="mathjax-tex">\({}_{n} C_{m}=\frac{n!}{m!(n-m)!}\)</span> descriptors.</p><p>From <i>m</i> points, <i>l</i> points are selected for computing a geometrical invariant. As the invariant, Nakai et al. selected a cross ratio as a perspective invariant (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e622">2005</a>) and a ratio of two triangles as an affine invariant (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552" href="/article/10.1007/s10055-010-0173-7#ref-CR17" id="ref-link-section-d99678e625">2006</a>). They concluded that the affine invariant was better because the perspective invariant was not stably computed. Even though the perspective invariant is utilized, the available range of keypoint matching is almost same as that by the affine invariant. In this work, we select the affine invariant (<i>l</i> = 4).</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig1">1</a>, four points are selected as <b>abcd</b> to compute a ratio of two triangles. For a hashing scheme, the value of the ratio is quantized into an index using a distribution histogram created in prior experiments (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e641">2005</a>). The histogram is segmented into the number of quantization level to assign an integer number at each segment. Because the number of the combination to select four points is equivalent to the dimension of the descriptor, the dimension is <span class="mathjax-tex">\({}_{m} C_{4}\)</span>.</p><p>For quick retrieval, a hash scheme is adopted. The descriptor is converted into an index using following equation:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \hbox{Index} = \left( \sum ^{{}_{m} C_{l}-1}_{i=0} r_{(i)} k^{i} \right) \bmod H_{\rm size} $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(r_{(i)}(i=0,1,\ldots,{}_{m} C_{l}-1)\)</span> are quantized values of geometrical invariants, <i>k</i> is the quantization level, and <i>H</i>
                        <sub>size</sub> is the pre-defined hash size. As a result, each keypoint is stored in a hash table as (Index, Document ID + Keypoint ID). The table has been pre-defined to a large size to access the element of each index by <i>O</i>(1). In addition, a keypoint database is prepared to store a 2D coordinate of each keypoint as (Document ID + Keypoint ID, 2D coordinate). In our implementation, a document ID and a keypoint ID are represented by a 32 bit ID by assigning 16 bits to both IDs.</p><p>In the hash table, a collision such that the same index is computed for different keypoints may occur. Because the discriminative power of such an index is considered to be too low, a keypoint is not stored at the index.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Proposed method</h2><div class="c-article-section__content" id="Sec4-content"><h3 class="c-article__sub-heading" id="Sec5">Target objects and their keypoints</h3><p>Our target objects for augmented reality applications are intersection maps (Uchiyama et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Uchiyama H, Saito H, Servières M, Moreau G (2008) AR representation system for 3D GIS based on camera pose estimation using distribution of intersections. In: Proceedings of ICAT, pp 218–225" href="/article/10.1007/s10055-010-0173-7#ref-CR23" id="ref-link-section-d99678e708">2008</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Uchiyama H, Saito H, Servières M, Moreau G (2009) AR GIS on a physical map based on map image retrieval using LLAH tracking. In: Proceedings of MVA, pp 382–385" href="/article/10.1007/s10055-010-0173-7#ref-CR24" id="ref-link-section-d99678e711">2009</a>) and text documents (Uchiyama and Saito <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Uchiyama H, Saito H, Servières M, Moreau G (2009) AR GIS on a physical map based on map image retrieval using LLAH tracking. In: Proceedings of MVA, pp 382–385" href="/article/10.1007/s10055-010-0173-7#ref-CR24" id="ref-link-section-d99678e714">2009</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig2">2</a>a is an example of intersection maps generated from GIS, and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig2">2</a>b is the visualization of 3D data on GIS. This application was developed to achieve a novel geo-visualization by relating GIS and paper maps. Also, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig2">2</a>c illustrates a virtual annotation system implemented as augmented reality on a document.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Target objects. <b>a</b> An example of intersection maps. <b>b</b> 3D building visualization on the intersection map. <b>c</b> Virtual annotation on a document</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In these applications, local patch–based tracking methods do not work because local texture patterns are not sufficiently distinctive. For this reason, we sought another approach using the local arrangement of keypoints as a descriptor.</p><p>For keypoint extraction, the intersections are extracted by color extraction because the intersections have a specific color, such as red. From text documents, word regions are extracted using adaptive thresholding in the same way as (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552" href="/article/10.1007/s10055-010-0173-7#ref-CR17" id="ref-link-section-d99678e760">2006</a>). A keypoint is the center of each region. Compared to normal textures, local texture patterns are similar, but keypoints are stably extracted.</p><h3 class="c-article__sub-heading" id="Sec6">Initialization</h3><p>In our applications, a camera pose with respect to a 2D printed paper (map or document) is tracked. Before pose tracking, an initial pose is estimated as an initialization. Because the initialization needs an initial hash table (descriptor database) and keypoint database, these databases are prepared as follows.</p><p>To create an intersection map, the 2D distribution of intersections is exported from GIS. The distribution is equivalent to the top view of a map. By dealing with intersections as keypoints, the initial databases are created. For a text document, a document image is prepared from a digital document, such as a PDF. The image is also regarded as the top view of the document. By extracting keypoints from the image, the initial databases are created.</p><p>Because the initial databases are equivalent to the databases from a top view, a camera needs to be set at the top view for the initialization.</p><p>The process is the same as that of document image retrieval by (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552" href="/article/10.1007/s10055-010-0173-7#ref-CR17" id="ref-link-section-d99678e777">2006</a>). In the initialization, keypoints are extracted from a captured image (keypoint extraction), and their indices (descriptors) are computed (descriptor computation). For each keypoint, a histogram of keypoint IDs is generated by retrieving keypoint IDs from the indices in the initial hash table. By selecting a peak of the histogram, each keypoint in the captured image has a corresponding keypoint in the keypoint database. In order to estimate a camera pose, a homography is computed from the correspondences, because the paper is set on a plane. Because there are outliers in the correspondences, we use RANSAC (Fischler and Bolles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Fischler MA, Bolles RC (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24:381–395" href="/article/10.1007/s10055-010-0173-7#ref-CR6" id="ref-link-section-d99678e780">1981</a>) to remove outliers and compute a refined homography.</p><h3 class="c-article__sub-heading" id="Sec7">Online learning process</h3><p>A tilted camera pose cannot be computed from the initial hash table because the keypoint matching by LLAH fails. In order to achieve wide base-line keypoint tracking, the online learning of descriptors is proposed.</p><p>The flowchart of pose tracking is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig3">3</a>. From keypoint extraction to pose estimation, the process is the same as the initialization. After pose estimation, the process moves to the descriptor update step. Because the camera pose is computed, keypoints in the keypoint database can be projected onto the image. By computing the distance between a projected keypoint and keypoints in the image, some of the keypoints in the image can get corresponding points in the keypoint database as matching by projection. For each keypoint for which correspondence is established, a descriptor update is performed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Online learning process. As well as the initialization, a camera pose is estimated by matching by LLAH. After pose estimation, the keypoints in the keypoint database are projected onto the captured image to find correspondences in the image. If a correspondence is established, a descriptor update is performed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec8">Neighbor keypoints’ selection</h3><p>As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec3">3</a>, the neighbors of each keypoint are necessary in order to compute the descriptors. If we compute the distances for all keypoints from a keypoint, the computational cost is <i>O</i>(<i>N</i>
                           <sup>2</sup>), where <i>N</i> is the number of keypoints. The computation of all possible distances would imply larger computational costs. To limit the computational costs in the neighbor keypoints’ selection, we limit the searching of candidates for distance computation to limited neighbor areas, in order to search neighbor keypoints efficiently.</p><p>As a pre-processing phase, the captured image is divided into square regions by segmenting them at a uniform interval, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig4">4</a>. When the keypoints are extracted in the captured image, we compute the region to which each keypoint belongs. In addition, each region maintains the list of keypoints included in that region.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Neighbor points selection. The image region is divided into square regions beforehand. If a keypoint is extracted in <b>a</b>, candidates of neighbor points are collected from <b>a</b> to <b>i</b>. If they are not sufficient, the candidates are collected from <b>j</b> to <b>y</b>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>When we search the neighbor keypoints of a target keypoint, we collect potential candidates from the surrounding regions. For example, if a target keypoint belongs to <b>a</b> in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig4">4</a>, the candidates are extracted from <b>abcdefghi</b>. If the number of candidates is less than <i>n</i> in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec3">3</a>, we collect more candidates from more surrounding regions. When the number is more than <i>n</i>, the neighbor points are selected from the candidates by computing each distance.</p><h3 class="c-article__sub-heading" id="Sec9">Matching by projection</h3><p>In the pose estimation, we compute homography <b>H</b> as:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[ \begin{array}{c} X\\ Y\\ 1\\ \end{array}\right] \sim {\bf H} \left[ \begin{array}{c} x\\ y\\ 1\\ \end{array}\right] $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where (<i>X</i>, <i>Y</i>) is a keypoint in the keypoint database and (<i>x</i>, <i>y</i>) is a keypoint in the image.</p><p>After the homography computation with RANSAC, we can obtain two types of outliers, as follows:</p><ul class="u-list-style-dash">
                    <li>
                      <p>Volatile keypoints by the instability of the keypoint extraction or motion blur.</p>
                    </li>
                    <li>
                      <p>Keypoints stored in the keypoint database, but their descriptors are changed because of the narrow range invariance of the descriptors.</p>
                    </li>
                  </ul><p> We ignore the former outliers because they are not useful for keypoint matching. For the latter outliers, a descriptor update is performed in order to keep these outliers as inliers.</p><p>When the homography is successfully computed, keypoints in the database can be projected onto the captured image by using inverse homography as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \left[ \begin{array}{c} x'\\ y'\\ 1\\\end{array} \right] \sim {{\bf H}^{-1}} \left[ \begin{array}{c} X\\ Y\\ 1\\ \end{array}\right] $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where (<i>x</i>′, <i>y</i>′) is a projected keypoint from the database. For each outlier in the image <b>x</b> = (<i>x</i>, <i>y</i>), we compute the distances between each projected keypoint <b>x</b>′ = (<i>x</i>′, <i>y</i>′) to find the nearest one. If the distance <span class="mathjax-tex">\(|{{\bf x}}-{{\bf x}'}|\)</span> is less than a threshold (usually two pixels), the outlier is matched with the nearest projected keypoint to assign the keypoint ID of the projected keypoint.</p><h3 class="c-article__sub-heading" id="Sec10">Descriptor update</h3><p>For all keypoints extracted from the image, <span class="mathjax-tex">\({}_{n} C_{m}\)</span> indices have already been computed in the descriptor computation. If a keypoint is an outlier, NULL is stored at some of the indices, because the indices (descriptors) were never computed. Because NULL means an empty index, we can update this index.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig5">5</a>, some of the computed indices have NULL. For these indices, we fill NULL with a keypoint ID computed from matching by projection. The updated indices can be utilized after the next frame or later, so that outliers in the current frame become inliers in matching by LLAH. If there is a keypoint ID in the indices, the update is not performed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Update of LLAH. A keypoint has the indices 14, 164, and 345. If the keypoint gets 311 as a keypoint ID from matching by projection, the keypoint ID is inserted into 164 and 345. For 14, the update is not performed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The update is performed using a threshold for the number of the inliers after RANSAC-based homography computation. If there are sufficient inliers in a frame, the update is not necessary for next frame. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec14">5.2</a>, the influence of a threshold for the hash table is discussed.</p><p>Even though we insert keypoint IDs in the hash table, the size of the hash table does not change, because it has been pre-defined to a large size to access each index by <i>O</i>(1). The pre-processing phase usually leads to many empty indices. In order to use those empty indices effectively, we insert keypoint IDs into the empty indices. If an index has already had a keypoint ID, another keypoint ID is not inserted. This computational cost is not affected by the number of inserted indices because of the property of a hash scheme.</p><p>This update helps the re-initialization of a camera pose when camera tracking fails. If there is no descriptor update, a camera should be set at the top view for the initialization, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec6">4.2</a>. With the update, a camera pose can be re-initialized by setting back the camera on the camera trajectory including the top view and last camera pose.</p><h3 class="c-article__sub-heading" id="Sec11">Parallel processing</h3><p>In order to develop AR applications, we have to reduce computational costs as much as possible for real-time processing. We thus have to use computer resources effectively. Because we have two processing units in Intel Core 2 Duo architecture, it is important to assign the same load to each processing unit. By measuring each processing time, the proper assignment is determined.</p><p>The average processing times were measured from an experiment by using 100 input images, as described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0173-7#Tab1">1</a>. In this case, the parameters for LLAH were as follows: <i>n</i> = 6, <i>m</i> = 5, and <i>l</i> = 4.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Processing time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0173-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The costs for keypoint extraction, descriptor computation, and matching by descriptor are influenced by the number of keypoints captured in the image. Because the number of extracted keypoints usually varied between 500 and 600, the times in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0173-7#Tab1">1</a> are averages, but also include a variation range. When there are many outliers, the homography computation takes time, especially to get a refined homography. We have limited the number of iterations in RANSAC to 500, thus the maximum time was 22 ms. When there are mostly inliers, this computation takes approximately 2 ms.</p><p>Given those results, we have divided our process into two parts. The first thread takes care of “Capture an image” and “Keypoint extraction” while the other does the remaining tasks.</p><p>For this parallel processing, we need two memory spaces to store keypoints. First, one memory has keypoints at the <i>t</i> frame extracted by the first thread. The other memory has keypoints at the <i>t</i> − 1 frame for the processes of the second thread. After all processes are finished in each thread, both threads are synchronized to copy the memory of the first thread to that of the second thread. If both threads work in the same way, the total cost per frame will be about 30 ms.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Experimental results</h2><div class="c-article-section__content" id="Sec12-content"><h3 class="c-article__sub-heading" id="Sec13">Influence of the parameters in LLAH for keypoint matching</h3><p>Because Nakai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e1242">2005</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552" href="/article/10.1007/s10055-010-0173-7#ref-CR17" id="ref-link-section-d99678e1245">2006</a>) evaluated only the accuracy of document image retrieval, we evaluate the influence of parameters in LLAH for keypoint matching by LLAH. In this experiment, we prepared a white paper with 100 black circular dots distributed randomly to eliminate the influence of the instability in the keypoint extraction. The initial hash table and keypoint database are prepared from a top view image.</p><p>In the parameters of LLAH, we evaluate the influence by two parameters: <i>n</i> and <i>m</i>, which mainly affect the computational costs and the accuracy. The other parameters were optimized to achieve the best result as <i>l</i> = 4, <i>k</i> = 32 and <span class="mathjax-tex">\(H_{\rm size}=2^{15}-1.\)</span> In this experiment, we tested the following combinations: (<i>n</i>, <i>m</i>) = (5, 5), (6, 5), (7, 5), (8, 5), and (7, 6).</p><p>For each combination, we apply matching by LLAH to the same video capturing the paper from a top view to an inclined view around the center of the paper. We compute the angle between the vector from the center of a document to a camera position and the document plane using the tracking with the descriptor update at every frame as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec6">4.2</a>. The number of inliers after the RANSAC-based homography computation is counted as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The number of inliers after the RANSAC-based homography computation. The combinations of (<i>n</i>, <i>m</i>) = (5, 5), (6, 5), (7, 5), (8, 5), and (7, 6) are tested to check the relationship between inliers and angles</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>First, we investigated the influence of <i>n</i> as (<i>n</i>, <i>m</i>) = (5, 5), (6, 5), (7, 5), and (8, 5). In these cases, the descriptor dimension is <span class="mathjax-tex">\({}_{5}C_{4}=5.\)</span> As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig6">6</a>, <i>n</i> = 5 got the least number of inliers. As <i>n</i> increased from 6 to 8, the number of inliers increased because the number of descriptors for a keypoint increased. However, the computational cost is increased as <span class="mathjax-tex">\({}_{5}C_{5}=1, {}_{6}C_{5}=6, {}_{7}C_{5}=21\)</span> and <span class="mathjax-tex">\({}_{8}C_{5}=56\)</span> in matching by LLAH. Because this is a trade-off, we have to select parameters depending on computer resources and accuracy requirements.</p><p>Afterward, we have examined the influence of the descriptor dimension. We compared these two combinations: (<i>n</i>, <i>m</i>) = (7, 5) and (7, 6) for which we have the following descriptor dimensions: <span class="mathjax-tex">\({}_{5}C_{4}=5\)</span> and <span class="mathjax-tex">\({}_{6}C_{4}=15\)</span>. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig6">6</a>, the result of (<i>n</i>, <i>m</i>) = (7, 6) was worse than that of (<i>n</i>, <i>m</i>) = (7, 5) because the discriminative power was too much.</p><h3 class="c-article__sub-heading" id="Sec14">Behavior of descriptor update</h3><p>In this section, we investigate the behavior of the descriptor update for camera tracking. Because the descriptor update is performed depending on a threshold as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec10">4.6</a>, the influence of a threshold is investigated.</p><p>We tested those cases: no update, update when the number of inliers is less than 20, less than 40, and update at every frame. They are applied to the video utilized in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0173-7#Sec13">5.1</a>. The LLAH parameters are as follows: <i>n</i> = 6, <i>m</i> = 5, <i>l</i> = 4, <i>k</i> = 32, and <span class="mathjax-tex">\(H_{\rm size}=2^{15}-1\)</span>.</p><p>The results are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig7">7</a>. The graph of less than 20 is overlapped with the one with no update from 0 degrees to 27 degrees. Camera tracking does not fail with the update at every frame and update when the number of inliers is less than 40. If the number of inliers is less than 20, a camera pose could not be estimated at some viewpoints. No update could track a camera pose up to 34 degrees. Compared to no update, the descriptor update contributes to camera tracking for a large range of viewpoint changes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Descriptor update with respect to a threshold. In case of no update, a camera could be tracked up to 34°. In other cases, a camera pose is tracked</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Next, we investigated the behavior of the number of indices in the hash table as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig8">8</a>. The results naturally depend on the number of the descriptor update times. However, collisions in the hash table often happen as the number of updated indices increases. In an application, update of every frame may be acceptable when we use only one document. If we use many documents, it is important to effectively update descriptors in order to add descriptors in each document. As discussed in (Nakai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94" href="/article/10.1007/s10055-010-0173-7#ref-CR16" id="ref-link-section-d99678e1489">2005</a>), the appropriate parameters can be selected from several experiments and system environment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Updated indices. The number of updated indices depends on the number of descriptor update times</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec15">Comparison with SURF</h3><p>For intersection maps, local descriptors do not obviously work because the maps have circular dots and their local textures are same. In addition, these descriptors do not work well for text documents because of their repetitive patterns. To prove that the proposed tracking method is superior than these descriptors for documents, both tracking results on a document are compared. Because SIFT cannot run in real-time for AR systems, SURF implemented on OpenCV (Willowgarage <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Willowgarage (2010) OpenCV 2.0. &#xA;                    http://opencv.willowgarage.com/wiki/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-010-0173-7#ref-CR27" id="ref-link-section-d99678e1518">2010</a>) is selected for the comparison.</p><p>In tracking by SURF, a document image is prepared and printed on a A4 paper. The image resolution is selected as <span class="mathjax-tex">\(672\times 950\)</span> because this made the best result compared to other resolutions. For each captured image, correspondences in the document image are established by the descriptors in SURF, which can be regarded as matching between the top view and the captured image. In our tracking method, a document image is tracked by descriptor update at every frame. We applied both methods to a video capturing from the top view to the inclined view around the center of a document. For each case, the number of inliers after RANSAC-based homography computation is counted as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0173-7/MediaObjects/10055_2010_173_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Comparison with SURF. The number of inliers is compared in each method. Matching by SURF failed at 32 degrees</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0173-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In case of SURF, the matching failed after 32 degrees because SURF descriptors are not invariant to the perspective distortion. In contrast, our proposed tracking succeeded at every frame and could estimate the angle of each captured image in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0173-7#Fig9">9</a>. Because our proposed method is a framework for tracking by the descriptor update, the descriptors in our tracking can be replaced with SURF.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions and future work</h2><div class="c-article-section__content" id="Sec16-content"><p>We proposed a camera-tracking method based on learning the local arrangements of keypoints for intersection maps and text documents. To describe the arrangements, we use LLAH used in the studies of document image retrieval. Because the descriptors in LLAH are not invariant to a wide range of viewpoints, we proposed a dynamic learning process of the descriptors, called tracking by the descriptor update.</p><p>In the updating process, the keypoints in the keypoint database are projected onto a captured image to establish the correspondences between the keypoints in the image and the projected keypoints as matching by projection. For each established correspondence, we insert the keypoint ID into the indices. From the experiment, the descriptor update contributes to a wide range of camera tracking.</p><p>In the future work, we have to handle a collision problem in a hash table efficiently. Because the purpose of the proposed method is to track a paper, an intersection map, or a document, the collision did not occur much. If we handle multiple papers, the collisions happen many times, and the structure of the hash table will be a list at each index. In addition, we will develop natural keypoint matching by the local arrangement of keypoints. The local arrangement may help the matching by local descriptors. The keypoint matching method will be utilized in various applications, such as SLAM.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Arya, DM. Mount, NS. Netanyahu, R. Silverman, A. Wu, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Arya S, Mount DM, Netanyahu NS, Silverman R, Wu A (1998) An optimal algorithm for approximate nearest neighbor" /><p class="c-article-references__text" id="ref-CR1">Arya S, Mount DM, Netanyahu NS, Silverman R, Wu A (1998) An optimal algorithm for approximate nearest neighbor searching fixed dimensions. J ACM 45:891–923</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1678846" aria-label="View reference 1 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1065.68650" aria-label="View reference 1 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F293347.293348" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20optimal%20algorithm%20for%20approximate%20nearest%20neighbor%20searching%20fixed%20dimensions&amp;journal=J%20ACM&amp;volume=45&amp;pages=891-923&amp;publication_year=1998&amp;author=Arya%2CS&amp;author=Mount%2CDM&amp;author=Netanyahu%2CNS&amp;author=Silverman%2CR&amp;author=Wu%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Bay, A. Ess, T. Tuytelaars, LV. Gool, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bay H, Ess A, Tuytelaars T, Gool LV (2008) SURF: speeded up robust features. Comput Vis Image Underst 110:346–" /><p class="c-article-references__text" id="ref-CR2">Bay H, Ess A, Tuytelaars T, Gool LV (2008) SURF: speeded up robust features. Comput Vis Image Underst 110:346–359</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2007.09.014" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SURF%3A%20speeded%20up%20robust%20features&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=110&amp;pages=346-359&amp;publication_year=2008&amp;author=Bay%2CH&amp;author=Ess%2CA&amp;author=Tuytelaars%2CT&amp;author=Gool%2CLV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Datar M, Indyk P, Immorlica N, Mirrokni VS (2004) Locality-sensitive hashing scheme based on p-stable distribu" /><p class="c-article-references__text" id="ref-CR3">Datar M, Indyk P, Immorlica N, Mirrokni VS (2004) Locality-sensitive hashing scheme based on p-stable distributions. In: Proceedings of SCG, pp 253–262</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Drummond T, Cipolla R (1999) Real-time tracking of complex structures with on-line camera calibration. In: Pro" /><p class="c-article-references__text" id="ref-CR4">Drummond T, Cipolla R (1999) Real-time tracking of complex structures with on-line camera calibration. In: Proceedings of BMVC, pp 574–583</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fiala M (2005) Artag, a fiducial marker system using digital techniques. In: Proc. CVPR, pp 590–596" /><p class="c-article-references__text" id="ref-CR5">Fiala M (2005) Artag, a fiducial marker system using digital techniques. In: Proc. CVPR, pp 590–596</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Fischler, RC. Bolles, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Fischler MA, Bolles RC (1981) Random sample consensus: a paradigm for model fitting with applications to image" /><p class="c-article-references__text" id="ref-CR6">Fischler MA, Bolles RC (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24:381–395</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20sample%20consensus%3A%20a%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography&amp;journal=Commun%20ACM&amp;volume=24&amp;pages=381-395&amp;publication_year=1981&amp;author=Fischler%2CMA&amp;author=Bolles%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of AVC, pp 147–151" /><p class="c-article-references__text" id="ref-CR7">Harris C, Stephens M (1988) A combined corner and edge detector. In: Proceedings of AVC, pp 147–151</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hull J, Erol B, Graham J, Ke Q, Kishi H, Moraleda J, Van Olst D (2007) Paper-based augmented reality. In: Proc" /><p class="c-article-references__text" id="ref-CR8">Hull J, Erol B, Graham J, Ke Q, Kishi H, Moraleda J, Van Olst D (2007) Paper-based augmented reality. In: Proceedings of ICAT, pp 205–209</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M (1999) Marker tracking and hmd calibration for a video-based augmented reality conferen" /><p class="c-article-references__text" id="ref-CR9">Kato H, Billinghurst M (1999) Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In: Proceedings of IWAR</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Klein G, Murray D (2008) Improving the agility of keyframe-based SLAM. In: Proceedings of ECCV, pp 802–815" /><p class="c-article-references__text" id="ref-CR10">Klein G, Murray D (2008) Improving the agility of keyframe-based SLAM. In: Proceedings of ECCV, pp 802–815</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kotake D, Satoh K, Uchiyama S, Yamamoto H (2007) A fast initialization method for edge-based registration usin" /><p class="c-article-references__text" id="ref-CR11">Kotake D, Satoh K, Uchiyama S, Yamamoto H (2007) A fast initialization method for edge-based registration using an inclination constraint. In: Proceedings of ISMAR, pp 239–248</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lamdan Y, Wolfson H (1988) Geometric hashing: a general and efficient model-based recognition scheme. In: Proc" /><p class="c-article-references__text" id="ref-CR12">Lamdan Y, Wolfson H (1988) Geometric hashing: a general and efficient model-based recognition scheme. In: Proceedings of ICCV, pp 238–249</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose es" /><p class="c-article-references__text" id="ref-CR13">Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Proceedings of CVPR, pp 244–250</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DG. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60:91–110" /><p class="c-article-references__text" id="ref-CR14">Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60:91–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=IJCV&amp;volume=60&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CDG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Mikolajczyk, C. Schmid, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Mikolajczyk K, Schmid C (2005) A performance evaluation of local descriptors. PAMI 27:1615–1630" /><p class="c-article-references__text" id="ref-CR15">Mikolajczyk K, Schmid C (2005) A performance evaluation of local descriptors. PAMI 27:1615–1630</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2005.188" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20performance%20evaluation%20of%20local%20descriptors&amp;journal=PAMI&amp;volume=27&amp;pages=1615-1630&amp;publication_year=2005&amp;author=Mikolajczyk%2CK&amp;author=Schmid%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to cam" /><p class="c-article-references__text" id="ref-CR16">Nakai T, Kise K, Iwamura M (2005) Hashing with local combinations of feature points and its application to camera-based document image retrieval: Retrieval in 0.14 second from 10,000 pages. In: Proceedings of CBDAR, pp 87–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-ba" /><p class="c-article-references__text" id="ref-CR17">Nakai T, Kise K, Iwamura M (2006) Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In: Proceedings of DAS, pp 541–552</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Proceedings of CVPR, pp 2161–216" /><p class="c-article-references__text" id="ref-CR18">Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Proceedings of CVPR, pp 2161–2168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pentenrieder K, Bade C, Doil F, Meier P (2007) Augmented reality-based factory planning—an application tailore" /><p class="c-article-references__text" id="ref-CR19">Pentenrieder K, Bade C, Doil F, Meier P (2007) Augmented reality-based factory planning—an application tailored to industrial needs. In: Proceedings of ISMAR, pp 1–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosten E, Drummond T (2006) Machine learning for high speed corner detection. In: Proceedings of ECCV, pp 430–" /><p class="c-article-references__text" id="ref-CR20">Rosten E, Drummond T (2006) Machine learning for high speed corner detection. In: Proceedings of ECCV, pp 430–443</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sinha S, Frahm J, Pollefeys M, Genc Y (2006) GPU-based video feature tracking and matching. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR21">Sinha S, Frahm J, Pollefeys M, Genc Y (2006) GPU-based video feature tracking and matching. In: Proceedings of EDGE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In:" /><p class="c-article-references__text" id="ref-CR22">Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: Proceedings of ISMAR, pp 95–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Saito H, Servières M, Moreau G (2008) AR representation system for 3D GIS based on camera pose est" /><p class="c-article-references__text" id="ref-CR23">Uchiyama H, Saito H, Servières M, Moreau G (2008) AR representation system for 3D GIS based on camera pose estimation using distribution of intersections. In: Proceedings of ICAT, pp 218–225</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Saito H, Servières M, Moreau G (2009) AR GIS on a physical map based on map image retrieval using " /><p class="c-article-references__text" id="ref-CR24">Uchiyama H, Saito H, Servières M, Moreau G (2009) AR GIS on a physical map based on map image retrieval using LLAH tracking. In: Proceedings of MVA, pp 382–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proce" /><p class="c-article-references__text" id="ref-CR25">Wagner D, Langlotz T, Schmalstieg D (2008a) Robust and unobtrusive marker tracking on mobile phones. In: Proceedings of ISMAR, pp 121–124</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobi" /><p class="c-article-references__text" id="ref-CR26">Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008b) Pose tracking from natural features on mobile phones. In: Proceedings of ISMAR, pp 125–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Willowgarage (2010) OpenCV 2.0. http://opencv.willowgarage.com/wiki/&#xA;                        " /><p class="c-article-references__text" id="ref-CR27">Willowgarage (2010) OpenCV 2.0. <a href="http://opencv.willowgarage.com/wiki/">http://opencv.willowgarage.com/wiki/</a>
                        </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0173-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We thank Dr. Julien Pilet for the discussion. This work was supported in part by Grant-in-Aid for JSPS Fellows and a Grant-in-Aid for the Global Center of Excellence for high-Level Global Cooperation for Leading-Edge Platform on Access Spaces from the Ministry of Education, Culture, Sport, Science, and Technology in Japan.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Keio University, 3-14-1 Hiyoshi, Kohoku-ku, 223-8522, Japan</p><p class="c-article-author-affiliation__authors-list">Hideaki Uchiyama &amp; Hideo Saito</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Ecole Centrale de Nantes - CERMA IRSTV, Rue de la Noë, BP 92101, 44321, Nantes Cedex 3, France</p><p class="c-article-author-affiliation__authors-list">Myriam Servières &amp; Guillaume Moreau</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Hideaki-Uchiyama"><span class="c-article-authors-search__title u-h3 js-search-name">Hideaki Uchiyama</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hideaki+Uchiyama&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hideaki+Uchiyama" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hideaki+Uchiyama%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hideo-Saito"><span class="c-article-authors-search__title u-h3 js-search-name">Hideo Saito</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hideo+Saito&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hideo+Saito" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hideo+Saito%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Myriam-Servi_res"><span class="c-article-authors-search__title u-h3 js-search-name">Myriam Servières</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Myriam+Servi%C3%A8res&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Myriam+Servi%C3%A8res" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Myriam+Servi%C3%A8res%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Guillaume-Moreau"><span class="c-article-authors-search__title u-h3 js-search-name">Guillaume Moreau</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guillaume+Moreau&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guillaume+Moreau" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guillaume+Moreau%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0173-7/email/correspondent/c1/new">Hideaki Uchiyama</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Camera%20tracking%20by%20online%20learning%20of%20keypoint%20arrangements%20using%20LLAH%20in%20augmented%20reality%20applications&amp;author=Hideaki%20Uchiyama%20et%20al&amp;contentID=10.1007%2Fs10055-010-0173-7&amp;publication=1359-4338&amp;publicationDate=2010-10-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Uchiyama, H., Saito, H., Servières, M. <i>et al.</i> Camera tracking by online learning of keypoint arrangements using LLAH in augmented reality applications.
                    <i>Virtual Reality</i> <b>15, </b>109–117 (2011). https://doi.org/10.1007/s10055-010-0173-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0173-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-20">20 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-09-23">23 September 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-10-07">07 October 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-06">June 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0173-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0173-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">LLAH</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Feature descriptor</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Camera tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0173-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=173;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

