<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Comparing head gesture, hand gesture and gamepad interfaces for answer"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-12-10"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in the context of virtual reality (VR) have been rarely studied. In the present paper, we focused on a typical VR interaction scenario&#8212;answering Yes/No questions in VR systems to compare the performance and the user preference of three types of interfaces. These interfaces included a head gesture interface, a hand gesture interface and a conventional gamepad interface. We designed a memorization task, in which participants were asked to memorize several everyday objects presented in a virtual room and later respond to questions on whether they saw a specific object through the given interfaces when these objects were absent. The performance of the interfaces was evaluated in terms of the real-time accuracy and the response time. A user interface questionnaire was also used to reveal the user preference for these interfaces. The results showed that head gesture is a very promising interface, which can be easily added to existing VR systems for answering Yes/No questions and other binary responses in virtual environments."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-12-10"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="10"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00416-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00416-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00416-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00416-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments"/>

    <meta name="citation_online_date" content="2019/12/10"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="10"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00416-7"/>

    <meta name="DOI" content="10.1007/s10055-019-00416-7"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00416-7"/>

    <meta name="description" content="A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and "/>

    <meta name="dc.creator" content="Jingbo Zhao"/>

    <meta name="dc.creator" content="Robert S. Allison"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Lang Comput; citation_title=An interactive virtual guide for the AR based visit of archaeological sites; citation_author=AF Abate, G Acampora, S Ricciardi; citation_volume=22; citation_publication_date=2011; citation_pages=415-425; citation_doi=10.1016/j.jvlc.2011.02.005; citation_id=CR1"/>

    <meta name="citation_reference" content="Cardoso JCS (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology, pp 319&#8211;320"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Intell Syst Technol; citation_title=LIBSVM: a library for support vector machines; citation_author=C Chang, C Lin; citation_volume=27; citation_issue=2; citation_publication_date=2011; citation_pages=1-27; citation_doi=10.1145/1961189.1961199; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Circuits Syst Video Technol; citation_title=Survey on 3D hand gesture recognition; citation_author=H Cheng, L Yang, Z Liu; citation_volume=26; citation_publication_date=2016; citation_pages=1659-1673; citation_doi=10.1109/TCSVT.2015.2469551; citation_id=CR3"/>

    <meta name="citation_reference" content="Coomer N, Bullard S, Clinton W, Williams B (2018) Evaluating the effects of four VR locomotion methods: joystick, arm-cycling, point-tugging, and teleporting. In: Proceedings of the 15th ACM symposium on applied perception, pp 7:1&#8211;7:8"/>

    <meta name="citation_reference" content="Kitson A, Hashemian AM, Stepanova ER, Kruijff E, Riecke BE (2017) Comparing leaning-based motion cueing interfaces for virtual reality locomotion. In: 2017 IEEE symposium on 3D user interfaces, pp 73&#8211;82"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Patt Recogn Artif Intell; citation_title=A survey of applications and human motion recognition with microsoft kinect; citation_author=R Lun, W Zhao; citation_volume=29; citation_publication_date=2015; citation_pages=1555008; citation_doi=10.1142/S0218001415550083; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Multimed Tools Appl; citation_title=Hand gesture recognition with jointly calibrated Leap Motion and depth sensor; citation_author=G Marin, F Dominio, P Zanuttigh; citation_volume=75; citation_publication_date=2016; citation_pages=14991-15015; citation_doi=10.1007/s11042-015-2451-6; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell; citation_title=Head gestures for perceptual interfaces: the role of context in improving recognition; citation_author=L-P Morency, C Sidner, C Lee, T Darrell; citation_volume=171; citation_publication_date=2007; citation_pages=568-585; citation_doi=10.1016/j.artint.2007.04.003; citation_id=CR8"/>

    <meta name="citation_reference" content="Morimoto C, Yacoob Y, Davis L (1996) Recognition of head gestures using hidden Markov models. In: Proceedings of 13th international conference on pattern recognition, pp 461&#8211;465"/>

    <meta name="citation_reference" content="Nabiyouni M, Saktheeswaran A, Bowman DA, Karanth A (2015) Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality. In: 2015 IEEE symposium on 3D user interfaces, pp 3&#8211;10"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=A tutorial on hidden Markov models and selected applications in speech recognition; citation_author=LR Rabiner; citation_volume=77; citation_publication_date=1989; citation_pages=257-286; citation_doi=10.1109/5.18626; citation_id=CR11"/>

    <meta name="citation_reference" content="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Proceedings of the 1992 symposium on interactive 3D graphics, pp 189&#8211;192"/>

    <meta name="citation_reference" content="citation_title=Robust Head Gestures Recognition for Assistive Technology; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2014; citation_pages=152-161; citation_id=CR13; citation_author=Juan R. Terven; citation_author=Joaquin Salas; citation_author=Bogdan Raducanu; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="Wille M, Wischniewski S (2015) Influence of head mounted display hardware on performance and strain. In: Proceedings of the HFES annual meeting"/>

    <meta name="citation_reference" content="Yan Z, Lindeman RW, Dey A (2016) Let your fingers do the walking: a unified approach for efficient short-, medium-, and long-distance travel in VR. In: 2016 IEEE symposium on 3D user interfaces (3DUI), pp 27&#8211;30"/>

    <meta name="citation_reference" content="Zhao J, Allison RS (2017) Real-time head gesture recognition on head-mounted displays using cascaded hidden Markov models. In: 2017 IEEE international conference on systems, man, and cybernetics (SMC), pp 2361&#8211;2366"/>

    <meta name="citation_reference" content="Zielasko D, Horn S, Freitag S, Weyers B, Kuhlen TW (2016) Evaluation of hands-free HMD-based navigation techniques for immersive data analysis. In: 2016 IEEE symposium on 3D user interfaces, pp 113&#8211;119"/>

    <meta name="citation_author" content="Jingbo Zhao"/>

    <meta name="citation_author_email" content="jingbo@eecs.yorku.ca"/>

    <meta name="citation_author_institution" content="College of Information and Electrical Engineering, China Agricultural University, Beijing, China"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Science, York University, Toronto, Canada"/>

    <meta name="citation_author" content="Robert S. Allison"/>

    <meta name="citation_author_email" content="allison@eecs.yorku.ca"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Science, York University, Toronto, Canada"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00416-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00416-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments"/>
        <meta property="og:description" content="A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in the context of virtual reality (VR) have been rarely studied. In the present paper, we focused on a typical VR interaction scenario—answering Yes/No questions in VR systems to compare the performance and the user preference of three types of interfaces. These interfaces included a head gesture interface, a hand gesture interface and a conventional gamepad interface. We designed a memorization task, in which participants were asked to memorize several everyday objects presented in a virtual room and later respond to questions on whether they saw a specific object through the given interfaces when these objects were absent. The performance of the interfaces was evaluated in terms of the real-time accuracy and the response time. A user interface questionnaire was also used to reveal the user preference for these interfaces. The results showed that head gesture is a very promising interface, which can be easily added to existing VR systems for answering Yes/No questions and other binary responses in virtual environments."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00416-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Head gesture, Hand gesture, Virtual reality, Usability","kwrd":["Head_gesture","Hand_gesture","Virtual_reality","Usability"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00416-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00416-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=416;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00416-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00416-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00416-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-12-10" itemprop="datePublished">10 December 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jingbo-Zhao" data-author-popup="auth-Jingbo-Zhao" data-corresp-id="c1">Jingbo Zhao<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-6279-9570"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-6279-9570</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="China Agricultural University" /><meta itemprop="address" content="grid.22935.3f, 0000 0004 0530 8290, College of Information and Electrical Engineering, China Agricultural University, No. 17 Tsinghua East Road, Beijing, 100083, China" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="York University" /><meta itemprop="address" content="grid.21100.32, 0000 0004 1936 9430, Department of Electrical Engineering and Computer Science, York University, 4700 Keele Street, Toronto, ON, M3J 1P3, Canada" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Robert_S_-Allison" data-author-popup="auth-Robert_S_-Allison">Robert S. Allison</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="York University" /><meta itemprop="address" content="grid.21100.32, 0000 0004 1936 9430, Department of Electrical Engineering and Computer Science, York University, 4700 Keele Street, Toronto, ON, M3J 1P3, Canada" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">254 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00416-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in the context of virtual reality (VR) have been rarely studied. In the present paper, we focused on a typical VR interaction scenario—answering Yes/No questions in VR systems to compare the performance and the user preference of three types of interfaces. These interfaces included a head gesture interface, a hand gesture interface and a conventional gamepad interface. We designed a memorization task, in which participants were asked to memorize several everyday objects presented in a virtual room and later respond to questions on whether they saw a specific object through the given interfaces when these objects were absent. The performance of the interfaces was evaluated in terms of the real-time accuracy and the response time. A user interface questionnaire was also used to reveal the user preference for these interfaces. The results showed that head gesture is a very promising interface, which can be easily added to existing VR systems for answering Yes/No questions and other binary responses in virtual environments.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Recent improvements in sensor technologies have enabled human body movements to be accurately tracked in real time. With novel depth sensors such as the Kinect and the Leap Motion, a large volume of algorithms has been proposed and developed for body gesture (Lun and Zhao <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lun R, Zhao W (2015) A survey of applications and human motion recognition with microsoft kinect. Int J Patt Recogn Artif Intell 29:1555008" href="/article/10.1007/s10055-019-00416-7#ref-CR6" id="ref-link-section-d112249e352">2015</a>) and hand gesture recognition (Cheng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Cheng H, Yang L, Liu Z (2016) Survey on 3D hand gesture recognition. IEEE Trans Circuits Syst Video Technol 26:1659–1673" href="/article/10.1007/s10055-019-00416-7#ref-CR3" id="ref-link-section-d112249e355">2016</a>). The accurate and fast head tracking sensors in head-mounted displays (HMDs), such as the Oculus Rift DK2, also make real-time head gesture recognition possible (Zhao and Allison <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Zhao J, Allison RS (2017) Real-time head gesture recognition on head-mounted displays using cascaded hidden Markov models. In: 2017 IEEE international conference on systems, man, and cybernetics (SMC), pp 2361–2366" href="/article/10.1007/s10055-019-00416-7#ref-CR16" id="ref-link-section-d112249e358">2017</a>) in addition to the systems that use cameras to track head movements (Morimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Morimoto C, Yacoob Y, Davis L (1996) Recognition of head gestures using hidden Markov models. In: Proceedings of 13th international conference on pattern recognition, pp 461–465" href="/article/10.1007/s10055-019-00416-7#ref-CR9" id="ref-link-section-d112249e361">1996</a>; Terven et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Terven JR, Salas J, Raducanu B (2014) Robust head gestures recognition for assistive technology. In: Pattern recognition, pp 152–161" href="/article/10.1007/s10055-019-00416-7#ref-CR13" id="ref-link-section-d112249e364">2014</a>). One possible application of gesture recognition is to integrate such algorithms into VR systems to interact with virtual worlds. A typical interaction scenario in VR systems is to answer Yes/No questions asked by virtual avatars or raised by VR systems. For instance, Abate et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Abate AF, Acampora G, Ricciardi S (2011) An interactive virtual guide for the AR based visit of archaeological sites. J Vis Lang Comput 22:415–425" href="/article/10.1007/s10055-019-00416-7#ref-CR1" id="ref-link-section-d112249e368">2011</a>) presented an augmented reality (AR)-based tour system that may require an interface for answering questions asked by virtual tour guides. Answering Yes/No questions in VR systems is usually done by buttons pressed on handheld devices or by using handheld devices to point to corresponding options in menus (as in the HTC Vive). However, potential problems for using handheld devices are that it may hinder or prevent users from using their hands to perform tasks, such as picking objects with their fingers or performing hands-free locomotion in virtual environments.</p><p>In the current study, we propose to use head gesture and hand gesture as alternatives to conventional gamepad interfaces or other interfaces that employ handheld devices to answer Yes/No questions in virtual environments. The head gesture interface and the hand gesture interface do not require the user to hold additional devices in their hands. This may give users much freedom in performing activities. Comparing to the hand gesture interface and the gamepad interface, the head gesture interface has a unique advantage that it does not require extra devices for tracking head movements as head movements are directly tracked in VR systems that use head-mounted displays (HMDs). Similarly, CAVEs are usually equipped with tracking glasses or computer vision systems that monitor head movements. Thus, head movements data are readily available for head gesture recognition. To implement a head gesture interface, one would only need to integrate existing head gesture recognition algorithms into the VR system.</p><p>However, head gesture interfaces and hand gesture interfaces may have their own problems. For example, in head gesture interfaces, when responses are given by head movements tracked by HMDs or tracking glasses, the heaviness of the head worn devices may give people strain (Wille and Wischniewski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Wille M, Wischniewski S (2015) Influence of head mounted display hardware on performance and strain. In: Proceedings of the HFES annual meeting" href="/article/10.1007/s10055-019-00416-7#ref-CR14" id="ref-link-section-d112249e377">2015</a>), typically neck pain, when performing rapid head movements such as nodding or shaking. Other issues include blurred images and increased simulator sickness when rapid head movements are made. In comparison, hand gesture interfaces may not be as physically demanding as head gesture interfaces since users’ hands can be directly tracked by optical sensors without users holding any devices. However, these optical sensors usually have limited tracking range and the tracking performance will degrade when these sensors are interfered with other infrared (IR) devices in a VR system. On the other hand, gamepad interfaces are familiar devices to people. They may be preferred to other types of interfaces as people may have substantial experience using the gamepad interface.</p><p>The goal of the present study is to systematically evaluate and compare a head gesture interface, a hand gesture interface and a gamepad interface to answer Yes/No questions (or make other binary decisions) in virtual environments. The results of the study may help researchers or designers to select a suitable interface in their VR systems to answer such type of questions.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Previous work on the comparison of interaction techniques or interfaces to the conventional gamepad interface in VR systems primarily focused on virtual locomotion and navigation.</p><p>Nabiyouni et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Nabiyouni M, Saktheeswaran A, Bowman DA, Karanth A (2015) Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality. In: 2015 IEEE symposium on 3D user interfaces, pp 3–10" href="/article/10.1007/s10055-019-00416-7#ref-CR10" id="ref-link-section-d112249e394">2015</a>) evaluated the Virtusphere technique, the real-walking interface and the gamepad interface. They showed that the Virtusphere as a moderate-fidelity technique was significantly outperformed by a high-fidelity real-walking interface and a well-designed low-fidelity gamepad interface as the Virtusphere was fatiguing and difficult to control due to its large inertia. Conversely, the real-walking interface was natural to people and the gamepad interface had a clear mapping between joystick movement and users’ intended direction of travel, so it was easy to use.</p><p>Yan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Yan Z, Lindeman RW, Dey A (2016) Let your fingers do the walking: a unified approach for efficient short-, medium-, and long-distance travel in VR. In: 2016 IEEE symposium on 3D user interfaces (3DUI), pp 27–30" href="/article/10.1007/s10055-019-00416-7#ref-CR15" id="ref-link-section-d112249e400">2016</a>) proposed three types of finger gestures based on the Synaptics ForcePad for virtual walking. The proposed methods were compared to the gamepad interface in terms of their performance. The gamepad interface was found to have shorter task completion time but have problems in overshooting. It was also either too sensitive or not responsive compared to finger gesture interfaces.</p><p>Zielasko et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Zielasko D, Horn S, Freitag S, Weyers B, Kuhlen TW (2016) Evaluation of hands-free HMD-based navigation techniques for immersive data analysis. In: 2016 IEEE symposium on 3D user interfaces, pp 113–119" href="/article/10.1007/s10055-019-00416-7#ref-CR18" id="ref-link-section-d112249e406">2016</a>) evaluated five locomotion techniques. Among these techniques, the Adapted Walking in Place and the Accelerator Pedal involved lower limb movements. Leaning required upper body movements while seated. The Shake Your Head technique used only head movements tracked by an HMD. Similarly, these techniques were also compared with the traditional gamepad interface. They found that the Accelerator Pedal and the leaning technique performed better than other techniques in terms of user preference and task performance.</p><p>Cardoso (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Cardoso JCS (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology, pp 319–320" href="/article/10.1007/s10055-019-00416-7#ref-CR17" id="ref-link-section-d112249e413">2016</a>) compared a hand gesture interface based on the Leap Motion sensor, a gamepad interface and a gaze-based interface for locomotion in VR. Results showed that the hand gesture performed better than the gaze-based interface but worse than the gamepad interface.</p><p>Kitson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Kitson A, Hashemian AM, Stepanova ER, Kruijff E, Riecke BE (2017) Comparing leaning-based motion cueing interfaces for virtual reality locomotion. In: 2017 IEEE symposium on 3D user interfaces, pp 73–82" href="/article/10.1007/s10055-019-00416-7#ref-CR5" id="ref-link-section-d112249e419">2017</a>) compared several seated leaning locomotion techniques to the joystick interface. They reported that participants in general preferred the leaning techniques as they are fun, engaging and more realistic, but the joystick interface was still easier to use and control.</p><p>More recently, Coomer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Coomer N, Bullard S, Clinton W, Williams B (2018) Evaluating the effects of four VR locomotion methods: joystick, arm-cycling, point-tugging, and teleporting. In: Proceedings of the 15th ACM symposium on applied perception, pp 7:1–7:8" href="/article/10.1007/s10055-019-00416-7#ref-CR4" id="ref-link-section-d112249e425">2018</a>) compared four locomotion methods, including the joystick interface, the Arm-Cycling, the Point-Tugging and teleporting. The Arm-Cycling is a locomotion technique that creates egocentric motion in VR based on the displacements of HTC Vive controllers held in users’ hands when users perform cycling motion of their arms with the triggers on the HTC Vive being pressed down. The Point-Tugging is method that requires users to grab a virtual point in virtual environments by pressing the triggers on the HTC Vive controllers and then tug to move themselves in virtual environments, followed by releasing the triggers to complete the movement. They concluded that the Arm-Cycling was the best locomotion method among these four techniques as it gave better sense of spatial awareness and lower simulator sickness scores.</p><p>In addition, the work by Morency et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Morency L-P, Sidner C, Lee C, Darrell T (2007) Head gestures for perceptual interfaces: the role of context in improving recognition. Artif Intell 171:568–585" href="/article/10.1007/s10055-019-00416-7#ref-CR8" id="ref-link-section-d112249e431">2007</a>) is related to ours. They showed that head gesture was preferred to mice or keyboards by participants for responding confirmation questions prompted by dialog boxes in their experiments.</p><p>But to the authors’ knowledge, there has been no research that evaluated and compared head gesture interfaces, hand gesture interfaces and gamepad interfaces to answer Yes/No questions in virtual environments.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Methods</h2><div class="c-article-section__content" id="Sec3-content"><p>The functionality of each interface and the associated ways to indicate Yes/No responses are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab1">1</a>. We discuss the algorithm of each interface in detail in this section.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Definition of interface</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00416-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec4">Head gesture interface</h3><p>Zhao and Allison (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Zhao J, Allison RS (2017) Real-time head gesture recognition on head-mounted displays using cascaded hidden Markov models. In: 2017 IEEE international conference on systems, man, and cybernetics (SMC), pp 2361–2366" href="/article/10.1007/s10055-019-00416-7#ref-CR16" id="ref-link-section-d112249e502">2017</a>) presented a real-time head gesture recognition algorithm on HMDs using cascaded hidden Markov models (HMMs) (Rabiner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE 77:257–286" href="/article/10.1007/s10055-019-00416-7#ref-CR11" id="ref-link-section-d112249e505">1989</a>). An HMM is governed by parameters including <span class="mathjax-tex">\(N\)</span> the number of hidden states, <span class="mathjax-tex">\(M\)</span> the number of observation symbols and the model parameter <span class="mathjax-tex">\(\lambda = \left( {A, B, \pi } \right)\)</span>, where <span class="mathjax-tex">\(A\)</span> is the matrix that represents the transition probability between states, <span class="mathjax-tex">\(B\)</span> the matrix that represents the emission probability of a symbol observed from a specific state and <span class="mathjax-tex">\(\pi\)</span> the initial state probabilities. The structure of the head gesture recognition framework (recapped in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig1">1</a>) consists of four components, which are the vector quantization model, the simple gesture layer, the complex gesture layer and the output selection module. The simple gesture layer has seven parallel left–right HMMs for recognizing simple gestures such as rotating left and rotating right, while the complex gesture layer has two parallel left–right HMMs for recognizing complex gestures: nodding and shaking. The Baum–Welch algorithm was used to train HMMs to obtain their respective model parameter <span class="mathjax-tex">\(\lambda\)</span>, and the forward procedure was used to evaluate an observation sequence <span class="mathjax-tex">\(S\)</span>, which consists of discrete symbols of quantized head angular velocities, using trained HMMs with their respective model parameter <span class="mathjax-tex">\(\lambda\)</span>. During real-time operation, the vector quantization module reads head angular velocities from the HMD—the Oculus Rift DK2, and quantizes the continuous head angular velocities into discrete symbols. These discrete symbols are further buffered and fed into the simple gesture layer to determine whether a simple gesture exists in the buffered sequence. The outputs from the simple gesture layer are further buffered and fed into the complex gesture layer to determine whether a complex gesture has been made. Finally, the output selection module determines the final gesture using the outputs from the simple gesture layer and the complex gesture layer. We used the original implementation from the authors for the head gesture interface, and the implementation was trained using head gesture data collected from nineteen participants. The average accuracy for recognizing complex gestures was 98.5%. We only used three types of gesture outputs, including remaining still, shaking and nodding, for the head gesture interface. We ignored other head gesture outputs. For this interface, nodding represents Yes and shaking denotes No.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig1_HTML.png" alt="figure1" loading="lazy" width="477" height="574" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Head gesture interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec5">Hand gesture interface</h3><p>Marin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Marin G, Dominio F, Zanuttigh P (2016) Hand gesture recognition with jointly calibrated Leap Motion and depth sensor. Multimed Tools Appl 75:14991–15015" href="/article/10.1007/s10055-019-00416-7#ref-CR7" id="ref-link-section-d112249e618">2016</a>) proposed a set of robust features for recognizing static hand gestures with the hand skeletons tracked by the Leap Motion sensor. The specific feature descriptor selected from the set for our implementation was:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} P_{i}^{x} &amp; = \left( {F_{i} - C} \right) \cdot \left( {n \times h} \right) \\ P_{i}^{y} &amp; = \left( {F_{i} - C} \right) \cdot h \\ P_{i}^{z} &amp; = \left( {F_{i} - C} \right) \cdot n \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(F_{i}\)</span> is the position of the fingertip and <span class="mathjax-tex">\(i\)</span> the index of a finger, <span class="mathjax-tex">\(C\)</span> the position of the palm center, <span class="mathjax-tex">\(n\)</span> the normal vector emanating from the palm and <span class="mathjax-tex">\(h\)</span> the vector from the palm center to the direction of the fingers. These parameters are directly available from the tracked hand skeleton of the Leap Motion sensor. <span class="mathjax-tex">\(P_{i}^{x}\)</span>, <span class="mathjax-tex">\(P_{i}^{y}\)</span> and <span class="mathjax-tex">\(P_{i}^{z}\)</span> are the extracted features. As pointed out by the authors, the set of equations normalize fingertip positions with respect to hand position and orientation. Fingertip angles, positions and elevations are embedded in the extracted features <span class="mathjax-tex">\(P_{i}^{x}\)</span>, <span class="mathjax-tex">\(P_{i}^{y}\)</span> and <span class="mathjax-tex">\(P_{i}^{z}\)</span>. The extracted features can be used to train classifiers such as the support vector machine (SVM) (Chang and Lin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 27(2):1–27" href="/article/10.1007/s10055-019-00416-7#ref-CR2" id="ref-link-section-d112249e827">2011</a>) to recognize static gestures.</p><p>Initially, we defined three types of hand gestures. The OK gesture represents Yes, and the extended hand gesture means No. The fist gesture was also defined as the standby gesture for resting (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig2">2</a>). A problem using only the static hand gesture recognition algorithm is that it is difficult to determine whether users intend to confirm their responses as static hand gestures are continuously recognized and a response can be determined before users finishing making their intended gestures. Thus, we included two HMMs to monitor the trajectory of the hand velocity to detect whether users are waving their hands or not. The outputs from the SVMs and the HMMs are fused by a set of rules to generate the final gesture: if the user waves a hand with an OK gesture, then the algorithm will confirm that the response from the user is Yes; similarly, if the user waves an extended hand, the response will be confirmed as No; otherwise, the algorithm considers that there are no meaningful responses given by users. Therefore, the types of hand gestures in the hand gesture interface were extended to six types, including: static OK gesture, static extended hand, static fist, waving OK gesture, waving extended hand and waving fist. The structure of the hand gesture recognition algorithm is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="281" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Hand gestures (OK gesture, extended hand and standby)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="383" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Hand gesture interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We collected hand gesture samples from twelve participants (age: 20–33, 7 males, 5 females) for the six types of gestures. Each participant was asked to perform four sessions of data collection. In each session, a participant was asked to perform the six types of hand gestures, respectively, and each type of hand gesture was recorded for 4 s. The features <span class="mathjax-tex">\(P_{i}^{x}\)</span>, <span class="mathjax-tex">\(P_{i}^{y}\)</span> and <span class="mathjax-tex">\(P_{i}^{z}\)</span> extracted from the collected samples were used to train three SVMs using the one against one approach. To recognize a gesture during real-time operation, the voting strategy was used, meaning that the type of gesture that received the highest number of the votes given by SVMs is the winner. The average accuracy for recognizing static hand gestures is 99.6%. Two HMMs were trained using hand velocity data calculated from the hand centers. Specifically, one HMM was trained using hand velocities of static gestures, while the other was trained using hand velocities of waving hands. During operation, the HMM that produces the highest posterior probability when evaluating hand velocity data with their respective model parameter <span class="mathjax-tex">\(\lambda\)</span> was chosen as the winner and its class label was given as the output. The average accuracy for recognizing whether a hand is waving or not is 98.1%. The theoretical average accuracy for recognizing dynamic hand gestures was 97.7% by multiplying the average accuracy for recognizing static hand gestures (99.6%) and the average accuracy for detecting moving hands (98.1%).</p><p>An advantage of the proposed hand gesture recognition framework is that it can be further extended to recognize combinations of different static hand gestures and different shapes of hand velocity trajectories. Thus, it has the potential to deal with more complex gesture recognition scenarios, but the average accuracy may decrease when more gestures are added.</p><h3 class="c-article__sub-heading" id="Sec6">Gamepad interface</h3><p>The gamepad interface (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig4">4</a>) was implemented based on a Logitech Dual Action gamepad. Specifically, users pressed button 5 on the gamepad for Yes and pressed button 6 for No.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="577" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Gamepad interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Experiment</h2><div class="c-article-section__content" id="Sec7-content"><h3 class="c-article__sub-heading" id="Sec8">Introduction</h3><p>The goal of the experiment was to evaluate and compare performance and user preference of the head gesture interface, the hand gesture interface and the gamepad interface for answering Yes/No questions in virtual environments. To achieve the goal, a memorization task was designed. The task asked participants to memorize the objects presented in a virtual room with a 30-s exposure period. Then, these objects were removed, and participants were asked whether they saw a specific object by answering Yes or No through a given interface. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig5">5</a>a–c, we show the three stages of the experiment, including initialization stage, memorization stage and question stage. When a participant made a response, a confirmation (Yes/No) was prompted as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig5">5</a>d.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="503" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Experimental stages</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">VR hardware and software</h3><p>The host machine for the experiment was a desktop computer equipped with an Intel i7 2.8 GHz. CPU, 4 GB memory and an AMD Radeon HD 6850 graphics card. The operating system was Windows 7. Other experimental devices included an Oculus Rift DK 2, a Leap Motion sensor and a Logitech Dual Action gamepad connected to the host machine.</p><p>The experimental application and the algorithms for three interfaces were implemented in the Worldviz Vizard 5.0 using Python 2.7. The training of HMMs was done in MATLAB as it offered an HMM library convenient to use. Model parameters of HMMs were obtained after training. For real-time application, we only needed to implement the forward procedure of HMMs using Python in Vizard with the model parameters to evaluate movement sequences captured in real time. The support vector machine used in our study came from the LibSVM library (Chang and Lin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 27(2):1–27" href="/article/10.1007/s10055-019-00416-7#ref-CR2" id="ref-link-section-d112249e962">2011</a>). Similarly, we also conducted training in MATLAB and then performed real-time application using the testing function of SVM with Python in Vizard.</p><h3 class="c-article__sub-heading" id="Sec10">Metrics</h3><p>The metrics consisted of objective measures and subjective measures. The objective measures were: response time, which is the time interval between when a question was prompted and when a response was made, and real-time accuracy, which is the percentage of the objects that were correctly classified as present or not during real-time operation. The objective measures were applied on the recorded experimental data to extract the corresponding parameters.</p><p>The subjective measures were: ease-to-learn, ease-to-use, natural-to-use, fun, tiredness, responsiveness and subjective accuracy. The subjective measures were evaluated using a user interface questionnaire modified from the one by Nabiyouni et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Nabiyouni M, Saktheeswaran A, Bowman DA, Karanth A (2015) Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality. In: 2015 IEEE symposium on 3D user interfaces, pp 3–10" href="/article/10.1007/s10055-019-00416-7#ref-CR10" id="ref-link-section-d112249e976">2015</a>). The items given in the questionnaire are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab2">2</a>. The seven-point Likert scale (from strongly disagree to strongly agree) was used to rate each factor.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 The user interface questionnaire</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00416-7/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec11">Procedure</h3><p>Informed consent was obtained from all twelve participants (age: 20–38, 7 males, 5 females) in accordance with a protocol approved by the Human Participants Review Subcommittee at York university. Twelve participants were divided into six groups. Each group covered a permutation of the gesture interfaces. Thus, six groups covered all six permutations of the three gesture interfaces. For each interface, four trials were conducted. The first trial was for training and was not considered for the data analysis, while the remaining three trials were the actual experiments. During experimental sessions, participants wore the Oculus Rift DK2 and sat 60 cm in front of the computer monitor, on which the tracking camera of the Oculus Rift DK2 was mounted. The Leap Motion sensor was attached onto a stand and was placed 30 cm in front of the participants. At the beginning of a trial, a participant was exposed to a virtual room, facing a bench placed in the front of the room. After the researcher pressed the start button, twenty objects were randomly selected from a list of thirty objects. The list consisted of everyday objects, including a camera, a cell phone and a chair, etc. The 3D models of the objects were obtained from <a href="http://www.turbosquid.com">www.turbosquid.com</a>. Among the selected objects, ten were placed onto the bench with a random order and participants were given 30 s to memorize the presence of these objects. Another ten objects were not presented in the room and were only used for generating questions. After the 30-second memorization period, the presented objects were removed from the room. Participants were sequentially asked about the existence of the twenty objects with a random sequence. Questions had the form “Did you see a cell phone?” (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig5">5</a>c). Each time the participant made a response through the given interface; a 3-s waiting period was introduced before the next question was prompted. The object names, the timestamps when the questions were prompted and the timestamps when the responses were made, the existence of objects and the responses of the participants were recorded for data analysis. After participants completed four trials for a given interface, they were asked to complete the user interface questionnaire to evaluate the interface they used.</p><h3 class="c-article__sub-heading" id="Sec12">Results</h3><p>We performed the data analysis in MATLAB 2016a and R 3.4.2. One-way repeated-measure ANOVAs were applied on each factor of the objective measures and subjective measures to reveal whether there were significant effects between the types of interfaces. Post hoc pairwise comparisons were made using Tukey’s range tests.</p><p>We found a significant effect on response time (<span class="mathjax-tex">\(F\left( {2,22} \right) = 50.84,\;p &lt; 0.001\)</span>). On average, the head gesture interface had the highest response time and the gamepad interface had the lowest, while the hand gesture was in the middle. A Tukey’s range test confirmed that the gamepad interface was significantly faster than the head gesture interface and the hand gesture interface in terms of response time and there was no significant difference between the head gesture interface and the hand gesture interface (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab3">3</a>). The results (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig6">6</a>, bars denote the mean value and error bars denote the standard error of the mean) were expected since the head gesture interface required nodding or shaking for at least one cycle. This typically took longer time than pressing a button on the gamepad or waving hands in front of the Leap Motion sensor. We also found a significant effect on real-time accuracy <span class="mathjax-tex">\((F\left( {2,22} \right) = 16.70,\;p &lt; 0.001)\)</span>. A Tukey’s range test showed that the hand gesture interface was significantly less accurate than the gamepad interface and the head gesture interface (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab3">3</a>). The real-time accuracies of the three interfaces are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig7">7</a>. (Bars denote the mean value, and error bars denote the standard of the mean.) The factor is primarily determined by the accuracy of memorization of the objects, the control of the interfaces and the recognition performance of the interfaces. Although, in theory, the gamepad interface should have the best recognition performance as Yes/No is recognized by two buttons, we found the head gesture had a slightly higher real-time accuracy than the gamepad interface with the assumption that the memorization of objects by participants across three interfaces was the same. This suggested that using the head gesture interface was less error-prone than the gamepad interface. The hand gesture interface had the lowest real-time accuracy, which suggested that using hand gesture interface might introduce more errors into responses.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Results of the Tukey’s range tests on factors with significant effects (95% confidence level)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00416-7/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="541" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Response time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="553" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Real-time accuracy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For subjective measures (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig8">8</a>, bars denote the mean value, and error bars denote the standard error of the mean), the gamepad interface was rated better than other interfaces in terms of ease-to-use, fun, tiredness, responsiveness and subjective accuracy, while the head gesture was rated slightly higher for ease-to-learn and natural-to-use. The hand gesture interface was not preferred for all factors except tiredness as the head gesture interface was considered as the most tiring interface. We found a significant effect on ease-to-use <span class="mathjax-tex">\((F\left( {2,22} \right) = 7.00, \;p = 0.004)\)</span>, and a Tukey’s range test showed that the gamepad was significantly easier to use than the hand gesture interface (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab3">3</a>). A significant effect was also found on tiredness <span class="mathjax-tex">\((F\left( {2,22} \right) = 4.22, \;p = 0.028)\)</span>, and a Tukey’s range test showed that the gamepad interface was significantly less tiring than the head gesture interface (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00416-7#Tab3">3</a>). However, we did not find significant effects on other factors: ease-to-learn <span class="mathjax-tex">\((F\left( {2,22} \right) = 2.27,\; p = 0.13)\)</span>, natural-to-use <span class="mathjax-tex">\((F\left( {2,22} \right) = 2.18, \;p = 0.14)\)</span>, fun <span class="mathjax-tex">\((F\left( {2,22} \right) = 0.65, \; p = 0.53)\)</span>, responsiveness <span class="mathjax-tex">\((F\left( {2,22} \right) = 2.89, \; p = 0.08)\)</span> and subjective accuracy <span class="mathjax-tex">\((F\left( {2,22} \right) = 2.84, \; p = 0.08)\)</span>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="292" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Subjective measures</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>One interesting finding was that responsiveness and subjective accuracy in the subjective measures did not agree with response time and real-time accuracy in the objective measures, respectively. For example, although subjectively participants indicated that the gamepad interface was more accurate than the head gesture interface, this was not the case when the accuracy was assessed objectively. Similarly, the head gesture interface took the longest time for making responses on average in the objective measure, but participants indicated that the hand gesture interface was less responsive than the head gesture.</p><p>The total scores of each interface rated by all participants are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig9">9</a>. The score for the factor tiredness was inverted (strongly agree received one point and strongly disagree received seven points) to indicate how positive participants’ attitudes were toward tiredness. We found that the gamepad interface was preferred by six participants, while the head gesture interface was preferred by five participants. Only one participant opted for the hand gesture interface. Thus, in general, the gamepad interface and the head gesture interface were equally liked, while the hand gesture was rejected. But in terms of the mean scores across participants (the 13<sup>th</sup> set of bars in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig9">9</a>), the gamepad interface still received the highest score.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig9_HTML.png?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00416-7/MediaObjects/10055_2019_416_Fig9_HTML.png" alt="figure9" loading="lazy" width="685" height="550" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Total score</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00416-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>Gamepads or other handheld devices are traditional interfaces for people to play games and have a long history for interaction in VR systems. For example, joysticks have been used for flying in virtual environments as a method for locomotion (Robinett and Holloway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Proceedings of the 1992 symposium on interactive 3D graphics, pp 189–192" href="/article/10.1007/s10055-019-00416-7#ref-CR12" id="ref-link-section-d112249e1540">1992</a>). Because of people’s familiarity and previous experience with these devices, it is possible that even when new interfaces appear, they would still prefer these traditional devices as such devices may be more reliable. In addition, handheld devices are more familiar and would not take extra efforts for people to learn how to use them. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig9">9</a>, six participants preferred the gamepad interface to other two interfaces. This showed that gamepads or handheld devices are still important devices for VR interactions.</p><p>Responding Yes/No through head nodding and shaking is a natural way for the interaction between people in the real world. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig8">8</a>, the rating of natural-to-use was higher than other interfaces. Similarly, the interface was rated easier to use than other interfaces. As has been discussed, the primary problem with the interface is the heaviness of the HMD, which probably made people consider the head gesture interface the most tiring one. We expect that by using an HMD or tracking glasses with lower weight or using computer vision systems for tracking, the tiredness for using the interface would be lowered. But given tiredness as the primary limitation, the interface was still preferred by five participants.</p><p>The hand gesture interface was only preferred by one participant probably because the definition of Yes/No using a waving OK gesture and a waving extended hand was not natural or unfamiliar to participants. To make a response, the hand of a participant needed to make a two-step movement. First, they need to make an OK gesture or extend their hands. Then, they need to wave their hands to confirm their responses. It is obvious that more efforts are required when using the hand gesture interface than other two interfaces, which required only a one-step movement, such as pressing a button or shaking their heads. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00416-7#Fig8">8</a>, the hand gesture was the most difficult to learn and most difficult to use. The factors fun, responsiveness and subjective accuracy were also lower than other two interfaces. It was only considered better than the head gesture interface in terms of tiredness. We expect that improved gestures for Yes and No might improve the usability of the hand gesture interface.</p><p>Finally, another option to implement the functionality to answer Yes/No questions in VR systems is to use speech recognition algorithms as interfaces to recognize people’s voice. The performance and user preference of the speech recognition interface also can be studied and compared to the motion-based interfaces presented in this paper. In practice, we can also design a multi-modal interface that integrates the head gesture interface, the hand gesture interface, the gamepad interface and the speech recognition interface into a single system and let users choose their preferred interface during actual usage.</p><p>Limitations of the experimental design were that extra time was taken for participants to recall the objects they memorized when responding to Yes/No questions and the ability of the participants to memorize the given objects might also affect the results of the real-time accuracy. In addition, we had a sample of twelve participants who were young university-educated students. They might not be the representative of the general population or specific subpopulations such as children or the elderly. Future research should consider including more participants from diverse groups to determine whether the interface preferences found here generalize to these groups.</p><p>The research is important in the way that it provides VR researchers and designers with an idea of user preference and performance when users answering Yes/No questions or other binary questions using a head gesture interface, a hand gesture interface and a gamepad interface. Designers may consider several factors such as cost and the factors we studied in the paper when they do actual design and make decisions to include a specific interface or provide all three interfaces in their systems.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusions</h2><div class="c-article-section__content" id="Sec14-content"><p>In this paper, we proposed to use the head gesture interface and the hand gesture interface to answer Yes/No questions in virtual environments. We evaluated their performance and user preference through a memorization task and compared them to the traditional gamepad interface. We showed that the head gesture interface was comparable to the gamepad interface. As adding the head gesture interface to a VR system usually does not require additional tracking devices, we suggest adding the head gesture interface to VR systems that require users to answer Yes/No questions. These techniques could also be readily adapted for other common binary responses (e.g., left versus right). We believe that interaction techniques using head gestures and hand gestures in VR systems are still underexplored. Thus, the utility of these interfaces in VR systems is worth further investigation.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AF. Abate, G. Acampora, S. Ricciardi, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Abate AF, Acampora G, Ricciardi S (2011) An interactive virtual guide for the AR based visit of archaeological" /><p class="c-article-references__text" id="ref-CR1">Abate AF, Acampora G, Ricciardi S (2011) An interactive virtual guide for the AR based visit of archaeological sites. J Vis Lang Comput 22:415–425</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jvlc.2011.02.005" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20interactive%20virtual%20guide%20for%20the%20AR%20based%20visit%20of%20archaeological%20sites&amp;journal=J%20Vis%20Lang%20Comput&amp;volume=22&amp;pages=415-425&amp;publication_year=2011&amp;author=Abate%2CAF&amp;author=Acampora%2CG&amp;author=Ricciardi%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cardoso JCS (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR17">Cardoso JCS (2016) Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds. In: Proceedings of the 22nd ACM conference on virtual reality software and technology, pp 319–320</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Chang, C. Lin, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 27(2):1–27" /><p class="c-article-references__text" id="ref-CR2">Chang C, Lin C (2011) LIBSVM: a library for support vector machines. ACM Trans Intell Syst Technol 27(2):1–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1961189.1961199" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=LIBSVM%3A%20a%20library%20for%20support%20vector%20machines&amp;journal=ACM%20Trans%20Intell%20Syst%20Technol&amp;volume=27&amp;issue=2&amp;pages=1-27&amp;publication_year=2011&amp;author=Chang%2CC&amp;author=Lin%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Cheng, L. Yang, Z. Liu, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Cheng H, Yang L, Liu Z (2016) Survey on 3D hand gesture recognition. IEEE Trans Circuits Syst Video Technol 26" /><p class="c-article-references__text" id="ref-CR3">Cheng H, Yang L, Liu Z (2016) Survey on 3D hand gesture recognition. IEEE Trans Circuits Syst Video Technol 26:1659–1673</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTCSVT.2015.2469551" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20on%203D%20hand%20gesture%20recognition&amp;journal=IEEE%20Trans%20Circuits%20Syst%20Video%20Technol&amp;volume=26&amp;pages=1659-1673&amp;publication_year=2016&amp;author=Cheng%2CH&amp;author=Yang%2CL&amp;author=Liu%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coomer N, Bullard S, Clinton W, Williams B (2018) Evaluating the effects of four VR locomotion methods: joysti" /><p class="c-article-references__text" id="ref-CR4">Coomer N, Bullard S, Clinton W, Williams B (2018) Evaluating the effects of four VR locomotion methods: joystick, arm-cycling, point-tugging, and teleporting. In: Proceedings of the 15th ACM symposium on applied perception, pp 7:1–7:8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kitson A, Hashemian AM, Stepanova ER, Kruijff E, Riecke BE (2017) Comparing leaning-based motion cueing interf" /><p class="c-article-references__text" id="ref-CR5">Kitson A, Hashemian AM, Stepanova ER, Kruijff E, Riecke BE (2017) Comparing leaning-based motion cueing interfaces for virtual reality locomotion. In: 2017 IEEE symposium on 3D user interfaces, pp 73–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Lun, W. Zhao, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Lun R, Zhao W (2015) A survey of applications and human motion recognition with microsoft kinect. Int J Patt R" /><p class="c-article-references__text" id="ref-CR6">Lun R, Zhao W (2015) A survey of applications and human motion recognition with microsoft kinect. Int J Patt Recogn Artif Intell 29:1555008</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1142%2FS0218001415550083" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20applications%20and%20human%20motion%20recognition%20with%20microsoft%20kinect&amp;journal=Int%20J%20Patt%20Recogn%20Artif%20Intell&amp;volume=29&amp;publication_year=2015&amp;author=Lun%2CR&amp;author=Zhao%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Marin, F. Dominio, P. Zanuttigh, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Marin G, Dominio F, Zanuttigh P (2016) Hand gesture recognition with jointly calibrated Leap Motion and depth " /><p class="c-article-references__text" id="ref-CR7">Marin G, Dominio F, Zanuttigh P (2016) Hand gesture recognition with jointly calibrated Leap Motion and depth sensor. Multimed Tools Appl 75:14991–15015</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11042-015-2451-6" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hand%20gesture%20recognition%20with%20jointly%20calibrated%20Leap%20Motion%20and%20depth%20sensor&amp;journal=Multimed%20Tools%20Appl&amp;volume=75&amp;pages=14991-15015&amp;publication_year=2016&amp;author=Marin%2CG&amp;author=Dominio%2CF&amp;author=Zanuttigh%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L-P. Morency, C. Sidner, C. Lee, T. Darrell, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Morency L-P, Sidner C, Lee C, Darrell T (2007) Head gestures for perceptual interfaces: the role of context in" /><p class="c-article-references__text" id="ref-CR8">Morency L-P, Sidner C, Lee C, Darrell T (2007) Head gestures for perceptual interfaces: the role of context in improving recognition. Artif Intell 171:568–585</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.artint.2007.04.003" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Head%20gestures%20for%20perceptual%20interfaces%3A%20the%20role%20of%20context%20in%20improving%20recognition&amp;journal=Artif%20Intell&amp;volume=171&amp;pages=568-585&amp;publication_year=2007&amp;author=Morency%2CL-P&amp;author=Sidner%2CC&amp;author=Lee%2CC&amp;author=Darrell%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morimoto C, Yacoob Y, Davis L (1996) Recognition of head gestures using hidden Markov models. In: Proceedings " /><p class="c-article-references__text" id="ref-CR9">Morimoto C, Yacoob Y, Davis L (1996) Recognition of head gestures using hidden Markov models. In: Proceedings of 13th international conference on pattern recognition, pp 461–465</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nabiyouni M, Saktheeswaran A, Bowman DA, Karanth A (2015) Comparing the performance of natural, semi-natural, " /><p class="c-article-references__text" id="ref-CR10">Nabiyouni M, Saktheeswaran A, Bowman DA, Karanth A (2015) Comparing the performance of natural, semi-natural, and non-natural locomotion techniques in virtual reality. In: 2015 IEEE symposium on 3D user interfaces, pp 3–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LR. Rabiner, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEE" /><p class="c-article-references__text" id="ref-CR11">Rabiner LR (1989) A tutorial on hidden Markov models and selected applications in speech recognition. Proc IEEE 77:257–286</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5.18626" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20tutorial%20on%20hidden%20Markov%20models%20and%20selected%20applications%20in%20speech%20recognition&amp;journal=Proc%20IEEE&amp;volume=77&amp;pages=257-286&amp;publication_year=1989&amp;author=Rabiner%2CLR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Proceeding" /><p class="c-article-references__text" id="ref-CR12">Robinett W, Holloway R (1992) Implementation of flying, scaling and grabbing in virtual worlds. In: Proceedings of the 1992 symposium on interactive 3D graphics, pp 189–192</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Juan R.. Terven, Joaquin. Salas, Bogdan. Raducanu, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Terven JR, Salas J, Raducanu B (2014) Robust head gestures recognition for assistive technology. In: Pattern r" /><p class="c-article-references__text" id="ref-CR13">Terven JR, Salas J, Raducanu B (2014) Robust head gestures recognition for assistive technology. In: Pattern recognition, pp 152–161</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lecture%20Notes%20in%20Computer%20Science&amp;pages=152-161&amp;publication_year=2014&amp;author=Terven%2CJuan%20R.&amp;author=Salas%2CJoaquin&amp;author=Raducanu%2CBogdan">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wille M, Wischniewski S (2015) Influence of head mounted display hardware on performance and strain. In: Proce" /><p class="c-article-references__text" id="ref-CR14">Wille M, Wischniewski S (2015) Influence of head mounted display hardware on performance and strain. In: Proceedings of the HFES annual meeting</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yan Z, Lindeman RW, Dey A (2016) Let your fingers do the walking: a unified approach for efficient short-, med" /><p class="c-article-references__text" id="ref-CR15">Yan Z, Lindeman RW, Dey A (2016) Let your fingers do the walking: a unified approach for efficient short-, medium-, and long-distance travel in VR. In: 2016 IEEE symposium on 3D user interfaces (3DUI), pp 27–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhao J, Allison RS (2017) Real-time head gesture recognition on head-mounted displays using cascaded hidden Ma" /><p class="c-article-references__text" id="ref-CR16">Zhao J, Allison RS (2017) Real-time head gesture recognition on head-mounted displays using cascaded hidden Markov models. In: 2017 IEEE international conference on systems, man, and cybernetics (SMC), pp 2361–2366</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zielasko D, Horn S, Freitag S, Weyers B, Kuhlen TW (2016) Evaluation of hands-free HMD-based navigation techni" /><p class="c-article-references__text" id="ref-CR18">Zielasko D, Horn S, Freitag S, Weyers B, Kuhlen TW (2016) Evaluation of hands-free HMD-based navigation techniques for immersive data analysis. In: 2016 IEEE symposium on 3D user interfaces, pp 113–119</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00416-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">College of Information and Electrical Engineering, China Agricultural University, No. 17 Tsinghua East Road, Beijing, 100083, China</p><p class="c-article-author-affiliation__authors-list">Jingbo Zhao</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Electrical Engineering and Computer Science, York University, 4700 Keele Street, Toronto, ON, M3J 1P3, Canada</p><p class="c-article-author-affiliation__authors-list">Jingbo Zhao &amp; Robert S. Allison</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jingbo-Zhao"><span class="c-article-authors-search__title u-h3 js-search-name">Jingbo Zhao</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jingbo+Zhao&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jingbo+Zhao" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jingbo+Zhao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Robert_S_-Allison"><span class="c-article-authors-search__title u-h3 js-search-name">Robert S. Allison</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Robert S.+Allison&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Robert S.+Allison" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Robert S.+Allison%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00416-7/email/correspondent/c1/new">Jingbo Zhao</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p><p>The original version of this article has been revised: Equation 1 has been corrected.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Comparing%20head%20gesture%2C%20hand%20gesture%20and%20gamepad%20interfaces%20for%20answering%20Yes%2FNo%20questions%20in%20virtual%20environments&amp;author=Jingbo%20Zhao%20et%20al&amp;contentID=10.1007%2Fs10055-019-00416-7&amp;publication=1359-4338&amp;publicationDate=2019-12-10&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00416-7" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00416-7" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Zhao, J., Allison, R.S. Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00416-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00416-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-11-13">13 November 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-11-27">27 November 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-10">10 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00416-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00416-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Head gesture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand gesture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Usability</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00416-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=416;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

