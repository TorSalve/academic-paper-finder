<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="The contribution of virtual reality to research on sensory feedback in"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality and telepresence in that research. We..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="The contribution of virtual reality to research on sensory feedback in remote control"/>

    <meta name="dc.source" content="Virtual Reality 2006 9:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-03-03"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality and telepresence in that research. We argue that full automation is a distant goal and that remote control deserves continued attention and improvement. Visual feedback to controllers has developed in various ways but autostereoscopic displays have yet to be proven. Haptic force feedback, in both real and virtual settings, has been demonstrated to offer much to the remote control environment and has led to a greater understanding of the kinesthetic and cutaneous components of haptics, and their role in multimodal processes, such as sensory capture and integration. We suggest that many displays using primarily visual feedback would benefit from the addition of haptic information but that much is yet to be learned about optimizing such displays."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-03-03"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="234"/>

    <meta name="prism.endingPage" content="242"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0020-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0020-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0020-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0020-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="The contribution of virtual reality to research on sensory feedback in remote control"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2006/04"/>

    <meta name="citation_online_date" content="2006/03/03"/>

    <meta name="citation_firstpage" content="234"/>

    <meta name="citation_lastpage" content="242"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0020-z"/>

    <meta name="DOI" content="10.1007/s10055-006-0020-z"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0020-z"/>

    <meta name="description" content="Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality"/>

    <meta name="dc.creator" content="Barry Richardson"/>

    <meta name="dc.creator" content="Mark Symmons"/>

    <meta name="dc.creator" content="Dianne Wuillemin"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_title=Fly-by-wire; citation_publication_date=1998; citation_id=CR1; citation_author=VR Schmitt; citation_author=JW Morris; citation_author=GD Jenney; citation_publisher=Society of Automotive Engineers"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Individ Dif; citation_title=Predicitng cognitive failures from boredom proneness and daytime sleepiness scores: an investigation within military and undergraduate samples; citation_author=JC Wallace, SJ Vodanovich, R Restino; citation_volume=34; citation_publication_date=2003; citation_pages=635-644; citation_doi=10.1016/S0191-8869(02)00050-8; citation_id=CR2"/>

    <meta name="citation_reference" content="Diolaiti N, Melchiorri C (2002) Tele-operation of a mobile robot through haptic feedback. In: HAVE, IEEE international workshop on haptic virtual environments and their applications"/>

    <meta name="citation_reference" content="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts&#8212;haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509&#8211;510"/>

    <meta name="citation_reference" content="Rastogi A, Milgram P, Drascic D (1996) Telerobotic control with stereoscopic augmented reality. In: Bolas M, Fisher S, Merritt J (eds) Stereoscopic displays and virtual reality systems, vol III. Proc SPIE 2635:115&#8211;122"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleop Virtual Environ; citation_title=Experiments using multimodal virtual environments in design for assembly analysis; citation_author=R Gupta, T Sheridan, D Whitney; citation_volume=6; citation_issue=3; citation_publication_date=1997; citation_pages=318-338; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Auton Robots; citation_title=Teleoperation user interfaces for mining robotics; citation_author=DW Hainsworth; citation_volume=11; citation_issue=1; citation_publication_date=2001; citation_pages=19-28; citation_doi=10.1023/A:1011299910904; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE/ASME Transactions on Mechatronics; citation_title=Haptic information in internet-based teleoperation; citation_author=I Elhajj, N Xi, WK Fung, YH Liu, WJ Li, T Kaga, T Fukuda; citation_volume=6; citation_issue=3; citation_publication_date=2001; citation_pages=295-304; citation_doi=10.1109/3516.951367; citation_id=CR8"/>

    <meta name="citation_reference" content="Roberts JW, Slattery OT, Swope B, Volker M, Comstock T (2002) Small-scale tactile graphics for virtual reality systems. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:422&#8211;429"/>

    <meta name="citation_reference" content="Grohn M (2002) Is audio useful in immersive visualization? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:411&#8211;421"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=The use of force feedback and auditory cues for performance of an assembly task in an immersive virtual environment; citation_author=GW Edwards, W Barfield, MA Nussbaum; citation_volume=7; citation_publication_date=2004; citation_pages=112-119; citation_doi=10.1007/s10055-004-0120-6; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput&#8211;Hum Interact; citation_title=Supporting presence in collaborative environments by haptic feedback; citation_author=EL K Sallnas Rassmus-Grohn, C Sjostrom; citation_volume=7; citation_issue=4; citation_publication_date=2001; citation_pages=461-476; citation_doi=10.1145/365058.365086; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Operator performance in surgical telemanipulation; citation_author=A Kazi; citation_volume=10; citation_publication_date=2001; citation_pages=495-510; citation_doi=10.1162/105474601753132678; citation_id=CR13"/>

    <meta name="citation_reference" content="Raspolli M, Avizzano CA, Facenza G, Bergamasco M (2005) HERMES: an angioplasty surgery simulator. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 148&#8211;156"/>

    <meta name="citation_reference" content="Jiang L, Girotra R, Cutkosky MR, Ullrich C (2005) Reducing error rates with low-cost haptic feedback in virtual reality-based training applications. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 420&#8211;425"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Seeing with the skin; citation_author=BW White, FA Saunders, L Scadden, P Bach-y-Rita, CC Collins; citation_volume=7; citation_publication_date=1970; citation_pages=23-27; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_title=The nature of haptics; citation_inbook_title=The perception of pictures; citation_publication_date=1980; citation_id=CR17; citation_author=JM Kennedy; citation_author=BL Richardson; citation_author=LE Magee; citation_publisher=Academic"/>

    <meta name="citation_reference" content="Baier H, Buss M, Freyberger F, Hoogen J, Kammermeier P, Schmidt G (1999) Distributed PC-based haptic, visual and acoustic telepresence system experiments in virtual and remote environments. In: Proceedings of IEEE virtual reality conference, p 118"/>

    <meta name="citation_reference" content="Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interface. In: Proceedings IMAGE conference, pp 23&#8211;35"/>

    <meta name="citation_reference" content="Grane C, Bengtsson P (2005) Menu selection with a rotary device founded on haptic and/or graphic information. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 475&#8211;476"/>

    <meta name="citation_reference" content="citation_title=Vision sciences; citation_publication_date=2002; citation_id=CR21; citation_author=S Palmer; citation_publisher=Bradford Books"/>

    <meta name="citation_reference" content="citation_title=Sensation and perception; citation_publication_date=1999; citation_id=CR22; citation_author=BE Goldstein; citation_publisher=Brooks Cole"/>

    <meta name="citation_reference" content="Bradshaw MF, Elliot KM, Watt SJ, Davies IR (2002) Do observers exploit binocular disparity information in motor tasks within dynamic telepresence environments? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:331&#8211;342"/>

    <meta name="citation_reference" content="Schmit A, Grasnik A (2002) Multiviewpoint autostereoscopic displays from 4D-vision. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:212&#8211;221"/>

    <meta name="citation_reference" content="Perlin K, Paxia S, Kollin J (2000) An autostereoscopic display. In: Proceedings of SIG-GRAPH, ACM conference on computer graphics and interactive techniques, pp 319&#8211;326"/>

    <meta name="citation_reference" content="McKnight S, Melder N, Barrow AL, Harwin WS, Wann JP (2005) Perceptual cues for orientation in a two finger haptic grasp task. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 549&#8211;550"/>

    <meta name="citation_reference" content="citation_journal_title=J Aust Inst Mining Metall; citation_title=Sensory feedback and remote control of machines in mining and extraterrestrial environments; citation_author=BL Richardson, DB Wuillemin, MA Symmons; citation_volume=2; citation_publication_date=2004; citation_pages=53-56; citation_id=CR27"/>

    <meta name="citation_reference" content="Woods A (2003) Seeing in depth at depth. Newsletter of the Centre for Marine Science &amp; Technology, Sept. Available at: 
                    http://www.cmst.curtin.edu.au/brochures/cmstnewsletter4.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="Kugah DA (1972) Experiments evaluating compliance and force feedback effect on manipulator performance. Genral Elec Corp NASA &#8211; CR 128605 Philadelphia"/>

    <meta name="citation_reference" content="Gunn C, Hutchins M, Adcock M, Hawkins R (2003) Trans-world Haptic collaboration, In: Proceedings of the SIGGRAPH Conference, Sketches and Applications, p 1"/>

    <meta name="citation_reference" content="Oakley I, O&#8217;Modhrain S (2005) Tilt to scroll: evaluating a motion based vibrotactile mobile interface. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 40&#8211;49"/>

    <meta name="citation_reference" content="Nojima T, Funabiki K (2005) Cockpit display using tactile sensations. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 501&#8211;502"/>

    <meta name="citation_reference" content="Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor location identification in zero-g. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 56&#8211;62"/>

    <meta name="citation_reference" content="Rauterberg M (1999) New directions in user-system interaction: augmented reality, ubiquitous and mobile computing. In: Proceedings of IEEE symposium on human interfacing, pp 105&#8211;133"/>

    <meta name="citation_reference" content="Lindeman RW, Sibert JL, Mendez-Mendez E, Patil S, Phifer D (2005) Effectiveness of directional vibrotactile cuing on a building-clearing task. In: Proceedings of ACM CHI, pp 271&#8211;280"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Tactile discrimination of competing sounds; citation_author=BL Richardson, DB Wuillemin, F Saunders; citation_volume=24; citation_publication_date=1978; citation_pages=546-550; citation_id=CR36"/>

    <meta name="citation_reference" content="Hoffman H, Groen J, Rousseau S, Hollander A, Winn W, Wells M, Furness T (1996) Tactile augmentation: enhancing presence in virtual reality with tactile feedback from real objects. In: Meeting of the American Psychological Society, San Francisco. Available at: 
                    http://www.hitl.washington.edu/publications/p-96&#8211;1/
                    
                  
                        "/>

    <meta name="citation_reference" content="McGrath BJ, Estrada A, Braithwaite MG, Raj AK, Rupert AH, (2004). Tactile situation awareness system flight demonstration final report USAARL Report 2004&#8211;10, March"/>

    <meta name="citation_reference" content="See details of cybergrasp at: 
                    http://www.immersion.com/3d/products/cyber_grasp.php
                    
                  
                        "/>

    <meta name="citation_reference" content="Richardson BL, Wuillemin DB, Symmons MA, Accardi R (2005) The Exograsp delivers tactile and kinaesthetic information about virtual objects. In: IEEE Tencon conference, November, Melbourne"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Display of holistic sensation by combined tactile and kinaesthetic feedback; citation_author=P Kammermeier, A Kron, J Hoogen, G Schmit; citation_volume=13; citation_issue=1; citation_publication_date=2004; citation_pages=1-15; citation_doi=10.1162/105474604774048199; citation_id=CR41"/>

    <meta name="citation_reference" content="Seay AF, Krum DM, Hodges L, Ribarsky W (2001) Simulator sickness and presence in a high FOV virtual environment. In: Proceedings of the virtual reality 2001 conference, pp 299&#8211;300"/>

    <meta name="citation_reference" content="citation_title=Interaction between the senses: vision and the vestibular system; citation_inbook_title=Signals and perception; citation_publication_date=2002; citation_id=CR43; citation_author=R Stott; citation_publisher=Palgrave Macmillan"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Hearing lips and seeing voices; citation_author=H McGurk, T MacDonald; citation_volume=264; citation_publication_date=1976; citation_pages=746-748; citation_doi=10.1038/264746a0; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Res; citation_title=Shape and color in apparent motion; citation_author=P Kohlers, M Grunau; citation_volume=16; citation_publication_date=1976; citation_pages=329-335; citation_doi=10.1016/0042-6989(76)90192-9; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Hum Percept Perform; citation_title=Cross-modal dynamic capture: congruency effects in the perception of motion across sensory modalities; citation_author=S Soto-Faraco, C Spence, A Kingstone; citation_volume=30; citation_issue=2; citation_publication_date=2004; citation_pages=330-345; citation_doi=10.1037/0096-1523.30.2.330; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_title=Sensation and perception; citation_publication_date=1996; citation_id=CR47; citation_author=HR Shiffman; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=Perception; citation_title=Haptic dominance in form perception with blurred vision; citation_author=MA Heller; citation_volume=122; citation_publication_date=1983; citation_pages=607-613; citation_doi=10.1068/p120607; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_title=Multisensory integration of dynamic information; citation_inbook_title=Handbook of multisensory processes; citation_publication_date=2004; citation_id=CR49; citation_author=S Soto-Faraco; citation_author=A Kingstone; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Tactile &#8220;capture&#8221; of audition; citation_author=A Caclin, S Soto-Faraco, A Kingstone, C Spence; citation_volume=18; citation_publication_date=2002; citation_pages=55-60; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Rehabilit Eng; citation_title=The TDS: A new device for comparing active and passive-guided touch; citation_author=BL Richardson, MA Symmons, R Accardi; citation_volume=8; citation_publication_date=2000; citation_pages=414-417; citation_doi=10.1109/86.867883; citation_id=CR51"/>

    <meta name="citation_reference" content="Symmons MA, Richardson BL, Wuillemin DB, VanDoorn GH (2005) Kinaesthetic and cutaneous contributions to raised-line stimulus interpretation. In: World haptics conference, 18&#8211;20 March, Pisa. Video clip at 
                    http://www-personal.monash.edu.au/~msymmons/images/6_9_qt.mov
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Nature (London); citation_title=Exploring pictures tactually; citation_author=LE Magee, JM Kennedy; citation_volume=283; citation_publication_date=1980; citation_pages=287; citation_doi=10.1038/283287a0; citation_id=CR53"/>

    <meta name="citation_reference" content="citation_title=The relative importance of cutaneous and kinesthetic cues in raised line drawing exploration; citation_inbook_title=Touch, blindness, and neuroscience; citation_publication_date=2004; citation_pages=247-250; citation_id=CR54; citation_author=BL Richardson; citation_author=M Symmons; citation_author=DB Wuillemin; citation_publisher=Universidad Nacional de Educaci&#243;n a Distancia"/>

    <meta name="citation_reference" content="citation_title=Active versus passive touch: Superiority depends more on the task than the mode; citation_inbook_title=Touch, blindness, and neuroscience; citation_publication_date=2004; citation_pages=179-185; citation_id=CR55; citation_author=M Symmons; citation_author=BL Richardson; citation_author=DB Wuillemin; citation_publisher=Universidad Nacional de Educaci&#243;n a Distancia"/>

    <meta name="citation_reference" content="Wuillemin DB, VanDoorn GH, Richardson BL, Symmons MA (2005) Haptic and visual size judgements in virtual and real environments. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 86&#8211;89"/>

    <meta name="citation_reference" content="Sarter N (1998) Turning automation into a teamplayer: The development of multisensory and graded feedback for highly automated (flight deck) systems. Willard Airport Aviation Research Lab, 
                    http://www.nsfworkshop.engr.ucf.edu/papers/Sarter.asp/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Haptic teleoperation of a mobile robot: a user study; citation_author=S Lee, GS Sukhatme, GJ Kim, C Park; citation_volume=14; citation_issue=3; citation_publication_date=2005; citation_pages=345-365; citation_doi=10.1162/105474605323384681; citation_id=CR58"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Man&#8211;Mach Syst; citation_title=Optical-to-tactile image conversion for the blind; citation_author=JC Bliss, MH Katcher, CH Roger, R Shepard; citation_volume=MMS-11; citation_publication_date=1970; citation_pages=58-64; citation_doi=10.1109/TMMS.1970.299963; citation_id=CR59"/>

    <meta name="citation_reference" content="citation_journal_title=J Speech Hear Res; citation_title=Word and feature identification by profoundly deaf teenagers using the Queen&#8217;s University Tactile Vocoder; citation_author=PL Brooks, BJ Frost, JL Mason, DM Gibson; citation_volume=30; citation_publication_date=1987; citation_pages=137-141; citation_id=CR60"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Cutaneous coding of optical signals: the Optohapt; citation_author=FA Geldard; citation_volume=1; citation_publication_date=1966; citation_pages=377-381; citation_id=CR61"/>

    <meta name="citation_reference" content="citation_journal_title=J Psychol; citation_title=Sensory substitution and the design of an artificial ear; citation_author=BL Richardson, BJ Frost; citation_volume=96; citation_publication_date=1977; citation_pages=259-285; citation_id=CR62"/>

    <meta name="citation_reference" content="Gallace A, Tan HZ, Spence C (2005) Tactile change detection. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 12&#8211;16"/>

    <meta name="citation_reference" content="van Erp JBF (2005) Vibrotactile spatial acuity on the torso: effects of location and timing parameters. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 80&#8211;85"/>

    <meta name="citation_author" content="Barry Richardson"/>

    <meta name="citation_author_email" content="barry.richardson@arts.monash.edu.au"/>

    <meta name="citation_author_institution" content="Bionics and Cognitive Science Centre, Monash University, Churchill, Australia"/>

    <meta name="citation_author" content="Mark Symmons"/>

    <meta name="citation_author_institution" content="Bionics and Cognitive Science Centre, Monash University, Churchill, Australia"/>

    <meta name="citation_author" content="Dianne Wuillemin"/>

    <meta name="citation_author_institution" content="Bionics and Cognitive Science Centre, Monash University, Churchill, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0020-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/04/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0020-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="The contribution of virtual reality to research on sensory feedback in remote control"/>
        <meta property="og:description" content="Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality and telepresence in that research. We argue that full automation is a distant goal and that remote control deserves continued attention and improvement. Visual feedback to controllers has developed in various ways but autostereoscopic displays have yet to be proven. Haptic force feedback, in both real and virtual settings, has been demonstrated to offer much to the remote control environment and has led to a greater understanding of the kinesthetic and cutaneous components of haptics, and their role in multimodal processes, such as sensory capture and integration. We suggest that many displays using primarily visual feedback would benefit from the addition of haptic information but that much is yet to be learned about optimizing such displays."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>The contribution of virtual reality to research on sensory feedback in remote control | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0020-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Haptic, Feedback, Active, Passive, Kinesthetic, Cutaneous, Virtual reality, Perception","kwrd":["Haptic","Feedback","Active","Passive","Kinesthetic","Cutaneous","Virtual_reality","Perception"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0020-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0020-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=20;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0020-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            The contribution of virtual reality to research on sensory feedback in remote control
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0020-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0020-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-03-03" itemprop="datePublished">03 March 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">The contribution of virtual reality to research on sensory feedback in remote control</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Barry-Richardson" data-author-popup="auth-Barry-Richardson" data-corresp-id="c1">Barry Richardson<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Monash University" /><meta itemprop="address" content="grid.1002.3, 0000000419367857, Bionics and Cognitive Science Centre, Monash University, Gippsland Campus, 3842, Churchill, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mark-Symmons" data-author-popup="auth-Mark-Symmons">Mark Symmons</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Monash University" /><meta itemprop="address" content="grid.1002.3, 0000000419367857, Bionics and Cognitive Science Centre, Monash University, Gippsland Campus, 3842, Churchill, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dianne-Wuillemin" data-author-popup="auth-Dianne-Wuillemin">Dianne Wuillemin</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Monash University" /><meta itemprop="address" content="grid.1002.3, 0000000419367857, Bionics and Cognitive Science Centre, Monash University, Gippsland Campus, 3842, Churchill, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">234</span>–<span itemprop="pageEnd">242</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">240 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0020-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Here we consider research on the kinds of sensory information most effective as feedback during remote control of machines, and the role of virtual reality and telepresence in that research. We argue that full automation is a distant goal and that remote control deserves continued attention and improvement. Visual feedback to controllers has developed in various ways but autostereoscopic displays have yet to be proven. Haptic force feedback, in both real and virtual settings, has been demonstrated to offer much to the remote control environment and has led to a greater understanding of the kinesthetic and cutaneous components of haptics, and their role in multimodal processes, such as sensory capture and integration. We suggest that many displays using primarily visual feedback would benefit from the addition of haptic information but that much is yet to be learned about optimizing such displays.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Automation or remote control?</h2><div class="c-article-section__content" id="Sec1-content"><p>Autonomous robots capture the imagination but not much else—yet. There is no doubt that machines are better than humans at certain complex tasks. Some controls of modern fighter aircraft, for example, require reaction times beyond human capability [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Schmitt VR, Morris JW, Jenney GD (1998) Fly-by-wire. Society of Automotive Engineers Philadelphia" href="/article/10.1007/s10055-006-0020-z#ref-CR1" id="ref-link-section-d81846e312">1</a>]. At the other extreme there are tasks so repetitive that humans become bored and error-prone as a consequence of reduced vigilance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Wallace JC, Vodanovich SJ, Restino R (2003) Predicitng cognitive failures from boredom proneness and daytime sleepiness scores: an investigation within military and undergraduate samples. Pers Individ Dif 34:635–644" href="/article/10.1007/s10055-006-0020-z#ref-CR2" id="ref-link-section-d81846e315">2</a>]. Between these two extremes there are myriad operations that can be performed either by humans using remotely controlled devices or autonomous machines, and though the choice is typically based on cost-effectiveness, it is not always easy to choose because technological advances in automation are often potentially useful in remote control, and vice versa. For example, better sensors on autonomous robots can be valuable in “master–slave” applications, particularly when haptic information is added to the visual information the master receives [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Diolaiti N, Melchiorri C (2002) Tele-operation of a mobile robot through haptic feedback. In: HAVE, IEEE international workshop on haptic virtual environments and their applications" href="/article/10.1007/s10055-006-0020-z#ref-CR3" id="ref-link-section-d81846e318">3</a>].</p><p>It could be argued that neural networks and other ways of making machines intelligent will speed up the automation process. While this may true, we argue that, in the meantime, autonomous robots remain best suited to relatively predictable and/or repetitive tasks requiring minimal adaptation to changes in the environment. Remote control is a pragmatic alternative to automation because:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>It is invariably cheaper and less prone to technical failure than full automation</p>
                  </li>
                  <li>
                    <p>It makes use of the flexibility and adaptability of humans (useful in unexpected situations or when there is a high degree of variation in the task environment)</p>
                  </li>
                  <li>
                    <p>Experience gained in remote control can teach us relatively inexpensive lessons about the features that a fully automated system might eventually have.</p>
                  </li>
                </ul><p>Automation and remote control are not, of course, mutually exclusive. Remotely operated vehicles (ROVs) used in underwater or extraterrestrial settings, for example, may be required to travel through environments with obstacles so hard to predict that remote control (e.g., from a nearby base) is called for. Manipulation and analysis of samples collected by the same machine could then be done automatically so that in one machine there may be both autonomous and remote control. The manner in which unexpected challenges are successfully dealt with in remote control can be used to guide the design of an autonomous robot to do the same job in the future.</p><p>Whatever the decision about the relative merits of full automation or remote control, virtual reality is playing an increasingly important role in the evaluation process, as we seek to illustrate in this paper. In addition, we argue that the contribution of haptics to effective remote control of machines has yet to be fully appreciated.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Problems with remote control</h2><div class="c-article-section__content" id="Sec2-content"><p>It is not hard to find examples of skilled operation of machinery, but removed from the machine and given remote controls, an operator’s efficiency can be markedly reduced [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510" href="/article/10.1007/s10055-006-0020-z#ref-CR4" id="ref-link-section-d81846e352">4</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Gupta R, Sheridan T, Whitney D (1997) Experiments using multimodal virtual environments in design for assembly analysis. Presence Teleop Virtual Environ 6(3):318–338" href="/article/10.1007/s10055-006-0020-z#ref-CR6" id="ref-link-section-d81846e355">6</a>]. This may be due to less then optimal vision [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Hainsworth DW (2001) Teleoperation user interfaces for mining robotics. Auton Robots 11(1):19–28" href="/article/10.1007/s10055-006-0020-z#ref-CR7" id="ref-link-section-d81846e358">7</a>], lack of haptic feedback [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Elhajj I, Xi N, Fung WK, Liu YH, Li WJ, Kaga T, Fukuda T (2001) Haptic information in internet-based teleoperation. IEEE/ASME Transactions on Mechatronics 6(3): 295–304" href="/article/10.1007/s10055-006-0020-z#ref-CR8" id="ref-link-section-d81846e361">8</a>], and/or reduced auditory information [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510" href="/article/10.1007/s10055-006-0020-z#ref-CR4" id="ref-link-section-d81846e364">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Roberts JW, Slattery OT, Swope B, Volker M, Comstock T (2002) Small-scale tactile graphics for virtual reality systems. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:422–429" href="/article/10.1007/s10055-006-0020-z#ref-CR9" id="ref-link-section-d81846e368">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Grohn M (2002) Is audio useful in immersive visualization? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:411–421" href="/article/10.1007/s10055-006-0020-z#ref-CR10" id="ref-link-section-d81846e371">10</a>]. It has also been noted that impoverished feedback reduces the sense of “being there” or what is often called “presence”. While presence is clearly critical in virtual reality applications (since it is defined in terms of subjective realism), it is not always associated with improved performance. In one study, for example, force feedback was judged by participants to be useful in a virtual assembly task and to improve realism, but it was also associated with an increase in collision errors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Edwards GW, Barfield W, Nussbaum MA (2004) The use of force feedback and auditory cues for performance of an assembly task in an immersive virtual environment. Virtual Real 7:112–119" href="/article/10.1007/s10055-006-0020-z#ref-CR11" id="ref-link-section-d81846e374">11</a>]. However, it is not clear whether subjects in this study were told to avoid collisions, and they may therefore have sought them as sources of haptic confirmation of the objects’ location, much as a blind person might use a cane to detect obstacles.</p><p>Even if presence is useful, it could be argued that it is not necessary for teleremote operation of machines, particularly when the critical information is largely visual. However, there is evidence that presence is positively correlated with performance in a variety of circumstances, including tasks involving assembly [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Grohn M (2002) Is audio useful in immersive visualization? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:411–421" href="/article/10.1007/s10055-006-0020-z#ref-CR10" id="ref-link-section-d81846e380">10</a>], collaboration [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Sallnas EL Rassmus-Grohn K, Sjostrom C (2001) Supporting presence in collaborative environments by haptic feedback. ACM Trans Comput–Hum Interact 7(4):461–476" href="/article/10.1007/s10055-006-0020-z#ref-CR12" id="ref-link-section-d81846e383">12</a>], surgical teleoperation and other medical applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Kazi A (2001) Operator performance in surgical telemanipulation. Presence 10:495–510" href="/article/10.1007/s10055-006-0020-z#ref-CR13" id="ref-link-section-d81846e386">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Raspolli M, Avizzano CA, Facenza G, Bergamasco M (2005) HERMES: an angioplasty surgery simulator. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 148–156" href="/article/10.1007/s10055-006-0020-z#ref-CR14" id="ref-link-section-d81846e389">14</a>], and clearing of buildings in virtual military and emergency settings [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Jiang L, Girotra R, Cutkosky MR, Ullrich C (2005) Reducing error rates with low-cost haptic feedback in virtual reality-based training applications. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 420–425" href="/article/10.1007/s10055-006-0020-z#ref-CR15" id="ref-link-section-d81846e392">15</a>]. Thus, how real a remote control situation is perceived to be may be one way of predicting the effectiveness of that system.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Presence and distal attribution</h2><div class="c-article-section__content" id="Sec3-content"><p>Any relationship between presence and performance depends on the sensory information available and this in turn affects the process of distal attribution or “externalisation” of the percept. This is an aspect of realism and refers to our tendency to attend not to the receptors actually stimulated (the proximal stimuli) but to the objects “out there”—the distal stimuli—which we perceive as being responsible for our receptor states. It is a remarkable skill that allows us to perceive objects in three-dimensional space provided that certain sensory conditions are met.</p><p>Using a device called the Tactile Vision Substitution System (TVSS), Bach-y-Rita et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="White BW, Saunders FA, Scadden L, Bach-y-Rita P, Collins CC (1970) Seeing with the skin. Percept Psychophys 7:23–27" href="/article/10.1007/s10055-006-0020-z#ref-CR16" id="ref-link-section-d81846e405">16</a>] and colleagues gave blind people tactile patterns whose regions of activity corresponded to light (or dark) areas of a visual field scanned by a TV camera. Behind the camera lens was an array of photoelectric cells that could activate a vibrator in contact with the blind subject’s skin. At first, the tactile patterns were felt as just that, but when given control over the camera, so that movements made by the subjects caused correlated apparent movements across the tactor array, some subjects reported the percept to be “out there” rather than located on the skin.</p><p>It may in fact be misleading to classify touch as a proximal sense, in contrast to vision and hearing as distal senses. Although proximal tactile stimuli <i>can</i> be attended to (in a manner not easily mirrored in vision, or echoed in audition), we often pay more attention to the distal attributes of a tactile stimulus. For example, when riding a bicycle on a rough surface, contact with the saddle, pedals, and handlebars can be our attentional focus, or we can instead perceive the surface of the road itself as the distal stimulus, and in fact the one of greatest interest [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Kennedy JM, Richardson BL, Magee LE (1980) The nature of haptics. In: Hagen M (ed) The perception of pictures. Academic, New York" href="/article/10.1007/s10055-006-0020-z#ref-CR17" id="ref-link-section-d81846e414">17</a>]. We may go even further and argue that the handlebars, though proximal in the sense that they touch the skin, are distal in the sense that they make up an object external to us, to which we pay more attention than the “state” of our skin receptors. Thus, most haptic percepts may be regarded as enjoying distal attribution similar to that characteristic of vision and hearing. Presence and distal attribution may be closely related perceptual phenomena because they both have to do with how we represent reality.</p><p>If this is correct, then haptic displays should, as far as possible, give rise to percepts (of objects for example) that agree with information from the other senses, rather than conflict with them, or bear no relation to them. In this way, advantages of multimodal processes can be realized. Such processes have been shown to enhance presence and performance in a variety of tasks relevant to remote control [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Baier H, Buss M, Freyberger F, Hoogen J, Kammermeier P, Schmidt G (1999) Distributed PC-based haptic, visual and acoustic telepresence system experiments in virtual and remote environments. In: Proceedings of IEEE virtual reality conference, p 118" href="/article/10.1007/s10055-006-0020-z#ref-CR18" id="ref-link-section-d81846e420">18</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Grane C, Bengtsson P (2005) Menu selection with a rotary device founded on haptic and/or graphic information. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 475–476" href="/article/10.1007/s10055-006-0020-z#ref-CR20" id="ref-link-section-d81846e423">20</a>] and Miner et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interface. In: Proceedings IMAGE conference, pp 23–35" href="/article/10.1007/s10055-006-0020-z#ref-CR19" id="ref-link-section-d81846e426">19</a>] suggest that distal attribution involves the integration of multimodal sensory experience into “single sense-making occurrences in the external world” and suggest that realism might be achieved by interaction between modalities dealing with the same stimulus.</p><p>Does it follow from this that displays consisting of codes that do not allow multimodal cross referencing, or that restrict stimuli to arrays that discourage interaction, may not be optimal? This question is central to designers of feedback in remote control.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Improving visual depth perception</h2><div class="c-article-section__content" id="Sec4-content"><p>Although depth perception relies on many sources of information, one of the most powerful is the “retinal disparity” that results from having two spatially separated eyes and therefore seeing slightly different aspects of an object, or objects [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Palmer S (2002) Vision sciences. Bradford Books Cambridge" href="/article/10.1007/s10055-006-0020-z#ref-CR21" id="ref-link-section-d81846e440">21</a>]. The greater the disparity, the nearer the object, although we are not conscious of using this information in judging depth. Three-dimensional displays in movies deliver a different image to each eye of an observer. This difference mimics the retinal disparity present in normal binocular vision and can be achieved with special glasses worn by the observer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Goldstein BE (1999) Sensation and perception 5th edn. Brooks Cole, Pacific Grove" href="/article/10.1007/s10055-006-0020-z#ref-CR22" id="ref-link-section-d81846e443">22</a>]. Each lens of the spectacles has a filter of some kind (e.g., polarisation, colour, or shutter) to produce the disparity, and the brain fuses the images into one to yield a single percept in which extent of disparity is inversely proportional to perceived depth (or distance) of the viewed object. Systems using such spectacles and displays have been developed to improve motor performance in dynamic telepresence environments [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Bradshaw MF, Elliot KM, Watt SJ, Davies IR (2002) Do observers exploit binocular disparity information in motor tasks within dynamic telepresence environments? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:331–342" href="/article/10.1007/s10055-006-0020-z#ref-CR23" id="ref-link-section-d81846e446">23</a>].</p><p>Recently there have been significant developments in autostereoscopic displays. These flat-screened LCD monitors yield stereoscopic images without the need for glasses [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Schmit A, Grasnik A (2002) Multiviewpoint autostereoscopic displays from 4D-vision. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:212–221" href="/article/10.1007/s10055-006-0020-z#ref-CR24" id="ref-link-section-d81846e452">24</a>]. Essentially the observer’s right and left eyes receive disparate views because the source image on the screen is covered with an invisible vertical grid that separates the two views. Alternate columns of pixels project either to the right or to the left eye only. The different views are recorded by up to nine cameras, the outputs of which are combined into “double image” displays by purpose-built software. Recent versions have overcome the restriction of having to view the monitor from directly in front only (the single “sweet” spot) and can now be viewed from several directions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Schmit A, Grasnik A (2002) Multiviewpoint autostereoscopic displays from 4D-vision. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:212–221" href="/article/10.1007/s10055-006-0020-z#ref-CR24" id="ref-link-section-d81846e455">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Perlin K, Paxia S, Kollin J (2000) An autostereoscopic display. In: Proceedings of SIG-GRAPH, ACM conference on computer graphics and interactive techniques, pp 319–326" href="/article/10.1007/s10055-006-0020-z#ref-CR25" id="ref-link-section-d81846e458">25</a>], although there is still variation in opinions about how compelling the three-dimensionality is and debate about how much it affects performance [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="McKnight S, Melder N, Barrow AL, Harwin WS, Wann JP (2005) Perceptual cues for orientation in a two finger haptic grasp task. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 549–550" href="/article/10.1007/s10055-006-0020-z#ref-CR26" id="ref-link-section-d81846e461">26</a>]. As a form of feedback during teleoperation, autostereoscopic displays have the potential to improve depth perception while reducing the negative reactions to head-mounted displays and glasses [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Richardson BL, Wuillemin DB, Symmons MA (2004) Sensory feedback and remote control of machines in mining and extraterrestrial environments. J Aust Inst Mining Metall 2:53–56" href="/article/10.1007/s10055-006-0020-z#ref-CR27" id="ref-link-section-d81846e464">27</a>]. Which kind of visual display is preferable, and indeed whether three-dimensional vision is better than two-dimensional plus haptics and/or audition, remains an empirical question. However, a device using three-dimensional technology has been used effectively for remote control in underwater exploration [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Woods A (2003) Seeing in depth at depth. Newsletter of the Centre for Marine Science &amp; Technology, Sept. Available at: &#xA;                    http://www.cmst.curtin.edu.au/brochures/cmstnewsletter4.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0020-z#ref-CR28" id="ref-link-section-d81846e468">28</a>].</p><p>Another consideration is general visibility. In well-lit areas without “clutter”, stereoscopic vision may be an unnecessary luxury, but in a novel or unpredictable environment, or where safe travel between way-points is needed, three-dimensional vision may be crucial because other visual cues may be absent or impoverished. For example, one powerful monocular cue to depth—the “retinal size” of an object—relies on previous experience. A tennis ball for instance is of known size and a novel object of a different size (e.g., a little green man, or a rock on Mars) can be judged in relation to the tennis ball if both are visible at the same time. But there is no “standard” size of a rock to remember, so its retinal size is not much help in unfamiliar settings, such as underground mining tunnels or other worlds. In contrast, cues that use retinal disparity do not require such experience, and their use is probably innate [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Palmer S (2002) Vision sciences. Bradford Books Cambridge" href="/article/10.1007/s10055-006-0020-z#ref-CR21" id="ref-link-section-d81846e474">21</a>].</p><p>Despite improvements in stereoscopic feedback, there will be times when vision alone is insufficient, and haptic feedback is then likely to prove extremely useful. For instance, the throttles on patrol boats used by the Norwegian Navy became harder to use when they were digitised and analogue haptic information was lost [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510" href="/article/10.1007/s10055-006-0020-z#ref-CR4" id="ref-link-section-d81846e480">4</a>]. The problem was exacerbated by the dark and noisy environment of a patrol boat—the lost haptic information could not readily be made visual or auditory. Similar problems may be encountered in a mining site or tunnel where dust is unavoidable. The utility of haptic feedback in mining-like environments was demonstrated some time ago [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Kugah DA (1972) Experiments evaluating compliance and force feedback effect on manipulator performance. Genral Elec Corp NASA – CR 128605 Philadelphia" href="/article/10.1007/s10055-006-0020-z#ref-CR29" id="ref-link-section-d81846e483">29</a>], and the feasibility of teleoperation of mining machinery is now well established [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Hainsworth DW (2001) Teleoperation user interfaces for mining robotics. Auton Robots 11(1):19–28" href="/article/10.1007/s10055-006-0020-z#ref-CR7" id="ref-link-section-d81846e486">7</a>]. However, only recently has the mining industry shown interest in the use of haptic feedback in addition to visual feedback [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Richardson BL, Wuillemin DB, Symmons MA (2004) Sensory feedback and remote control of machines in mining and extraterrestrial environments. J Aust Inst Mining Metall 2:53–56" href="/article/10.1007/s10055-006-0020-z#ref-CR27" id="ref-link-section-d81846e489">27</a>].</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Haptic feedback</h2><div class="c-article-section__content" id="Sec5-content"><p>In relation to teleoperated robots, Elhajj et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Elhajj I, Xi N, Fung WK, Liu YH, Li WJ, Kaga T, Fukuda T (2001) Haptic information in internet-based teleoperation. IEEE/ASME Transactions on Mechatronics 6(3): 295–304" href="/article/10.1007/s10055-006-0020-z#ref-CR8" id="ref-link-section-d81846e500">8</a>] state: “Many tasks done easily by humans turn out to be very difficult to accomplish with a teleoperated robot. The main reason for this is the lack of tactile sensing” (p. 295). These authors argue that with visual information alone, teleoperation resembles an open loop system, but becomes closed loop with the introduction of haptic feedback (although visual feedback may still be present). The loop is closed, they suggest, when the operator’s manipulation of the controls causes not only corresponding movement on the machine, but the sensors on the machine feed back the <i>consequences</i> of the operator’s commands. This often leads to improvements in performance and can be attributed to kinesthetic information (typically provided in the form of force feedback), or tactile information delivered to the skin as cutaneous stimulation.</p><h3 class="c-article__sub-heading" id="Sec6">Force feedback</h3><p>A significant component of haptic information can be conveyed with force feedback devices such as the Phantom. The user holds a probe connected to shafts linked through several joints giving a total of six degrees of freedom. Sensors indicate the position of the probe in relation to a virtual object, the surfaces of which are defined by spatial coordinates as part of a software collision algorithm. When the probe (seen on the PC screen as a cursor) “reaches” the virtual surface, motors at linkage points are activated and resistance is felt in a manner analogous to a blind person sensing a surface at the end of a cane. The motors can also “drive” the probe, and therefore the hand holding it, as would happen if the object being explored is elastic and bounces back, or is in motion so as to push the probe. Virtual objects of great variety, complexity, and texture can be computer-generated, explored, and moved in haptic space within limits set only by the software that defines the object’s characteristics, and the capacity of the human kinesthetic system to detect and interpret the inputs. Two users in different countries can share the same virtual space and “shake hands” (or sticks) over the Internet by holding and moving their respective probes [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Gunn C, Hutchins M, Adcock M, Hawkins R (2003) Trans-world Haptic collaboration, In: Proceedings of the SIGGRAPH Conference, Sketches and Applications, p 1" href="/article/10.1007/s10055-006-0020-z#ref-CR30" id="ref-link-section-d81846e513">30</a>].</p><p>This technology can be adapted for use with a remote machine that has sensors for movement and vibrations that are transmitted to controls felt by the operator. The haptic feedback may be the same as that available at the remote machine (the usual choice), or possibly a transformation or enhancement of it. For example, vibratory tactile feedback has been used successfully to navigate through a list of items on a hand-held computer, with movement through the list controlled by tilting the device [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Oakley I, O’Modhrain S (2005) Tilt to scroll: evaluating a motion based vibrotactile mobile interface. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 40–49" href="/article/10.1007/s10055-006-0020-z#ref-CR31" id="ref-link-section-d81846e519">31</a>]. Vibration has also been used to convey altitude to aircraft pilots [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Nojima T, Funabiki K (2005) Cockpit display using tactile sensations. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 501–502" href="/article/10.1007/s10055-006-0020-z#ref-CR32" id="ref-link-section-d81846e522">32</a>], and tactile stimulation of the torso has been used to indicate attitude in zero gravity [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor location identification in zero-g. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 56–62" href="/article/10.1007/s10055-006-0020-z#ref-CR33" id="ref-link-section-d81846e525">33</a>].</p><p>In visual displays, a virtual image may be superimposed on one delivered by the camera to create augmented reality—a mixture of real and virtual images—that conveys extra information such as temperature. Similarly, haptic feedback may be made better than that available in a real setting. As an example, hydraulic controls could be augmented with feedback indicating mass or resistance at the workface of a mining machine [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Richardson BL, Wuillemin DB, Symmons MA (2004) Sensory feedback and remote control of machines in mining and extraterrestrial environments. J Aust Inst Mining Metall 2:53–56" href="/article/10.1007/s10055-006-0020-z#ref-CR27" id="ref-link-section-d81846e531">27</a>], and information that may be lost due to decoupling of the machine and operator, as described by Bjelland et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510" href="/article/10.1007/s10055-006-0020-z#ref-CR4" id="ref-link-section-d81846e534">4</a>], could be restored in either a faithful or an augmented form. Hybrids are possible too such that a ROV may have controls that allow it to be driven by a person, or it may be remotely controlled for greater safety, duration, and distance. Alternatively, the machine may not be manually “drivable” at all. The controls could be present at the control site only, with commands and feedback signals received and transmitted by specialised systems on the machine.</p><p>Whatever the remote system envisaged, feedback is likely to enhance performance. The boom used on spacecraft for maintenance and repair of exterior parts is an example of a remote system likely to benefit from haptic feedback. For instance, a screw head in which a screwdriver has repeatedly slipped, or a rounded hexagonal bolt head, may become extremely difficult or impossible to remove. However, a skilled mechanic can sometimes feel (though not see) when there is risk of such damage and make adjustments requiring fine control over pressure and torque while testing for feedback indicating incipient slip. In general, the contribution of haptic information in such demanding environments has been neglected [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Rauterberg M (1999) New directions in user-system interaction: augmented reality, ubiquitous and mobile computing. In: Proceedings of IEEE symposium on human interfacing, pp 105–133" href="/article/10.1007/s10055-006-0020-z#ref-CR34" id="ref-link-section-d81846e540">34</a>], partly because, as Bjelland et al. put it, “People’s haptic interaction with the world is often subconscious, and many of the qualities of touch are easily overlooked, as they are subtle and difficult to verbalise” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510" href="/article/10.1007/s10055-006-0020-z#ref-CR4" id="ref-link-section-d81846e543">4</a>, p. 509]</p><h3 class="c-article__sub-heading" id="Sec7">Tactile information</h3><p>Tactile vibratory feedback has been shown to improve performance and a sense of presence in a variety of real and virtual spatial tasks [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Lindeman RW, Sibert JL, Mendez-Mendez E, Patil S, Phifer D (2005) Effectiveness of directional vibrotactile cuing on a building-clearing task. In: Proceedings of ACM CHI, pp 271–280" href="/article/10.1007/s10055-006-0020-z#ref-CR35" id="ref-link-section-d81846e554">35</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Richardson BL, Wuillemin DB, Saunders F (1978) Tactile discrimination of competing sounds. Percept Psychophys 24: 546–550" href="/article/10.1007/s10055-006-0020-z#ref-CR36" id="ref-link-section-d81846e557">36</a>], and in a mixture of the two in which a real object is touched to improve the sense of reality in a visually virtual environment [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Hoffman H, Groen J, Rousseau S, Hollander A, Winn W, Wells M, Furness T (1996) Tactile augmentation: enhancing presence in virtual reality with tactile feedback from real objects. In: Meeting of the American Psychological Society, San Francisco. Available at: &#xA;                    http://www.hitl.washington.edu/publications/p-96–1/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0020-z#ref-CR37" id="ref-link-section-d81846e560">37</a>]. To be useful as information, the tactile experience need not be a faithful copy of what would be felt by the skin in the real situation. For example, and previously mentioned the Tactile Situation Awareness System (TSAS) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="McGrath BJ, Estrada A, Braithwaite MG, Raj AK, Rupert AH, (2004). Tactile situation awareness system flight demonstration final report USAARL Report 2004–10, March" href="/article/10.1007/s10055-006-0020-z#ref-CR38" id="ref-link-section-d81846e563">38</a>] is a vest containing several tactual vibrators, activation of which helps aircraft pilots and astronauts overcome spatial disorientation, or incorrect perception of attitude, altitude and motion. In this example, touch is used as a conduit for information either unavailable or unreliable through other senses, but this application may have limited value because it requires conscious (cognitive) attention to the locus of stimulation [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor location identification in zero-g. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 56–62" href="/article/10.1007/s10055-006-0020-z#ref-CR33" id="ref-link-section-d81846e566">33</a>], and this is sometimes undesirable if cognitive load is already high. Many sensory psychologists argue that haptic displays should be as intuitive as possible and that the more “natural” a perceptual process is, the less cognitive effort is required. According to this view, perceptual processes typically occur at the unconscious level where parallel processing is possible, in contrast to the more pedestrian sequential, conscious, cognitive processes. The TVSS system is an example of such an intuitive vibratory display.</p><p>Tactile information may also accompany kinesthetic feedback that is present when using exoskeletons such as the CyberGrasp [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="See details of cybergrasp at: &#xA;                    http://www.immersion.com/3d/products/cyber_grasp.php&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0020-z#ref-CR39" id="ref-link-section-d81846e572">39</a>] or Exograsp [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Richardson BL, Wuillemin DB, Symmons MA, Accardi R (2005) The Exograsp delivers tactile and kinaesthetic information about virtual objects. In: IEEE Tencon conference, November, Melbourne" href="/article/10.1007/s10055-006-0020-z#ref-CR40" id="ref-link-section-d81846e575">40</a>]. The motors in these devices, instead of applying a force to open the hand, tighten cables to prevent the wearer’s fingers from closing to the point that the space occupied by a virtual object is invaded. These devices lack the degrees of freedom of the Phantom but do offer some advantages. For example, tactile information (e.g., temperature and pressure) can be delivered at the same time that “contact” with the virtual object is registered via kinesthesis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Richardson BL, Wuillemin DB, Symmons MA, Accardi R (2005) The Exograsp delivers tactile and kinaesthetic information about virtual objects. In: IEEE Tencon conference, November, Melbourne" href="/article/10.1007/s10055-006-0020-z#ref-CR40" id="ref-link-section-d81846e578">40</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Kammermeier P, Kron A, Hoogen J, Schmit G (2004) Display of holistic sensation by combined tactile and kinaesthetic feedback. Presence 13(1):1–15" href="/article/10.1007/s10055-006-0020-z#ref-CR41" id="ref-link-section-d81846e581">41</a>]. A single Phantom typically offers a single point of contact with the virtual object; however, two or more Phantoms can be attached to fingers to allow a pinching action similar to that possible with hand-worn exoskeletons [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="McKnight S, Melder N, Barrow AL, Harwin WS, Wann JP (2005) Perceptual cues for orientation in a two finger haptic grasp task. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 549–550" href="/article/10.1007/s10055-006-0020-z#ref-CR26" id="ref-link-section-d81846e584">26</a>].</p><p>These developments in visual and haptic virtual environments promise rich multimodal stimuli for many applications, including remote control. However, the fact that stimuli are virtual means that congruence or agreement between the inputs provided is not guaranteed, as it tends to be with real stimuli, and discordance may lead to conflict. Though this could result in nothing more serious than mild annoyance, such as that experienced by movie-goers when lip-movements and speech sounds are desynchronised, it can have more serious consequences, such as disorientation and nausea; so-called “simulator sickness” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Seay AF, Krum DM, Hodges L, Ribarsky W (2001) Simulator sickness and presence in a high FOV virtual environment. In: Proceedings of the virtual reality 2001 conference, pp 299–300" href="/article/10.1007/s10055-006-0020-z#ref-CR42" id="ref-link-section-d81846e590">42</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Stott R (2002) Interaction between the senses: vision and the vestibular system. In: Roberts D (ed) Signals and perception. Palgrave Macmillan, New York" href="/article/10.1007/s10055-006-0020-z#ref-CR43" id="ref-link-section-d81846e593">43</a>]. Disagreement between inputs can also result in a compromise percept such as that observed when a TV shows a person’s lips saying “ga”, and delivers the sound “ba”, but the final percept is “da” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="McGurk H, MacDonald T (1976) Hearing lips and seeing voices. Nature 264:746–748" href="/article/10.1007/s10055-006-0020-z#ref-CR44" id="ref-link-section-d81846e596">44</a>]. There can be competition within a sense such that, for example, a colour may change from red to blue with intermediate hues if the red and blue dots are presented in a way that engenders apparent movement between them [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Kohlers P, von Grunau M (1976) Shape and color in apparent motion. Vis Res 16:329–335" href="/article/10.1007/s10055-006-0020-z#ref-CR45" id="ref-link-section-d81846e599">45</a>]. Another possibility is that one sense may dominate or capture another—a phenomenon exploited by ventriloquists.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Sensory capture in real and virtual environments</h2><div class="c-article-section__content" id="Sec8-content"><p>When we “hear” words coming from the mouth of a ventriloquist’s dummy or when we hear the sound coming from the direction of a screen in front of us, when it is in infact coming from the projector behind us, we are experiencing visual capture [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Soto-Faraco S, Spence C, Kingstone A (2004) Cross-modal dynamic capture: congruency effects in the perception of motion across sensory modalities. J Exp Psychol Hum Percept Perform 30(2):330–345" href="/article/10.1007/s10055-006-0020-z#ref-CR46" id="ref-link-section-d81846e612">46</a>].</p><p>In most situations involving sensory conflict, vision is reported to “capture” or “dominate” haptics if information from these two modalities is discrepant [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Shiffman HR (1996) Sensation and perception, 4th edn. Wiley, New York" href="/article/10.1007/s10055-006-0020-z#ref-CR47" id="ref-link-section-d81846e618">47</a>]. Touch has rarely been reported to capture vision, although the sharpness of a knife is best tested with the skin, counterfeit money can be detected because its texture is “wrong”, and the haptic sense can dominate when visual acuity is reduced [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Soto-Faraco S, Spence C, Kingstone A (2004) Cross-modal dynamic capture: congruency effects in the perception of motion across sensory modalities. J Exp Psychol Hum Percept Perform 30(2):330–345" href="/article/10.1007/s10055-006-0020-z#ref-CR46" id="ref-link-section-d81846e621">46</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Heller MA (1983) Haptic dominance in form perception with blurred vision. Perception 122:607–613" href="/article/10.1007/s10055-006-0020-z#ref-CR48" id="ref-link-section-d81846e624">48</a>]. In an experimental situation that took full advantage of what virtual reality can offer, Miner et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interface. In: Proceedings IMAGE conference, pp 23–35" href="/article/10.1007/s10055-006-0020-z#ref-CR19" id="ref-link-section-d81846e627">19</a>] reported that haptics “captured” vision and audition separately, and in combination, when a virtual surface looked and sounded soft, but felt hard.</p><p>In current theories, intersensory conflicts have been reinterpreted in terms of models of multimodal processing that place less emphasis on competition and more on integration of information [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Soto-Faraco S, Kingstone A (2004) Multisensory integration of dynamic information. In: Calvert G, Spence C, Stein B (eds) Handbook of multisensory processes. MIT Press Cambridge" href="/article/10.1007/s10055-006-0020-z#ref-CR49" id="ref-link-section-d81846e633">49</a>]. Thus, one sense may <i>appear</i> to dominate another when a more accurate analysis is that more attention is directed to one modality’s input than another, according to the salience of the input of that modality in that particular circumstance. This has become known as the “modality appropriateness” interpretation of sensory conflict [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile “capture” of audition. Percept Psychophys 18:55–60" href="/article/10.1007/s10055-006-0020-z#ref-CR50" id="ref-link-section-d81846e639">50</a>]. Looked at this way, there is not so much a conflict between vision and touch when the sharpness of a knife is being tested, but rather a choice of which sense is best suited for the particular job. More attention is directed to the relevant channel.</p><p>Attentional preference may appear intramodally too. We asked what would happen when attention could be directed to either the kinesthetic or cutaneous aspect of a single raised line drawing (RLD).</p><p>We used a device called the Tactile Display System (TDS) see [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Richardson BL, Symmons MA, Accardi R (2000) The TDS: A new device for comparing active and passive-guided touch. IEEE Trans Rehabilit Eng 8:414–417" href="/article/10.1007/s10055-006-0020-z#ref-CR51" id="ref-link-section-d81846e648">51</a>] for full details, to separate cutaneous and kinaesthetic aspects of the same stimulus presented in two orientations, each 180° rotations of the other (e.g., p and d, 6 and 9). While exploring a 6 in the lower tray of the TDS with one finger, the subject’s other finger felt a 9 move under it (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0020-z#Fig1">1</a>). If the RLD of the 6 was removed from the lower tray, but the moving finger continued to control what the upper (stationary) finger felt, subjects indicated that they were attending to the cutaneous information provided at a stationary fingertip (the 9) in preference to the kinesthetic information simultaneously present at the other finger (a 6) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Symmons MA, Richardson BL, Wuillemin DB, VanDoorn GH (2005) Kinaesthetic and cutaneous contributions to raised-line stimulus interpretation. In: World haptics conference, 18–20 March, Pisa. Video clip at &#xA;                    http://www-personal.monash.edu.au/~msymmons/images/6_9_qt.mov&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0020-z#ref-CR52" id="ref-link-section-d81846e654">52</a>]. This result was surprising because previous research suggested that kinesthesis is a more powerful component of haptics than touch and that the primary role of touch during RLD explorations is to signal “you are on or off the line” while the kinesthetic system bears the responsibility of registering the shape depicted [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Magee LE, Kennedy JM (1980) Exploring pictures tactually. Nature (London) 283:287" href="/article/10.1007/s10055-006-0020-z#ref-CR53" id="ref-link-section-d81846e657">53</a>]. This finding was, however, consistent with the results of an earlier study in which we found cutaneous information to offer as much as kinaesthetic information about raised line drawings [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Richardson BL, Symmons M, Wuillemin DB (2004) The relative importance of cutaneous and kinesthetic cues in raised line drawing exploration. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 247–250" href="/article/10.1007/s10055-006-0020-z#ref-CR54" id="ref-link-section-d81846e660">54</a>]. If information delivered via dynamic displays to a stationary fingertip offers as much as movement (kinesthesis) then “passive” touch may offer more as a channel of communication than hitherto supposed.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1.</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0020-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0020-z/MediaObjects/10055_2006_20_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0020-z/MediaObjects/10055_2006_20_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The tactile display system (TDS) is shown here being used to allow a subject to explore a raised line drawing (RLD) of a 6 on the lower tray and have these movements cause a RLD of a 9 to be felt under the stationary fingertip above. Subjects were asked to report under varying conditions. If the raised line 6 is removed from the lower tray, leaving kinesthetic information as the major cue for “6”, and cutaneous RLD information as the major cue for “9”, most subjects reported a 9, indicating attention to cutaneous rather than kinesthetic information</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0020-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The TDS is capable of recording the movements of a finger, gripped lightly on the sides by a cradle attached to the machine, while a RLD is being explored. These movements (in the <i>x</i>–<i>y</i> plane) can be played back to guide another person’s finger over the same drawing, with speed and direction of movements precisely matched.</p><p>Using the TDS in this way we have found passive-guided performance to be better than active for outline shapes from an infinitely large set—a task with significant cognitive demands [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Symmons M, Richardson BL, Wuillemin DB (2004) Active versus passive touch: Superiority depends more on the task than the mode. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 179–185" href="/article/10.1007/s10055-006-0020-z#ref-CR55" id="ref-link-section-d81846e694">55</a>]. This finding agreed with earlier research reported in <i>Nature</i> [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Magee LE, Kennedy JM (1980) Exploring pictures tactually. Nature (London) 283:287" href="/article/10.1007/s10055-006-0020-z#ref-CR53" id="ref-link-section-d81846e700">53</a>]. However, when cognitive load was reduced by requiring only discrimination among nonsense shapes, the passive-guided superiority disappeared [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 55" title="Symmons M, Richardson BL, Wuillemin DB (2004) Active versus passive touch: Superiority depends more on the task than the mode. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 179–185" href="/article/10.1007/s10055-006-0020-z#ref-CR55" id="ref-link-section-d81846e703">55</a>]. We also found that drawings were identified just as well when passed under a stationary fingertip (cutaneous information only), as they were with only kinesthetic information present (as was the case when the subject was guided along a pathway corresponding to the outline shape, but with nothing under the fingertip) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Richardson BL, Symmons M, Wuillemin DB (2004) The relative importance of cutaneous and kinesthetic cues in raised line drawing exploration. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 247–250" href="/article/10.1007/s10055-006-0020-z#ref-CR54" id="ref-link-section-d81846e706">54</a>]. This finding challenged those of the earlier researchers [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Magee LE, Kennedy JM (1980) Exploring pictures tactually. Nature (London) 283:287" href="/article/10.1007/s10055-006-0020-z#ref-CR53" id="ref-link-section-d81846e710">53</a>].</p><p>We are currently investigating the extent to which discrepancies in the visually and haptically perceived size of <i>virtual</i> objects might affect capture and have found that sizes of real spheres were judged with small errors that did not differ significantly for either the visual or haptic modalities. However, virtual haptic spheres were perceived <i>larger</i> than their virtual visual counterparts [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 56" title="Wuillemin DB, VanDoorn GH, Richardson BL, Symmons MA (2005) Haptic and visual size judgements in virtual and real environments. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 86–89" href="/article/10.1007/s10055-006-0020-z#ref-CR56" id="ref-link-section-d81846e722">56</a>]. Thus, it cannot be assumed that sizes of objects as depicted by designers of virtual environments will be perceived in the same way across modalities. However, by manipulating and measuring the extent to which visual and haptic images agree, we can determine tolerance for a degraded image in one modality given higher fidelity in another. Virtual environments can, for instance, be used to assess the value of haptic feedback when vision is poor, such as in dusty conditions or low light.</p><p>Some of this research would not have been possible without the availability of a virtual environment. For example, it is hard to imagine how a real surface could be made to look and sound soft but feel hard [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interface. In: Proceedings IMAGE conference, pp 23–35" href="/article/10.1007/s10055-006-0020-z#ref-CR19" id="ref-link-section-d81846e728">19</a>], or how spheres could be made to change haptically while always looking the same. Interestingly, research on sensory capture/dominance seemed to languish after the 1980s, but with the coming of virtual environments, interest has returned and fresh perspectives, such as those based on multisensory integration and modality appropriateness, have emerged. Other research has not directly depended on virtual reality but has been stimulated by it. Some of these topics have theoretical as well as practical implications.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Theoretical issues</h2><div class="c-article-section__content" id="Sec9-content"><p>An implicit argument in the above is that multisensory displays are better than unimodal displays [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 57" title="Sarter N (1998) Turning automation into a teamplayer: The development of multisensory and graded feedback for highly automated (flight deck) systems. Willard Airport Aviation Research Lab, &#xA;                    http://www.nsfworkshop.engr.ucf.edu/papers/Sarter.asp/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-006-0020-z#ref-CR57" id="ref-link-section-d81846e739">57</a>]. However, to take advantage of the benefits of multimodal signals, it is not enough to simply “throw in” other stimuli or dimensions and hope for the best. The relationship between the state of the machine, or robot, and how that state is conveyed to the remote operator is important, particularly when autonomy is shared between controller and robot, as is increasingly common. For example, in one study, a mobile robot was programmed to avoid close-range obstacles, and relayed visual information to the controller from a front-facing camera. In addition, distance from obstacles could be registered as force feedback. Presence and performance (e.g., fewer collisions) was better with the haptic feedback than without it [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 58" title="Lee S, Sukhatme GS, Kim GJ, Park C (2005) Haptic teleoperation of a mobile robot: a user study. Presence 14(3):345–365" href="/article/10.1007/s10055-006-0020-z#ref-CR58" id="ref-link-section-d81846e742">58</a>].</p><p>In this kind of interaction, the feedback is intuitive and informs the controller, in different ways, about the same environment. In general, congruent or redundant inputs can be expected to complement each other and may substitute for one another to improve perception when fidelity is poor. However, optimal human–machine interfaces remain elusive. For example, with respect to the feedback described above in the remote control scenario, could modulation of tactile vibration or sound frequency, or locus of tactile pulses, have served just as well as the haptic force feedback that was actually used? If a tactile display is chosen instead of force feedback, should elements of the display be mentally separable and distinct, or should they be spatio-temporally close enough to allow interaction and integration?</p><p>There seems to be two fundamentally different approaches to this question and they cannot always be explained in terms of different purposes or applications.</p><h3 class="c-article__sub-heading" id="Sec10">“Make it complex” versus “keep it simple”</h3><p>Some researchers include as much sensory information as possible in order to approximate normal conditions in which attention is directed to <i>selected</i> features of complex arrays containing, as they typically do, many redundancies and potential distractors. The brain does the selection and is suited to this task, which it performs at an unconscious level. Examples of these displays include the TVSS system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="White BW, Saunders FA, Scadden L, Bach-y-Rita P, Collins CC (1970) Seeing with the skin. Percept Psychophys 7:23–27" href="/article/10.1007/s10055-006-0020-z#ref-CR16" id="ref-link-section-d81846e759">16</a>], the Optacon [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 59" title="Bliss JC, Katcher MH, Roger CH, Shepard R (1970) Optical-to-tactile image conversion for the blind. IEEE Trans Man–Mach Syst MMS-11:58–64" href="/article/10.1007/s10055-006-0020-z#ref-CR59" id="ref-link-section-d81846e762">59</a>] which delivers a tactile image of print and pictures to a blind person’s fingertip, and the Queen’s Vocoder, which tranduces speech sounds onto an array of vibrotactors [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 60" title="Brooks PL, Frost BJ, Mason JL, Gibson DM (1987) Word and feature identification by profoundly deaf teenagers using the Queen’s University Tactile Vocoder. J Speech Hear Res 30:137–141" href="/article/10.1007/s10055-006-0020-z#ref-CR60" id="ref-link-section-d81846e765">60</a>].</p><p>This approach contrasts with attempts to simplify stimulus arrays with the intention of <i>removing</i> redundancy, complexity, and distraction, so as to leave (ideally) only the essential elements that can be dealt with as distinct entities for combination into meaningful sequences. This approach aims to relieve selection and attention mechanisms of their responsibility so that confusion among pattern elements is minimized.</p><p>Proponents of the complex approach argue that the simple approach risks overly impoverished displays that deny the haptic system the chance to do what all senses do well—selecting what is important from a complex array. White et al. said, in the context of the TVSS system:</p><blockquote class="c-blockquote"><div class="c-blockquote__body">
                    <p>We would never have been able to say that it was possible to determine the identity and layout in 3 dimensions of a group of familiar objects if we had designed our system to deliver 400 maximally discriminable sensations to the skin. The perceptual systems of living organisms are the most remarkable information reduction mechanisms known. They are not seriously embarrassed in situations where an enormous proportion of the input must be filtered out or ignored, but they are invariably handicapped when the input is drastically curtailed or artificially encoded. (pp 57–58)</p>
                  </div></blockquote><p>The argument has a long history and is well illustrated by two very different views about how to use the skin to convey linguistic or symbolic information. Designers of the Optohapt [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 61" title="Geldard FA (1966) Cutaneous coding of optical signals: the Optohapt. Percept Psychophys 1:377–381" href="/article/10.1007/s10055-006-0020-z#ref-CR61" id="ref-link-section-d81846e782">61</a>] argued for the need to keep all points of tactile information distinct and separable. To achieve this, fewer than ten points throughout the whole body were found, and none could be contralateral sites if their perceptual distinctiveness was to be assured. Stimuli that were too close in time or space suffered masking effects and localization errors. To the opposing camp, the extreme difficulty in assuring this distinctiveness was a hint that an inappropriate method was being used and that one <i>encouraging</i> interaction among the sites of tactile stimulation was preferable [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Richardson BL, Frost BJ (1977) Sensory substitution and the design of an artificial ear. J Psychol 96:259–285" href="/article/10.1007/s10055-006-0020-z#ref-CR62" id="ref-link-section-d81846e788">62</a>]. Paradoxically, the latter argument was ultimately supported by the finding that the optimal rate of stimulation for elements of the Optohapt was a rate that promoted temporal integration of tactile stimuli, producing apparent movement, not their separation or individual distinctiveness [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 62" title="Richardson BL, Frost BJ (1977) Sensory substitution and the design of an artificial ear. J Psychol 96:259–285" href="/article/10.1007/s10055-006-0020-z#ref-CR62" id="ref-link-section-d81846e791">62</a>].</p><p>In a more contemporary study, vibratory patterns of 200 ms duration were presented at up to three (at a time) of seven widely separated body points, and it was observed that subjects had difficulty in detecting changes if the interval between successive patterns was 800 ms or less. The authors suggested that these limitations may be even more severe in real life situations where perceptual load is likely to be greater [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 63" title="Gallace A, Tan HZ, Spence C (2005) Tactile change detection. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 12–16" href="/article/10.1007/s10055-006-0020-z#ref-CR63" id="ref-link-section-d81846e797">63</a>]. This prediction seems to be born out by the finding that localization of tactors on the torso was worse under higher cognitive load [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor location identification in zero-g. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 56–62" href="/article/10.1007/s10055-006-0020-z#ref-CR33" id="ref-link-section-d81846e800">33</a>].</p><p>However, it might be argued that the limitations are more in the type of display chosen than in the tactile sense. This Optohapt involves cognitive effort, and is not intuitive. It seems designed to thwart any tendency towards integration among elements and might be likened to asking someone palpating an object to report not what the object is, but differences in the sequence of sensations at each part of the fingers and hands. This might not be impossible to do but it would hardly be taking advantage of the haptic system’s special abilities.</p><p>Equally contemporary is fresh evidence for improved tactile acuity on the torso, even for closely spaced sites, provided that the temporal parameters are within a range that elicits apparent movement [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 64" title="van Erp JBF (2005) Vibrotactile spatial acuity on the torso: effects of location and timing parameters. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 80–85" href="/article/10.1007/s10055-006-0020-z#ref-CR64" id="ref-link-section-d81846e809">64</a>]. The researcher concludes, “even the most advanced displays have not reached the borders of the processing capacity of the torso” (p. 80).</p><p>The choice of display may be important for realism or presence and it would appear that those preserving some kind of isomorphism with the real world and avoiding artificial codes are more likely to promote presence than are displays that require some kind of cognitive translation or conscious attention. Unfortunately, we know less about haptic perception than we do about vision and hearing, though that is changing rapidly, partly thanks to fresh approaches made possible with virtual environments.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Conclusions</h2><div class="c-article-section__content" id="Sec11-content"><p>Virtual reality technology has made it possible to test the effectiveness of visual and tactile displays as modality-specific perceptual processes, and as contributors in multimodal settings. Much can now be achieved with laboratory simulations, or virtual environments, in the place of more expensive real world trials. In particular, reports concerning haptic devices offering force feedback have stimulated long overdue attention to the benefits of such feedback intrinsically (e.g., to improve presence or externalisation) and in conjunction with other senses for multimodal inputs that improve performance in remote control. This focus has, in turn, prompted a new look at multimodal processes (capture and integration) and a closer examination of haptic components (kinesthesis and touch), in active and passive conditions. The increase in knowledge has broad implications and applications in remote control of machinery, teleremote surgery, simulators for training, and devices to alleviate sensory handicap.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="VR. Schmitt, JW. Morris, GD. Jenney, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Schmitt VR, Morris JW, Jenney GD (1998) Fly-by-wire. Society of Automotive Engineers Philadelphia" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Schmitt VR, Morris JW, Jenney GD (1998) Fly-by-wire. Society of Automotive Engineers Philadelphia</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fly-by-wire&amp;publication_year=1998&amp;author=Schmitt%2CVR&amp;author=Morris%2CJW&amp;author=Jenney%2CGD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Wallace, SJ. Vodanovich, R. Restino, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Wallace JC, Vodanovich SJ, Restino R (2003) Predicitng cognitive failures from boredom proneness and daytime s" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Wallace JC, Vodanovich SJ, Restino R (2003) Predicitng cognitive failures from boredom proneness and daytime sleepiness scores: an investigation within military and undergraduate samples. Pers Individ Dif 34:635–644</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0191-8869%2802%2900050-8" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Predicitng%20cognitive%20failures%20from%20boredom%20proneness%20and%20daytime%20sleepiness%20scores%3A%20an%20investigation%20within%20military%20and%20undergraduate%20samples&amp;journal=Pers%20Individ%20Dif&amp;volume=34&amp;pages=635-644&amp;publication_year=2003&amp;author=Wallace%2CJC&amp;author=Vodanovich%2CSJ&amp;author=Restino%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Diolaiti N, Melchiorri C (2002) Tele-operation of a mobile robot through haptic feedback. In: HAVE, IEEE inter" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Diolaiti N, Melchiorri C (2002) Tele-operation of a mobile robot through haptic feedback. In: HAVE, IEEE international workshop on haptic virtual environments and their applications</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, ele" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Bjelland HV, Roed BK, Hoff T (2005) Studies on throttle sticks in high speed crafts—haptics in mechanical, electronic and haptic feedback interfaces. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 509–510</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rastogi A, Milgram P, Drascic D (1996) Telerobotic control with stereoscopic augmented reality. In: Bolas M, F" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Rastogi A, Milgram P, Drascic D (1996) Telerobotic control with stereoscopic augmented reality. In: Bolas M, Fisher S, Merritt J (eds) Stereoscopic displays and virtual reality systems, vol III. Proc SPIE 2635:115–122</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Gupta, T. Sheridan, D. Whitney, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Gupta R, Sheridan T, Whitney D (1997) Experiments using multimodal virtual environments in design for assembly" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Gupta R, Sheridan T, Whitney D (1997) Experiments using multimodal virtual environments in design for assembly analysis. Presence Teleop Virtual Environ 6(3):318–338</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Experiments%20using%20multimodal%20virtual%20environments%20in%20design%20for%20assembly%20analysis&amp;journal=Presence%20Teleop%20Virtual%20Environ&amp;volume=6&amp;issue=3&amp;pages=318-338&amp;publication_year=1997&amp;author=Gupta%2CR&amp;author=Sheridan%2CT&amp;author=Whitney%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DW. Hainsworth, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hainsworth DW (2001) Teleoperation user interfaces for mining robotics. Auton Robots 11(1):19–28" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Hainsworth DW (2001) Teleoperation user interfaces for mining robotics. Auton Robots 11(1):19–28</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1011299910904" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0983.68554" aria-label="View reference 7 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Teleoperation%20user%20interfaces%20for%20mining%20robotics&amp;journal=Auton%20Robots&amp;volume=11&amp;issue=1&amp;pages=19-28&amp;publication_year=2001&amp;author=Hainsworth%2CDW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Elhajj, N. Xi, WK. Fung, YH. Liu, WJ. Li, T. Kaga, T. Fukuda, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Elhajj I, Xi N, Fung WK, Liu YH, Li WJ, Kaga T, Fukuda T (2001) Haptic information in internet-based teleopera" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Elhajj I, Xi N, Fung WK, Liu YH, Li WJ, Kaga T, Fukuda T (2001) Haptic information in internet-based teleoperation. IEEE/ASME Transactions on Mechatronics 6(3): 295–304</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F3516.951367" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20information%20in%20internet-based%20teleoperation&amp;journal=IEEE%2FASME%20Transactions%20on%20Mechatronics&amp;volume=6&amp;issue=3&amp;pages=295-304&amp;publication_year=2001&amp;author=Elhajj%2CI&amp;author=Xi%2CN&amp;author=Fung%2CWK&amp;author=Liu%2CYH&amp;author=Li%2CWJ&amp;author=Kaga%2CT&amp;author=Fukuda%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roberts JW, Slattery OT, Swope B, Volker M, Comstock T (2002) Small-scale tactile graphics for virtual reality" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Roberts JW, Slattery OT, Swope B, Volker M, Comstock T (2002) Small-scale tactile graphics for virtual reality systems. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:422–429</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grohn M (2002) Is audio useful in immersive visualization? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) " /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Grohn M (2002) Is audio useful in immersive visualization? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:411–421</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GW. Edwards, W. Barfield, MA. Nussbaum, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Edwards GW, Barfield W, Nussbaum MA (2004) The use of force feedback and auditory cues for performance of an a" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Edwards GW, Barfield W, Nussbaum MA (2004) The use of force feedback and auditory cues for performance of an assembly task in an immersive virtual environment. Virtual Real 7:112–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-004-0120-6" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20use%20of%20force%20feedback%20and%20auditory%20cues%20for%20performance%20of%20an%20assembly%20task%20in%20an%20immersive%20virtual%20environment&amp;journal=Virtual%20Real&amp;volume=7&amp;pages=112-119&amp;publication_year=2004&amp;author=Edwards%2CGW&amp;author=Barfield%2CW&amp;author=Nussbaum%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EL K. Sallnas Rassmus-Grohn, C. Sjostrom, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Sallnas EL Rassmus-Grohn K, Sjostrom C (2001) Supporting presence in collaborative environments by haptic feed" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Sallnas EL Rassmus-Grohn K, Sjostrom C (2001) Supporting presence in collaborative environments by haptic feedback. ACM Trans Comput–Hum Interact 7(4):461–476</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365086" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20presence%20in%20collaborative%20environments%20by%20haptic%20feedback&amp;journal=ACM%20Trans%20Comput%E2%80%93Hum%20Interact&amp;volume=7&amp;issue=4&amp;pages=461-476&amp;publication_year=2001&amp;author=Sallnas%20Rassmus-Grohn%2CEL%20K&amp;author=Sjostrom%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kazi, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kazi A (2001) Operator performance in surgical telemanipulation. Presence 10:495–510" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Kazi A (2001) Operator performance in surgical telemanipulation. Presence 10:495–510</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474601753132678" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Operator%20performance%20in%20surgical%20telemanipulation&amp;journal=Presence&amp;volume=10&amp;pages=495-510&amp;publication_year=2001&amp;author=Kazi%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raspolli M, Avizzano CA, Facenza G, Bergamasco M (2005) HERMES: an angioplasty surgery simulator. In: Proceedi" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Raspolli M, Avizzano CA, Facenza G, Bergamasco M (2005) HERMES: an angioplasty surgery simulator. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 148–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jiang L, Girotra R, Cutkosky MR, Ullrich C (2005) Reducing error rates with low-cost haptic feedback in virtua" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Jiang L, Girotra R, Cutkosky MR, Ullrich C (2005) Reducing error rates with low-cost haptic feedback in virtual reality-based training applications. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 420–425</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BW. White, FA. Saunders, L. Scadden, P. Bach-y-Rita, CC. Collins, " /><meta itemprop="datePublished" content="1970" /><meta itemprop="headline" content="White BW, Saunders FA, Scadden L, Bach-y-Rita P, Collins CC (1970) Seeing with the skin. Percept Psychophys 7:" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">White BW, Saunders FA, Scadden L, Bach-y-Rita P, Collins CC (1970) Seeing with the skin. Percept Psychophys 7:23–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Seeing%20with%20the%20skin&amp;journal=Percept%20Psychophys&amp;volume=7&amp;pages=23-27&amp;publication_year=1970&amp;author=White%2CBW&amp;author=Saunders%2CFA&amp;author=Scadden%2CL&amp;author=Bach-y-Rita%2CP&amp;author=Collins%2CCC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JM. Kennedy, BL. Richardson, LE. Magee, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Kennedy JM, Richardson BL, Magee LE (1980) The nature of haptics. In: Hagen M (ed) The perception of pictures." /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Kennedy JM, Richardson BL, Magee LE (1980) The nature of haptics. In: Hagen M (ed) The perception of pictures. Academic, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20perception%20of%20pictures&amp;publication_year=1980&amp;author=Kennedy%2CJM&amp;author=Richardson%2CBL&amp;author=Magee%2CLE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baier H, Buss M, Freyberger F, Hoogen J, Kammermeier P, Schmidt G (1999) Distributed PC-based haptic, visual a" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Baier H, Buss M, Freyberger F, Hoogen J, Kammermeier P, Schmidt G (1999) Distributed PC-based haptic, visual and acoustic telepresence system experiments in virtual and remote environments. In: Proceedings of IEEE virtual reality conference, p 118</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interfa" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Miner N, Gillespie B, Caudell T (1996) Examining the influence of audio and visual stimuli on a haptic interface. In: Proceedings IMAGE conference, pp 23–35</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grane C, Bengtsson P (2005) Menu selection with a rotary device founded on haptic and/or graphic information. " /><span class="c-article-references__counter">20.</span><p class="c-article-references__text" id="ref-CR20">Grane C, Bengtsson P (2005) Menu selection with a rotary device founded on haptic and/or graphic information. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 475–476</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Palmer, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Palmer S (2002) Vision sciences. Bradford Books Cambridge" /><span class="c-article-references__counter">21.</span><p class="c-article-references__text" id="ref-CR21">Palmer S (2002) Vision sciences. Bradford Books Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision%20sciences&amp;publication_year=2002&amp;author=Palmer%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="BE. Goldstein, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Goldstein BE (1999) Sensation and perception 5th edn. Brooks Cole, Pacific Grove" /><span class="c-article-references__counter">22.</span><p class="c-article-references__text" id="ref-CR22">Goldstein BE (1999) Sensation and perception 5th edn. Brooks Cole, Pacific Grove</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensation%20and%20perception&amp;publication_year=1999&amp;author=Goldstein%2CBE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bradshaw MF, Elliot KM, Watt SJ, Davies IR (2002) Do observers exploit binocular disparity information in moto" /><span class="c-article-references__counter">23.</span><p class="c-article-references__text" id="ref-CR23">Bradshaw MF, Elliot KM, Watt SJ, Davies IR (2002) Do observers exploit binocular disparity information in motor tasks within dynamic telepresence environments? In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:331–342</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schmit A, Grasnik A (2002) Multiviewpoint autostereoscopic displays from 4D-vision. In: Woods AJ, Merrit JO, B" /><span class="c-article-references__counter">24.</span><p class="c-article-references__text" id="ref-CR24">Schmit A, Grasnik A (2002) Multiviewpoint autostereoscopic displays from 4D-vision. In: Woods AJ, Merrit JO, Benton SA, Bolas MT (eds) Stereoscopic displays and virtual reality systems, IX. Proc SPIE 4660:212–221</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Perlin K, Paxia S, Kollin J (2000) An autostereoscopic display. In: Proceedings of SIG-GRAPH, ACM conference o" /><span class="c-article-references__counter">25.</span><p class="c-article-references__text" id="ref-CR25">Perlin K, Paxia S, Kollin J (2000) An autostereoscopic display. In: Proceedings of SIG-GRAPH, ACM conference on computer graphics and interactive techniques, pp 319–326</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McKnight S, Melder N, Barrow AL, Harwin WS, Wann JP (2005) Perceptual cues for orientation in a two finger hap" /><span class="c-article-references__counter">26.</span><p class="c-article-references__text" id="ref-CR26">McKnight S, Melder N, Barrow AL, Harwin WS, Wann JP (2005) Perceptual cues for orientation in a two finger haptic grasp task. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 549–550</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BL. Richardson, DB. Wuillemin, MA. Symmons, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Richardson BL, Wuillemin DB, Symmons MA (2004) Sensory feedback and remote control of machines in mining and e" /><span class="c-article-references__counter">27.</span><p class="c-article-references__text" id="ref-CR27">Richardson BL, Wuillemin DB, Symmons MA (2004) Sensory feedback and remote control of machines in mining and extraterrestrial environments. J Aust Inst Mining Metall 2:53–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%20feedback%20and%20remote%20control%20of%20machines%20in%20mining%20and%20extraterrestrial%20environments&amp;journal=J%20Aust%20Inst%20Mining%20Metall&amp;volume=2&amp;pages=53-56&amp;publication_year=2004&amp;author=Richardson%2CBL&amp;author=Wuillemin%2CDB&amp;author=Symmons%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Woods A (2003) Seeing in depth at depth. Newsletter of the Centre for Marine Science &amp; Technology, Sept. Avail" /><span class="c-article-references__counter">28.</span><p class="c-article-references__text" id="ref-CR28">Woods A (2003) Seeing in depth at depth. Newsletter of the Centre for Marine Science &amp; Technology, Sept. Available at: <a href="http://www.cmst.curtin.edu.au/brochures/cmstnewsletter4.pdf">http://www.cmst.curtin.edu.au/brochures/cmstnewsletter4.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kugah DA (1972) Experiments evaluating compliance and force feedback effect on manipulator performance. Genral" /><span class="c-article-references__counter">29.</span><p class="c-article-references__text" id="ref-CR29">Kugah DA (1972) Experiments evaluating compliance and force feedback effect on manipulator performance. Genral Elec Corp NASA – CR 128605 Philadelphia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gunn C, Hutchins M, Adcock M, Hawkins R (2003) Trans-world Haptic collaboration, In: Proceedings of the SIGGRA" /><span class="c-article-references__counter">30.</span><p class="c-article-references__text" id="ref-CR30">Gunn C, Hutchins M, Adcock M, Hawkins R (2003) Trans-world Haptic collaboration, In: Proceedings of the SIGGRAPH Conference, Sketches and Applications, p 1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oakley I, O’Modhrain S (2005) Tilt to scroll: evaluating a motion based vibrotactile mobile interface. In: Pro" /><span class="c-article-references__counter">31.</span><p class="c-article-references__text" id="ref-CR31">Oakley I, O’Modhrain S (2005) Tilt to scroll: evaluating a motion based vibrotactile mobile interface. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 40–49</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nojima T, Funabiki K (2005) Cockpit display using tactile sensations. In: Proceedings of World Haptics Confere" /><span class="c-article-references__counter">32.</span><p class="c-article-references__text" id="ref-CR32">Nojima T, Funabiki K (2005) Cockpit display using tactile sensations. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 501–502</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor " /><span class="c-article-references__counter">33.</span><p class="c-article-references__text" id="ref-CR33">Bhargava A, Scott M, Traylor R, Chung R, Mrozek K, Wolter J, Tan HZ (2005) Effect of cognitive load on tactor location identification in zero-g. In: Proceedings of World Haptics Conference. IEEE Computer Society, Los Alamitos, pp 56–62</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rauterberg M (1999) New directions in user-system interaction: augmented reality, ubiquitous and mobile comput" /><span class="c-article-references__counter">34.</span><p class="c-article-references__text" id="ref-CR34">Rauterberg M (1999) New directions in user-system interaction: augmented reality, ubiquitous and mobile computing. In: Proceedings of IEEE symposium on human interfacing, pp 105–133</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lindeman RW, Sibert JL, Mendez-Mendez E, Patil S, Phifer D (2005) Effectiveness of directional vibrotactile cu" /><span class="c-article-references__counter">35.</span><p class="c-article-references__text" id="ref-CR35">Lindeman RW, Sibert JL, Mendez-Mendez E, Patil S, Phifer D (2005) Effectiveness of directional vibrotactile cuing on a building-clearing task. In: Proceedings of ACM CHI, pp 271–280</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BL. Richardson, DB. Wuillemin, F. Saunders, " /><meta itemprop="datePublished" content="1978" /><meta itemprop="headline" content="Richardson BL, Wuillemin DB, Saunders F (1978) Tactile discrimination of competing sounds. Percept Psychophys " /><span class="c-article-references__counter">36.</span><p class="c-article-references__text" id="ref-CR36">Richardson BL, Wuillemin DB, Saunders F (1978) Tactile discrimination of competing sounds. Percept Psychophys 24: 546–550</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=750999" aria-label="View reference 36 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tactile%20discrimination%20of%20competing%20sounds&amp;journal=Percept%20Psychophys&amp;volume=24&amp;pages=546-550&amp;publication_year=1978&amp;author=Richardson%2CBL&amp;author=Wuillemin%2CDB&amp;author=Saunders%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hoffman H, Groen J, Rousseau S, Hollander A, Winn W, Wells M, Furness T (1996) Tactile augmentation: enhancing" /><span class="c-article-references__counter">37.</span><p class="c-article-references__text" id="ref-CR37">Hoffman H, Groen J, Rousseau S, Hollander A, Winn W, Wells M, Furness T (1996) Tactile augmentation: enhancing presence in virtual reality with tactile feedback from real objects. In: Meeting of the American Psychological Society, San Francisco. Available at: <a href="http://www.hitl.washington.edu/publications/p-96-1/">http://www.hitl.washington.edu/publications/p-96–1/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McGrath BJ, Estrada A, Braithwaite MG, Raj AK, Rupert AH, (2004). Tactile situation awareness system flight de" /><span class="c-article-references__counter">38.</span><p class="c-article-references__text" id="ref-CR38">McGrath BJ, Estrada A, Braithwaite MG, Raj AK, Rupert AH, (2004). Tactile situation awareness system flight demonstration final report USAARL Report 2004–10, March</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="See details of cybergrasp at: http://www.immersion.com/3d/products/cyber_grasp.php&#xA;                        " /><span class="c-article-references__counter">39.</span><p class="c-article-references__text" id="ref-CR39">See details of cybergrasp at: <a href="http://www.immersion.com/3d/products/cyber_grasp.php">http://www.immersion.com/3d/products/cyber_grasp.php</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Richardson BL, Wuillemin DB, Symmons MA, Accardi R (2005) The Exograsp delivers tactile and kinaesthetic infor" /><span class="c-article-references__counter">40.</span><p class="c-article-references__text" id="ref-CR40">Richardson BL, Wuillemin DB, Symmons MA, Accardi R (2005) The Exograsp delivers tactile and kinaesthetic information about virtual objects. In: IEEE Tencon conference, November, Melbourne</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Kammermeier, A. Kron, J. Hoogen, G. Schmit, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kammermeier P, Kron A, Hoogen J, Schmit G (2004) Display of holistic sensation by combined tactile and kinaest" /><span class="c-article-references__counter">41.</span><p class="c-article-references__text" id="ref-CR41">Kammermeier P, Kron A, Hoogen J, Schmit G (2004) Display of holistic sensation by combined tactile and kinaesthetic feedback. Presence 13(1):1–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474604774048199" aria-label="View reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Display%20of%20holistic%20sensation%20by%20combined%20tactile%20and%20kinaesthetic%20feedback&amp;journal=Presence&amp;volume=13&amp;issue=1&amp;pages=1-15&amp;publication_year=2004&amp;author=Kammermeier%2CP&amp;author=Kron%2CA&amp;author=Hoogen%2CJ&amp;author=Schmit%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seay AF, Krum DM, Hodges L, Ribarsky W (2001) Simulator sickness and presence in a high FOV virtual environmen" /><span class="c-article-references__counter">42.</span><p class="c-article-references__text" id="ref-CR42">Seay AF, Krum DM, Hodges L, Ribarsky W (2001) Simulator sickness and presence in a high FOV virtual environment. In: Proceedings of the virtual reality 2001 conference, pp 299–300</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Stott, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Stott R (2002) Interaction between the senses: vision and the vestibular system. In: Roberts D (ed) Signals an" /><span class="c-article-references__counter">43.</span><p class="c-article-references__text" id="ref-CR43">Stott R (2002) Interaction between the senses: vision and the vestibular system. In: Roberts D (ed) Signals and perception. Palgrave Macmillan, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Signals%20and%20perception&amp;publication_year=2002&amp;author=Stott%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. McGurk, T. MacDonald, " /><meta itemprop="datePublished" content="1976" /><meta itemprop="headline" content="McGurk H, MacDonald T (1976) Hearing lips and seeing voices. Nature 264:746–748" /><span class="c-article-references__counter">44.</span><p class="c-article-references__text" id="ref-CR44">McGurk H, MacDonald T (1976) Hearing lips and seeing voices. Nature 264:746–748</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F264746a0" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=1012311" aria-label="View reference 44 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hearing%20lips%20and%20seeing%20voices&amp;journal=Nature&amp;volume=264&amp;pages=746-748&amp;publication_year=1976&amp;author=McGurk%2CH&amp;author=MacDonald%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Kohlers, M. Grunau, " /><meta itemprop="datePublished" content="1976" /><meta itemprop="headline" content="Kohlers P, von Grunau M (1976) Shape and color in apparent motion. Vis Res 16:329–335" /><span class="c-article-references__counter">45.</span><p class="c-article-references__text" id="ref-CR45">Kohlers P, von Grunau M (1976) Shape and color in apparent motion. Vis Res 16:329–335</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0042-6989%2876%2990192-9" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=941407" aria-label="View reference 45 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Shape%20and%20color%20in%20apparent%20motion&amp;journal=Vis%20Res&amp;volume=16&amp;pages=329-335&amp;publication_year=1976&amp;author=Kohlers%2CP&amp;author=Grunau%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Soto-Faraco, C. Spence, A. Kingstone, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Soto-Faraco S, Spence C, Kingstone A (2004) Cross-modal dynamic capture: congruency effects in the perception " /><span class="c-article-references__counter">46.</span><p class="c-article-references__text" id="ref-CR46">Soto-Faraco S, Spence C, Kingstone A (2004) Cross-modal dynamic capture: congruency effects in the perception of motion across sensory modalities. J Exp Psychol Hum Percept Perform 30(2):330–345</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0096-1523.30.2.330" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cross-modal%20dynamic%20capture%3A%20congruency%20effects%20in%20the%20perception%20of%20motion%20across%20sensory%20modalities&amp;journal=J%20Exp%20Psychol%20Hum%20Percept%20Perform&amp;volume=30&amp;issue=2&amp;pages=330-345&amp;publication_year=2004&amp;author=Soto-Faraco%2CS&amp;author=Spence%2CC&amp;author=Kingstone%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="HR. Shiffman, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Shiffman HR (1996) Sensation and perception, 4th edn. Wiley, New York" /><span class="c-article-references__counter">47.</span><p class="c-article-references__text" id="ref-CR47">Shiffman HR (1996) Sensation and perception, 4th edn. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensation%20and%20perception&amp;publication_year=1996&amp;author=Shiffman%2CHR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Heller, " /><meta itemprop="datePublished" content="1983" /><meta itemprop="headline" content="Heller MA (1983) Haptic dominance in form perception with blurred vision. Perception 122:607–613" /><span class="c-article-references__counter">48.</span><p class="c-article-references__text" id="ref-CR48">Heller MA (1983) Haptic dominance in form perception with blurred vision. Perception 122:607–613</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1068%2Fp120607" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20dominance%20in%20form%20perception%20with%20blurred%20vision&amp;journal=Perception&amp;volume=122&amp;pages=607-613&amp;publication_year=1983&amp;author=Heller%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Soto-Faraco, A. Kingstone, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Soto-Faraco S, Kingstone A (2004) Multisensory integration of dynamic information. In: Calvert G, Spence C, St" /><span class="c-article-references__counter">49.</span><p class="c-article-references__text" id="ref-CR49">Soto-Faraco S, Kingstone A (2004) Multisensory integration of dynamic information. In: Calvert G, Spence C, Stein B (eds) Handbook of multisensory processes. MIT Press Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20multisensory%20processes&amp;publication_year=2004&amp;author=Soto-Faraco%2CS&amp;author=Kingstone%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Caclin, S. Soto-Faraco, A. Kingstone, C. Spence, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile “capture” of audition. Percept Psychophys 18:55–" /><span class="c-article-references__counter">50.</span><p class="c-article-references__text" id="ref-CR50">Caclin A, Soto-Faraco S, Kingstone A, Spence C (2002) Tactile “capture” of audition. Percept Psychophys 18:55–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tactile%20%E2%80%9Ccapture%E2%80%9D%20of%20audition&amp;journal=Percept%20Psychophys&amp;volume=18&amp;pages=55-60&amp;publication_year=2002&amp;author=Caclin%2CA&amp;author=Soto-Faraco%2CS&amp;author=Kingstone%2CA&amp;author=Spence%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BL. Richardson, MA. Symmons, R. Accardi, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Richardson BL, Symmons MA, Accardi R (2000) The TDS: A new device for comparing active and passive-guided touc" /><span class="c-article-references__counter">51.</span><p class="c-article-references__text" id="ref-CR51">Richardson BL, Symmons MA, Accardi R (2000) The TDS: A new device for comparing active and passive-guided touch. IEEE Trans Rehabilit Eng 8:414–417</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F86.867883" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20TDS%3A%20A%20new%20device%20for%20comparing%20active%20and%20passive-guided%20touch&amp;journal=IEEE%20Trans%20Rehabilit%20Eng&amp;volume=8&amp;pages=414-417&amp;publication_year=2000&amp;author=Richardson%2CBL&amp;author=Symmons%2CMA&amp;author=Accardi%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Symmons MA, Richardson BL, Wuillemin DB, VanDoorn GH (2005) Kinaesthetic and cutaneous contributions to raised" /><span class="c-article-references__counter">52.</span><p class="c-article-references__text" id="ref-CR52">Symmons MA, Richardson BL, Wuillemin DB, VanDoorn GH (2005) Kinaesthetic and cutaneous contributions to raised-line stimulus interpretation. In: World haptics conference, 18–20 March, Pisa. Video clip at <a href="http://www-personal.monash.edu.au/~msymmons/images/6_9_qt.mov">http://www-personal.monash.edu.au/~msymmons/images/6_9_qt.mov</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LE. Magee, JM. Kennedy, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Magee LE, Kennedy JM (1980) Exploring pictures tactually. Nature (London) 283:287" /><span class="c-article-references__counter">53.</span><p class="c-article-references__text" id="ref-CR53">Magee LE, Kennedy JM (1980) Exploring pictures tactually. Nature (London) 283:287</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F283287a0" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20pictures%20tactually&amp;journal=Nature%20%28London%29&amp;volume=283&amp;publication_year=1980&amp;author=Magee%2CLE&amp;author=Kennedy%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="BL. Richardson, M. Symmons, DB. Wuillemin, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Richardson BL, Symmons M, Wuillemin DB (2004) The relative importance of cutaneous and kinesthetic cues in rai" /><span class="c-article-references__counter">54.</span><p class="c-article-references__text" id="ref-CR54">Richardson BL, Symmons M, Wuillemin DB (2004) The relative importance of cutaneous and kinesthetic cues in raised line drawing exploration. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 247–250</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Touch%2C%20blindness%2C%20and%20neuroscience&amp;pages=247-250&amp;publication_year=2004&amp;author=Richardson%2CBL&amp;author=Symmons%2CM&amp;author=Wuillemin%2CDB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Symmons, BL. Richardson, DB. Wuillemin, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Symmons M, Richardson BL, Wuillemin DB (2004) Active versus passive touch: Superiority depends more on the tas" /><span class="c-article-references__counter">55.</span><p class="c-article-references__text" id="ref-CR55">Symmons M, Richardson BL, Wuillemin DB (2004) Active versus passive touch: Superiority depends more on the task than the mode. In: Ballesteros S, Heller MA (eds) Touch, blindness, and neuroscience. Universidad Nacional de Educación a Distancia, Madrid, pp 179–185</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Touch%2C%20blindness%2C%20and%20neuroscience&amp;pages=179-185&amp;publication_year=2004&amp;author=Symmons%2CM&amp;author=Richardson%2CBL&amp;author=Wuillemin%2CDB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wuillemin DB, VanDoorn GH, Richardson BL, Symmons MA (2005) Haptic and visual size judgements in virtual and r" /><span class="c-article-references__counter">56.</span><p class="c-article-references__text" id="ref-CR56">Wuillemin DB, VanDoorn GH, Richardson BL, Symmons MA (2005) Haptic and visual size judgements in virtual and real environments. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 86–89</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sarter N (1998) Turning automation into a teamplayer: The development of multisensory and graded feedback for " /><span class="c-article-references__counter">57.</span><p class="c-article-references__text" id="ref-CR57">Sarter N (1998) Turning automation into a teamplayer: The development of multisensory and graded feedback for highly automated (flight deck) systems. Willard Airport Aviation Research Lab, <a href="http://www.nsfworkshop.engr.ucf.edu/papers/Sarter.asp/">http://www.nsfworkshop.engr.ucf.edu/papers/Sarter.asp/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Lee, GS. Sukhatme, GJ. Kim, C. Park, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Lee S, Sukhatme GS, Kim GJ, Park C (2005) Haptic teleoperation of a mobile robot: a user study. Presence 14(3)" /><span class="c-article-references__counter">58.</span><p class="c-article-references__text" id="ref-CR58">Lee S, Sukhatme GS, Kim GJ, Park C (2005) Haptic teleoperation of a mobile robot: a user study. Presence 14(3):345–365</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474605323384681" aria-label="View reference 58">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%20teleoperation%20of%20a%20mobile%20robot%3A%20a%20user%20study&amp;journal=Presence&amp;volume=14&amp;issue=3&amp;pages=345-365&amp;publication_year=2005&amp;author=Lee%2CS&amp;author=Sukhatme%2CGS&amp;author=Kim%2CGJ&amp;author=Park%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Bliss, MH. Katcher, CH. Roger, R. Shepard, " /><meta itemprop="datePublished" content="1970" /><meta itemprop="headline" content="Bliss JC, Katcher MH, Roger CH, Shepard R (1970) Optical-to-tactile image conversion for the blind. IEEE Trans" /><span class="c-article-references__counter">59.</span><p class="c-article-references__text" id="ref-CR59">Bliss JC, Katcher MH, Roger CH, Shepard R (1970) Optical-to-tactile image conversion for the blind. IEEE Trans Man–Mach Syst MMS-11:58–64</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMMS.1970.299963" aria-label="View reference 59">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 59 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optical-to-tactile%20image%20conversion%20for%20the%20blind&amp;journal=IEEE%20Trans%20Man%E2%80%93Mach%20Syst&amp;volume=MMS-11&amp;pages=58-64&amp;publication_year=1970&amp;author=Bliss%2CJC&amp;author=Katcher%2CMH&amp;author=Roger%2CCH&amp;author=Shepard%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PL. Brooks, BJ. Frost, JL. Mason, DM. Gibson, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Brooks PL, Frost BJ, Mason JL, Gibson DM (1987) Word and feature identification by profoundly deaf teenagers u" /><span class="c-article-references__counter">60.</span><p class="c-article-references__text" id="ref-CR60">Brooks PL, Frost BJ, Mason JL, Gibson DM (1987) Word and feature identification by profoundly deaf teenagers using the Queen’s University Tactile Vocoder. J Speech Hear Res 30:137–141</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3560893" aria-label="View reference 60 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Word%20and%20feature%20identification%20by%20profoundly%20deaf%20teenagers%20using%20the%20Queen%E2%80%99s%20University%20Tactile%20Vocoder&amp;journal=J%20Speech%20Hear%20Res&amp;volume=30&amp;pages=137-141&amp;publication_year=1987&amp;author=Brooks%2CPL&amp;author=Frost%2CBJ&amp;author=Mason%2CJL&amp;author=Gibson%2CDM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FA. Geldard, " /><meta itemprop="datePublished" content="1966" /><meta itemprop="headline" content="Geldard FA (1966) Cutaneous coding of optical signals: the Optohapt. Percept Psychophys 1:377–381" /><span class="c-article-references__counter">61.</span><p class="c-article-references__text" id="ref-CR61">Geldard FA (1966) Cutaneous coding of optical signals: the Optohapt. Percept Psychophys 1:377–381</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 61 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cutaneous%20coding%20of%20optical%20signals%3A%20the%20Optohapt&amp;journal=Percept%20Psychophys&amp;volume=1&amp;pages=377-381&amp;publication_year=1966&amp;author=Geldard%2CFA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BL. Richardson, BJ. Frost, " /><meta itemprop="datePublished" content="1977" /><meta itemprop="headline" content="Richardson BL, Frost BJ (1977) Sensory substitution and the design of an artificial ear. J Psychol 96:259–285" /><span class="c-article-references__counter">62.</span><p class="c-article-references__text" id="ref-CR62">Richardson BL, Frost BJ (1977) Sensory substitution and the design of an artificial ear. J Psychol 96:259–285</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=886486" aria-label="View reference 62 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sensory%20substitution%20and%20the%20design%20of%20an%20artificial%20ear&amp;journal=J%20Psychol&amp;volume=96&amp;pages=259-285&amp;publication_year=1977&amp;author=Richardson%2CBL&amp;author=Frost%2CBJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gallace A, Tan HZ, Spence C (2005) Tactile change detection. In: Proceedings of world haptics conference. IEEE" /><span class="c-article-references__counter">63.</span><p class="c-article-references__text" id="ref-CR63">Gallace A, Tan HZ, Spence C (2005) Tactile change detection. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 12–16</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van Erp JBF (2005) Vibrotactile spatial acuity on the torso: effects of location and timing parameters. In: Pr" /><span class="c-article-references__counter">64.</span><p class="c-article-references__text" id="ref-CR64">van Erp JBF (2005) Vibrotactile spatial acuity on the torso: effects of location and timing parameters. In: Proceedings of world haptics conference. IEEE Computer Society, Los Alamitos, pp 80–85</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0020-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The research conducted at the Bionics and Cognitive Science Centre of Monash University was supported by a grant from the Australian Commonwealth Government’s Sustainable Regions Programme.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Bionics and Cognitive Science Centre, Monash University, Gippsland Campus, 3842, Churchill, Australia</p><p class="c-article-author-affiliation__authors-list">Barry Richardson, Mark Symmons &amp; Dianne Wuillemin</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Barry-Richardson"><span class="c-article-authors-search__title u-h3 js-search-name">Barry Richardson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Barry+Richardson&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Barry+Richardson" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Barry+Richardson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mark-Symmons"><span class="c-article-authors-search__title u-h3 js-search-name">Mark Symmons</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mark+Symmons&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mark+Symmons" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mark+Symmons%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Dianne-Wuillemin"><span class="c-article-authors-search__title u-h3 js-search-name">Dianne Wuillemin</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Dianne+Wuillemin&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dianne+Wuillemin" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dianne+Wuillemin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0020-z/email/correspondent/c1/new">Barry Richardson</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=The%20contribution%20of%20virtual%20reality%20to%20research%20on%20sensory%20feedback%20in%20remote%20control&amp;author=Barry%20Richardson%20et%20al&amp;contentID=10.1007%2Fs10055-006-0020-z&amp;publication=1359-4338&amp;publicationDate=2006-03-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Richardson, B., Symmons, M. &amp; Wuillemin, D. The contribution of virtual reality to research on sensory feedback in remote control.
                    <i>Virtual Reality</i> <b>9, </b>234–242 (2006). https://doi.org/10.1007/s10055-006-0020-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0020-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-29">29 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-02-13">13 February 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03-03">03 March 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04">April 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0020-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0020-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Haptic</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Active</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Passive</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Kinesthetic</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cutaneous</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Perception</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0020-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=20;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

