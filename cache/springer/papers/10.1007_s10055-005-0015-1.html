<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Communication in a networked haptic virtual environment for temporal b"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Networked virtual environments using haptic interfaces can be used for surgical training and support both a simulation component and a communication component. We present such an environment for..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Communication in a networked haptic virtual environment for temporal bone surgery training"/>

    <meta name="dc.source" content="Virtual Reality 2005 9:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-12-09"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Networked virtual environments using haptic interfaces can be used for surgical training and support both a simulation component and a communication component. We present such an environment for training in surgery of the temporal bone, which emphasises communication between an instructor and a student. We give an overview of the learning requirements for surgeons in this area and present the details of our implementation with a focus on the way communication is supported. We describe a training trial that was undertaken with a group of surgical trainees and carry out a qualitative analysis of transcripts from the teaching sessions. We conclude that the virtual environment supports a rich dialogue between the instructor and student, allowing them to ground their conversation in the shared model. Haptic interfaces are an important enabling technology for the simulation and communication and are used in conjunction with other modes and media to support situated learning."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-12-09"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="97"/>

    <meta name="prism.endingPage" content="107"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0015-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0015-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0015-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0015-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Communication in a networked haptic virtual environment for temporal bone surgery training"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2006/03"/>

    <meta name="citation_online_date" content="2005/12/09"/>

    <meta name="citation_firstpage" content="97"/>

    <meta name="citation_lastpage" content="107"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0015-1"/>

    <meta name="DOI" content="10.1007/s10055-005-0015-1"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0015-1"/>

    <meta name="description" content="Networked virtual environments using haptic interfaces can be used for surgical training and support both a simulation component and a communication compon"/>

    <meta name="dc.creator" content="Matthew A. Hutchins"/>

    <meta name="dc.creator" content="Duncan R. Stevenson"/>

    <meta name="dc.creator" content="Chris Gunn"/>

    <meta name="dc.creator" content="Alexander Krumpholz"/>

    <meta name="dc.creator" content="Tony Adriaansen"/>

    <meta name="dc.creator" content="Brian Pyman"/>

    <meta name="dc.creator" content="Stephen O&#8217;Leary"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Leigh J, Johnson A, Vasilakis C, DeFanti T (1996) Multi-pespective collaborative design in persistent networked virtual environments. In: Proceeding of the IEEE Virtual Reality Annual International Symposium (VRAIS 96), March 1996, pp 253&#8211;262"/>

    <meta name="citation_reference" content="Gunn C, Hutchins M, Adcock M and Hawkins R (2003) Trans-World Haptic Collaboration. In: Proceedings of the ACM SIGGRAPH 2003 conference (Sketches and Applications), San Diego, July 2003"/>

    <meta name="citation_reference" content="citation_title=Surgical training using haptics over long internet distances; citation_inbook_title=Medicine Meets Virtual Reality 2004; citation_publication_date=2004; citation_pages=121-123; citation_id=CR3; citation_author=C Gunn; citation_author=M Hutchins; citation_author=M Adcock; citation_author=R Hawkins; citation_publisher=IOS Press"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoper Virtual
Environ; citation_title=Transatlantic Touch: a Study of Haptic Collaboration over Long Distance; citation_author=J Kim, H Kim, M Muniyandi, M Srinivasan, J Jorden, J Mortensen, M Oliveira, M Slater; citation_volume=13; citation_issue=3; citation_publication_date=2004; citation_pages=328-337; citation_id=CR4"/>

    <meta name="citation_reference" content="Rasmussen M, Mason TP, Millman A, Evenhouse R, Sandin D (1998) The virtual temporal bone, a tele-immersive educational environment. Future Generation Computing Systems 14, Elsevier, pp 125&#8211;130"/>

    <meta name="citation_reference" content="Wiet GJ, Bryan J, Dodson E, Sessnna D, Stredney D, Schmalbrock P, Welling B (2000) Virtual Temporal Bone Dissection Simulation. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2000. IOS Press, pp 378&#8211;384"/>

    <meta name="citation_reference" content="Stredney D, Wiet GJ, Bryan J, Sessanna D, Murakami J, Schmalbrock P, Powell K, Welling, B (2002) Temporal Bone Dissection Simulation&#8212;An Update. In: Procedings of MMVR 02/10, Long Beach, CA,USA January 2002"/>

    <meta name="citation_reference" content="Agus M, Giachetti A, Gobbetti E, Zanetti G, Zorcolo A (2002) Real-time haptic and visual simulation of bone dissection. In: Procedings of the IEEE Virtual Reality 2002 (VR&#8217;02), March 2002"/>

    <meta name="citation_reference" content="Petersik A, Pflesser B, Tiede U, Hohne KH (2002) Haptic Rendering of Volumetric Anatomic Models at Sub-voxel Resolution. In: Proceedings of 10th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (Haptics 2002), Orlando, FL, USA 2002"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Surgery; citation_title=Virtual Reality Training Improves Operating Room Performance; citation_author=N Seymour, A Gallagher, S Roman, M V O&#8217;Brien Bansai, D Andersen, R Satava; citation_volume=236; citation_issue=4; citation_publication_date=2002; citation_pages=458-464; citation_doi=10.1097/00000658-200210000-00008; citation_id=CR10"/>

    <meta name="citation_reference" content="Morris D, Sewell C, Blevins N, Barbagli F, Salisbury K (2004) A collaborative virtual environment for the simulation of temporal bone surgery, In: Procedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI 2004), Springer-Verlag Lecture Notes in Computer Science, vol 3217, pp 319&#8211;327"/>

    <meta name="citation_reference" content="Stevenson D, Smith K, McLaughlin J, Gunn C, Veldkamp JP, Dixon M (1999) Haptic workbench: a multisensory virtual environment. In Proc. SPIE&#8212;The International Society for Optical Engineering, 3639, pp 356&#8211;366"/>

    <meta name="citation_reference" content="citation_title=The Visualization Toolkit; citation_publication_date=1998; citation_id=CR13; citation_author=W Schroeder; citation_author=K Martin; citation_author=B Lorensen; citation_publisher=Prentice-Hall Inc"/>

    <meta name="citation_reference" content="Hutchins M, Adcock M, Stevenson D, Gunn C, Krumpholz A (2005) The design of perceptual representations for practical networked multimodal virtual training environments. In: Proceedings of the 11th International Conference on Human-Computer Interaction (HCI International 2005), July 2005, Las Vegas, Nevada, USA, CD-ROM"/>

    <meta name="citation_reference" content="citation_title=Conversation Analysis; citation_publication_date=1998; citation_id=CR15; citation_author=I Hutchby; citation_author=W Woofitt; citation_publisher=Polity Press"/>

    <meta name="citation_reference" content="citation_title=Grounding in Communication; citation_inbook_title=Perspectives on Socially Shared Cognition; citation_publication_date=1991; citation_pages=127-149; citation_id=CR16; citation_author=H Clark; citation_author=S Brennan; citation_publisher=APA Books"/>

    <meta name="citation_reference" content="Krumpholz A (2005) Building a virtual trainer for an immersive haptic virtual reality environment. In: Kaschek R (ed) Perspectives of Intelligent Systems&#8217; Assistance, Technical Report TR 2005/2, Department of Information Systems, Massey University, Palmerston North, New Zealand"/>

    <meta name="citation_author" content="Matthew A. Hutchins"/>

    <meta name="citation_author_email" content="Matthew.Hutchins@csiro.au"/>

    <meta name="citation_author_institution" content="CSIRO ICT Centre, Canberra, Australia"/>

    <meta name="citation_author" content="Duncan R. Stevenson"/>

    <meta name="citation_author_institution" content="CSIRO ICT Centre, Canberra, Australia"/>

    <meta name="citation_author" content="Chris Gunn"/>

    <meta name="citation_author_institution" content="CSIRO ICT Centre, Canberra, Australia"/>

    <meta name="citation_author" content="Alexander Krumpholz"/>

    <meta name="citation_author_institution" content="CSIRO ICT Centre, Canberra, Australia"/>

    <meta name="citation_author" content="Tony Adriaansen"/>

    <meta name="citation_author_institution" content="CSIRO ICT Centre, Canberra, Australia"/>

    <meta name="citation_author" content="Brian Pyman"/>

    <meta name="citation_author_institution" content="Department of Otolaryngology, University of Melbourne, Melbourne, Australia"/>

    <meta name="citation_author" content="Stephen O&#8217;Leary"/>

    <meta name="citation_author_institution" content="Department of Otolaryngology, University of Melbourne, Melbourne, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0015-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0015-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Communication in a networked haptic virtual environment for temporal bone surgery training"/>
        <meta property="og:description" content="Networked virtual environments using haptic interfaces can be used for surgical training and support both a simulation component and a communication component. We present such an environment for training in surgery of the temporal bone, which emphasises communication between an instructor and a student. We give an overview of the learning requirements for surgeons in this area and present the details of our implementation with a focus on the way communication is supported. We describe a training trial that was undertaken with a group of surgical trainees and carry out a qualitative analysis of transcripts from the teaching sessions. We conclude that the virtual environment supports a rich dialogue between the instructor and student, allowing them to ground their conversation in the shared model. Haptic interfaces are an important enabling technology for the simulation and communication and are used in conjunction with other modes and media to support situated learning."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Communication in a networked haptic virtual environment for temporal bone surgery training | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0015-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Haptic workbench, Networked virtual environments, Haptic surgical training, Conversation analysis","kwrd":["Haptic_workbench","Networked_virtual_environments","Haptic_surgical_training","Conversation_analysis"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0015-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0015-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=15;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0015-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Communication in a networked haptic virtual environment for temporal bone surgery training
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0015-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0015-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-12-09" itemprop="datePublished">09 December 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Communication in a networked haptic virtual environment for temporal bone surgery training</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Matthew_A_-Hutchins" data-author-popup="auth-Matthew_A_-Hutchins" data-corresp-id="c1">Matthew A. Hutchins<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CSIRO ICT Centre" /><meta itemprop="address" content="grid.417671.2, CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Duncan_R_-Stevenson" data-author-popup="auth-Duncan_R_-Stevenson">Duncan R. Stevenson</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CSIRO ICT Centre" /><meta itemprop="address" content="grid.417671.2, CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chris-Gunn" data-author-popup="auth-Chris-Gunn">Chris Gunn</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CSIRO ICT Centre" /><meta itemprop="address" content="grid.417671.2, CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Alexander-Krumpholz" data-author-popup="auth-Alexander-Krumpholz">Alexander Krumpholz</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CSIRO ICT Centre" /><meta itemprop="address" content="grid.417671.2, CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tony-Adriaansen" data-author-popup="auth-Tony-Adriaansen">Tony Adriaansen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CSIRO ICT Centre" /><meta itemprop="address" content="grid.417671.2, CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brian-Pyman" data-author-popup="auth-Brian-Pyman">Brian Pyman</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Melbourne" /><meta itemprop="address" content="grid.1008.9, 000000012179088X, Department of Otolaryngology, University of Melbourne, Melbourne, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Stephen-O_Leary" data-author-popup="auth-Stephen-O_Leary">Stephen O’Leary</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Melbourne" /><meta itemprop="address" content="grid.1008.9, 000000012179088X, Department of Otolaryngology, University of Melbourne, Melbourne, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">97</span>–<span itemprop="pageEnd">107</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">156 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">16 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0015-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Networked virtual environments using haptic interfaces can be used for surgical training and support both a simulation component and a communication component. We present such an environment for training in surgery of the temporal bone, which emphasises communication between an instructor and a student. We give an overview of the learning requirements for surgeons in this area and present the details of our implementation with a focus on the way communication is supported. We describe a training trial that was undertaken with a group of surgical trainees and carry out a qualitative analysis of transcripts from the teaching sessions. We conclude that the virtual environment supports a rich dialogue between the instructor and student, allowing them to ground their conversation in the shared model. Haptic interfaces are an important enabling technology for the simulation and communication and are used in conjunction with other modes and media to support situated learning.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The development of haptic interface hardware has opened up new possibilities for teaching in domains where the sense of touch is crucial to task performance. One such domain is surgery, where simulation-based training is being increasingly recognised as one of the key components in the training of future surgeons. Many surgical simulation systems focus exclusively on the interaction between a student and a virtual model. However, communication between the student and an instructor is also a crucial component in teaching complex tasks. Networked virtual environments can combine the benefits of simulation-based training with the benefits of networked communication.</p><p>We have developed a networked virtual environment for surgical training. The environment includes a simulation component, using visualisation and haptic interaction to support the performance of a surgical procedure and a communication component, allowing an instructor and a student to share the simulation and communicate about it. The system could also allow two students to work together, exchange knowledge and compare performances. The networking capabilities of the environment allow the instructor and student to be separated by geographical distance. The initial motivation was to support training programs where instructors are scarce and students may be widely dispersed. We have found, however, that the facility for an instructor and student to share a virtual model is so valuable that we have deployed the collaborative training system on adjacent desks in the same room.</p><p>The surgical domain being taught in our system is ear surgery and in particular the drilling of the temporal bone. We provide some background on this surgery in the next section, followed by a review of related work in collaborative haptic virtual environments and other training systems for temporal bone surgery. We then describe the configuration of our system and give an overview of the features that support the task of drilling the temporal bone and the communication between the networked users.</p><p>We have conducted an initial training trial of our system. Eleven surgical trainees, who were interested in specialising in Ear, Nose and Throat surgery, received tuition on a basic temporal bone dissection procedure in the virtual environment and then demonstrated their acquired knowledge by drilling a real temporal bone in the laboratory. The training sessions were videotaped. We describe the trial and present a qualitative analysis of transcripts of the training sessions, describing what they reveal about the communication patterns exhibited between the instructor and students.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">The application: temporal bone surgery training</h2><div class="c-article-section__content" id="Sec2-content"><p>The temporal bones are part of the skull. There is one on each side of the head. They house the organs of hearing and balance that are the middle and inner ears. Any surgery that requires access to these organs, for example, to clean out an infection, to locate and remove a tumour or other abnormal growth, or to insert the electrode of a cochlear hearing implant, requires part of the temporal bone to be drilled away. Some neurosurgical procedures also require access to the inside of the skull through the temporal bone.</p><p>Surgical drilling of the temporal bone is not like drilling a bolt hole in a piece of wood with a threaded drill. The bone is eroded away gradually using a rapidly spinning almost spherical burr. Two types of burr are typically used. Cutting burrs have tiny equatorially orientated blades and erode bone most quickly. Polishing or diamond burrs are coated with a rough diamond surface and polish off layers gradually, and are used when precision is important, such as when working near a nerve. A range of burrs in different sizes of both types are used in a typical procedure. The drill speed is usually controlled by a foot pedal. The bone being eroded and the drill burr, must be prevented from overheating by constantly irrigating the area being drilled with water. The dust that comes from the eroded bone is mixed with the irrigation water and sucked away. The surgeon will have a drill in one hand and a sucker/irrigator in the other. The area being operated on is very small, so most of the surgery is conducted with the aid of a binocular surgical microscope.</p><p>The temporal bone is made up of bony regions with varying properties. The outer layer of bone, the cortex, is relatively thick and solid. The bone within the mastoid region is composed of thin plates arranged in a honeycombed pattern, interspersed with air cells. The canals and vessels that pass through it are lined with denser, more continuous bone. Surrounding the mastoid are plates of bone that connect to the surrounding skull. The inside of the temporal bone is lined with dura, the thick membrane that contains the brain. The bone surrounding the inner ear is hard and dense, the densest bone in the body.</p><p>The temporal bone can exhibit significant variations in geometry and topology of structures between patients. Safe surgery proceeds by identifying and exposing a series of anatomical landmarks. Each landmark that is correctly identified and exposed serves to both safeguard that landmark during the remainder of the surgery and also point the way to the next landmark. The surgeon uses surface features of the bone to determine where to commence drilling. A large vein called the sigmoid sinus and the dura must be discovered and exposed, before heading deeper to find the antrum, a cavity of the middle ear. One of the three semicircular canals should be visible in the antrum and points the way to safe drilling to expose a tiny bone called the incus. This, in turn, enables safe drilling to locate the facial nerve. The facial nerve animates the face and any damage to it during surgery can lead to facial paralysis.</p><p>Surgical trainees learning to operate on the temporal bone must first master the anatomy. There is a complex three-dimensional arrangement of interconnected parts packed into a small and mostly solid space. The trainees must understand the surgical approach, that is, which landmarks to locate in which order and the correct technique to locate each. They must know which size and type of drill burr to use at each stage, how the drill and sucker must be used in combination, what magnification to use in the microscope and so on. Finally, they must master the craft of using the drill burr to achieve just the correct amount of erosion, leaving behind a smoothly polished surface and not damaging any of the vital structures.</p><p>Trainees typically learn all these things by drilling human cadaveric bone samples under close instruction from an expert teacher. Although this is an excellent method of learning, there are obvious drawbacks. In particular, it requires access to a large number of carefully prepared bone samples, which can be difficult to obtain and costly to administrate. A trainee will need to drill approximately 20 bones to become proficient and learning opportunities on the earlier bones may be wasted while the student is still struggling to learn the basics. In this paper, we describe a collaborative haptic virtual environment for teaching temporal bone surgery that has been designed to complement the traditional training approaches. We keep the focus on teacher-student interactions, but replace the real temporal bones with virtual ones. Arguably, the student should learn the fundamentals in a virtual environment and then get maximum value out of the real samples.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Related and prior work</h2><div class="c-article-section__content" id="Sec3-content"><p>Early work on collaborative virtual environments dealt with a shared virtual world populated by avatars representing real or virtual people, possibly at multiple physical sites. The avatars moved about the virtual world interacting with other avatars and with objects in the world. Much of the research effort focussed on supporting these interactions and on culling the views into the world to minimise the network traffic between the different physical sites. An example is the work published by Leigh et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Leigh J, Johnson A, Vasilakis C, DeFanti T (1996) Multi-pespective collaborative design in persistent networked virtual environments. In: Proceeding of the IEEE Virtual Reality Annual International Symposium (VRAIS 96), March 1996, pp 253–262" href="/article/10.1007/s10055-005-0015-1#ref-CR1" id="ref-link-section-d2670e395">1</a>] on collaborative architectural design and review, with multiple perspectives of the proposed buildings.</p><p>Our work follows this broad model but is restricted to two participants, notionally sitting side-by-side at a laboratory bench. They see minimalist avatars representing the hand-tools that they each hold. Adequate bandwidth between the two physical sites is assumed and much effort has been devoted to supporting communication between the two participants. This includes both physical links—low-latency digital video and echo cancelled audio streams—and virtual—application data and interface features that support technical conversations about the task at hand.</p><p>Collaborative haptic interaction for both participants is an important component of our system. It allows them each to independently interact with virtual objects and supports their action-gesture-voice dialogue with the other person. This includes direct haptic interaction, one person physically guiding the other person’s haptic device to correctly position them in the scene and incidental haptic interaction where the two haptic tools collide with each other. Our early work on collaborative haptics was first demonstrated in July 2001. Our system was described by Gunn et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Gunn C, Hutchins M, Adcock M and Hawkins R (2003) Trans-World Haptic Collaboration. In: Proceedings of the ACM SIGGRAPH 2003 conference (Sketches and Applications), San Diego, July 2003" href="/article/10.1007/s10055-005-0015-1#ref-CR2" id="ref-link-section-d2670e403">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Gunn C, Hutchins M, Adcock M, Hawkins R (2004) Surgical training using haptics over long internet distances. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2004. IOS Press, Amsterdam, pp 121–123" href="/article/10.1007/s10055-005-0015-1#ref-CR3" id="ref-link-section-d2670e406">3</a>] and forms the basis of our current application. At its core are algorithms for handling latency in a networked haptic environment and ways of permitting independent use of the haptic tools at each site while maintaining consistency between them.</p><p>A similar long-distance collaborative haptics system was developed by a team working at Massachusetts Institute of Technology and University College London and was presented in 2002. The recent paper by Kim et al. in 2004 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Kim J, Kim H, Muniyandi M, Srinivasan M, Jorden J, Mortensen J, Oliveira M, Slater M (2004) Transatlantic Touch: a Study of Haptic Collaboration over Long Distance. Presence: Teleoper Virtual Environ 13(3):328–337" href="/article/10.1007/s10055-005-0015-1#ref-CR4" id="ref-link-section-d2670e412">4</a>] describes this work and reports on their experiment, which consisted of a simple task—two people on either side of the Atlantic working together to lift a virtual box—under four different experimental conditions.</p><p>In 1998 Rasmussen et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Rasmussen M, Mason TP, Millman A, Evenhouse R, Sandin D (1998) The virtual temporal bone, a tele-immersive educational environment. Future Generation Computing Systems 14, Elsevier, pp 125–130" href="/article/10.1007/s10055-005-0015-1#ref-CR5" id="ref-link-section-d2670e419">5</a>] described a “tele-immersive educational environment” for teaching temporal bone anatomy, which used the ImmersaDesk, a drafting-table-style 3-D display with spatially tracked “wand” for interaction developed at the University of Illinois at Chicago. They built a 3-D model of a human temporal bone from scanned histologic sections. This system contained two basic ideas, interaction with a 3-D model of anatomy for teaching purposes and sharing that interaction over a network.</p><p>Wiet, et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Wiet GJ, Bryan J, Dodson E, Sessnna D, Stredney D, Schmalbrock P, Welling B (2000) Virtual Temporal Bone Dissection Simulation. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2000. IOS Press, pp 378–384" href="/article/10.1007/s10055-005-0015-1#ref-CR6" id="ref-link-section-d2670e425">6</a>] developed this idea to include actual surgical simulation, with a haptic device to simulate the drilling of bone, using a voxel model of the data and a head-mounted display to show the simulated stereo view of the model and interaction. They continued their work, publishing updates [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Stredney D, Wiet GJ, Bryan J, Sessanna D, Murakami J, Schmalbrock P, Powell K, Welling, B (2002) Temporal Bone Dissection Simulation—An Update. In: Procedings of MMVR 02/10, Long Beach, CA,USA January 2002" href="/article/10.1007/s10055-005-0015-1#ref-CR7" id="ref-link-section-d2670e428">7</a>] and proposing an international trial of their developed system.</p><p>Agus et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Agus M, Giachetti A, Gobbetti E, Zanetti G, Zorcolo A (2002) Real-time haptic and visual simulation of bone dissection. In: Procedings of the IEEE Virtual Reality 2002 (VR’02), March 2002" href="/article/10.1007/s10055-005-0015-1#ref-CR8" id="ref-link-section-d2670e434">8</a>] developed a surgical simulator for temporal bone surgery, also using visual and haptic feedback, in which they paid close attention to modelling the physical interaction between the drilling burr and the bone. They also presented a system architecture that partitioned the fast haptic sub-system from the slower visual sub-system and implemented this on two closely coupled PCs. They used a bench-mounted binocular system to display the stereo video output.</p><p>Petersik et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Petersik A, Pflesser B, Tiede U, Hohne KH (2002) Haptic Rendering of Volumetric Anatomic Models at Sub-voxel Resolution. In: Proceedings of 10th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (Haptics 2002), Orlando, FL, USA 2002" href="/article/10.1007/s10055-005-0015-1#ref-CR9" id="ref-link-section-d2670e440">9</a>] present an approach that combines haptic interaction with a voxel model of the anatomy with a ray-casting algorithm for representing the surface of the model at sub-voxel resolution. They also use a tool-based collision detection algorithm to model the interaction between the drill and the bone. Their work has resulted in a commercially available surgical trainer called Voxel-Man TempoSurg.</p><p>Our system design was driven by two criteria: our teaching scenario would be a one-on-one lesson between instructor and student using two networked virtual environments, which echoes the early work of Rasmussen et al. mentioned above. The features of the system would be driven by explicit teaching requirements: the data would be anatomically correct, there would be appropriate shared 3-D interaction to support the teaching and the haptic drilling performance would at least match actual tool speeds and responses during surgery. This meant that we did not focus as tightly on specific components of the simulation as did the teams described above, but rather we looked for the best ways to make the required teaching points while still maintaining system performance.</p><p>Our approach to designing the teaching curriculum and ultimately the assessment of trainees, was modelled on the error-based approach to assessing virtual-reality training described by Seymour et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Seymour N, Gallagher A, Roman S, O’Brien M Bansai V, Andersen D, Satava R (2002) Virtual Reality Training Improves Operating Room Performance. Ann Surgery 236(4):458–464" href="/article/10.1007/s10055-005-0015-1#ref-CR10" id="ref-link-section-d2670e448">10</a>] in 2002. For example, there is an emphasis on the danger zones for making errors in the simulation and an indicative but exaggerated response when errors are made. The error condition can be corrected and the step in the procedure repeated until the trainee’s work is error-free.</p><p>In a recent publication (2004) Morris et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Morris D, Sewell C, Blevins N, Barbagli F, Salisbury K (2004) A collaborative virtual environment for the simulation of temporal bone surgery, In: Procedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI 2004), Springer-Verlag Lecture Notes in Computer Science, vol 3217, pp 319–327" href="/article/10.1007/s10055-005-0015-1#ref-CR11" id="ref-link-section-d2670e455">11</a>] describe work with many similarities to ours. They have a similar hybrid data structure for graphical rendering of the bone model under dissection. Their work includes modelling the contact sound between drill-burr and bone, which is an advance on ours. They have also implemented a networking capability using a private Gigabit connection to send a continuous stream of position and force data between two systems. This contrasts with our networking approach, which is to send updates for the model and tool position and to compute locally any required forces and which does not require Gigabit bandwidth.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Hardware and software configuration</h2><div class="c-article-section__content" id="Sec4-content"><p>Our training system for temporal bone surgery has been demonstrated using a general purpose hardware configuration we call a haptic workbench [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Stevenson D, Smith K, McLaughlin J, Gunn C, Veldkamp JP, Dixon M (1999) Haptic workbench: a multisensory virtual environment. In Proc. SPIE—The International Society for Optical Engineering, 3639, pp 356–366" href="/article/10.1007/s10055-005-0015-1#ref-CR12" id="ref-link-section-d2670e466">12</a>], although we have slightly tailored our default configuration to more closely match the needs of this particular surgery. The training system can be run by a student at a single workbench, but has been designed for deployment on a pair of workbenches connected by a network. This creates a virtual environment shared by an instructor and a student. The precise configuration of each workbench can vary, so here we describe the configuration used for the student machine in the trial described later in this paper. The computing platform is a Dell Precision 650MT with two Intel Xeon CPUs at 3.2 GHz running Windows XP, 3.37 GB of main memory and an nVIDIA Quadro4 900 XGL graphic card. The scene is displayed in stereo, using CrystalEyes liquid crystal shutter glasses from StereoGraphics. The workbench incorporates a 21” CRT monitor mounted above a horizontal mirror, creating a work volume in which the three-dimensional scene is coincident with the motion of the user’s hands. Located in this work volume are two PHANToM haptic devices from SensAble Technologies, one used for each hand. We use the PHANToM Premium 1.5 for the dominant hand and the PHANToM Desktop for the other hand. There is also a Logitech Magellan SpaceMouse for six degree of freedom manipulations. We provide cushioned wrist rests of various heights for support during long drilling sessions. An analog foot pedal is connected to the system via an Immersion Interface Box. The hardware configuration is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0015-1#Fig1">1</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0015-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0015-1/MediaObjects/10055_2005_15_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0015-1/MediaObjects/10055_2005_15_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The Haptic Workbench configured for temporal bone surgery training. <b>a</b> Instructor and student seated at networked haptic workbenches. <b>b</b> One hand uses a haptic device for drilling. <b>c</b> The other hand uses a haptic device for suction, or 6DOF input device to orient the bone. <b>d</b> Active stereo shutter glasses generate 3-D views. <b>e</b> A foot pedal controls the drill speed</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0015-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            <p>The instructor and student systems are connected together via a local area Ethernet network connection. Although not used in the trial, the haptic workbenches incorporate video cameras, monitors, microphones and speakers, suitable for establishing an audiovisual link between participants when the systems are not located in the same room. We have demonstrated this capability for other surgical scenarios [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Gunn C, Hutchins M, Adcock M and Hawkins R (2003) Trans-World Haptic Collaboration. In: Proceedings of the ACM SIGGRAPH 2003 conference (Sketches and Applications), San Diego, July 2003" href="/article/10.1007/s10055-005-0015-1#ref-CR2" id="ref-link-section-d2670e509">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Gunn C, Hutchins M, Adcock M, Hawkins R (2004) Surgical training using haptics over long internet distances. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2004. IOS Press, Amsterdam, pp 121–123" href="/article/10.1007/s10055-005-0015-1#ref-CR3" id="ref-link-section-d2670e512">3</a>].</p><p>The software system is built on the Reachin API Version 3.2 from Reachin Technologies AB. This API is an extensible C++ class library that supports the graphic and haptic rendering of scene graphs, in the style of the VRML97 Specification, ISO/IEC 14772-1:1997. In this model, an application is created by constructing a scene graph built from a variety of different types of nodes, organised in 3-D space. A typical cluster of nodes might specify the geometry and appearance of a particular object in the scene. Nodes are graphically rendered using Open GL and haptically rendered by specifying surface collision behaviour with a haptic tool. The dynamics of the application are specified by connecting together fields of those nodes via routes, specified either in VRML notation or via Python scripts. A propagation mechanism updates the values of those fields according to the routes defined.</p><p>One of the extension libraries we have built adds the capability of the propagation of fields over network connections, allowing distributed applications to be built. Using this collaboration library, we based our application on a replicated homogeneous scene database, i.e. the same model is loaded on both machines, streaming only the changes backwards and forwards between them.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Supporting task and communication</h2><div class="c-article-section__content" id="Sec5-content"><p>The central feature of our virtual environment is an anatomical model of the temporal bone region. The model includes both the bone itself and some surrounding and contained anatomy. The bone primarily must support the operation of drilling. We model the bone using a hybrid format. A volumetric model, based on an octree, stores the information about the presence and absence of bone throughout the region. As drilling progresses, voxels in the model are transformed from full to empty. A triangular surface mesh is generated from the volumetric model to graphically display the changing surface of the bone. An alternate volumetric format is generated locally in the vicinity of the haptic device. This small subset of the model supports fast collision detection and response for real time haptic rendering.</p><p>Surgical trainees must learn to identify anatomical landmarks within the bone. Different regions of the bone have different appearances and textures and different behaviour under drilling. To model this, each voxel in the bone model is tagged with a bone type and a lookup table provides the attributes for that voxel. For example, the bone around the labyrinth erodes more slowly than the bone in the mastoid. We have implemented a false colouring scheme within the bone to highlight the different regions and aid the student in learning the anatomy initially. This is a design decision that supports the task adequately, without the expense (in both development time and computation resources) of aiming for a highly realistic bone representation. The bone can also be rendered partially transparent to reveal the anatomy and spaces hidden within and this mode can be switched on an off interactively. When visually transparent, the bone is not rendered haptically.</p><p>There are several anatomical structures that are modelled as tubes, specified by points along the centre and a variable radius. These include the sigmoid sinus, carotid artery and facial nerve. The tube specification is converted to a deformable triangular mesh for graphic and haptic rendering. Other structures, such as parts of the skull, the dura, external ear canal and the ossicles, are represented directly using surface meshes. These take advantage of the surface modelling and rendering features included with the Reachin API. Some structures are enhanced with specific behaviours that serve as learning cues for the student. For example, the blood vessels will bleed if struck with a spinning drill. The facial nerve will trigger feedback from the facial nerve monitor if injured.</p><p>The models for the system are derived from CT scans of temporal bone samples. Computed tomography (CT) is a three-dimensional X-ray that provides a density reading for each voxel in a volumetric region. It is relatively easy to segment bone from a CT using thresholding; soft tissues are more complex. At present, we use a manual segmentation process to produce models that include the information we need to give different attributes to different bone regions. The surface and tube models are produced using a variety of algorithms, using the open source Visualization Toolkit (VTK) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Schroeder W, Martin K, Lorensen B (1998) The Visualization Toolkit, 2nd edn. Prentice-Hall Inc., NJ" href="/article/10.1007/s10055-005-0015-1#ref-CR13" id="ref-link-section-d2670e533">13</a>]. We currently have four models available, three left ears and one right. Deriving the models from real scans ensures the anatomical accuracy and relevance necessary for supporting the learning task.</p><p>The complete temporal bone model is presented in the system through a simulated microscope view. The microscope supports several levels of magnification, typical of those found in surgery. We implemented the microscope by changing the graphic rendering of objects that appear within it, while leaving the haptic rendering unchanged. As in the real world, magnified objects look bigger, but do not feel bigger. The magnification applies not only to the anatomical model, but also to the various tools that can be used. The tools appear magnified when within the microscope region, but normal size when in other parts of the scene. The model can be moved and rotated within the microscope view using the Magellan SpaceMouse. Note that all microscope controls and in fact all controls in the system, can be operated equally by either user. All have on screen user interfaces, as well as keyboard short cuts. In the trials described later in this paper, the user interface was mostly operated by the instructing surgeon, to leave the student free to concentrate on the drilling task in the limited time available.</p><p>The system includes a range of different tools for operating on the model and teaching about the surgery. Each tool is modelled as a visual appearance and a range of behaviours that are associated with one of the haptic devices at any time. Each tool can be picked up by either user and in either hand on a system with two haptic devices. Some of the tools are restricted so that there can only be one active at any time. For example, there is only one drill, so one user must put it down before the other can pick it up. There is a short cut for swapping tools between hands.</p><p>The drill tool is actually a drill handle that can hold one of a range of burrs. The burrs are arranged on a rack and can be selected by touching the chosen burr with the drill handle. The system includes cutting and diamond burrs in a range of sizes. The drilling simulation treats the different types of burrs differently: diamond burrs erode bone more slowly than the cutting burrs. The drill speed is controlled with a foot pedal. We visualise the distinction between on and off by changing the texture on the head of the burr. The distinctive visual texture of a cutting or diamond burr is replaced by a blurred motion texture when the drill is operating, providing a convincing illusion of rapid spinning. The speed of the motor of the drill is also represented with a simulated sound effect. Varying drill speeds affect the rate of bone erosion.</p><p>Eroding bone creates bone dust, which obscures visibility and can clog the drill. The system includes a simple simulation of bone dust. Particles of dust are generated at the point of drilling contact and move outwards randomly, remaining in the area to obscure the view. They are visually rendered as small bone coloured points. There is a sucker tool that will cause the particles to move towards the end of the tool and disappear, enabling the surgeon to clean the area of dust. The sucker tool has two suction strengths activated by the button on the haptic device, typical of real suckers, which can be made to suck harder by covering a small hole in the tube with a fingertip.</p><p>The bone dust and suction simulation are not highly realistic and we do not include irrigation with fluid and the consequent formation of slurry. Like many of the features of the system, dust and suction are simulated in an indicative manner that is sufficient to allow the task to be completed and to support communication between the instructor and student on the subject. We have discussed this design approach further in another paper [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Hutchins M, Adcock M, Stevenson D, Gunn C, Krumpholz A (2005) The design of perceptual representations for practical networked multimodal virtual training environments. In: Proceedings of the 11th International Conference on Human-Computer Interaction (HCI International 2005), July 2005, Las Vegas, Nevada, USA, CD-ROM" href="/article/10.1007/s10055-005-0015-1#ref-CR14" id="ref-link-section-d2670e548">14</a>].</p><p>Some of the available tools are included solely to support communication. We include a marker and eraser that allows the instructor (or student) to draw anywhere in the scene, in a variety of colours and styles. This can be used for annotation of the model and to indicate the position of features not included in the model. In a typical example, the instructor marked the location of the initial strokes that the student should make on the surface of the bone, then erased them once drilling was under way. The system includes a white board area for more abstract drawing. Another communication tool is the guiding hand. This enables the instructor to reach out and grasp the end of the student’s tool, creating a two-way haptic connection between them across the network. The instructor can use this to guide the student’s tool to particular locations within the scene, or demonstrate stroke directions.</p><p>Finally, we have included in the system some areas away from the microscope view of the model, to support different styles of instruction and communication. The white board area was mentioned previously. We also include a light box, where the CT scans from which the models are derived can be viewed. This supports a different approach to describing and learning the anatomy of the region. There is also a video player, with a selection of clips from real temporal bone surgery. This allows the student to learn the actual appearance of the bone to be expected in the operating theatre. The video clips also include material on some of the soft tissue incisions that are not simulated by the model. A stereo setup screen allows the student to become comfortable with the three-dimensional environment and the use of the haptic devices, before encountering the bone model. We have found that first time users of the system are more comfortable if they are given some general purpose familiarisation exercises before embarking on their learning task. Three-dimensional virtual reality environments with haptics are still a novelty to many people! Software features of the system are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0015-1#Fig2">2</a>.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0015-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0015-1/MediaObjects/10055_2005_15_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0015-1/MediaObjects/10055_2005_15_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Screen captures of the system in operation. <b>a</b> A student drills the bone in the simulated microscope view, with guidance from the instructor. <b>b</b> The bone is made transparent to show the anatomical landmarks beneath. <b>c</b> The instructor annotates a slice from the CT scan of the bone</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0015-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
            </div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Analysis of communication</h2><div class="c-article-section__content" id="Sec6-content"><p>In November 2004 we conducted an experimental trial to measure the transfer of learning from the training system to a laboratory drilling situation. We collected data for a quantitative analysis of the results and also videotaped parts of the trial for further analysis. In this paper, we discuss the qualitative analysis of the videotapes of the training sessions. In conducting the analysis, we have been partly influenced by the field of social science called Conversation Analysis [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Hutchby I, Woofitt W (1998) Conversation Analysis. Polity Press, Cambridge, UK" href="/article/10.1007/s10055-005-0015-1#ref-CR15" id="ref-link-section-d2670e597">15</a>]. We base our analysis on recorded (in this case, videotaped) conversations, presented in the form of annotated transcripts. We have used some of the common transcription conventions found in Conversation Analysis texts. We treat the collected recordings as holding potentially interesting specimens of conversational events. Our goal is to identify and describe some of these specimens. Each specimen acts as evidence that a particular type of conversational structure occurs in the collection and can be used as a template to look for other occurrences. We have not used an artificial or pre-conceived coding scheme to classify or count parts of the conversation. Rather, the transcripts are studied to reveal specimens of interest as they occur. In this case we are looking for particular types of specimens—those that demonstrate how a shared virtual environment assists communication.</p><p>For the trial, we recruited 11 surgical trainees who were approaching the end of their basic surgical training and were thinking of specialising in otology. Most were broadly familiar with the subject, but had little or no former specific experience with temporal bone drilling. After being given information about the trial, giving their consent and completing a preliminary anatomy and surgical planning examination, they were trained for about an hour on the training system. After a short break, they completed another examination and then went on to drill a real temporal bone in the laboratory. The session on the training system was divided into two phases. In the first phase, which we will call the instruction phase, the subjects were given extensive instruction and step by step directions as they performed a simulated cortical mastoidectomy. During the second phase, which we will call the demonstration phase, they were asked to repeat the procedure to demonstrate what they had learned. Instruction was still available, but was to be initiated by the subject when required. During the demonstration phase, the subjects were asked to verbalise their progress, for example, identifying the surgical landmarks as they came across them. The communication discussed below took place between the instructor and the subjects during the instruction and demonstration phases of the sessions on the training system. All subjects were trained by the same instructor, Stephen O’Leary (a co-author on this paper and part of the research team that designed the training system and conducted the trial).</p><p>Due to the circumstances under which they were recorded, we would not expect the conversations we are analysing to have the structure of typical examples of generic conversation. The instructional scenario necessarily means that the two participants are not playing equal roles. The instructor dominates the communication and must deliver large amounts of didactic material to the subject, particularly in the instructional phase of the training session. Furthermore, the physical environment for the conversation and the existence of the shared virtual environment, could be expected to change the dynamics of the conversation. For the trial, we had the instructor and student workstations situated adjacent to each other in the same room. There was direct voice communication between them, but the visual attention of each was mainly on the virtual environment, so face and body cues were mostly absent.</p><p>Nevertheless, we can find in these conversations some typical patterns that have been identified in other types of talk. In this first transcript segment, the second subject is undergoing training in the instruction phase. “I” is the instructor and “S” is the student subject.</p>
                <h3 class="c-article__sub-heading">Transcript 1: Subject 2, Instruction Phase, Tape 2, Start 0:19:06.3</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK, now that’s a good - you’ve found the dura nicely there so suck that away and now you might want to find the sigmoid, OK?
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:What are you- you see you’ve got a big island here?
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:You want to avoid that because you don’t know - the sigmoid, for all you know, might be up here.
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah, [sure
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:[OK? It could be anterior, so you want to keep this all at a similar depth.
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So I can ero- get rid of all this?
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yes, that’s right. You want to keep this all at a similar depth. You don’t want a big mountain in the middle, OK?
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:That’s the way.
                          </td></tr></tbody></table></div></div></div>
                
              <p>We can see examples of turn taking (throughout), question and answer pairs (lines 4 and 5; 9 and 10), repairs (lines 4 and 9) and acknowledgments (lines 2 and 7). Note that the open square brackets on adjacent lines (lines 7 and 8) indicate the start of incidents of overlapping speech.</p><p>The instructor and student must work to establish <i>grounding</i>, in the conversational sense, as described by Clark and Brennan [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Clark H, Brennan S (1991) Grounding in Communication. In: Resnick LJL, Teasley S (eds) Perspectives on Socially Shared Cognition. APA Books, Washington, pp 127–149" href="/article/10.1007/s10055-005-0015-1#ref-CR16" id="ref-link-section-d2670e832">16</a>]. That is, they must establish that each is attending to and understanding the conversation as it proceeds. Without visual communication, they must do this verbally. This is clearly demonstrated by the student’s contributions in the following transcript segment.</p>
                <h3 class="c-article__sub-heading">Transcript 2: Subject 4, Instruction Phase, Tape 5, Start 0:14:24.8</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So, that means that we will be able to, ah, if we find the antrum, we’re going to basically find the lateral semicircular canal, which is going to be in this orientation.
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Now, the facial nerve is down here.
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So to go down here is going to be dangerous,
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:until we’ve found the lateral canal.
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So the way we’re going to do that is we’re going to basically open, make sure you’re deepest here at all times,
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:then open up in other directions.
                          </td></tr><tr><td class="u-text-left ">
                            12.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeh
                          </td></tr><tr><td class="u-text-left ">
                            13.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK, you’re going to, so
                          </td></tr><tr><td class="u-text-left ">
                            14.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So we go up and down here?
                          </td></tr><tr><td class="u-text-left ">
                            15.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yeah, that’s right, 
                          </td></tr><tr><td class="u-text-left ">
                            16.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes
                          </td></tr><tr><td class="u-text-left ">
                            17.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and you make sure you’re deeper superiorly than anywhere else, because there’s no structure up here to injure.
                          </td></tr></tbody></table></div></div></div>
                
              <p>The introduction of a shared virtual model allows the instructor and student to share grounded references to objects and events that are represented in the system. This is one of the crucial benefits of collaborative virtual environment technology and is clearly demonstrated in this segment, where the student is searching for the lateral semicircular canal.</p>
                <h3 class="c-article__sub-heading">Transcript 3: Subject 2, Demonstration Phase, Tape 3, Start 0:07:03.6</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So I’m in the antrum, [now
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:[Uh-hmm
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Er, is that th- 
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            {<i>Student waves the drill over the antrum where the lateral canal has been revealed</i>}
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Is that it?
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            {<i>Student makes a loose pointing gesture with the sucker</i>}
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:This here?
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            {<i>Instructor touches the lateral canal with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yes, that’s right.
                          </td></tr></tbody></table></div></div></div>
                
              <p>In lines 4–6, the student clearly expects the instructor to see and understand what she is referring to as “that” and this expectation is confirmed in lines 8–10. This would not necessarily be significant with a real object (although it is more difficult to have identical viewpoints), but in this case the behaviour is indicative of a shared acceptance of the illusion of the virtual object. Note also that the lateral canal is not explicitly mentioned in this segment of dialogue, but the referent of “it” in line 6 is assumed from the context of the procedure.</p><p>In this longer segment we can see a variety of references grounded in the shared virtual model.</p>
                <h3 class="c-article__sub-heading">Transcript 4: Subject 3, Instruction Phase, Tape 4, Start 0:18:23.9</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Now do you get the hint there’s a change of colour there?
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            {<i>The instructor points with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:I can see a pinkish here.
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            {<i>The student points with the sucker</i>}
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yeah, that’s, that could well be the beginning of the facial nerve.
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Um, so you be aware of that and think well OK I, I, I’ve got to follow this inferiorly, OK
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures with the probe to indicate inferiorly</i>}
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Um, and this is actually a facial nerve monitor,
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            {<i>The instructor waves the probe</i>} 
                          </td></tr><tr><td class="u-text-left ">
                            12.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            13.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:which we’d use in surgery
                          </td></tr><tr><td class="u-text-left ">
                            14.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            15.
                          </td><td class="u-text-left ">
                            {<i>The instructor touches the bone and the rhythmic sound of the facial nerve monitor is heard</i>}
                          </td></tr><tr><td class="u-text-left ">
                            16.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Hey presto!
                          </td></tr><tr><td class="u-text-left ">
                            17.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            18.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:It’s working.
                          </td></tr><tr><td class="u-text-left ">
                            19.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            20.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK, so, yes indeed that’s the facial nerve
                          </td></tr><tr><td class="u-text-left ">
                            21.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            22.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So you follow that now from <u>here</u> inferiorly
                          </td></tr><tr><td class="u-text-left ">
                            23.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures again to show drilling inferiorly</i>}
                          </td></tr><tr><td class="u-text-left ">
                            24.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            25.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and see if you can find a canal or a line of the nerve.
                          </td></tr><tr><td class="u-text-left ">
                            26.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So don’t want to i- I don’t want to um,
                          </td></tr><tr><td class="u-text-left ">
                            27.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:[Once you’ve got
                          </td></tr><tr><td class="u-text-left ">
                            28.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[go through that
                          </td></tr><tr><td class="u-text-left ">
                            29.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:[No
                          </td></tr><tr><td class="u-text-left ">
                            30.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[canal obviously, so I just take the, OK.
                          </td></tr><tr><td class="u-text-left ">
                            31.
                          </td><td class="u-text-left ">
                            {<i>The student points at the nerve with the sucker, then repeats the instructor’s gesture</i>}
                          </td></tr><tr><td class="u-text-left ">
                            32.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yeah, once you’ve got a hint of the hint of the nerve you follow [it
                          </td></tr><tr><td class="u-text-left ">
                            33.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[Yeah
                          </td></tr><tr><td class="u-text-left ">
                            34.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:in its expected direction, which is somewhere straight, you know, down here
                          </td></tr><tr><td class="u-text-left ">
                            35.
                          </td><td class="u-text-left ">
                            {<i>The instructor repeats the gesture, showing drilling along the nerve</i>} 
                          </td></tr><tr><td class="u-text-left ">
                            36.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Got it
                          </td></tr><tr><td class="u-text-left ">
                            37.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and you take it from there and see if you can expand this kind of view.
                          </td></tr><tr><td class="u-text-left ">
                            38.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK
                          </td></tr></tbody></table></div></div></div>
                
              <p>Again we observe an easy acceptance of the shared experience, with the assumption that each can perceive the intended referents in the other’s speech. In this example we also observe the importance of different types of gesture in the communication. The haptic devices we use have fast and stable tracking and placing these into the co-located haptic and graphic workbench creates an environment where gesture seems to be a very natural way of communicating. In the transcript above we can observe pointing at objects (line 2), gesturing to indicate direction (line 8), gestures intended to be imitated (line 35), waving the tool to attract attention to it (line 11) and active touch (line 15). In other segments we observed gesturing to indicate broad areas and to outline regions. These kinds of spatial gestures can only be used in the context of a shared spatial experience.</p><p>In Transcript 4 we also observe the multimodal nature of the communication between the instructor and the student. Speech and gesture are naturally combined, but other modes are also at work. For example, various kinds of visual representations are present in the scene, including the pseudo-colouring of the bone regions discussed in lines 1 and 3. Sound is also used, as in the tone made by the simulated facial nerve monitor (line 15), which also includes an accompanying visual signal. One of the features of the system that the students frequently praised was the ability to make the bone transparent to reveal the structures underneath. Interactive presentation techniques, like transparency, can be considered as alternate modes of communication. In this next extended transcript segment, we see how speech, gesture and use of the interactive transparency are combined in the explanation of a moderately complex spatial concept. There are also many further examples of turn taking, conversational grounding and referential grounding in the shared model.</p>
                <h3 class="c-article__sub-heading">Transcript 5: Subject 3, Instruction Phase, Tape 3, Start 0:34:43.4</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:You’ve found enough, though, to allow you to safely find the lateral semicircular canal, which is down here,
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            {<i>The bone is made transparent</i>}
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:straight ahead, it’s in that orientation. The canal’s in this orientation,
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures along the canal with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and it’s straight down below here.
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            {<i>Transparency is turned off. The instructor gestures above the location of the canal.</i>}
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:But what- Above the, ah, lateral semicircular canal is the antrum.
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            {<i>The instructor broadly outlines the extent of the antrum with the probe, above the bone</i>}
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            12.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:The antrum is an air filled space,
                          </td></tr><tr><td class="u-text-left ">
                            13.
                          </td><td class="u-text-left ">
                            {<i>The bone is made transparent</i>}
                          </td></tr><tr><td class="u-text-left ">
                            14.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:that flows direct, is in direct continuity with the middle ear and the epitympanic space, OK?
                          </td></tr><tr><td class="u-text-left ">
                            15.
                          </td><td class="u-text-left ">
                            {<i>The instructor again outlines the extent of the space, within the transparent bone</i>}
                          </td></tr><tr><td class="u-text-left ">
                            16.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and it comes around like this, see what I mean?
                          </td></tr><tr><td class="u-text-left ">
                            17.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Right, OK
                          </td></tr><tr><td class="u-text-left ">
                            18.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So it’s an air-filled space. And it’s very convenient, because you’re going to come across that before you get to the lateral semicircular canal.
                          </td></tr><tr><td class="u-text-left ">
                            19.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Aaah, OK.
                          </td></tr><tr><td class="u-text-left ">
                            20.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK?
                          </td></tr><tr><td class="u-text-left ">
                            21.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            22.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So, and if you can see, if you go too far forward, you’re going to hit the incus first.
                          </td></tr><tr><td class="u-text-left ">
                            23.
                          </td><td class="u-text-left ">
                            {<i>Transparency is turned off</i>}
                          </td></tr><tr><td class="u-text-left ">
                            24.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            25.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So you’re in the right spot to find the lateral canal,
                          </td></tr><tr><td class="u-text-left ">
                            26.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures above the bone to where the canal is beneath</i>}
                          </td></tr><tr><td class="u-text-left ">
                            27.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:and what you want to do is you want to open up along in this area here,
                          </td></tr><tr><td class="u-text-left ">
                            28.
                          </td><td class="u-text-left ">
                            {<i>The instructor outlines the area to be opened up</i>}
                          </td></tr><tr><td class="u-text-left ">
                            29.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:again, not digging a hole, not making it too steep, but if you stick- Basically, open initially at the top and then broaden it down, you can’t injure anything.
                          </td></tr><tr><td class="u-text-left ">
                            30.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:This is the top, here?
                          </td></tr><tr><td class="u-text-left ">
                            31.
                          </td><td class="u-text-left ">
                            {<i>The student gestures with the drill</i>}
                          </td></tr><tr><td class="u-text-left ">
                            32.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yeah, [this is the top,
                          </td></tr><tr><td class="u-text-left ">
                            33.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[I mean, OK, yeah
                          </td></tr><tr><td class="u-text-left ">
                            34.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:The dura. 
                          </td></tr><tr><td class="u-text-left ">
                            35.
                          </td><td class="u-text-left ">
                            {<i>The instructor points with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            36.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:So if you open it up near the dura,
                          </td></tr><tr><td class="u-text-left ">
                            37.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            38.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:go deep near the dura, <u>then</u> open it up a little bit more down here,
                          </td></tr><tr><td class="u-text-left ">
                            39.
                          </td><td class="u-text-left ">
                            {<i>The instructor indicates with the probe the areas to be drilled</i>}
                          </td></tr><tr><td class="u-text-left ">
                            40.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:mm-hmm
                          </td></tr><tr><td class="u-text-left ">
                            41.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:deep near the dura then open it up a little bit more down here, you can’t injure anything.
                          </td></tr><tr><td class="u-text-left ">
                            42.
                          </td><td class="u-text-left ">
                            {<i>The instructor repeats the previous gesture, as the words are repeated</i>}
                          </td></tr><tr><td class="u-text-left ">
                            43.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Can we just have a look at it without the thing again? Would that be alright?
                          </td></tr><tr><td class="u-text-left ">
                            44.
                          </td><td class="u-text-left ">
                            {<i>The bone is made transparent, then not transparent</i>}
                          </td></tr><tr><td class="u-text-left ">
                            45.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Sorry?
                          </td></tr><tr><td class="u-text-left ">
                            46.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yep, that’s it, good, yeah.
                          </td></tr><tr><td class="u-text-left ">
                            47.
                          </td><td class="u-text-left ">
                            {<i>The bone is made transparent again</i>}
                          </td></tr><tr><td class="u-text-left ">
                            48.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:This is the transparency there.
                          </td></tr><tr><td class="u-text-left ">
                            49.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK, I see, yep. So I see that there’s more of a space here near the top.
                          </td></tr><tr><td class="u-text-left ">
                            50.
                          </td><td class="u-text-left ">
                            {<i>The student gestures with the drill to indicate the spacious area</i>}
                          </td></tr><tr><td class="u-text-left ">
                            51.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:There is.
                          </td></tr><tr><td class="u-text-left ">
                            52.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK
                          </td></tr><tr><td class="u-text-left ">
                            53.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:There’s always more of a space, and th-s- the superior canal here is way, way, [way back.
                          </td></tr><tr><td class="u-text-left ">
                            54.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>: [way way back.
                          </td></tr><tr><td class="u-text-left ">
                            55.
                          </td><td class="u-text-left ">
                            {<i>The instructor points to the superior canal with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            56.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK, 
                          </td></tr><tr><td class="u-text-left ">
                            57.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK
                          </td></tr><tr><td class="u-text-left ">
                            58.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:so, you know, this is, ah, you’re going to get to this and then you’d have to drill through hard bone to get down to here
                          </td></tr><tr><td class="u-text-left ">
                            59.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:yeah
                          </td></tr></tbody></table></div></div></div>
                
              <p>Note how the virtual environment helps to demonstrate spatial concepts, such as the location of the superior canal (“way, way, way back”) introduced in line 53.</p><p>The student’s question about “the top” in line 30 is probably caused by the fact that it is conventional in surgery to discuss spatial terms with respect to a standing patient, so that the top is towards the top of the patient’s head. However, the model is oriented in the display so that the top of the patient’s head is towards the right. Thus, “the top” of the antrum is on the right of the model and the student needed to clarify this. In sharing a procedure, the instructor and student also get the opportunity to share language and convention in the domain, which is an important part of the training. The previous transcripts have shown how the students must become comfortable with spatial terms like “superiorly” and “anteriorly” and the shared virtual experience gives them a context to do this (although in this case they would most likely be familiar with them from their previous general training).</p><p>So far, we have not explicitly mentioned the role of haptics in the communication. Although we provided the facility for direct haptic communication (in the form of a “guiding hand”), this mode was not used by the instructor during the training trials. This may have been due to unfamiliarity with this mode, but also suggests that other modes were sufficient. We noted above that the haptic devices used provided strong support for gesture in the interaction. The most significant role of haptics in the system is in naturally supporting the drilling task. By enabling the students to perform the task, the inclusion of haptics contributes strongly to an overall environment in which situated learning can take place. That is, the students learn by doing and the process of doing creates situations from which they can learn, situations that may not have arisen in a static or less interactive model. The following transcript segment shows a typical example.</p>
                <h3 class="c-article__sub-heading">Transcript 6: Subject 4, Instruction Phase, Tape 5, Start 0:11:34.4</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:OK, now one thing you have done here that I have noticed, you see you’ve got this bit of bone here?
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yes.
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Probably, really by now you would have been best to have [drilled that away.
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[Should get rid of that, away.
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:That’s right, keep it a similar depth.
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah, alright.
                          </td></tr></tbody></table></div></div></div>
                
              <p>Although the student has largely been following instructions up to this point, one part of the instruction was missed. The missing instruction created a situation where a part of the bone was not removed properly. The instructor was able to identify this situation and bring it to the attention of the student, who may then be able to recognise it himself in the future.</p><p>Situating the learning within the task of drilling the virtual bone enables the instructor to recognise and correct mistakes as they occur, as well as praising correct behaviour so that it can be repeated. In this next segment, the instructor is able to identify both a potential mistake, and the satisfactory correction of that mistake.</p>
                <h3 class="c-article__sub-heading">Transcript 7: Subject 2, Demonstration Phase, Tape 3, Start 0:05:05.9</h3>
                
                  <h4 class="c-article-table__inline-caption u-h4"> </h4><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            1.
                          </td><td class="u-text-left ">
                            {<i>drilling reveals the dura</i>}
                          </td></tr><tr><td class="u-text-left ">
                            2.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah that’s, that’s the dura, up in there.
                          </td></tr><tr><td class="u-text-left ">
                            3.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Good, [OK, excellent.
                          </td></tr><tr><td class="u-text-left ">
                            4.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[Yeah
                          </td></tr><tr><td class="u-text-left ">
                            5.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So, um, next I want to find th- the lateral canal?
                          </td></tr><tr><td class="u-text-left ">
                            6.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Yep.
                          </td></tr><tr><td class="u-text-left ">
                            7.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            8.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Don’t dig too deep a hole, OK? That’s - you’re in the right direction, but don’t-You see what you’re doing here, you’re digging a really <u>deep</u> hole.
                          </td></tr><tr><td class="u-text-left ">
                            9.
                          </td><td class="u-text-left ">
                            {<i>The instructor points with the probe</i>}
                          </td></tr><tr><td class="u-text-left ">
                            10.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:Yeah
                          </td></tr><tr><td class="u-text-left ">
                            11.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:You should saucerise it a bit, OK?
                          </td></tr><tr><td class="u-text-left ">
                            12.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures with the probe to explain saucerise</i>}
                          </td></tr><tr><td class="u-text-left ">
                            13.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Until you get down to the level of the lateral canal you’re extremely unlikely to come across the facial. So you go from where you’ve done, out.
                          </td></tr><tr><td class="u-text-left ">
                            14.
                          </td><td class="u-text-left ">
                            {<i>The instructor gestures with the probe to demonstrate</i>}
                          </td></tr><tr><td class="u-text-left ">
                            15.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:OK, yeah.
                          </td></tr><tr><td class="u-text-left ">
                            16.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            17.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:I just want to see a it more saucerised, you know, [rather than a deep dark hole
                          </td></tr><tr><td class="u-text-left ">
                            18.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:[yes
                          </td></tr><tr><td class="u-text-left ">
                            19.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:yes
                          </td></tr><tr><td class="u-text-left ">
                            20.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            21.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:Now that’s better, you see it’s a better shape, isn’t it.
                          </td></tr><tr><td class="u-text-left ">
                            22.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:yes
                          </td></tr><tr><td class="u-text-left ">
                            23.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:It’s not a deep dark hole, but you still re- heading very much in the right direction.
                          </td></tr><tr><td class="u-text-left ">
                            24.
                          </td><td class="u-text-left ">
                            {<i>drilling</i>}
                          </td></tr><tr><td class="u-text-left ">
                            25.
                          </td><td class="u-text-left ">
                            
                                            <b>S</b>:So I’m in the antrum, [now.
                          </td></tr><tr><td class="u-text-left ">
                            26.
                          </td><td class="u-text-left ">
                            
                                            <b>I</b>:[Uh-hmm
                          </td></tr></tbody></table></div></div></div>
                
              <p>We can also see in this segment that the student is oriented within the task, although still somewhat uncertain. After locating the dura (line 2), the next step in the procedure is to locate the lateral canal (line 5, the uncertainty revealed by the phrasing of this as a question), which can be seen in the antrum (line 25).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Conclusions and future work</h2><div class="c-article-section__content" id="Sec7-content"><p>In this paper we have presented a networked virtual environment for teaching temporal bone surgery. The system emphasises collaborative work between an instructor and a student and provides a shared simulation of the anatomy that permits the student to perform a complete procedure, observed and assisted by the instructor. The system represents the objects in the simulation using a mix of model types and media. Haptics are an integral part of the user interface and overall user experience and enable the simulated drilling to be competently performed, accepted and in some sense believed, by the trainees. Haptic interfaces are an essential enabling technology for this class of application.</p><p>A qualitative analysis of transcripts of video tapes taken during a training trial has shown that that the networked virtual environment supports a rich dialogue between an instructor and a student. The conversation is grounded by the shared model, allowing detailed spatial and anatomical concepts to be conveyed. The conversation is further enriched by the availability of multiple communication modes: speech is enhanced by gesture, interactive visualisations and sound. The student can perform the task from beginning to end, supported by a simulation that provides the essential visual, auditory and haptic cues for understanding and completing the process. This task performance provides a context, both spatial and procedural, to situate learning and generate opportunities for new discoveries. The qualitative analysis presented here is complementary to a more traditional quantitative analysis of training transfer that we intend to publish in a future paper. The trial subjects were examined for their knowledge of the surgery before and after the training and had their surgical technique assessed on a real bone in the laboratory. This provides the basis for measurements of learning within the environment. The transcript analysis, on the other hand, gives us insight into the processes of communication within the environment, and in particular how they are supported by a shared virtual model.</p><p>The results reported here are from the first formal trial of the system, and open many possibilities for future work. Of particular interest is exploring the effect of separating the instructor and student geographically on the communication and the training. We have demonstrated that a shared environment supports instruction within the same room, but we expect that the system will also facilitate instruction at a distance. With the basics of the system in place, it would be useful to explore our design decisions with respect to the quality and realism of the simulation and determine which features should be improved to provide the maximum training benefit. Comparisons with other methods of teaching the same material, for example, physical models or multimedia systems, would help to establish the case for this level of technology in training. Although we have concentrated on supporting one-on-one training, there is also a need for individual training when an instructor is not available. There is, therefore, scope for integrating intelligent tutoring into the system [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Krumpholz A (2005) Building a virtual trainer for an immersive haptic virtual reality environment. In: Kaschek R (ed) Perspectives of Intelligent Systems’ Assistance, Technical Report TR 2005/2, Department of Information Systems, Massey University, Palmerston North, New Zealand" href="/article/10.1007/s10055-005-0015-1#ref-CR17" id="ref-link-section-d2670e3747">17</a>]. A library of interesting bone models would be a valuable addition to the system and in the long term would require the development of automated segmentation and modelling processes. Expanding the range of procedures supported by the system, by including more anatomy, tools and features, would also be necessary to make the system versatile enough for general deployment. An important next step will be to have the system adopted into the local curriculum. This will provide ongoing use, which will benefit the surgical community and provide further feedback for improving the system.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leigh J, Johnson A, Vasilakis C, DeFanti T (1996) Multi-pespective collaborative design in persistent networke" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Leigh J, Johnson A, Vasilakis C, DeFanti T (1996) Multi-pespective collaborative design in persistent networked virtual environments. In: Proceeding of the IEEE Virtual Reality Annual International Symposium (VRAIS 96), March 1996, pp 253–262</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gunn C, Hutchins M, Adcock M and Hawkins R (2003) Trans-World Haptic Collaboration. In: Proceedings of the ACM" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Gunn C, Hutchins M, Adcock M and Hawkins R (2003) Trans-World Haptic Collaboration. In: Proceedings of the ACM SIGGRAPH 2003 conference (Sketches and Applications), San Diego, July 2003</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Gunn, M. Hutchins, M. Adcock, R. Hawkins, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Gunn C, Hutchins M, Adcock M, Hawkins R (2004) Surgical training using haptics over long internet distances. I" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Gunn C, Hutchins M, Adcock M, Hawkins R (2004) Surgical training using haptics over long internet distances. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2004. IOS Press, Amsterdam, pp 121–123</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Medicine%20Meets%20Virtual%20Reality%202004&amp;pages=121-123&amp;publication_year=2004&amp;author=Gunn%2CC&amp;author=Hutchins%2CM&amp;author=Adcock%2CM&amp;author=Hawkins%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Kim, H. Kim, M. Muniyandi, M. Srinivasan, J. Jorden, J. Mortensen, M. Oliveira, M. Slater, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Kim J, Kim H, Muniyandi M, Srinivasan M, Jorden J, Mortensen J, Oliveira M, Slater M (2004) Transatlantic Touc" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Kim J, Kim H, Muniyandi M, Srinivasan M, Jorden J, Mortensen J, Oliveira M, Slater M (2004) Transatlantic Touch: a Study of Haptic Collaboration over Long Distance. Presence: Teleoper Virtual Environ 13(3):328–337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transatlantic%20Touch%3A%20a%20Study%20of%20Haptic%20Collaboration%20over%20Long%20Distance&amp;journal=Presence%3A%20Teleoper%20Virtual%20Environ&amp;volume=13&amp;issue=3&amp;pages=328-337&amp;publication_year=2004&amp;author=Kim%2CJ&amp;author=Kim%2CH&amp;author=Muniyandi%2CM&amp;author=Srinivasan%2CM&amp;author=Jorden%2CJ&amp;author=Mortensen%2CJ&amp;author=Oliveira%2CM&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rasmussen M, Mason TP, Millman A, Evenhouse R, Sandin D (1998) The virtual temporal bone, a tele-immersive edu" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Rasmussen M, Mason TP, Millman A, Evenhouse R, Sandin D (1998) The virtual temporal bone, a tele-immersive educational environment. Future Generation Computing Systems 14, Elsevier, pp 125–130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wiet GJ, Bryan J, Dodson E, Sessnna D, Stredney D, Schmalbrock P, Welling B (2000) Virtual Temporal Bone Disse" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Wiet GJ, Bryan J, Dodson E, Sessnna D, Stredney D, Schmalbrock P, Welling B (2000) Virtual Temporal Bone Dissection Simulation. In: Westwood JD, et al (eds) Medicine Meets Virtual Reality 2000. IOS Press, pp 378–384</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stredney D, Wiet GJ, Bryan J, Sessanna D, Murakami J, Schmalbrock P, Powell K, Welling, B (2002) Temporal Bone" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Stredney D, Wiet GJ, Bryan J, Sessanna D, Murakami J, Schmalbrock P, Powell K, Welling, B (2002) Temporal Bone Dissection Simulation—An Update. In: Procedings of MMVR 02/10, Long Beach, CA,USA January 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Agus M, Giachetti A, Gobbetti E, Zanetti G, Zorcolo A (2002) Real-time haptic and visual simulation of bone di" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Agus M, Giachetti A, Gobbetti E, Zanetti G, Zorcolo A (2002) Real-time haptic and visual simulation of bone dissection. In: Procedings of the IEEE Virtual Reality 2002 (VR’02), March 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Petersik A, Pflesser B, Tiede U, Hohne KH (2002) Haptic Rendering of Volumetric Anatomic Models at Sub-voxel R" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Petersik A, Pflesser B, Tiede U, Hohne KH (2002) Haptic Rendering of Volumetric Anatomic Models at Sub-voxel Resolution. In: Proceedings of 10th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems (Haptics 2002), Orlando, FL, USA 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Seymour, A. Gallagher, S. Roman, M V. O’Brien Bansai, D. Andersen, R. Satava, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Seymour N, Gallagher A, Roman S, O’Brien M Bansai V, Andersen D, Satava R (2002) Virtual Reality Training Impr" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Seymour N, Gallagher A, Roman S, O’Brien M Bansai V, Andersen D, Satava R (2002) Virtual Reality Training Improves Operating Room Performance. Ann Surgery 236(4):458–464</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F00000658-200210000-00008" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20Reality%20Training%20Improves%20Operating%20Room%20Performance&amp;journal=Ann%20Surgery&amp;volume=236&amp;issue=4&amp;pages=458-464&amp;publication_year=2002&amp;author=Seymour%2CN&amp;author=Gallagher%2CA&amp;author=Roman%2CS&amp;author=O%E2%80%99Brien%20Bansai%2CM%20V&amp;author=Andersen%2CD&amp;author=Satava%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Morris D, Sewell C, Blevins N, Barbagli F, Salisbury K (2004) A collaborative virtual environment for the simu" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Morris D, Sewell C, Blevins N, Barbagli F, Salisbury K (2004) A collaborative virtual environment for the simulation of temporal bone surgery, In: Procedings of Medical Image Computing and Computer-Assisted Intervention (MICCAI 2004), Springer-Verlag Lecture Notes in Computer Science, vol 3217, pp 319–327</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stevenson D, Smith K, McLaughlin J, Gunn C, Veldkamp JP, Dixon M (1999) Haptic workbench: a multisensory virtu" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Stevenson D, Smith K, McLaughlin J, Gunn C, Veldkamp JP, Dixon M (1999) Haptic workbench: a multisensory virtual environment. In Proc. SPIE—The International Society for Optical Engineering, 3639, pp 356–366</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="W. Schroeder, K. Martin, B. Lorensen, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Schroeder W, Martin K, Lorensen B (1998) The Visualization Toolkit, 2nd edn. Prentice-Hall Inc., NJ" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Schroeder W, Martin K, Lorensen B (1998) The Visualization Toolkit, 2nd edn. Prentice-Hall Inc., NJ</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Visualization%20Toolkit&amp;publication_year=1998&amp;author=Schroeder%2CW&amp;author=Martin%2CK&amp;author=Lorensen%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hutchins M, Adcock M, Stevenson D, Gunn C, Krumpholz A (2005) The design of perceptual representations for pra" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Hutchins M, Adcock M, Stevenson D, Gunn C, Krumpholz A (2005) The design of perceptual representations for practical networked multimodal virtual training environments. In: Proceedings of the 11th International Conference on Human-Computer Interaction (HCI International 2005), July 2005, Las Vegas, Nevada, USA, CD-ROM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="I. Hutchby, W. Woofitt, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Hutchby I, Woofitt W (1998) Conversation Analysis. Polity Press, Cambridge, UK" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Hutchby I, Woofitt W (1998) Conversation Analysis. Polity Press, Cambridge, UK</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Conversation%20Analysis&amp;publication_year=1998&amp;author=Hutchby%2CI&amp;author=Woofitt%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Clark, S. Brennan, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Clark H, Brennan S (1991) Grounding in Communication. In: Resnick LJL, Teasley S (eds) Perspectives on Sociall" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Clark H, Brennan S (1991) Grounding in Communication. In: Resnick LJL, Teasley S (eds) Perspectives on Socially Shared Cognition. APA Books, Washington, pp 127–149</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perspectives%20on%20Socially%20Shared%20Cognition&amp;pages=127-149&amp;publication_year=1991&amp;author=Clark%2CH&amp;author=Brennan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krumpholz A (2005) Building a virtual trainer for an immersive haptic virtual reality environment. In: Kaschek" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Krumpholz A (2005) Building a virtual trainer for an immersive haptic virtual reality environment. In: Kaschek R (ed) Perspectives of Intelligent Systems’ Assistance, Technical Report TR 2005/2, Department of Information Systems, Massey University, Palmerston North, New Zealand</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0015-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>We would like to thank the surgical trainees for their enthusiastic participation in the trial and surgical colleagues Drs Michael Tycocinski and Marcus Dahm, who assisted with the running of the trial. This work was substantially funded under the CeNTIE project. The CeNTIE project is supported by the Australian Government through the Advanced Networks Program of the Department of Communications, Information Technology and the Arts. Support was also provided by the Department of Otolaryngology The University of Melbourne and Medtronic Xomed.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CSIRO ICT Centre, GPO BOX 664, 2601, Canberra, ACT, Australia</p><p class="c-article-author-affiliation__authors-list">Matthew A. Hutchins, Duncan R. Stevenson, Chris Gunn, Alexander Krumpholz &amp; Tony Adriaansen</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Otolaryngology, University of Melbourne, Melbourne, Australia</p><p class="c-article-author-affiliation__authors-list">Brian Pyman &amp; Stephen O’Leary</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Matthew_A_-Hutchins"><span class="c-article-authors-search__title u-h3 js-search-name">Matthew A. Hutchins</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Matthew A.+Hutchins&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Matthew A.+Hutchins" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Matthew A.+Hutchins%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Duncan_R_-Stevenson"><span class="c-article-authors-search__title u-h3 js-search-name">Duncan R. Stevenson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Duncan R.+Stevenson&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Duncan R.+Stevenson" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Duncan R.+Stevenson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Chris-Gunn"><span class="c-article-authors-search__title u-h3 js-search-name">Chris Gunn</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Chris+Gunn&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chris+Gunn" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chris+Gunn%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Alexander-Krumpholz"><span class="c-article-authors-search__title u-h3 js-search-name">Alexander Krumpholz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Alexander+Krumpholz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Alexander+Krumpholz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Alexander+Krumpholz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tony-Adriaansen"><span class="c-article-authors-search__title u-h3 js-search-name">Tony Adriaansen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tony+Adriaansen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tony+Adriaansen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tony+Adriaansen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Brian-Pyman"><span class="c-article-authors-search__title u-h3 js-search-name">Brian Pyman</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Brian+Pyman&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Brian+Pyman" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brian+Pyman%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Stephen-O_Leary"><span class="c-article-authors-search__title u-h3 js-search-name">Stephen O’Leary</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Stephen+O%E2%80%99Leary&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Stephen+O%E2%80%99Leary" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Stephen+O%E2%80%99Leary%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0015-1/email/correspondent/c1/new">Matthew A. Hutchins</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Communication%20in%20a%20networked%20haptic%20virtual%20environment%20for%20temporal%20bone%20surgery%20training&amp;author=Matthew%20A.%20Hutchins%20et%20al&amp;contentID=10.1007%2Fs10055-005-0015-1&amp;publication=1359-4338&amp;publicationDate=2005-12-09&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Hutchins, M.A., Stevenson, D.R., Gunn, C. <i>et al.</i> Communication in a networked haptic virtual environment for temporal bone surgery training.
                    <i>Virtual Reality</i> <b>9, </b>97–107 (2006). https://doi.org/10.1007/s10055-005-0015-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0015-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-07-18">18 July 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-10-07">07 October 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12-09">09 December 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-03">March 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0015-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0015-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Haptic workbench</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Networked virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic surgical training</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Conversation analysis</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0015-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=15;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

