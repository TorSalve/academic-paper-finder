<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Evaluation of direct manipulation using finger tracking for complex ta"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="
A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube"/>

    <meta name="dc.source" content="Virtual Reality 2014 18:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-02-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="
A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate objects with fingers of both hands in a close-to-natural manner for moderately complex, general purpose tasks. Our solution couples finger tracking with a real-time physics engine, combined with a heuristic approach for hand manipulation, which is robust to tracker noise and simulation instabilities. A first study has been performed to evaluate our interface, with tasks involving complex manipulations, such as balancing objects while walking in the cube. The user&#8217;s finger-tracked manipulation was compared to manipulation with a 6 degree-of-freedom wand (or flystick), as well as with carrying out the same task in the real world. Users were also asked to perform a free task, allowing us to observe their perceived level of presence in the scene. Our results show that our approach provides a feasible interface for immersive cube environments and is perceived by users as being closer to the real experience compared to the wand. However, the wand outperforms direct manipulation in terms of speed and precision. We conclude with a discussion of the results and implications for further research."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-02-13"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="203"/>

    <meta name="prism.endingPage" content="217"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0246-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0246-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0246-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0246-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2014/09"/>

    <meta name="citation_online_date" content="2014/02/13"/>

    <meta name="citation_firstpage" content="203"/>

    <meta name="citation_lastpage" content="217"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0246-0"/>

    <meta name="DOI" content="10.1007/s10055-014-0246-0"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0246-0"/>

    <meta name="description" content="
A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditi"/>

    <meta name="dc.creator" content="Emmanuelle Chapoulie"/>

    <meta name="dc.creator" content="Maud Marchal"/>

    <meta name="dc.creator" content="Evanthia Dimara"/>

    <meta name="dc.creator" content="Maria Roussou"/>

    <meta name="dc.creator" content="Jean-Christophe Lombardo"/>

    <meta name="dc.creator" content="George Drettakis"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Agarawala A, Balakrishnan R (2006) Keepin&#8217; it real: pushing the desktop metaphor with physics, piles and the pen. CHI &#8217;06, pp 1283&#8211;1292"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Physics-based virtual reality for task learning and intelligent disassembly planning; citation_author=J Aleotti, S Caselli; citation_volume=15; citation_publication_date=2011; citation_pages=41-54; citation_doi=10.1007/s10055-009-0145-y; citation_id=CR2"/>

    <meta name="citation_reference" content="Bolt RA (1980) &amp;ldquo;put-that-there&amp;rdquo;: Voice and gesture at the graphics interface. In: Proceedings of the 7th annual conference on computer graphics and interactive techniques, ACM, New York, NY, USA, SIGGRAPH &#8217;80, pp 262&#8211;270. doi:
                    10.1145/800250.807503
                    
                  
                "/>

    <meta name="citation_reference" content="Borst C, Indugula A (2005) Realistic virtual grasping. In: IEEE VR 2005, pp 91&#8211;98"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=3D user interfaces: new directions and perspectives; citation_author=D Bowman, S Coquillart, B Froehlich, M Hirose, Y Kitamura, K Kiyokawa, W Stuerzlinger; citation_volume=28; citation_issue=6; citation_publication_date=2008; citation_pages=20-36; citation_doi=10.1109/MCG.2008.109; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Questioning naturalism in 3d user interfaces; citation_author=D Bowman, R McMahan, E Ragan; citation_volume=55; citation_issue=9; citation_publication_date=2012; citation_pages=78-88; citation_doi=10.1145/2330667.2330687; citation_id=CR6"/>

    <meta name="citation_reference" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) Fingartips: gesture based direct manipulation in augmented reality. In: Proceedings GRAPHITE &#8217;04, pp 212&#8211;221"/>

    <meta name="citation_reference" content="Cabral MC, Morimoto CH, Zuffo MK (2005) On the usability of gesture interfaces in virtual reality environments. In: Proceedings CLIHC &#8217;05, pp 100&#8211;108"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part C; citation_title=A survey of glove-based systems and their applications; citation_author=L Dipietro, AM Sabatini, P Dario; citation_volume=38; citation_issue=4; citation_publication_date=2008; citation_pages=461-482; citation_doi=10.1109/TSMCC.2008.923862; citation_id=CR9"/>

    <meta name="citation_reference" content="Fr&#246;hlich B, Tramberend H, Beers A, Agrawala M, Baraff D (2000) Physically-based manipulation on the responsive workbench. In: IEEE VR 2000"/>

    <meta name="citation_reference" content="Heumer G, Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves&#8212;a comparison of classification methods. In: Virtual reality conference, 2007. VR &#8217;07. IEEE, pp 19 &#8211;26. doi:
                    10.1109/VR.2007.352459
                    
                  
                "/>

    <meta name="citation_reference" content="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI &#8217;12, pp 2421&#8211;2430"/>

    <meta name="citation_reference" content="Hirota K, Hirose M (2003) Dexterous object manipulation based on collision response. In: IEEE VR &#8217;03, IEEE Comput Soc, vol 2003, pp 232&#8211;239"/>

    <meta name="citation_reference" content="citation_journal_title=J Virtual Real Broadcast; citation_title=Multi-contact grasp interaction for virtual environments; citation_author=D Holz, S Ullrich, M Wolter, T Kuhlen; citation_volume=5; citation_issue=7; citation_publication_date=2008; citation_pages=1860-2037; citation_id=CR14"/>

    <meta name="citation_reference" content="Jacobs J, Froehlich B (2011) A soft hand model for physically-based manipulation of virtual objects. In: IEEE VR 2011, IEEE"/>

    <meta name="citation_reference" content="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI&#8217;2012, IEEE, pp 43&#8211;51"/>

    <meta name="citation_reference" content="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: Conference companion on human factors in computing systems, ACM, New York, NY, USA, CHI &#8217;94, pp 453&#8211;454. doi:
                    10.1145/259963.260487
                    
                  
                "/>

    <meta name="citation_reference" content="Latoschik M, Frohlich M, Jung B, Wachsmuth I (1998) Utilize speech and gestures to realize natural interaction in a virtual environment. In: Industrial electronics society, 1998. IECON &#8217;98. Proceedings of the 24th annual conference of the IEEE, vol 4, pp 2028 &#8211;2033. doi:
                    10.1109/IECON.1998.724030
                    
                  
                "/>

    <meta name="citation_reference" content="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, ACM, New York, NY, USA, AFRIGRAPH &#8217;01, pp 95&#8211;100. doi:
                    10.1145/513867.513888
                    
                  
                "/>

    <meta name="citation_reference" content="McMahan R, Alon A, Lazem S, Beaton R, Machaj D, Schaefer M, Silva M, Leal A, Hagan R, Bowman D (2010) Evaluating natural interaction techniques in video games. In: 3DUI, IEEE, pp 11&#8211;14"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE TVCG; citation_title=Natural interaction metaphors for functional validations of virtual car models; citation_author=M Moehring, B Froehlich; citation_volume=17; citation_issue=9; citation_publication_date=2011; citation_pages=1195-1208; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=Visual gesture interfaces for virtual environments; citation_author=R O&#8217;Hagan, A Zelinsky, S Rougeaux; citation_volume=14; citation_issue=3; citation_publication_date=2002; citation_pages=231-250; citation_doi=10.1016/S0953-5438(01)00050-9; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE TVCG; citation_title=A six degree-of-freedom god-object method for haptic display of rigid bodies with surface properties; citation_author=M Ortega, S Redon, S Coquillart; citation_volume=13; citation_issue=3; citation_publication_date=2007; citation_pages=458-469; citation_id=CR23"/>

    <meta name="citation_reference" content="Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEEE, pp 39&#8211;42"/>

    <meta name="citation_reference" content="citation_journal_title=Philos Trans R Soc B Biol Sci; citation_title=Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments; citation_author=M Slater; citation_volume=364; citation_issue=1535; citation_publication_date=2009; citation_pages=3549-3557; citation_doi=10.1098/rstb.2009.0138; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=A survey of glove-based input; citation_author=DJ Sturman, D Zeltzer; citation_volume=14; citation_issue=1; citation_publication_date=1994; citation_pages=30-39; citation_doi=10.1109/38.250916; citation_id=CR26"/>

    <meta name="citation_reference" content="Sturman DJ, Zeltzer D, Pieper S (1989) Hands-on interaction with virtual environments. UIST &#8217;89, pp 19&#8211;24"/>

    <meta name="citation_reference" content="Ullmann T, Sauer J (2000) Intuitive virtual grasping for non haptic environments. In: Pacific Graphics &#8217;00, pp 373&#8211;381"/>

    <meta name="citation_reference" content="Wang RY, Popovi&#263; J (2009) Real-time hand-tracking with a color glove. In: ACM SIGGRAPH 2009 Papers, ACM, New York, NY, USA, SIGGRAPH &#8217;09, pp 63:1&#8211;63:8. doi:
                    10.1145/1576246.1531369
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=An approach to natural gesture in virtual environments; citation_author=A Wexelblat; citation_volume=2; citation_issue=3; citation_publication_date=1995; citation_pages=179-200; citation_doi=10.1145/210079.210080; citation_id=CR30"/>

    <meta name="citation_reference" content="Wilson AD, Izadi S, Hilliges O, Garcia-Mendoza A, Kirk D (2008) Bringing physics to the surface. In: ACM UIST &#8217;08, pp 67&#8211;76"/>

    <meta name="citation_author" content="Emmanuelle Chapoulie"/>

    <meta name="citation_author_email" content="george.drettakis@inria.fr"/>

    <meta name="citation_author_institution" content="Inria, REVES, Sophia Antipolis, France"/>

    <meta name="citation_author" content="Maud Marchal"/>

    <meta name="citation_author_institution" content="Inria, Hybrid, Rennes, France"/>

    <meta name="citation_author_institution" content="IRISA-INSA, Rennes, France"/>

    <meta name="citation_author" content="Evanthia Dimara"/>

    <meta name="citation_author_institution" content="University of Athens, Athens, Greece"/>

    <meta name="citation_author" content="Maria Roussou"/>

    <meta name="citation_author_institution" content="University of Athens, Athens, Greece"/>

    <meta name="citation_author_institution" content="Makebelieve Design and Consulting, Athens, Greece"/>

    <meta name="citation_author" content="Jean-Christophe Lombardo"/>

    <meta name="citation_author_institution" content="Inria, Sophia Antipolis, France"/>

    <meta name="citation_author" content="George Drettakis"/>

    <meta name="citation_author_institution" content="Inria, REVES, Sophia Antipolis, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0246-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0246-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube"/>
        <meta property="og:description" content="A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate objects with fingers of both hands in a close-to-natural manner for moderately complex, general purpose tasks. Our solution couples finger tracking with a real-time physics engine, combined with a heuristic approach for hand manipulation, which is robust to tracker noise and simulation instabilities. A first study has been performed to evaluate our interface, with tasks involving complex manipulations, such as balancing objects while walking in the cube. The user’s finger-tracked manipulation was compared to manipulation with a 6 degree-of-freedom wand (or flystick), as well as with carrying out the same task in the real world. Users were also asked to perform a free task, allowing us to observe their perceived level of presence in the scene. Our results show that our approach provides a feasible interface for immersive cube environments and is perceived by users as being closer to the real experience compared to the wand. However, the wand outperforms direct manipulation in terms of speed and precision. We conclude with a discussion of the results and implications for further research."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0246-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Direct manipulation, Immersive cube, Finger tracking","kwrd":["Virtual_reality","Direct_manipulation","Immersive_cube","Finger_tracking"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0246-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0246-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=246;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0246-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0246-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0246-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-02-13" itemprop="datePublished">13 February 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Emmanuelle-Chapoulie" data-author-popup="auth-Emmanuelle-Chapoulie" data-corresp-id="c1">Emmanuelle Chapoulie<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Inria" /><meta itemprop="address" content="grid.5328.c, 0000000121863954, Inria, REVES, Sophia Antipolis, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Maud-Marchal" data-author-popup="auth-Maud-Marchal">Maud Marchal</a></span><sup class="u-js-hide"><a href="#Aff2">2</a>,<a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Inria" /><meta itemprop="address" content="grid.5328.c, 0000000121863954, Inria, Hybrid, Rennes, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="IRISA-INSA" /><meta itemprop="address" content="IRISA-INSA, Rennes, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Evanthia-Dimara" data-author-popup="auth-Evanthia-Dimara">Evanthia Dimara</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Athens" /><meta itemprop="address" content="grid.5216.0, 0000000121550800, University of Athens, Athens, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Maria-Roussou" data-author-popup="auth-Maria-Roussou">Maria Roussou</a></span><sup class="u-js-hide"><a href="#Aff4">4</a>,<a href="#Aff5">5</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Athens" /><meta itemprop="address" content="grid.5216.0, 0000000121550800, University of Athens, Athens, Greece" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Makebelieve Design and Consulting" /><meta itemprop="address" content="Makebelieve Design and Consulting, Athens, Greece" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jean_Christophe-Lombardo" data-author-popup="auth-Jean_Christophe-Lombardo">Jean-Christophe Lombardo</a></span><sup class="u-js-hide"><a href="#Aff6">6</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Inria" /><meta itemprop="address" content="grid.5328.c, 0000000121863954, Inria, Sophia Antipolis, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-George-Drettakis" data-author-popup="auth-George-Drettakis">George Drettakis</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Inria" /><meta itemprop="address" content="grid.5328.c, 0000000121863954, Inria, REVES, Sophia Antipolis, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">203</span>–<span itemprop="pageEnd">217</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">445 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0246-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>
A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate objects with fingers of both hands in a close-to-natural manner for moderately complex, general purpose tasks. Our solution couples finger tracking with a real-time physics engine, combined with a heuristic approach for hand manipulation, which is robust to tracker noise and simulation instabilities. A first study has been performed to evaluate our interface, with tasks involving complex manipulations, such as balancing objects while walking in the cube. The user’s finger-tracked manipulation was compared to manipulation with a 6 degree-of-freedom wand (or flystick), as well as with carrying out the same task in the real world. Users were also asked to perform a free task, allowing us to observe their perceived level of presence in the scene. Our results show that our approach provides a feasible interface for immersive cube environments and is perceived by users as being closer to the real experience compared to the wand. However, the wand outperforms direct manipulation in terms of speed and precision. We conclude with a discussion of the results and implications for further research.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Interaction in immersive virtual reality systems (e.g., CAVEs) has always been challenging, especially for novice users. In most systems, 6 degree-of-freedom (6DOF) devices such as “wands” or “flysticks” are used for navigation as well as selection and manipulation tasks. Such devices are well established and can be very powerful since they allow users to perform actions which cannot be performed naturally, such as picking objects from afar, or navigating while physically staying in the same place. However, one goal of fully immersive systems is to enhance presence and immersion (Slater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Philos Trans R Soc B Biol Sci 364(1535):3549–3557" href="/article/10.1007/s10055-014-0246-0#ref-CR25" id="ref-link-section-d81434e429">2009</a>). In such a context, flysticks can potentially degrade the realism and naturalness of the virtual environment (VE). To avoid this shortcoming, the use of direct manipulation (DM) using finger tracking as close as possible to the manipulation used in the real world is investigated (Moehring and Froehlich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models. IEEE TVCG 17(9):1195–1208" href="/article/10.1007/s10055-014-0246-0#ref-CR21" id="ref-link-section-d81434e432">2011</a>; Wexelblat <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):179–200" href="/article/10.1007/s10055-014-0246-0#ref-CR30" id="ref-link-section-d81434e435">1995</a>; Jacobs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51" href="/article/10.1007/s10055-014-0246-0#ref-CR16" id="ref-link-section-d81434e438">2012</a>; Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430" href="/article/10.1007/s10055-014-0246-0#ref-CR12" id="ref-link-section-d81434e441">2012</a>).</p><p>With the advent of hand and finger tracking solutions, there has been recent interest in using DM and gestures in immersive systems to achieve specific tasks, such as automotive design (Jacobs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51" href="/article/10.1007/s10055-014-0246-0#ref-CR16" id="ref-link-section-d81434e447">2012</a>; Moehring and Froehlich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models. IEEE TVCG 17(9):1195–1208" href="/article/10.1007/s10055-014-0246-0#ref-CR21" id="ref-link-section-d81434e450">2011</a>), focusing on carefully handling the physics of collisions between hands and virtual objects in specific tasks. However, little has been done to investigate the usability of DM in a fully immersive setting for relatively complex, general-purpose tasks. Moreover, the effect of DM on presence or the similarity with real-world manipulations has not been sufficiently researched.</p><p>A solution which incorporates DM for grasping and moving objects, using both one and two hands, based on finger tracking with a “glove-like” device is presented and evaluated. Our system operates in a four-sided immersive cube (IC), i.e., a room with three rear-projected walls and rear-projected floor, providing a high sense of presence. To provide the most immersive and plausible experience, our solution uses a real-time physics simulator in addition to finger tracking. A physics engine, or simulator, is a software library that approximates the physical behavior of virtual objects by computing collision detection, the effect of gravity, etc., and the corresponding object transformations. It allows close-to-natural manipulation of objects in the scene. An approach for DM which is robust to tracker noise and instabilities of the physics simulation is presented.</p><p>The goals of our study are to evaluate (a) whether DM is a feasible alternative to traditional IC interfaces such as a wand, (b) the effect of using DM on presence, and (c) the similarity to real-world manipulation.</p><p>To demonstrate feasibility, we purposely choose a task that involves quite complex translations and rotations, while the user walks in the cube, and at the same time balances objects on a tray. DM is compared to traditional wand-based interaction and, most importantly, compared to a real-world reproduction of the virtual task, through a study focused on the sense of immersion. We record both objective measurements (time to completion and errors) and subjective judgments through the use of a questionnaire.</p><p>In summary, our study shows that two-handed DM enhances the sense of presence for some tasks, and users consider it more natural and closer to reality than the wand, clearly demonstrating its utility. Our objective measurements show that the wand and DM are in most cases equivalent in terms of speed and precision, but are slower than doing the task in the real world. The unconstrained user experience also shows several informal effects of enhanced presence due to DM, such as reflex reactions of participants trying to grasp dropped objects, or using two hands to adjust the position of a plate on a table.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous work</h2><div class="c-article-section__content" id="Sec2-content"><p>Gesture-based interaction has received significant interest in virtual or augmented reality research (Sturman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Sturman DJ, Zeltzer D, Pieper S (1989) Hands-on interaction with virtual environments. UIST ’89, pp 19–24" href="/article/10.1007/s10055-014-0246-0#ref-CR27" id="ref-link-section-d81434e470">1989</a>; O’Hagan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="O’Hagan R, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments. Interact Comput 14(3):231–250" href="/article/10.1007/s10055-014-0246-0#ref-CR22" id="ref-link-section-d81434e473">2002</a>; Buchmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) Fingartips: gesture based direct manipulation in augmented reality. In: Proceedings GRAPHITE ’04, pp 212–221" href="/article/10.1007/s10055-014-0246-0#ref-CR7" id="ref-link-section-d81434e476">2004</a>; Cabral et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Cabral MC, Morimoto CH, Zuffo MK (2005) On the usability of gesture interfaces in virtual reality environments. In: Proceedings CLIHC ’05, pp 100–108" href="/article/10.1007/s10055-014-0246-0#ref-CR8" id="ref-link-section-d81434e479">2005</a>). A thorough review of natural gestures for virtual reality (VR) (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Bowman D, McMahan R, Ragan E (2012) Questioning naturalism in 3d user interfaces. Commun ACM 55(9):78–88" href="/article/10.1007/s10055-014-0246-0#ref-CR6" id="ref-link-section-d81434e482">2012</a>) underlines the many positive features of natural, but also discusses the utility of “hyper-natural” interfaces. In a previous survey of 3D user interfaces (UI) (Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bowman D, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interfaces: new directions and perspectives. IEEE Comput Graph Appl 28(6):20–36" href="/article/10.1007/s10055-014-0246-0#ref-CR5" id="ref-link-section-d81434e486">2008</a>), natural gestures are mentioned as one of the important future directions for 3D UIs. In many cases, gestures are used to define a vocabulary or language (Buchmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) Fingartips: gesture based direct manipulation in augmented reality. In: Proceedings GRAPHITE ’04, pp 212–221" href="/article/10.1007/s10055-014-0246-0#ref-CR7" id="ref-link-section-d81434e489">2004</a>) even if the number of gestures is often limited (Sturman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Sturman DJ, Zeltzer D, Pieper S (1989) Hands-on interaction with virtual environments. UIST ’89, pp 19–24" href="/article/10.1007/s10055-014-0246-0#ref-CR27" id="ref-link-section-d81434e492">1989</a>; O’Hagan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="O’Hagan R, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments. Interact Comput 14(3):231–250" href="/article/10.1007/s10055-014-0246-0#ref-CR22" id="ref-link-section-d81434e495">2002</a>; Cabral et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Cabral MC, Morimoto CH, Zuffo MK (2005) On the usability of gesture interfaces in virtual reality environments. In: Proceedings CLIHC ’05, pp 100–108" href="/article/10.1007/s10055-014-0246-0#ref-CR8" id="ref-link-section-d81434e498">2005</a>). The early work by Bolt (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Bolt RA (1980) &amp;ldquo;put-that-there&amp;rdquo;: Voice and gesture at the graphics interface. In: Proceedings of the 7th annual conference on computer graphics and interactive techniques, ACM, New York, NY, USA, SIGGRAPH ’80, pp 262–270. doi:&#xA;                    10.1145/800250.807503&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR3" id="ref-link-section-d81434e501">1980</a>) already investigates the combined use of gestures and voice inputs through a set of commands to manipulate simple shapes. However, the use of a specific vocabulary can create an overhead for the user who must remember the meaning of each gesture. An early solution providing more natural interfaces involved the use of multimodal interactions, combining depictive gestures with speech (Koons and Sparrell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: Conference companion on human factors in computing systems, ACM, New York, NY, USA, CHI ’94, pp 453–454. doi:&#xA;                    10.1145/259963.260487&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR17" id="ref-link-section-d81434e505">1994</a>; Latoschik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Latoschik M, Frohlich M, Jung B, Wachsmuth I (1998) Utilize speech and gestures to realize natural interaction in a virtual environment. In: Industrial electronics society, 1998. IECON ’98. Proceedings of the 24th annual conference of the IEEE, vol 4, pp 2028 –2033. doi:&#xA;                    10.1109/IECON.1998.724030&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR18" id="ref-link-section-d81434e508">1998</a>; Latoschik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, ACM, New York, NY, USA, AFRIGRAPH ’01, pp 95–100. doi:&#xA;                    10.1145/513867.513888&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR19" id="ref-link-section-d81434e511">2001</a>). We are more interested in the case of DM, i.e., interacting with the environment in a natural manner with the users hands (Moehring and Froehlich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models. IEEE TVCG 17(9):1195–1208" href="/article/10.1007/s10055-014-0246-0#ref-CR21" id="ref-link-section-d81434e514">2011</a>; Wexelblat <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):179–200" href="/article/10.1007/s10055-014-0246-0#ref-CR30" id="ref-link-section-d81434e517">1995</a>; Jacobs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51" href="/article/10.1007/s10055-014-0246-0#ref-CR16" id="ref-link-section-d81434e520">2012</a>; Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430" href="/article/10.1007/s10055-014-0246-0#ref-CR12" id="ref-link-section-d81434e524">2012</a>).</p><p>Using the hand as an interaction metaphor results from an interest in applying the skills, dexterity and naturalness of the human hand directly to human–computer interfaces. Such metaphors are typically achieved either by detecting the hand through computer vision-based algorithms or by wearing-specific devices such as gloves (Sturman and Zeltzer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39. doi:&#xA;                    10.1109/38.250916&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR26" id="ref-link-section-d81434e530">1994</a>). However, Wang and Popović (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wang RY, Popović J (2009) Real-time hand-tracking with a color glove. In: ACM SIGGRAPH 2009 Papers, ACM, New York, NY, USA, SIGGRAPH ’09, pp 63:1–63:8. doi:&#xA;                    10.1145/1576246.1531369&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR29" id="ref-link-section-d81434e533">2009</a>) recently proposed a solution combining both technologies to provide simple and accurate real-time hand tracking for desktop VR applications, using only one camera and a color glove with a specific pattern. They demonstrate the validity of their solution through typical applications such as virtual assembly or gesture recognition. We are interested in fully immersive setups, and thus will focus on finger tracking technologies with high-range detection. Sturman and Zeltzer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39. doi:&#xA;                    10.1109/38.250916&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR26" id="ref-link-section-d81434e536">1994</a>) and Dipietro et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Dipietro L, Sabatini AM, Dario P (2008) A survey of glove-based systems and their applications. IEEE Trans Syst Man Cybern Part C 38(4):461–482" href="/article/10.1007/s10055-014-0246-0#ref-CR9" id="ref-link-section-d81434e539">2008</a>) provide thorough surveys of such devices and their applications in various fields, such as design for construction or 3D modeling, data visualization, robot control, entertainment and sign language interpretation, while the health sector shows an increasing interest in such interfaces for motor rehabilitation (hand functional assessment), ergonomics or training.</p><p>The addition of physics simulation to DM provides truly intuitive interaction with the objects in the scene. Such approaches for interface design have received much interest in recent years, often linked with tabletop systems (Agarawala and Balakrishnan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Agarawala A, Balakrishnan R (2006) Keepin’ it real: pushing the desktop metaphor with physics, piles and the pen. CHI ’06, pp 1283–1292" href="/article/10.1007/s10055-014-0246-0#ref-CR1" id="ref-link-section-d81434e545">2006</a>; Wilson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wilson AD, Izadi S, Hilliges O, Garcia-Mendoza A, Kirk D (2008) Bringing physics to the surface. In: ACM UIST ’08, pp 67–76" href="/article/10.1007/s10055-014-0246-0#ref-CR31" id="ref-link-section-d81434e548">2008</a>). The use of physics simulation to provide natural feedback in the environment has also been of great interest in VR research. One remarkable early result was that of Fröhlich et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Fröhlich B, Tramberend H, Beers A, Agrawala M, Baraff D (2000) Physically-based manipulation on the responsive workbench. In: IEEE VR 2000" href="/article/10.1007/s10055-014-0246-0#ref-CR10" id="ref-link-section-d81434e551">2000</a>), which demonstrated the use of a fast physics solver and hand-based interaction, in the context of a workbench environment. The physics solver presented was one of the first providing sufficiently fast simulation to allow realistic interaction. A major difficulty is how to handle objects controlled by the users hands (often called “God-objects,” sometimes referred to as kinematic objects; they can apply forces to dynamic objects in the scene, but no object can affect them) with respect to the simulation of the rest of the environment, i.e., correctly providing external forces from the hands.</p><p>Over the last 10 years, both physics simulation solutions and gesture-based input hardware have progressed immensely, providing the ability for much more accurate simulation. Much previous work concentrates on the more technical aspects of gesture recognition and its integration with physics, and often includes the calculation of forces for haptic force-feedback systems (Ortega et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ortega M, Redon S, Coquillart S (2007) A six degree-of-freedom god-object method for haptic display of rigid bodies with surface properties. IEEE TVCG 13(3):458–469" href="/article/10.1007/s10055-014-0246-0#ref-CR23" id="ref-link-section-d81434e557">2007</a>). Initial work, before physics simulation became widely available, focused on the definition of appropriate gestures for certain kinds of operations, and notably grasping (Ullmann and Sauer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Ullmann T, Sauer J (2000) Intuitive virtual grasping for non haptic environments. In: Pacific Graphics ’00, pp 373–381" href="/article/10.1007/s10055-014-0246-0#ref-CR28" id="ref-link-section-d81434e560">2000</a>). Experiments with force-feedback systems allowed the simulation of several quite complex manipulations (Hirota and Hirose <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hirota K, Hirose M (2003) Dexterous object manipulation based on collision response. In: IEEE VR ’03, IEEE Comput Soc, vol 2003, pp 232–239" href="/article/10.1007/s10055-014-0246-0#ref-CR13" id="ref-link-section-d81434e563">2003</a>). A spring model coupled with a commercially available physics simulator was used in Borst and Indugula (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Borst C, Indugula A (2005) Realistic virtual grasping. In: IEEE VR 2005, pp 91–98" href="/article/10.1007/s10055-014-0246-0#ref-CR4" id="ref-link-section-d81434e566">2005</a>) to simulate various kinds of grasping operations with objects of varying complexity and to avoid hand-object interpenetrations. A simpler approach was proposed by Holz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Holz D, Ullrich S, Wolter M, Kuhlen T (2008) Multi-contact grasp interaction for virtual environments. J Virtual Real Broadcast 5(7):1860–2037" href="/article/10.1007/s10055-014-0246-0#ref-CR14" id="ref-link-section-d81434e569">2008</a>), where grasping is simulated without complex physics. Grasping and interpenetration were also the focus of the work by Prachyabrued and Borst (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEEE, pp 39–42" href="/article/10.1007/s10055-014-0246-0#ref-CR24" id="ref-link-section-d81434e573">2012</a>).</p><p>In Moehring and Froehlich (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models. IEEE TVCG 17(9):1195–1208" href="/article/10.1007/s10055-014-0246-0#ref-CR21" id="ref-link-section-d81434e580">2011</a>), DM was used based on grasping heuristics and constraint-based solutions for the specific case of a car interior. Their approach is based on an analysis of the types of objects, their constraints and the typical grasps to derive a set of pseudophysical interaction metaphors. A quantitative comparison to a real-world car interior (mirror, door, etc.) was performed. More recent work has concentrated on developing appropriate soft models (Jacobs and Froehlich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Jacobs J, Froehlich B (2011) A soft hand model for physically-based manipulation of virtual objects. In: IEEE VR 2011, IEEE" href="/article/10.1007/s10055-014-0246-0#ref-CR15" id="ref-link-section-d81434e583">2011</a>) and efficient solvers to avoid interpenetration of God-objects and other objects in the scene (Jacobs et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51" href="/article/10.1007/s10055-014-0246-0#ref-CR16" id="ref-link-section-d81434e586">2012</a>), mainly in the context of automotive project review. The latter uses a setup similar to ours, with the same finger tracking system; however, a three-sided display system is used, with the floor projection from above. Usability and presence were not studied in this work.</p><p>The use of consumer-level depth sensors also opens numerous possibilities for natural interaction with collocation of virtual hands and a virtual scene, albeit in a limited workspace subject to various system constraints, e.g., the need for a split screen setup and the limited range of the depth sensor (Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430" href="/article/10.1007/s10055-014-0246-0#ref-CR12" id="ref-link-section-d81434e592">2012</a>). Our 4-sided IC also allows hand collocation and interaction, but does involve a number of hard constraints related to the geometry of the IC and the design of the tracking system. In particular, the three surrounding walls restrict the possible positions for the trackers and result in relatively large regions of “shadow” for the tracking system.</p><p>Our focus will be on presence, usability and user satisfaction when using DM in fully immersive environments. Some recent work exists for example on the effect of using physics on task learning (Aleotti and Caselli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Aleotti J, Caselli S (2011) Physics-based virtual reality for task learning and intelligent disassembly planning. Virtual Reality 15:41–54" href="/article/10.1007/s10055-014-0246-0#ref-CR2" id="ref-link-section-d81434e598">2011</a>) or the use of natural interaction for video games (McMahan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="McMahan R, Alon A, Lazem S, Beaton R, Machaj D, Schaefer M, Silva M, Leal A, Hagan R, Bowman D (2010) Evaluating natural interaction techniques in video games. In: 3DUI, IEEE, pp 11–14" href="/article/10.1007/s10055-014-0246-0#ref-CR20" id="ref-link-section-d81434e601">2010</a>). Heumer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Heumer G, Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves—a comparison of classification methods. In: Virtual reality conference, 2007. VR ’07. IEEE, pp 19 –26. doi:&#xA;                    10.1109/VR.2007.352459&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-014-0246-0#ref-CR11" id="ref-link-section-d81434e604">2007</a>) proposed to evaluate recognition methods through classification to avoid the calibration of gesture-based input devices. However, the study of usability for DM in general tasks and in a fully IC-like environment with walking users has not received much attention.</p><p>Our goal is to compare DM to the use of a wand and to real-world manipulations, in the challenging context of full immersion, which allows a close-to-natural interaction with the environment.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">A heuristic approach for direct manipulation with physics</h2><div class="c-article-section__content" id="Sec3-content"><p>There are several difficulties in developing a DM interface in an IC-like environment. In contrast to systems with a restricted workspace, in which the user is sitting (Prachyabrued and Borst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEEE, pp 39–42" href="/article/10.1007/s10055-014-0246-0#ref-CR24" id="ref-link-section-d81434e617">2012</a>; Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430" href="/article/10.1007/s10055-014-0246-0#ref-CR12" id="ref-link-section-d81434e620">2012</a>), a room-sized environment is targeted, and the user is allowed to walk around the scene while manipulating objects. There are three main difficulties discussed below: finger tracking, dynamic constraints between objects and the physics simulation.</p><p>First, finger tracking in the IC is challenging. Even though a high-end finger tracking system is used (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0246-0#Sec10">4.2</a>), it is prone to noise and interruptions in the signal for the fingers. In addition, a freely walking and moving user can often be in, or close to, “shadow” regions of trackers where the signal is deteriorated. This is due to occlusion from the user himself, and tracking calibration which is also hindered by the enclosing walls and the user occlusion. Tracking “shadow” regions thus occur in the zone 10–20 cm away from the screen. To overcome these issues, our tasks are designed to avoid activities where the user’s body blocks the tracking cameras or involves objects in the “shadow” regions. Our treatment of finger tracking and tracker signal filtering is detailed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0246-0#Sec4">3.1</a>.</p><p>Second, given our goal of assessing feasibility of DM, users are asked to perform relatively complex tasks: grasping and translating objects, including balancing objects one on top of the other while walking. In previous work, users often manipulate a single dynamic object per hand, with simple constraints, e.g., collision detection with static objects. In contrast, our tasks involve several dynamic objects and multiple indirect constraints from physics-based interactions between objects. This requires the robust tracking and grasping solutions developed.</p><p>Third, our tasks require the use of a fast physics engine; we use <i>Bullet</i>
                <sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0246-0#Sec7">3.4</a> for details), which is fast enough to handle quite complex scenes in real time (e.g., our user experience scene in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig14">14</a>. Due to the low light condition in the IC, photos were taken with a long exposure which results in some blur in the photographs). <i>Bullet</i> uses an impact-based simulation approach. As a result, objects tend to bounce between the fingers instead of being grasped, and the contacts are unstable. Many recent solutions improve physics simulation, most notably to avoid interpenetration of the hand and other objects in the scene [e.g., Jacobs et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51" href="/article/10.1007/s10055-014-0246-0#ref-CR16" id="ref-link-section-d81434e663">2012</a>), Prachyabrued and Borst (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEEE, pp 39–42" href="/article/10.1007/s10055-014-0246-0#ref-CR24" id="ref-link-section-d81434e666">2012</a>)], while other approaches (Ullmann and Sauer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Ullmann T, Sauer J (2000) Intuitive virtual grasping for non haptic environments. In: Pacific Graphics ’00, pp 373–381" href="/article/10.1007/s10055-014-0246-0#ref-CR28" id="ref-link-section-d81434e669">2000</a>; Holz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Holz D, Ullrich S, Wolter M, Kuhlen T (2008) Multi-contact grasp interaction for virtual environments. J Virtual Real Broadcast 5(7):1860–2037" href="/article/10.1007/s10055-014-0246-0#ref-CR14" id="ref-link-section-d81434e672">2008</a>) avoid the need for complex—and often expensive—precise simulation using specific algorithms
. In contrast, a fast but simple physics simulation is used, combined with an approach for basic DM such as grasping and releasing, and a finite-state machine to handle sequences of manipulations (e.g., moving from one-hand to two-hand grasping, etc.) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A user in the four-sided cube holding a tray with two hands</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In addition, the physics simulation requires careful synchronization with the displays. The simulation is run on one machine, the transformation information of all dynamic objects is propagated to all slaves at each frame. The solutions adopted to address these problems are next discussed.</p><h3 class="c-article__sub-heading" id="Sec4">Finger tracking and signal filtering</h3><p>The user is presented with a representation of the palm, thumb, index and middle fingertips in the form of small white cubes, providing visual feedback of hand position and orientation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig2">2</a>, left). Cubes are used for efficiency; they are mostly hidden by physical fingers, and thus did not interfere with interaction. In the following, only the thumb and index of each hand are used; these are called <i>active fingers</i>; the middle finger is ignored in our current implementation. Two <i>active fingers</i> proved to be largely sufficient to provide a natural-feeling DM interface, as seen in the evaluation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>
                          <i>Left</i> visual representation of the palm and three fingertips in the VE (cubes are shown away from the fingers for clarity of illustration).<i> Right</i> wand selection used for comparison</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>When grasping with one hand, the <i>main contacts</i> are the actual contacts of the thumb and index with the object. When grasping with two hands, there is one <i>main contact</i> per hand. This is the actual contact between the finger and the object if only one finger is applied, or a “mean contact” if two fingers are applied with this hand. The “mean contact” is set at the midpoint between the two actual finger contacts and is useful for the grasp/release finite-state machine described next.</p><p>The finger tracking signal in the IC lacks precision involving noise (“trembling”) and loss of signal (“jumps”). A Kalman filter is applied to each finger in time and we track the variance of the signal over a sliding window. If variance is above a threshold, we test if there is a plateau in the signal, in which case we identify this as a loss of signal and do not apply the motion to the object.</p><h3 class="c-article__sub-heading" id="Sec5">Grasp/release heuristics</h3><p>The physics engine is directly used to simulate the manipulations in the environment. Cubic objects are attached to the fingertips and the top of the palm. These are “kinematic objects” in <i>Bullet</i> terminology and can apply forces to the other dynamic objects being simulated. Kinematic objects are not affected by other objects. For each object, contact events with the fingers provided by the physics simulation are tracked. Once two kinematic contacts on a given object are identified, the object is marked as “grasped,” until the contacts are released. Grasped objects are removed from the physics simulation, and the transformation and speed of the trackers are enforced so that the simulation of objects in contact with the object selected is still correct.</p><p>When grasping, specific data are stored (e.g., position and orientation of the selected object), called <i>grasping data</i>, which is reset each time the number of contacts on the object changes. These data are used to compute the transformations of the selected object, depending on the fingers movements.</p><p>If we simply use the information from the tracker and the physics engine, users “drop” objects very quickly, or objects may move in an unstable manner. The selection is made more robust by marking an object as “released” only when the distance between the contacts is &gt;10 % of the distance stored in the <i>grasping data</i>. It is important to understand that contacts are determined by the physics engine in a natural manner when treated with our approach; they are not “pinch”-like gestures as the selection is determined by the actual contacts and distance between contacts and not by the movements (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig4">4</a>). If a hand participates in holding an object by applying a single finger, it releases as soon as the distance between the two main contacts is 10 % longer than that stored in the <i>grasping data</i>. These thresholds have been chosen by pilot trial-and-error tests in several different settings. While releasing, the object being handled follows the translation of the midpoint between the two <i>main contacts</i>. Accurately placing objects thus requires a short learning period (see the video and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig3">3</a> for an example). However, users did not complain about this limitation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Accurately placing a cube in a corner</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The threshold for release is the main restriction of our approach in terms of “naturalness.” If a user grasps a “thick” object with one hand, it is not possible for her to directly release it in the current implementation if she cannot open her hand by at least 10 % of the current gap. To release the selected object, the user would need to switch from a one-handed grasp to a two-handed grasp to reset the <i>grasping data</i> and then release the object.
 This scenario did not occur in our tasks (Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig4">4</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>User spreads her arms to release a large object without the need to further open her hands</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Finite-state machine graph detailing transitions between grasping and releasing states. A representation of the hand is shown in<i> green</i>;<i> dots</i> represent fingers. When only one active finger is touching a dynamic object, the corresponding<i> dot</i> becomes<i> plain green</i>. When two active fingers of the same hand are grasping an object, the corresponding<i> dots</i> turn<i> blue</i>. When the object is grasped with both hands, the<i> dots</i> in contact with the object become<i> red</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As mentioned before, selected objects are not handled by the physics simulation. The translation applied is that of the midpoint between the main contacts, and the rotation applied is that of the vector defined by the main contacts. When released, the objects are reinserted into the physics simulation.</p><h3 class="c-article__sub-heading" id="Sec6">Finite-state machine for object manipulation</h3><p>When performing complex tasks, users naturally grasp and release objects, and often mix one- and two-handed manipulations. To treat such transitions, a finite-state machine approach is designed. The different possible transitions for each manipulation are next described.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Grasp</h4><p> An object is grasped when the user applies at least two fingers on it, with only one hand or both hands (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig6">6</a>, left). The user can switch from a one-handed grasp to a two-handed grasp by touching the object with one or both active fingers of the second hand.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>
                            <i>Left</i> two-handed grasp.<i> Right</i> two-handed release</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>There is no alignment test on contacts, so users could lift an object with two contacts on the same side of a cube. However, this would be unnatural, and no user attempted this gesture in our tests.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec290">Release</h4><p> An object is released when there is at most one finger in contact with the object. For a grasping hand to release the object, the user simply has to open her fingers (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig6">6</a>, right). If one or both hands are grasping just by applying one of their active fingers, the user simply has to remove these hand(s) from the object. If both hands are grasping, the user can switch to a one-handed grasp by opening one hand. Finally, to completely release an object grasped with two hands, the user must spread her arms to ensure that the fingers are no longer in contact with the object.</p><p>The full list of possible transitions is provided in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig5">5</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec300">Translate</h4><p> The grasped object is translated when the fingers in contact translate.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec301">Rotate</h4><p> The grasped object is rotated as soon as the fingers in contact rotate. The rotation with two hands is currently limited to simplify the implementation, avoiding problems with the combined effect of tracker noise on each hand: the user can rotate one hand with respect to the other (e.g., tipping out balls from a tray).</p><p>The user can switch from a one-handed grasp to a two-handed grasp and vice versa; two one-handed grasps can be used at the same time to manipulate two distinct objects (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig7">7</a>, right).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>
                            <i>Left</i> one-handed grasp.<i> Right</i> two hands grasping two objects</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Experimental setup. The four-sided IC is represented in<b> a</b> with its eight infrared cameras. On the right are photos of the tracked input devices used:<b> b</b> Infitec glasses with frame,<b> c</b> ART flystick and<b> d</b> ART finger tracking gloves</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec7">Implementation</h3><p>Color coding is applied to the representations of fingertips. By default, the cubes are white. When only one active finger is touching a dynamic object, the corresponding cube becomes green. When two active fingers of the same hand grasp an object, the corresponding cubes turn blue. When the object is grasped with both hands, the cubes in contact with the object become red.</p><p>We use our in-house VR software system which is based on OpenSceneGraph.
<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> The Bullet Physics open source physics engine has been adapted to communicate the data needed by our grasping/release approach and our finite-state machine through extra object properties, as the transformations of the handled objects are enforced coherently with those controlled by the physics simulation. The interocular distance is calibrated, as well as Y–Z positions of the eyes for each participant at the beginning of each experiment. This provides a much better immersive experience in the IC by improving the projection from the user’s point of view (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0246-0#Sec10">4.2</a>). This is also more comfortable for the user as it is expected to reduce the risk of cybersickness.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">User study</h2><div class="c-article-section__content" id="Sec8-content"><p>Our goal is to evaluate whether DM is a feasible alternative to traditional interaction interfaces such as wands, whether it affects presence, and how similar it is to real-world manipulations. To provide a meaningful evaluation of feasibility, a complex task has purposely been chosen, which involves quite complex translations and rotations, while walking and balancing objects on a tray (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0246-0#Sec12">4.3</a> for details). By <i>complex task</i>, we mean a task requiring the user’s attention as it combines several aspects of everyday movements, such as accuracy and balance, such that the task is difficult to perform. We do this evaluation by comparing DM to a traditional wand-based interaction and by comparing both the wand and DM to a real-world task: the virtual world used is a replica of a real space in which the same tasks are performed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig12">12</a>). Our hypotheses are as follows: (1) using the wand will be more precise and faster than finger tracking based manipulation and 2) using finger tracking based DM will be more natural and will provide a higher sense of presence.</p><p>Both objective measurements, namely time to complete a task and precision or errors, and subjective judgments based on a questionnaire are recorded.</p><h3 class="c-article__sub-heading" id="Sec9">Population</h3><p>The experiment has been run with 18 participants, 10 men and 8 women aged between 24 and 59 years old, (mean age 32.5 years, standard deviation 10.7 years). Most had no experience with virtual reality (13 out of 18); 5 had experienced VR demonstrations before. Four participants had previously used a wand, including three who had previously manipulated virtual objects.</p><h3 class="c-article__sub-heading" id="Sec10">Experimental apparatus</h3><p>A four-sided IC (the Barco iSpace <sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>) is used, comprising three walls and a floor, which has four retro-projected “black” screens. The front and side screens are 3.2 m wide × 2.4 m high, and the floor is 3.2 m wide × 2.4 m long. Stereo is provided using Infitec technology, and for tracking, an ART<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> infrared optical system is used, with eight cameras (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig8">8</a> left). The head is tracked with a frame mounted on glasses. The tracked devices provided are the wand and the finger tracking system of ART to track the palm, thumb, index and middle finger of each hand (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig8">8</a>, right). The finger tracking system tracks the tips of the fingers giving the relative positions of these frames with respect to a 6DOF active tracker which is on the back of the hand.</p><p>Normally, six cameras are sufficient for head tracking. However, to allow better quality finger tracking and reduce “shadow" regions, two additional cameras were added. As these cameras must be placed in the bottom front corners of the IC and the cables connecting them to the system are visible in certain areas of the side walls, this can affect the user’s immersive experience. Our scenarios were thus designed to avoid eye gaze on these areas as much as possible, and our models were also designed to hide the cables and the cameras by adding very dark baseboards to the room.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Wand interaction</h4><p>For the wand, a standard virtual ray emanating from the wand is used. To grasp an object with the wand, the user points the wand toward it and presses a trigger button. Small blue spheres are displayed at contact points between the ray and the object when grasping (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig2">2</a>, right). The trigger is kept pressed during manipulation. To release an object with the wand, the user simply has to release the trigger.</p><p>The ray has been implemented to be thin to limit occlusion and to be long enough (1 m) to allow sufficient coverage of the 3.2 m × 3.2 m × 2.4 m IC.</p><h3 class="c-article__sub-heading" id="Sec12">Experimental procedures and environments</h3><p>The experiment lasted 90 min on average. As every participant performed all the tasks, it is a within-subject design. The experiment consists of a calibration step, a training session, a usability test, a “free form” user experience and a questionnaire completion. The usability evaluation has three conditions: using the wand, DM, and the real-world condition. Hence, we had six groups of three participants to test all the possible orders of conditions, which were randomized. For DM and real, the users always performed the tasks first with two hands and then with only one hand. We decided not to vary this order within the conditions using hands to permit the user to learn progressively. Specifically, when handling larger objects such as a tray, using two hands provides better control and balance overall. In contrast, using one hand involves grasping the border of the virtual tray; slight motion of one finger with respect to the other can result in a large motion of the tray, making it harder to control. In the design of all tasks, we tried to minimize the cases of the hand incorrectly occluding virtual objects; evidently this is not always possible, but we did not receive any negative feedback about this from participants.</p><p>The training session and the free-form user experience session are not performed in the real condition, but the order of the other conditions is the same as in the usability task. Concerning the DM, training session and the usability task are performed with two hands and one hand separately, whereas in the user experience, the user can freely manipulate objects with one or both hands. The best way to appreciate these experimental procedures is to watch the accompanying video.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Calibration step</h4><p>The finger tracking devices are calibrated for each user so that the signals are more reliable. Both devices are calibrated separately to avoid interference, using the procedure defined by the manufacturer.</p><p>The position of the eyes is also calibrated. A pilot test has been performed where the interocular distance was simply measured and set: this proved to be insufficient since the environment still displayed a “swimming” effect. To overcome this, the other two coordinates of the eye positions are also adjusted (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig9">9</a>). This is done using a simple scene, i.e., a floor and a stool (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig10">10</a>, left). The experimenter progressively modifies the coordinates of the eye positions until the cubes representing the fingers are closer to the fingertips and the user feels comfortable with the projection. A subsequent step consists in modifying values to minimize perceived movement of static objects when moving in the IC.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Position of the user’s eyes is calibrated in the three dimensions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>
                            <i>Left</i> calibration scene.<i> Right</i> training scene</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Training session</h4><p>The goal of the training session is to familiarize the participant with the interaction techniques. The experimenter first explains the color coding of the cubes and how to use the techniques to grasp, release and move the objects.</p><p>To maximize immersion, the virtual scene consists of a closed room exactly the size of the actual IC, with two tables, and three cardboard posts in between (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig10">10</a>, right). On the left table, the user is provided with a colored cube and a tray containing two balls. There are red crosses marked on the tables: one under the cube on the left table, and two on the right table which serve as targets (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig11">11</a>, left).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>
                            <i>Left</i> in the training session, target crosses are used to guide the participants.<i> Right</i> usability task scene contains posts: the user passes the tray through them</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The experimenter demonstrates the movements in the IC before the training session starts, explains that an alarm sounds when the tray or hands hit the posts, and asks the user to test this. The goal of keeping as many balls as possible on the tray is explained, and that the participant should avoid hitting the posts with the tray or hands. The participant starts the training by lifting the tray off the table. The tray must then be rotated by 90°, passed between the two first posts, rotated again by 90° and passed between the two second posts before being released on one of the tables (preferably the one on the right). The user can then repeat these steps and try other movements until she feels comfortable with manipulating the tray. Time and trials are not restricted during the training session.</p><p>Once this is done, the user ends the session by placing the colored cube onto the red crosses in a specific order. The experimenter explains that the sound heard when the cube touches the marks means that the subtask is validated, and that the same sound will be used when validating a subtask in the following session.</p><p>Once the session is complete, the scene is reset so that the user can train with the other technique (wand or DM), and when it is complete for the second time, the next session is automatically loaded.</p><p>The tray and the cube are manipulated with the wand and one hand; the tray is also manipulated with two hands.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Usability task</h4><p>Usability is evaluated in a single task which tests all the manipulations.</p><p>The main evaluation scenario takes place in the same virtual room as in the training session; only this room contains a stool on the left, the same table on the right, the same three cardboard posts in between and a cupboard in the back. Again, the virtual scene has the exact size of our four-sided IC. A tray with nine balls is placed on the stool. There is also a bowl on the front half of the table (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig12">12</a>, left).</p><p>This session is also performed in real conditions: the same scene has been built in the vicinity of the IC (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig12">12</a>, right).</p><p>The experimenter first explains the entire task to the user by mimicking the required operations at the beginning of each condition, insisting on the order of the steps. The beginning and end of each subtask is determined automatically by the system. The task is composed as follows (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig13">13</a>):
</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <i>Subtask 1:</i> The user grasps the tray and passes between the two sets of posts after rotating the tray by 90° each time, and then releases it on the table. The participant is instructed to avoid dropping balls as well as to avoid touching the posts with the tray or the hands (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig13">13</a>a, b, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig11">11</a>, right).</p>
                      </li>
                      <li>
                        <p>
                          <i>Subtask 2:</i> The user lifts the tray off the table and empties it into the bowl before releasing it on the table again. The user is instructed to keep as many balls as possible into the bowl (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig13">13</a>c). If the tray is empty at the end of subtask 1, this subtask is skipped.</p>
                      </li>
                      <li>
                        <p>
                          <i>Subtask 3:</i> The user picks up the empty tray again and places it inside the cupboard (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig13">13</a>d).</p>
                      </li>
                    </ul>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>
                            <i>Left</i> virtual usability environment.<i> Right</i> real usability space</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Subtasks of the usability task (see text)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>This task is quite challenging, even in the real world. Notice that the first subtask corresponds to what the user already had to do in the training session, as this is the most challenging part of the task.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Making real and virtual equally difficult</h4><p>To compare real and virtual tasks, we need to have approximately the same level of difficulty between the two. However, the physics simulator is only approximate in terms of material properties (friction, etc.) and handles dynamics with impulses which can sometimes be unrealistic. For a fair comparison, a pilot test with four participants who performed the task of balancing the tray avoiding the posts and minimizing the number of balls dropped has been carried out before the actual experiments, in both the real and virtual environments. Difficulty is measured by counting the number of balls dropped. The adjusted parameters are the height of the borders of the virtual tray and the type of the real balls.</p><p>Because of the impulses, virtual balls tend to bounce more on the tray in the VE than those in the real world. The borders of the virtual tray have thus been increased. In addition, the real balls had different friction so two ball types were mixed: ping-pong balls and foam balls. The pilot test showed that equivalent difficulty level between real and virtual tasks is obtained with a tray border raised to 3/4 the height of the balls, and mixing 3 foam balls and 6 ping-pong balls. This configuration has been used in all experiments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">User experience</h4><p>The free-form user experience session corresponds to a qualitative observational evaluation, to see what the user will do with almost no instruction.</p><p>The scene is a dining room, with a cupboard, shelves and a table. The table is empty, and plates, glasses, forks and knives for six people are placed in the cupboard on the left and on the shelves in the back (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig14">14</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>User experience scene:<i> Left</i>; at the outset, the table is clear.<i> Right</i>; at the end, the table is set with plates, cups and cutlery</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The user is introduced to the scene and receives no other instructions than “you have 4 min to set the table as best as you can.” Our goal is to observe general behavior, i.e., whether participants use one or two hands, whether they focus on completely setting the table or correctly placing objects, the order of setting the table, etc. We are interested in observing whether participants behave naturally, e.g., walk around the virtual table, catch falling objects, etc., as well as their degree of presence.</p><h3 class="c-article__sub-heading" id="Sec18">Measurements</h3><p>In each session, head position and orientation of the users at each frame are recorded. The position and orientation of the fingers and palm, as well as the time to complete each task and subtask are also recorded. Every object collision and ball dropped are also recorded. The sessions were videotaped, and the completion times for the real environment were manually extracted.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Objective metrics</h4><p>During the usability task, accuracy is measured by recording the following: (1) position of the tray with respect to the posts to make sure that the user does pass the tray between them; (2) number of times the tray touches the posts; (3) number of times the hands touch the posts; (4) number of balls remaining on the tray when releasing the tray onto the table; (5) number of balls inside the bowl after emptying the tray.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Subjective measurements</h4><p>At the end of the experiment, participants complete a questionnaire. For each technique in virtual conditions (wand, one hand and two hands), they are asked to rate various criteria on a Likert's scale between 1 and 7. We evaluate: ease of use, fatigue caused by using the technique, sensation of “being there,” plausibility of the interaction with the environment and reaction of the environment to actions, similarity to the real condition (for one hand and two hands), precision, naturalness and cybersickness. Participants are also asked to rate the similarity of each virtual task to the real task. Then, they have to answer open questions related to their strategies for using the interfaces, and their opinion on advantages, drawbacks and difficulties of each interface. Finally, they are free to make additional comments. For further detail, please refer to the questionnaire in supplemental material.</p><h3 class="c-article__sub-heading" id="Sec21">Results</h3><p>The statistical analysis of the results for both our objective measurements, i.e., speed and errors, and the responses to our subjective questionnaire is next presented. For the completion times, a Shapiro's test has been performed that rejected the normality hypothesis on the data distribution. Thus, a non-parametric Friedman's test for differences among the conditions has been used. Post hoc comparisons were performed using Wilcoxon's signed-rank tests with a threshold of 0.05 for significance.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Objective measurements</h4><p>A Friedman's test has first been performed on the time performances between the 5 conditions, i.e., 1- and 2-handed real, 1- and 2-handed DM and the wand. The following abbreviations are used: R1H and R2H are real one and two-handed conditions respectively, and DM1H and DM2H are virtual DM one and two-handed conditions respectively. The reported<i> p</i> values were adjusted for multiple comparisons. A significant effect (<i>χ</i>
                    <sup>2</sup> = 4.62, <i>p</i> &lt; 0.001) of condition has been found. Post hoc analysis revealed that the time to complete the task was significantly lower for R1H with a median time to completion of 61.33 s compared to DM2H where time to completion was 113.8 s (<i>p</i> &lt; 0.001). The time for R2H (median = 65.67 s) was significantly lower than DM1H (median = 86.95 s, <i>p</i> = 0.04), DM2H (<i>p</i> &lt; 0.001) and wand (median = 88.86s, <i>p</i> = 0.04). No significant effect was found between virtual conditions.</p><p>Errors were measured in two manners: First, the number of balls lost throughout the task, and second, the number of times the tray or the hands hit the posts in the virtual tasks. Real and virtual conditions for both criteria are also compared.</p><p>A Friedman's test has been performed for the number of lost balls during the task. A significant effect of condition (<i>χ</i>
                    <sup>2</sup> = 3.35, <i>p</i> = 0.007) has been found. The number of lost balls was significantly lower for wand compared to R1H and DM1H (<i>p</i> = 0.007 and <i>p</i> = 0.01, respectively), as revealed by post hoc analysis.</p><p>Finally, a Friedman's test has been performed for the number of hits during the task. The test revealed a significant effect of condition (χ<sup>2</sup> = 5.32, <i>p</i> &lt; 0.001). The number of hits was significantly lower for R1H compared to all virtual conditions as shown by post hoc analysis (<i>p</i> &lt; 0.001 for DM1H, <i>p</i> = 0.02 for DM2H and <i>p</i> &lt; 0.001 for wand). The number of hits was also significantly lower for R2H compared to all virtual conditions (<i>p</i> &lt; 0.001 for DM1H, <i>p</i> = 0.003 for DM2H and <i>p</i> &lt; 0.001 for wand). No significant effect has been found between the virtual conditions.</p><p>Those results are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig15">15</a> which shows “lower than” relationships between conditions for each parameter when it is significant.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Significant “lower than” relationships between conditions for objective measurements</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Subjective questionnaire</h4><p>For the responses to the subjective questionnaire, a Friedman's test has been performed for the different criteria between the three virtual conditions. No significant effect has been found for <i>Plausibility</i> for the usability task, <i>Being there</i> for the user experience and <i>Cybersickness</i>. A significant effect has been found for 7 criteria: <i>Ease of Use</i> for the usability task (<i>χ</i>
                    <sup>2</sup> = 4.95, <i>p</i> &lt; 0.001), <i>Being there</i> for the user experience (<i>χ</i>
                    <sup>2</sup> = 2.69, <i>p</i> = 0.02), <i>Plausibility</i> for the user experience (<i>χ</i>
                    <sup>2</sup> = 2.54, <i>p</i> = 0.03), <i>Fatigue</i> (<i>χ</i>
                    <sup>2</sup> = 4.27, <i>p</i> &lt; 0.001), <i>Similarity with real experience</i> (<i>χ</i>
                    <sup>2</sup> = 3.24,  <i>p</i> = 0.003), <i>Precision</i> (<i>χ</i>
                    <sup>2</sup> = 4.35, <i>p</i> &lt; 0.001) and <i>Naturalness</i> (<i>χ</i>
                    <sup>2</sup> = 2.70, <i>p</i> = 0.02) (See Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig16">16</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig17">17</a>). Post hoc analysis showed that wand was preferred to DM1H and DM2H for <i>Ease of Use</i> during the usability task (<i>p</i> &lt; 0.001 and <i>p</i> = 0.003, respectively), <i>Fatigue</i> (<i>p</i> &lt; 0.001 and <i>p</i> = 0.007, respectively) and <i>Precision</i> (<i>p</i> &lt; 0.001 for both). In contrast, DM1H and DM2H conditions were significantly better rated than wand for <i>Being there</i> for the user experience (<i>p</i> = 0.04 and <i>p</i> = 0.02, respectively). The two hands condition was preferred to wand for <i>Plausibility</i> for the user experience (<i>p</i> = 0.03), <i>Similarity with real experience</i> (<i>p</i> = 0.003) and <i>Naturalness</i> (<i>p</i> = 0.02).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Significant “lower than” relationships between conditions for subjective measurements</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>
                            <i>Boxplots</i> of the statistical results for some of the most interesting studied criteria. Each<i> boxplot</i> is delimited by the 25 % quantile and 75 % quantile of the distribution of the effect over the individuals. The median is also represented as a<i> red line</i> for each effect (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0246-0/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Those results are summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0246-0#Fig16">16</a> which shows “lower than” relationships between conditions for each parameter when it is significant.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Discussion</h2><div class="c-article-section__content" id="Sec24-content"><p>Our goals were to evaluate the feasibility of DM in a fully immersive space, and its effect on presence as well as the similarity to real-world manipulation. We hypothesized that the wand would be more precise and efficient than DM, but we believed that DM would positively affect the sense of presence. The experimental results support these hypotheses.</p><p>Concerning the first hypothesis, the results show that all virtual tasks took longer than the real-world task and that the wand and DM are equivalent in terms of speed (even though the two-handed DM condition had a longer median completion time). We take this as an encouraging indication that DM does not penalize speed. However, as a general remark, it also indicates that for such complex tasks involving balance and rotations, we are still not at the point where virtual tasks can be performed at the same speed as their real equivalent. In the real scene, the user is influenced by the kinesthetic perception (of touch, weight and muscle tension) which guides balance of the tray but increases fatigue. In contrast, this sense is missing in the virtual setting; note, however, that current solutions for haptics in ICs are unsatisfactory, and we thus chose not to opt for such a solution.</p><p>In terms of accuracy, users dropped fewer balls using the wand than with the one-handed virtual DM, as well as with the one-handed <i>real</i> life condition. There was no significant difference with the two-handed DM, real or virtual. Evidently, the wand is a “hyper-natural” interface in the terminology of Bowman et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Bowman D, McMahan R, Ragan E (2012) Questioning naturalism in 3d user interfaces. Commun ACM 55(9):78–88" href="/article/10.1007/s10055-014-0246-0#ref-CR6" id="ref-link-section-d81434e1768">2012</a>), so it is unsurprising that it allows better performance than the real world in some cases. The fact that this occurred for the one-handed case rather than two hands is due to the inherent difficulty of the one-handed condition: the tray is a relatively long object, and a small movement of the fingers of the holding hand can result in a large movement of the tray, and thus, a loss of the balls. When using two hands, the tray is more stable. The above observations are true for the virtual setting, but also for the real tasks; several participants complained that the one-handed <i>real</i> task was hard and tiring.</p><p>The above two results for speed and accuracy indicate that the virtual DM is a feasible alternative for interaction. There is also an indication that DM could be considered better for applications such as training since performance is closer to the real world than the wand which <i>augments</i> interaction capabilities of the user.</p><p>The participants found the wand easier to use and less tiring. We also noticed that users subjectively considered the wand to be more precise than the DM even if the objective performance did not always confirm this, notably in terms of hitting the posts. Again, the “hyper-natural” aspect of the wand is a plausible explanation for this perception and this discrepancy.</p><p>For the user experience, participants rated the sense of <i>being there</i> to be higher for hands (both one and two-handed) compared to the wand. In informal interviews after completing the study, participants explained that in the usability task, they were so concentrated on completing the task that they did not pay much attention to the environment; this was not the case, however, for the user experience, where there were no constraints. Similarly, two hands were rated higher than the wand for the sense of <i>plausibility</i>, again in the user experience. We believe that this is not the case for the one-hand case because of the lack of precision when manipulating objects, as explained above.</p><p>The above results on the subjective ratings show that our DM interface does have an effect on the two components of presence, <i>plausibility</i> and <i>being there</i> (Slater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Philos Trans R Soc B Biol Sci 364(1535):3549–3557" href="/article/10.1007/s10055-014-0246-0#ref-CR25" id="ref-link-section-d81434e1801">2009</a>). In addition, for the two-handed case, participants perceived them as more natural and closer to the real experience than the wand. We believe that these are encouraging results, which support our hypothesis that DM can improve the sense of immersion in such VEs and provide an experience that is closer to reality than using more traditional device-based interfaces.</p><p>We also observed participants behavior informally, which revealed several interesting cases of their reactions to our VE. Some of these are illustrated in the sequences of the accompanying video. The IC offers a high level of immersion in and of itself; for example, participants attempted to place the wand on the virtual tables at the end of the different sessions. However, users tended to avoid walking through the virtual objects when they used their hands. In several cases, participants tried to catch the dropping objects as an automatic reaction when using DM. During the user experience, participants tended to place the objects in a specific order, as they do in real life: They begin with the plates, then the glasses, and they finish with the forks and the knives. Participants also tend to develop strategies to use the different techniques in the user experience. When using the wand, participants would stay in a convenient place to pick and place objects from a distance. However, they found it harder to rotate the objects, although different techniques for rotation could be implemented to alleviate this problem. When using DM, they used both hands to adjust the orientation of the plates; otherwise, they used both hands to pick two objects separately, and thus can set the table faster. These behaviors witness the immersion of our VE, as people behave as they do in real life.</p><p>Our results show that, even if the wand outperforms DM in terms of usability, our finger-based DM interface conveys a high sense of presence and naturalness. We also believe that improved tracking technology and possibly the use of five fingers will allow our approach to outperform the wand, while increasing difference in presence between the two.</p></div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusions and future work</h2><div class="c-article-section__content" id="Sec25-content"><p>In this paper, a complete system has been presented which integrates DM with real-time physics in a fully immersive space, allowing close-to-natural manipulation of objects in a VE. Interaction is based on the physics engine, enhanced by a heuristic approach to manipulate objects. Users can thus perform moderately complex tasks, involving translations and rotations of objects and maintaining balance while walking.</p><p>A first user study has been performed, which included a usability task, and a free-form user experience task. Our DM has been compared to the traditional wand, and for the controlled setting, the virtual task has been replicated in the real world. Both the objective measures (speed, accuracy) and the responses to the subjective questionnaire indicated that DM is a feasible alternative to the more traditional wand interface. The results of our study also indicate that in several cases, especially when using two hands, the use of DM enhances the sense of presence in the VE and is perceived as being closer to reality.</p><p>In this study, feasibility of DM has been examined, leading us to test relatively complex tasks. Given that the feasibility of the interaction with DM is now quite clear, it will be interesting to examine the different parameters of our system in separate, more specific studies. An interesting direction to future work would also involve the use of a full hand model with complete tracking, similar to Hilliges et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430" href="/article/10.1007/s10055-014-0246-0#ref-CR12" id="ref-link-section-d81434e1821">2012</a>), which, however, requires improvements in depth sensor technology before becoming realizable.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>
                    <a href="http://www.bulletphysics.org">http://www.bulletphysics.org</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.openscenegraph.org/">http://www.openscenegraph.org/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.barco.com/en/products-solutions/visual-display-systems/3d-video-walls/multi-walled-stereoscopic-environment.aspx">http://www.barco.com/en/products-solutions/visual-display-systems/3d-video-walls/multi-walled-stereoscopic-environment.aspx</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>
                      <a href="http://www.ar-tracking.com">http://www.ar-tracking.com</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Agarawala A, Balakrishnan R (2006) Keepin’ it real: pushing the desktop metaphor with physics, piles and the p" /><p class="c-article-references__text" id="ref-CR1">Agarawala A, Balakrishnan R (2006) Keepin’ it real: pushing the desktop metaphor with physics, piles and the pen. CHI ’06, pp 1283–1292</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Aleotti, S. Caselli, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Aleotti J, Caselli S (2011) Physics-based virtual reality for task learning and intelligent disassembly planni" /><p class="c-article-references__text" id="ref-CR2">Aleotti J, Caselli S (2011) Physics-based virtual reality for task learning and intelligent disassembly planning. Virtual Reality 15:41–54</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-009-0145-y" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physics-based%20virtual%20reality%20for%20task%20learning%20and%20intelligent%20disassembly%20planning&amp;journal=Virtual%20Real&amp;volume=15&amp;pages=41-54&amp;publication_year=2011&amp;author=Aleotti%2CJ&amp;author=Caselli%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolt RA (1980) &amp;ldquo;put-that-there&amp;rdquo;: Voice and gesture at the graphics interface. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR3">Bolt RA (1980) &amp;ldquo;put-that-there&amp;rdquo;: Voice and gesture at the graphics interface. In: Proceedings of the 7th annual conference on computer graphics and interactive techniques, ACM, New York, NY, USA, SIGGRAPH ’80, pp 262–270. doi:<a href="https://doi.org/10.1145/800250.807503">10.1145/800250.807503</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Borst C, Indugula A (2005) Realistic virtual grasping. In: IEEE VR 2005, pp 91–98" /><p class="c-article-references__text" id="ref-CR4">Borst C, Indugula A (2005) Realistic virtual grasping. In: IEEE VR 2005, pp 91–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Bowman, S. Coquillart, B. Froehlich, M. Hirose, Y. Kitamura, K. Kiyokawa, W. Stuerzlinger, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bowman D, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interface" /><p class="c-article-references__text" id="ref-CR5">Bowman D, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interfaces: new directions and perspectives. IEEE Comput Graph Appl 28(6):20–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2008.109" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20new%20directions%20and%20perspectives&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=28&amp;issue=6&amp;pages=20-36&amp;publication_year=2008&amp;author=Bowman%2CD&amp;author=Coquillart%2CS&amp;author=Froehlich%2CB&amp;author=Hirose%2CM&amp;author=Kitamura%2CY&amp;author=Kiyokawa%2CK&amp;author=Stuerzlinger%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Bowman, R. McMahan, E. Ragan, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Bowman D, McMahan R, Ragan E (2012) Questioning naturalism in 3d user interfaces. Commun ACM 55(9):78–88" /><p class="c-article-references__text" id="ref-CR6">Bowman D, McMahan R, Ragan E (2012) Questioning naturalism in 3d user interfaces. Commun ACM 55(9):78–88</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2330667.2330687" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Questioning%20naturalism%20in%203d%20user%20interfaces&amp;journal=Commun%20ACM&amp;volume=55&amp;issue=9&amp;pages=78-88&amp;publication_year=2012&amp;author=Bowman%2CD&amp;author=McMahan%2CR&amp;author=Ragan%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) Fingartips: gesture based direct manipulation in augm" /><p class="c-article-references__text" id="ref-CR7">Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) Fingartips: gesture based direct manipulation in augmented reality. In: Proceedings GRAPHITE ’04, pp 212–221</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cabral MC, Morimoto CH, Zuffo MK (2005) On the usability of gesture interfaces in virtual reality environments" /><p class="c-article-references__text" id="ref-CR8">Cabral MC, Morimoto CH, Zuffo MK (2005) On the usability of gesture interfaces in virtual reality environments. In: Proceedings CLIHC ’05, pp 100–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Dipietro, AM. Sabatini, P. Dario, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Dipietro L, Sabatini AM, Dario P (2008) A survey of glove-based systems and their applications. IEEE Trans Sys" /><p class="c-article-references__text" id="ref-CR9">Dipietro L, Sabatini AM, Dario P (2008) A survey of glove-based systems and their applications. IEEE Trans Syst Man Cybern Part C 38(4):461–482</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCC.2008.923862" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20glove-based%20systems%20and%20their%20applications&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20C&amp;volume=38&amp;issue=4&amp;pages=461-482&amp;publication_year=2008&amp;author=Dipietro%2CL&amp;author=Sabatini%2CAM&amp;author=Dario%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fröhlich B, Tramberend H, Beers A, Agrawala M, Baraff D (2000) Physically-based manipulation on the responsive" /><p class="c-article-references__text" id="ref-CR10">Fröhlich B, Tramberend H, Beers A, Agrawala M, Baraff D (2000) Physically-based manipulation on the responsive workbench. In: IEEE VR 2000</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heumer G, Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves—a comparison of class" /><p class="c-article-references__text" id="ref-CR11">Heumer G, Amor H, Weber M, Jung B (2007) Grasp recognition with uncalibrated data gloves—a comparison of classification methods. In: Virtual reality conference, 2007. VR ’07. IEEE, pp 19 –26. doi:<a href="https://doi.org/10.1109/VR.2007.352459">10.1109/VR.2007.352459</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-thro" /><p class="c-article-references__text" id="ref-CR12">Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display. In: CHI ’12, pp 2421–2430</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hirota K, Hirose M (2003) Dexterous object manipulation based on collision response. In: IEEE VR ’03, IEEE Com" /><p class="c-article-references__text" id="ref-CR13">Hirota K, Hirose M (2003) Dexterous object manipulation based on collision response. In: IEEE VR ’03, IEEE Comput Soc, vol 2003, pp 232–239</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Holz, S. Ullrich, M. Wolter, T. Kuhlen, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Holz D, Ullrich S, Wolter M, Kuhlen T (2008) Multi-contact grasp interaction for virtual environments. J Virtu" /><p class="c-article-references__text" id="ref-CR14">Holz D, Ullrich S, Wolter M, Kuhlen T (2008) Multi-contact grasp interaction for virtual environments. J Virtual Real Broadcast 5(7):1860–2037</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-contact%20grasp%20interaction%20for%20virtual%20environments&amp;journal=J%20Virtual%20Real%20Broadcast&amp;volume=5&amp;issue=7&amp;pages=1860-2037&amp;publication_year=2008&amp;author=Holz%2CD&amp;author=Ullrich%2CS&amp;author=Wolter%2CM&amp;author=Kuhlen%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacobs J, Froehlich B (2011) A soft hand model for physically-based manipulation of virtual objects. In: IEEE " /><p class="c-article-references__text" id="ref-CR15">Jacobs J, Froehlich B (2011) A soft hand model for physically-based manipulation of virtual objects. In: IEEE VR 2011, IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interaction" /><p class="c-article-references__text" id="ref-CR16">Jacobs J, Stengel M, Froehlich B (2012) A generalized god-object method for plausible finger-based interactions in virtual environments. In: 3DUI’2012, IEEE, pp 43–51</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: Confere" /><p class="c-article-references__text" id="ref-CR17">Koons DB, Sparrell CJ (1994) Iconic: speech and depictive gestures at the human-machine interface. In: Conference companion on human factors in computing systems, ACM, New York, NY, USA, CHI ’94, pp 453–454. doi:<a href="https://doi.org/10.1145/259963.260487">10.1145/259963.260487</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Latoschik M, Frohlich M, Jung B, Wachsmuth I (1998) Utilize speech and gestures to realize natural interaction" /><p class="c-article-references__text" id="ref-CR18">Latoschik M, Frohlich M, Jung B, Wachsmuth I (1998) Utilize speech and gestures to realize natural interaction in a virtual environment. In: Industrial electronics society, 1998. IECON ’98. Proceedings of the 24th annual conference of the IEEE, vol 4, pp 2028 –2033. doi:<a href="https://doi.org/10.1109/IECON.1998.724030">10.1109/IECON.1998.724030</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedi" /><p class="c-article-references__text" id="ref-CR19">Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. In: Proceedings of the 1st international conference on computer graphics, virtual reality and visualisation, ACM, New York, NY, USA, AFRIGRAPH ’01, pp 95–100. doi:<a href="https://doi.org/10.1145/513867.513888">10.1145/513867.513888</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McMahan R, Alon A, Lazem S, Beaton R, Machaj D, Schaefer M, Silva M, Leal A, Hagan R, Bowman D (2010) Evaluati" /><p class="c-article-references__text" id="ref-CR20">McMahan R, Alon A, Lazem S, Beaton R, Machaj D, Schaefer M, Silva M, Leal A, Hagan R, Bowman D (2010) Evaluating natural interaction techniques in video games. In: 3DUI, IEEE, pp 11–14</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Moehring, B. Froehlich, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models." /><p class="c-article-references__text" id="ref-CR21">Moehring M, Froehlich B (2011) Natural interaction metaphors for functional validations of virtual car models. IEEE TVCG 17(9):1195–1208</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20interaction%20metaphors%20for%20functional%20validations%20of%20virtual%20car%20models&amp;journal=IEEE%20TVCG&amp;volume=17&amp;issue=9&amp;pages=1195-1208&amp;publication_year=2011&amp;author=Moehring%2CM&amp;author=Froehlich%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. O’Hagan, A. Zelinsky, S. Rougeaux, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="O’Hagan R, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments. Interact Comput 1" /><p class="c-article-references__text" id="ref-CR22">O’Hagan R, Zelinsky A, Rougeaux S (2002) Visual gesture interfaces for virtual environments. Interact Comput 14(3):231–250</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0953-5438%2801%2900050-9" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20gesture%20interfaces%20for%20virtual%20environments&amp;journal=Interact%20Comput&amp;volume=14&amp;issue=3&amp;pages=231-250&amp;publication_year=2002&amp;author=O%E2%80%99Hagan%2CR&amp;author=Zelinsky%2CA&amp;author=Rougeaux%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ortega, S. Redon, S. Coquillart, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Ortega M, Redon S, Coquillart S (2007) A six degree-of-freedom god-object method for haptic display of rigid b" /><p class="c-article-references__text" id="ref-CR23">Ortega M, Redon S, Coquillart S (2007) A six degree-of-freedom god-object method for haptic display of rigid bodies with surface properties. IEEE TVCG 13(3):458–469</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20six%20degree-of-freedom%20god-object%20method%20for%20haptic%20display%20of%20rigid%20bodies%20with%20surface%20properties&amp;journal=IEEE%20TVCG&amp;volume=13&amp;issue=3&amp;pages=458-469&amp;publication_year=2007&amp;author=Ortega%2CM&amp;author=Redon%2CS&amp;author=Coquillart%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEE" /><p class="c-article-references__text" id="ref-CR24">Prachyabrued M, Borst C (2012) Visual interpenetration tradeoffs in whole-hand virtual grasping. In: 3DUI, IEEE, pp 39–42</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Slater, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environme" /><p class="c-article-references__text" id="ref-CR25">Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Philos Trans R Soc B Biol Sci 364(1535):3549–3557</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1098%2Frstb.2009.0138" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Place%20illusion%20and%20plausibility%20can%20lead%20to%20realistic%20behaviour%20in%20immersive%20virtual%20environments&amp;journal=Philos%20Trans%20R%20Soc%20B%20Biol%20Sci&amp;volume=364&amp;issue=1535&amp;pages=3549-3557&amp;publication_year=2009&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DJ. Sturman, D. Zeltzer, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39. doi:10.1109/38" /><p class="c-article-references__text" id="ref-CR26">Sturman DJ, Zeltzer D (1994) A survey of glove-based input. IEEE Comput Graph Appl 14(1):30–39. doi:<a href="https://doi.org/10.1109/38.250916">10.1109/38.250916</a>
                </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.250916" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20glove-based%20input&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;doi=10.1109%2F38.250916&amp;volume=14&amp;issue=1&amp;pages=30-39&amp;publication_year=1994&amp;author=Sturman%2CDJ&amp;author=Zeltzer%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sturman DJ, Zeltzer D, Pieper S (1989) Hands-on interaction with virtual environments. UIST ’89, pp 19–24" /><p class="c-article-references__text" id="ref-CR27">Sturman DJ, Zeltzer D, Pieper S (1989) Hands-on interaction with virtual environments. UIST ’89, pp 19–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ullmann T, Sauer J (2000) Intuitive virtual grasping for non haptic environments. In: Pacific Graphics ’00, pp" /><p class="c-article-references__text" id="ref-CR28">Ullmann T, Sauer J (2000) Intuitive virtual grasping for non haptic environments. In: Pacific Graphics ’00, pp 373–381</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wang RY, Popović J (2009) Real-time hand-tracking with a color glove. In: ACM SIGGRAPH 2009 Papers, ACM, New Y" /><p class="c-article-references__text" id="ref-CR29">Wang RY, Popović J (2009) Real-time hand-tracking with a color glove. In: ACM SIGGRAPH 2009 Papers, ACM, New York, NY, USA, SIGGRAPH ’09, pp 63:1–63:8. doi:<a href="https://doi.org/10.1145/1576246.1531369">10.1145/1576246.1531369</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Wexelblat, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):" /><p class="c-article-references__text" id="ref-CR30">Wexelblat A (1995) An approach to natural gesture in virtual environments. ACM Trans Comput Hum Interact 2(3):179–200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F210079.210080" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20approach%20to%20natural%20gesture%20in%20virtual%20environments&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=2&amp;issue=3&amp;pages=179-200&amp;publication_year=1995&amp;author=Wexelblat%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wilson AD, Izadi S, Hilliges O, Garcia-Mendoza A, Kirk D (2008) Bringing physics to the surface. In: ACM UIST " /><p class="c-article-references__text" id="ref-CR31">Wilson AD, Izadi S, Hilliges O, Garcia-Mendoza A, Kirk D (2008) Bringing physics to the surface. In: ACM UIST ’08, pp 67–76</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0246-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>The authors wish to thank Rachid Guerchouche for his help in many aspects of the project, also Theophanis Tsandilas, Martin Hachet, Anatole Lecuyer, Peter Vangorp and Sylvain Duchêne for their help and their useful comments. We also thank all the participants of the study. This work was supported in part by the Regional Council of Provence Alpes-Côte d’Azur and the EU project VERVE (<a href="http://www.verveconsortium.eu">http://www.verveconsortium.eu</a>). The textures in the VEs come from the following websites: <a href="http://archivetextures.net">http://archivetextures.net</a>, <a href="http://www.freestockphotos.biz">http://www.freestockphotos.biz</a>, <a href="http://www.flickr.com">http://www.flickr.com</a>, <a href="http://www.defcon-x.de">http://www.defcon-x.de</a>, <a href="http://www.public-domain-image.com">http://www.public-domain-image.com</a>, <a href="http://images.meredith.com">http://images.meredith.com</a>, <a href="http://onlyhdwallpapers.com">http://onlyhdwallpapers.com</a>, <a href="http://www.merveilles-russie.com">http://www.merveilles-russie.com</a> and <a href="http://www.zastavki.com">http://www.zastavki.com</a>.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Inria, REVES, Sophia Antipolis, France</p><p class="c-article-author-affiliation__authors-list">Emmanuelle Chapoulie &amp; George Drettakis</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Inria, Hybrid, Rennes, France</p><p class="c-article-author-affiliation__authors-list">Maud Marchal</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">IRISA-INSA, Rennes, France</p><p class="c-article-author-affiliation__authors-list">Maud Marchal</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">University of Athens, Athens, Greece</p><p class="c-article-author-affiliation__authors-list">Evanthia Dimara &amp; Maria Roussou</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">Makebelieve Design and Consulting, Athens, Greece</p><p class="c-article-author-affiliation__authors-list">Maria Roussou</p></li><li id="Aff6"><p class="c-article-author-affiliation__address">Inria, Sophia Antipolis, France</p><p class="c-article-author-affiliation__authors-list">Jean-Christophe Lombardo</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Emmanuelle-Chapoulie"><span class="c-article-authors-search__title u-h3 js-search-name">Emmanuelle Chapoulie</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Emmanuelle+Chapoulie&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Emmanuelle+Chapoulie" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Emmanuelle+Chapoulie%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Maud-Marchal"><span class="c-article-authors-search__title u-h3 js-search-name">Maud Marchal</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Maud+Marchal&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Maud+Marchal" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Maud+Marchal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Evanthia-Dimara"><span class="c-article-authors-search__title u-h3 js-search-name">Evanthia Dimara</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Evanthia+Dimara&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Evanthia+Dimara" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Evanthia+Dimara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Maria-Roussou"><span class="c-article-authors-search__title u-h3 js-search-name">Maria Roussou</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Maria+Roussou&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Maria+Roussou" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Maria+Roussou%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jean_Christophe-Lombardo"><span class="c-article-authors-search__title u-h3 js-search-name">Jean-Christophe Lombardo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jean-Christophe+Lombardo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jean-Christophe+Lombardo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jean-Christophe+Lombardo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-George-Drettakis"><span class="c-article-authors-search__title u-h3 js-search-name">George Drettakis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;George+Drettakis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=George+Drettakis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22George+Drettakis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0246-0/email/correspondent/c1/new">Emmanuelle Chapoulie</a>.</p></div></div></section><section aria-labelledby="Sec26"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec26-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdivBzqKgfc-NzwN3qy9RDgj4m"><div mi24-video-player="true" video-id="BzqKgfc-NzwN3qy9RDgj4m" player-id="8PcXmCm9nWqE6posBEkd1h" config-type="vmpro" flash-path="//e.video-cdn.net/v2/" api-url="//d.video-cdn.net/play"></div><script src="//e.video-cdn.net/v2/embed.js"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>MP4 (47,620 KB)</p></div></div>
                </div></div></div></section><section aria-labelledby="Sec27"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec27-content"><div data-test="supplementary-info"><p>Below is the link to the electronic supplementary material.

</p>
                <div class="c-article-supplementary__item" data-test="supp-item" id="MOESM2"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-track-label="link" data-test="supp-info-link" href="https://static-content.springer.com/esm/art%3A10.1007%2Fs10055-014-0246-0/MediaObjects/10055_2014_246_MOESM2_ESM.odt" data-supp-info-image="">ODT (699 KB)</a></h3></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Evaluation%20of%20direct%20manipulation%20using%20finger%20tracking%20for%20complex%20tasks%20in%20an%20immersive%20cube&amp;author=Emmanuelle%20Chapoulie%20et%20al&amp;contentID=10.1007%2Fs10055-014-0246-0&amp;publication=1359-4338&amp;publicationDate=2014-02-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chapoulie, E., Marchal, M., Dimara, E. <i>et al.</i> Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube.
                    <i>Virtual Reality</i> <b>18, </b>203–217 (2014). https://doi.org/10.1007/s10055-014-0246-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0246-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-03-14">14 March 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-01-17">17 January 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-02-13">13 February 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-09">September 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0246-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0246-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Direct manipulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive cube</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Finger tracking</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0246-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=246;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

