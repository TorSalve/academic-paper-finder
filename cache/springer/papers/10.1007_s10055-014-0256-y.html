<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Projection-based visualization of tangential deformation of nonrigid s"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In this paper, we propose a projection-based mixed
 reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphics directly onto the surface by projected..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture"/>

    <meta name="dc.source" content="Virtual Reality 2014 19:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-12-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In this paper, we propose a projection-based mixed
 reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphics directly onto the surface by projected imagery. The superimposed graphics are deformed according to the surface deformation. To achieve this goal, we develop a computer vision technique that estimates the tangential deformation by measuring the frame-by-frame movement of an infrared (IR) texture on the surface. IR ink, which can be captured by an IR camera under IR light, but is invisible to the human eye, is used to provide the surface texture. Consequently, the texture does not degrade the image quality of the augmented graphics. The proposed technique measures individually the surface motion between two successive frames. Therefore, it does not suffer from occlusions caused by interactions and allows touching, pushing, pulling, and pinching, etc. The moving least squares technique interpolates the measured result to estimate denser surface deformation. The proposed method relies only on the apparent motion measurement; thus, it is not limited to a specific deformation characteristic, but is flexible for multiple deformable materials, such as viscoelastic and elastic materials. Experiments confirm that, with the proposed method, we can visualize the surface deformation of various materials by projected illumination, even when the user&#8217;s hand occludes the surface from the camera."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-12-11"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="45"/>

    <meta name="prism.endingPage" content="56"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0256-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0256-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0256-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0256-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2015/03"/>

    <meta name="citation_online_date" content="2014/12/11"/>

    <meta name="citation_firstpage" content="45"/>

    <meta name="citation_lastpage" content="56"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0256-y"/>

    <meta name="DOI" content="10.1007/s10055-014-0256-y"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0256-y"/>

    <meta name="description" content="In this paper, we propose a projection-based mixed
 reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphi"/>

    <meta name="dc.creator" content="Parinya Punpongsanon"/>

    <meta name="dc.creator" content="Daisuke Iwai"/>

    <meta name="dc.creator" content="Kosuke Sato"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Reliable estimation of dense optical flow fields with large displacements; citation_author=L Alvarez, J Weickert, J Snchez; citation_volume=39; citation_issue=1; citation_publication_date=2000; citation_pages=41-56; citation_doi=10.1023/A:1008170101536; citation_id=CR1"/>

    <meta name="citation_reference" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of the IEEE/ACM international symposium on augmented reality, pp 207&#8211;216"/>

    <meta name="citation_reference" content="citation_title=Spatial augmented reality: merging real and virtual worlds; citation_publication_date=2005; citation_id=CR3; citation_author=O Bimber; citation_author=R Raskar; citation_publisher=A. K. Peters, Ltd."/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Superimposing dynamic range; citation_author=O Bimber, D Iwai; citation_volume=27; citation_issue=5; citation_publication_date=2008; citation_pages=15:1-15:8; citation_doi=10.1145/1409060.1409103; citation_id=CR4"/>

    <meta name="citation_reference" content="Bluteau J, Kitahara I, Kameda Y, Noma H, Kogure K, Ohta Y (2005) Visual support for medical communication by using projector-based augmented reality and thermal markers. In: Proceedings of the international conference on artificial reality and telexistence, pp 98&#8211;105"/>

    <meta name="citation_reference" content="Chang RC, Tseng FC (2010) Automatic detection and correction for glossy reflections in digital photograph. In: 3rd IEEE international conference on Ubi-media computing (U-Media), pp 44&#8211;49"/>

    <meta name="citation_reference" content="Follmer S, Johnson M, Adelson E, Ishii H (2011) deform: An interactive malleable surface for capturing 2.5d arbitrary objects, tools and touch. In: Proceedings of the 24th annual ACM symposium on user interface software and technology. ACM, New York, UIST&#8217;11, pp 527&#8211;536"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Geometrically-correct projection-based texture mapping onto a deformable object; citation_author=Y Fujimoto, R Smith, T Taketomi, G Yamamoto, J Miyazaki, H Kato, B Thomas; citation_volume=20; citation_issue=4; citation_publication_date=2014; citation_pages=540-549; citation_doi=10.1109/TVCG.2014.25; citation_id=CR8"/>

    <meta name="citation_reference" content="Haouchine N, Dequidt J, Kerrien E, Berger MO, Cotin S (2012) Physics-based augmented reality for 3d deformable object. In: Workshop on virtual reality interaction and physical simulation"/>

    <meta name="citation_reference" content="Heo S, Lee G (2013) Indirect shear force estimation for multi-point shear force operations. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, CHI&#8217;13, pp 281&#8211;284"/>

    <meta name="citation_reference" content="Hisada M, Yamamoto K, Kanaya I, Sato K (2006) Free-form shape design system using stereoscopic projector&#8212;hyperreal 2.0. In: SICE-ICASE international joint conference, pp 4832&#8211;4835"/>

    <meta name="citation_reference" content="Ito Y, Kim Y, Obinata G (2014) Acquisition of contact force and slippage using a vision-based tactile sensor with a fluid-type touchpad for the dexterous handling of robots. Adv Robot Autom 3(116)"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Document search support by making physical documents transparent in projection-based mixed reality; citation_author=D Iwai, K Sato; citation_volume=15; citation_issue=2&#8211;3; citation_publication_date=2010; citation_pages=147-160; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Microgeometry capture using an elastomeric sensor; citation_author=MK Johnson, F Cole, A Raj, EH Adelson; citation_volume=30; citation_issue=4; citation_publication_date=2011; citation_pages=46:1-46:8; citation_doi=10.1145/2010324.1964941; citation_id=CR14"/>

    <meta name="citation_reference" content="Jones BR, Benko H, Ofek E, Wilson AD (2013) Illumiroom: peripheral projected illusions for interactive experiences. In: Proceedings of the ACM annual conference on human factors in computing systems, pp 869&#8211;878"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Vision-based sensor for real-time measuring of surface traction fields; citation_author=K Kamiyama, K Vlack, T Mizota, H Kajimoto, N Kawakami, S Tachi; citation_volume=25; citation_issue=1; citation_publication_date=2005; citation_pages=68-75; citation_doi=10.1109/MCG.2005.27; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Assist Radiol Surg; citation_title=Projector-based surgeon&#8211;computer interaction on deformable surfaces; citation_author=B Kocev, F Ritter, L Linsen; citation_volume=8; citation_issue=6; citation_publication_date=2013; citation_pages=1015-1025; citation_doi=10.1007/s11548-013-0897-4; citation_id=CR17"/>

    <meta name="citation_reference" content="Matoba Y, Sato T, Takahashi N, Koike H (2012) Claytricsurface: an interactive surface with dynamic softness control capability. In: ACM SIGGRAPH emerging technologies, p 6:1"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput; citation_title=Projection-based augmented reality in disney theme parks; citation_author=M Mine, J Baar, A Grundhofer, D Rose, B Yang; citation_volume=45; citation_issue=7; citation_publication_date=2012; citation_pages=32-40; citation_doi=10.1109/MC.2012.154; citation_id=CR19"/>

    <meta name="citation_reference" content="Ni T, Karlson AK, Wigdor D (2011) AnatOnMe: facilitating doctor-patient communication using a projection-based handheld device. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 3333&#8211;3342"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Fast non-rigid surface detection, registration and realistic augmentation; citation_author=J Pilet, V Lepetit, P Fua; citation_volume=76; citation_issue=2; citation_publication_date=2008; citation_pages=109-122; citation_doi=10.1007/s11263-006-0017-9; citation_id=CR21"/>

    <meta name="citation_reference" content="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-d tangible interface for landscape analysis. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 355&#8211;362"/>

    <meta name="citation_reference" content="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of the eurographics workshop on rendering, pp 89&#8211;102"/>

    <meta name="citation_reference" content="Raskar R, Ziegler R, Willwacher T (2006) Cartoon dioramas in motion. In: Proceedings of the international symposium on non-photorealistic animation and rendering, pp 7&#8211;12"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Sculpting by numbers; citation_author=A Rivers, A Adams, F Durand; citation_volume=31; citation_issue=6; citation_publication_date=2012; citation_pages=157:1-157:7; citation_doi=10.1145/2366145.2366176; citation_id=CR25"/>

    <meta name="citation_reference" content="Saakes D, Chiu K, Hutchison T, Buczyk BM, Koizumi N, Inami M, Raskar R (2010) Slow display. In: ACM SIGGRAPH emerging technologies, p 22"/>

    <meta name="citation_reference" content="Sato T, Mamiya H, Koike H, Fukuchi K (2009) PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology&#8212;UIST &#8217;09. ACM Press, New York, pp 43&#8211;50"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Image deformation using moving least squares; citation_author=S Schaefer, T McPhail, J Warren; citation_volume=25; citation_issue=3; citation_publication_date=2006; citation_pages=533-540; citation_doi=10.1145/1141911.1141920; citation_id=CR28"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 593&#8211;600"/>

    <meta name="citation_reference" content="Shimazu S, Iwai D, Sato K (2011) 3d high dynamic range display system. In: Proceedings of the 10th IEEE/ACM international symposium on mixed and augmented reality, pp 235&#8211;236"/>

    <meta name="citation_reference" content="Shimizu N, Yoshida T, Hayashi T, de Sorbier F, Saito H (2013) Non-rigid surface tracking for virtual fitting system. In: International conference on computer vision theory and applications, pp 1&#8211;7"/>

    <meta name="citation_reference" content="Steimle J, Jordt A, Maes P (2013) Flexpad: highly flexible bending interactions for projected handheld displays. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 237&#8211;246"/>

    <meta name="citation_reference" content="Uchiyama H, Marchand E (2011) Deformable random dot markers. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 237&#8211;238"/>

    <meta name="citation_reference" content="Uchiyama H, Saito H (2011) Random dot markers. In: 2014 IEEE virtual reality (VR), pp 35&#8211;38"/>

    <meta name="citation_reference" content="Vlack K, Mizota T, Kawakami N, Kamiyama K, Kajimoto H, Tachi S (2005) GelForce: a vision-based traction field computer interface. In: CHI &#8217;05 extended abstracts on human factors in computing systems. ACM, New York, pp 1154&#8211;1155"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Eulerian video magnification for revealing subtle changes in the world; citation_author=HY Wu, M Rubinstein, E Shih, J Guttag, F Durand, W Freeman; citation_volume=31; citation_issue=4; citation_publication_date=2012; citation_pages=65-72; citation_doi=10.1145/2185520.2185561; citation_id=CR36"/>

    <meta name="citation_author" content="Parinya Punpongsanon"/>

    <meta name="citation_author_email" content="parinya@sens.sys.es.osaka-u.ac.jp"/>

    <meta name="citation_author_institution" content="Osaka University, Toyonaka, Japan"/>

    <meta name="citation_author" content="Daisuke Iwai"/>

    <meta name="citation_author_email" content="daisuke.iwai@sys.es.osaka-u.ac.jp"/>

    <meta name="citation_author_institution" content="Osaka University, Toyonaka, Japan"/>

    <meta name="citation_author" content="Kosuke Sato"/>

    <meta name="citation_author_email" content="sato@sys.es.osakau.ac.jp"/>

    <meta name="citation_author_institution" content="Osaka University, Toyonaka, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0256-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0256-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture"/>
        <meta property="og:description" content="In this paper, we propose a projection-based mixed reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphics directly onto the surface by projected imagery. The superimposed graphics are deformed according to the surface deformation. To achieve this goal, we develop a computer vision technique that estimates the tangential deformation by measuring the frame-by-frame movement of an infrared (IR) texture on the surface. IR ink, which can be captured by an IR camera under IR light, but is invisible to the human eye, is used to provide the surface texture. Consequently, the texture does not degrade the image quality of the augmented graphics. The proposed technique measures individually the surface motion between two successive frames. Therefore, it does not suffer from occlusions caused by interactions and allows touching, pushing, pulling, and pinching, etc. The moving least squares technique interpolates the measured result to estimate denser surface deformation. The proposed method relies only on the apparent motion measurement; thus, it is not limited to a specific deformation characteristic, but is flexible for multiple deformable materials, such as viscoelastic and elastic materials. Experiments confirm that, with the proposed method, we can visualize the surface deformation of various materials by projected illumination, even when the user’s hand occludes the surface from the camera."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0256-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Projection-based mixed reality, User interaction, Deformable surface","kwrd":["Projection-based_mixed_reality","User_interaction","Deformable_surface"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0256-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0256-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=256;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0256-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0256-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0256-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-12-11" itemprop="datePublished">11 December 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Parinya-Punpongsanon" data-author-popup="auth-Parinya-Punpongsanon" data-corresp-id="c1">Parinya Punpongsanon<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Daisuke-Iwai" data-author-popup="auth-Daisuke-Iwai">Daisuke Iwai</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kosuke-Sato" data-author-popup="auth-Kosuke-Sato">Kosuke Sato</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Osaka University" /><meta itemprop="address" content="grid.136593.b, 0000000403733971, Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">45</span>–<span itemprop="pageEnd">56</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">626 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">25 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0256-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In this paper, we propose a projection-based mixed
 reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphics directly onto the surface by projected imagery. The superimposed graphics are deformed according to the surface deformation. To achieve this goal, we develop a computer vision technique that estimates the tangential deformation by measuring the frame-by-frame movement of an infrared (IR) texture on the surface. IR ink, which can be captured by an IR camera under IR light, but is invisible to the human eye, is used to provide the surface texture. Consequently, the texture does not degrade the image quality of the augmented graphics. The proposed technique measures individually the surface motion between two successive frames. Therefore, it does not suffer from occlusions caused by interactions and allows touching, pushing, pulling, and pinching, etc. The moving least squares technique interpolates the measured result to estimate denser surface deformation. The proposed method relies only on the apparent motion measurement; thus, it is not limited to a specific deformation characteristic, but is flexible for multiple deformable materials, such as viscoelastic and elastic materials. Experiments confirm that, with the proposed method, we can visualize the surface deformation of various materials by projected illumination, even when the user’s hand occludes the surface from the camera.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Projection-based mixed reality (MR) refers to a technique in which computer graphics are projected on an actual object to augment its appearance without refabricating the object or requiring any user-worn equipment (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of the eurographics workshop on rendering, pp 89–102" href="/article/10.1007/s10055-014-0256-y#ref-CR23" id="ref-link-section-d16634e315">2001</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Raskar R, Ziegler R, Willwacher T (2006) Cartoon dioramas in motion. In: Proceedings of the international symposium on non-photorealistic animation and rendering, pp 7–12" href="/article/10.1007/s10055-014-0256-y#ref-CR24" id="ref-link-section-d16634e318">2006</a>). Compared with MR techniques that use video/optical see-through displays, projection-based MR is more suitable for multi-user scenarios because it provides users with a natural field of view without requiring them to wear or hold any additional physically constraining devices (Bimber and Raskar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., USA" href="/article/10.1007/s10055-014-0256-y#ref-CR3" id="ref-link-section-d16634e321">2005</a>). This enhances communication, particularly the sharing of augmented information during collaborative tasks (Bluteau et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bluteau J, Kitahara I, Kameda Y, Noma H, Kogure K, Ohta Y (2005) Visual support for medical communication by using projector-based augmented reality and thermal markers. In: Proceedings of the international conference on artificial reality and telexistence, pp 98–105" href="/article/10.1007/s10055-014-0256-y#ref-CR5" id="ref-link-section-d16634e324">2005</a>; Ni et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ni T, Karlson AK, Wigdor D (2011) AnatOnMe: facilitating doctor-patient communication using a projection-based handheld device. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 3333–3342" href="/article/10.1007/s10055-014-0256-y#ref-CR20" id="ref-link-section-d16634e327">2011</a>). Projection-based MR can be integrated into many application fields such as simulation (Jones et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Jones BR, Benko H, Ofek E, Wilson AD (2013) Illumiroom: peripheral projected illusions for interactive experiences. In: Proceedings of the ACM annual conference on human factors in computing systems, pp 869–878" href="/article/10.1007/s10055-014-0256-y#ref-CR15" id="ref-link-section-d16634e331">2013</a>), designer support (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of the IEEE/ACM international symposium on augmented reality, pp 207–216" href="/article/10.1007/s10055-014-0256-y#ref-CR2" id="ref-link-section-d16634e334">2001</a>; Rivers et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Rivers A, Adams A, Durand F (2012) Sculpting by numbers. ACM Trans Graph 31(6):157:1–157:7" href="/article/10.1007/s10055-014-0256-y#ref-CR25" id="ref-link-section-d16634e337">2012</a>), and entertainment (Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Mine M, van Baar J, Grundhofer A, Rose D, Yang B (2012) Projection-based augmented reality in disney theme parks. IEEE Comput 45(7):32–40" href="/article/10.1007/s10055-014-0256-y#ref-CR19" id="ref-link-section-d16634e340">2012</a>; Saakes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Saakes D, Chiu K, Hutchison T, Buczyk BM, Koizumi N, Inami M, Raskar R (2010) Slow display. In: ACM SIGGRAPH emerging technologies, p 22" href="/article/10.1007/s10055-014-0256-y#ref-CR26" id="ref-link-section-d16634e343">2010</a>). However, current technologies typically support only rigid objects and are not applicable to deformable or malleable objects. Note that in this study, we refer to “deformation” as a tangential deformation of a surface rather than a bump deformation.</p><p>The projection-based augmentation of deformable objects has a broad range of application possibilities. For example, visualizing surface deformation can improve the study of fluid dynamics by allowing students to touch a deformable object and observe its deformation. It can also support a graphical designer in creating deformed graphics with a free-form tangible interface on which the deformation is visualized by projection. Moreover, projection-based MR with deformation has the potential to enhance advertisements for nonrigid products in a retail store by allowing customers to interact with the projected ads on the product by deforming (e.g., pressing and bending) the product.</p><p>The aim of this study was to provide a projection-based MR system that visualizes tangential deformation on a surface by deforming the projection graphics to match the actual surface deformation in real time. To achieve this goal, we propose a computer vision technique to measure the tangential motion of visual feature points on a deformable surface. Because of occlusions and non-uniformity of the surface texture density, we cannot measure the motion in some areas of the surface. Therefore, we interpolate the measured result using the moving least squares (MLS) method (Schaefer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schaefer S, McPhail T, Warren J (2006) Image deformation using moving least squares. ACM Trans Graph 25(3):533–540" href="/article/10.1007/s10055-014-0256-y#ref-CR28" id="ref-link-section-d16634e351">2006</a>) to estimate the final deformation information. Because our feature tracking method relies only on the measurement of apparent surface motion, it is not limited to a specific deformation characteristic. The proposed method is flexible for multiple deformable materials, such as viscoelastic and elastic materials. To avoid tracking failures caused by occlusions, we propose tracking visual features individually on the surface between two successive frames, rather than through an entire capture sequence. We refer to this technique as <i>within-two-frames feature tracking</i>. We use infrared (IR) ink, rather than visible ink, to provide a surface texture to make the surface motion detectable by an IR camera without degrading the image quality of visible projected graphics. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig1">1</a> shows an overview of the proposed concept.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>System concept</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The primary contributions of this work can be summarized as follows:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Image-based approach for tangential deformation estimation of various materials;</p>
                  </li>
                  <li>
                    <p>Within-two-frames feature tracking method for measuring tangential motion of deformation surface, which is robust against occlusions caused by user interactions;</p>
                  </li>
                  <li>
                    <p>IR texture on a surface for stable feature detection under visible projector illumination;</p>
                  </li>
                  <li>
                    <p>Combination of sparse feature tracking and MLS-based interpolation for real-time deformation estimation.</p>
                  </li>
                </ul>
              </div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Projection-based MR, which provides virtual graphics to augment physical objects, can be applied to enrich interactive content (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of the IEEE/ACM international symposium on augmented reality, pp 207–216" href="/article/10.1007/s10055-014-0256-y#ref-CR2" id="ref-link-section-d16634e412">2001</a>; Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Raskar R, Ziegler R, Willwacher T (2006) Cartoon dioramas in motion. In: Proceedings of the international symposium on non-photorealistic animation and rendering, pp 7–12" href="/article/10.1007/s10055-014-0256-y#ref-CR24" id="ref-link-section-d16634e415">2006</a>; Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Mine M, van Baar J, Grundhofer A, Rose D, Yang B (2012) Projection-based augmented reality in disney theme parks. IEEE Comput 45(7):32–40" href="/article/10.1007/s10055-014-0256-y#ref-CR19" id="ref-link-section-d16634e418">2012</a>), enhance appearances (Bimber and Iwai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bimber O, Iwai D (2008) Superimposing dynamic range. ACM Trans Graph 27(5):15:1–15:8" href="/article/10.1007/s10055-014-0256-y#ref-CR4" id="ref-link-section-d16634e421">2008</a>; Iwai and Sato <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Iwai D, Sato K (2010) Document search support by making physical documents transparent in projection-based mixed reality. Virtual Real 15(2–3):147–160" href="/article/10.1007/s10055-014-0256-y#ref-CR13" id="ref-link-section-d16634e424">2010</a>; Shimazu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Shimazu S, Iwai D, Sato K (2011) 3d high dynamic range display system. In: Proceedings of the 10th IEEE/ACM international symposium on mixed and augmented reality, pp 235–236" href="/article/10.1007/s10055-014-0256-y#ref-CR30" id="ref-link-section-d16634e428">2011</a>), and create a deformed illusion on rigid objects (Hisada et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hisada M, Yamamoto K, Kanaya I, Sato K (2006) Free-form shape design system using stereoscopic projector—hyperreal 2.0. In: SICE-ICASE international joint conference, pp 4832–4835" href="/article/10.1007/s10055-014-0256-y#ref-CR11" id="ref-link-section-d16634e431">2006</a>). To date, projection-based MR researchers have primarily focused on rigid objects, and little work has been conducted on nonrigid objects.</p><p>Pioneering work on projection-based MR for a nonrigid surface has been performed by Piper et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-d tangible interface for landscape analysis. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 355–362" href="/article/10.1007/s10055-014-0256-y#ref-CR22" id="ref-link-section-d16634e437">2002</a>). Their system consisted of a depth sensor, a projector, and clay. Users interact with the clay (e.g., building a terrain map) onto which the projector superimposes depth-related imagery, such as pseudo-color representation of the height information of the map. Recently, Steimle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Steimle J, Jordt A, Maes P (2013) Flexpad: highly flexible bending interactions for projected handheld displays. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 237–246" href="/article/10.1007/s10055-014-0256-y#ref-CR32" id="ref-link-section-d16634e440">2013</a>) proposed estimating the bump deformation of a sheet of paper by using a depth sensor with depth-related imagery projected onto it. Other work has applied projection-based MR to a medical surgery scenario, in which the navigation information is projected directly onto a patient’s body (Kocev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Kocev B, Ritter F, Linsen L (2013) Projector-based surgeon–computer interaction on deformable surfaces. Int J Comput Assist Radiol Surg 8(6):1015–1025" href="/article/10.1007/s10055-014-0256-y#ref-CR17" id="ref-link-section-d16634e443">2013</a>). These studies have focused on the bump deformation of a projection surface rather than tangential deformation.</p><p>We apply a visual feature tracking approach to measure tangential deformation. Several studies have tackled this issue. Pilet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Pilet J, Lepetit V, Fua P (2008) Fast non-rigid surface detection, registration and realistic augmentation. Int J Comput Vis 76(2):109–122" href="/article/10.1007/s10055-014-0256-y#ref-CR21" id="ref-link-section-d16634e449">2008</a>) presented a method that detects and tracks deformable objects in monocular image sequences, in which wide baseline matching is applied to find feature correspondences between a reference image and a captured deformed object. However, this method has a high computational cost. Other studies have proposed real-time deformation estimation methods using richer information, i.e., both depth images and visual features (Haouchine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Haouchine N, Dequidt J, Kerrien E, Berger MO, Cotin S (2012) Physics-based augmented reality for 3d deformable object. In: Workshop on virtual reality interaction and physical simulation" href="/article/10.1007/s10055-014-0256-y#ref-CR9" id="ref-link-section-d16634e452">2012</a>; Shimizu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shimizu N, Yoshida T, Hayashi T, de Sorbier F, Saito H (2013) Non-rigid surface tracking for virtual fitting system. In: International conference on computer vision theory and applications, pp 1–7" href="/article/10.1007/s10055-014-0256-y#ref-CR31" id="ref-link-section-d16634e455">2013</a>). These methods work for <i>global deformation</i>, by which the appearance of a small area around a feature point, and consequently the feature vector, does not change significantly. Thus, the global deformation does not introduce distraction to the process of finding feature correspondences. On the other hand, these methods do not work for <i>local deformation</i>, which changes the appearance of a small area dramatically, making feature matching impossible.</p><p>Kamiyama et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Kamiyama K, Vlack K, Mizota T, Kajimoto H, Kawakami N, Tachi S (2005) Vision-based sensor for real-time measuring of surface traction fields. IEEE Comput Graph Appl 25(1):68–75" href="/article/10.1007/s10055-014-0256-y#ref-CR16" id="ref-link-section-d16634e467">2005</a>) proposed GelForce, method for estimating local deformation. This method estimates the magnitude and direction of the tangential force of a deformable silicone surface by measuring the deformation of red and blue markers embedded in the object. However, the materials that can be used in the system are limited because the method requires a physical model of the material as well as a dense marker array on the entire surface. Ito et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ito Y, Kim Y, Obinata G (2014) Acquisition of contact force and slippage using a vision-based tactile sensor with a fluid-type touchpad for the dexterous handling of robots. Adv Robot Autom 3(116)" href="/article/10.1007/s10055-014-0256-y#ref-CR12" id="ref-link-section-d16634e470">2014</a>) proposed method for estimating the force of a dexterous handling robot. This method estimates the orientation as well as direction of tangential force by measuring the displacement of the center dot using the contact area and contact depth of a fluid-type elastic touchpad. Although the method achieved the estimation of the rotation and direction of the tangential force, it is designed for a specific application, i.e., contact force detection in a robot, and cannot be used for our applications. We aim to estimate local tangential deformation of various deformable surfaces (e.g., elastic, viscous, and viscoelastic materials) without assuming any specific physical models, while the marker density is not related to the size of the surface.</p><p>An alternative method for detecting the local deformation is to apply indirect sensing techniques. Sato et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Sato T, Mamiya H, Koike H, Fukuchi K (2009) PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology—UIST ’09. ACM Press, New York, pp 43–50" href="/article/10.1007/s10055-014-0256-y#ref-CR27" id="ref-link-section-d16634e477">2009</a>) detected the natural changes in the photoelastic property in order to detect the place where transparent silicone was deformed. Heo and Lee (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Heo S, Lee G (2013) Indirect shear force estimation for multi-point shear force operations. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, CHI’13, pp 281–284" href="/article/10.1007/s10055-014-0256-y#ref-CR10" id="ref-link-section-d16634e480">2013</a>) used the normal force of a finger to estimate the shear force applied to the touch screen surface. Other works have detected bump deformation of elastomer using a depth from structure light (Follmer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Follmer S, Johnson M, Adelson E, Ishii H (2011) deform: An interactive malleable surface for capturing 2.5d arbitrary objects, tools and touch. In: Proceedings of the 24th annual ACM symposium on user interface software and technology. ACM, New York, UIST’11, pp 527–536" href="/article/10.1007/s10055-014-0256-y#ref-CR7" id="ref-link-section-d16634e483">2011</a>; Matoba et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Matoba Y, Sato T, Takahashi N, Koike H (2012) Claytricsurface: an interactive surface with dynamic softness control capability. In: ACM SIGGRAPH emerging technologies, p 6:1" href="/article/10.1007/s10055-014-0256-y#ref-CR18" id="ref-link-section-d16634e486">2012</a>) or using an illumination of different position red, green, blue light (Johnson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Johnson MK, Cole F, Raj A, Adelson EH (2011) Microgeometry capture using an elastomeric sensor. ACM Trans Graph 30(4):46:1–46:8" href="/article/10.1007/s10055-014-0256-y#ref-CR14" id="ref-link-section-d16634e489">2011</a>). These methods have focused on the bump deformation from touch (or pressure) to reconstruct the input shape and cannot be applied to the tangential deformation of a dilatation surface.</p><p>We propose the within-two-frames feature tracking method for the estimation of local tangential deformation. We assume that surface deformation between two successive frames can be regarded as global deformation; thus, feature correspondences between these frames can be determined. The proposed method measures feature displacement between the previous and the current frame and adds it to the estimated deformation result from the previous frame. A previous study Wu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Wu HY, Rubinstein M, Shih E, Guttag J, Durand F, Freeman W (2012) Eulerian video magnification for revealing subtle changes in the world. ACM Trans Graph 31(4):65–72" href="/article/10.1007/s10055-014-0256-y#ref-CR36" id="ref-link-section-d16634e495">2012</a>) has shown that the summation of frame-by-frame displacement of a point over a long sequence (i.e., an Eulerian-based approach) is equivalent to the displacement of the point from the first to the last frame of the sequence (i.e., a Lagrangian-based approach).</p><p>Visual features can be moved in any arbitrary directions on a deformable surface exhibiting both global and local deformations. In other words, the features are not always evenly distributed on surfaces, and there may be a region without any features, for which the feature tracking method fails. Therefore, we need to interpolate the measured displacement results by the within-two-frames feature tracking. For this purpose, we apply an MLS method, which is a sophisticated computer graphics technique designed for generating realistic deformation of a graphical character (Schaefer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schaefer S, McPhail T, Warren J (2006) Image deformation using moving least squares. ACM Trans Graph 25(3):533–540" href="/article/10.1007/s10055-014-0256-y#ref-CR28" id="ref-link-section-d16634e501">2006</a>). MLS produces smooth deformations to the entire graphic character using a small number of control points.</p><p>As discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec1">1</a>, there are many application fields that require deformation visualization for various materials. To realize such applications, we need a deformation estimation method of a nonrigid surface that can be applied for different materials in real time. Several deformation visualization methods have been proposed, such as tracking visual features on the surface from overhead image sensors (Uchiyama and Marchand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Uchiyama H, Marchand E (2011) Deformable random dot markers. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 237–238" href="/article/10.1007/s10055-014-0256-y#ref-CR33" id="ref-link-section-d16634e510">2011</a>; Uchiyama and Saito <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Uchiyama H, Saito H (2011) Random dot markers. In: 2014 IEEE virtual reality (VR), pp 35–38" href="/article/10.1007/s10055-014-0256-y#ref-CR34" id="ref-link-section-d16634e513">2011</a>; Fujimoto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Fujimoto Y, Smith R, Taketomi T, Yamamoto G, Miyazaki J, Kato H, Thomas B (2014) Geometrically-correct projection-based texture mapping onto a deformable object. IEEE Trans Vis Comput Graph 20(4):540–549" href="/article/10.1007/s10055-014-0256-y#ref-CR8" id="ref-link-section-d16634e516">2014</a>), or under the surface (Sato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Sato T, Mamiya H, Koike H, Fukuchi K (2009) PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology—UIST ’09. ACM Press, New York, pp 43–50" href="/article/10.1007/s10055-014-0256-y#ref-CR27" id="ref-link-section-d16634e519">2009</a>; Vlack et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Vlack K, Mizota T, Kawakami N, Kamiyama K, Kajimoto H, Tachi S (2005) GelForce: a vision-based traction field computer interface. In: CHI ’05 extended abstracts on human factors in computing systems. ACM, New York, pp 1154–1155" href="/article/10.1007/s10055-014-0256-y#ref-CR35" id="ref-link-section-d16634e523">2005</a>), and embedded sensors (Heo and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Heo S, Lee G (2013) Indirect shear force estimation for multi-point shear force operations. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, CHI’13, pp 281–284" href="/article/10.1007/s10055-014-0256-y#ref-CR10" id="ref-link-section-d16634e526">2013</a>). However, these methods only work for specific materials and/or cannot work in real time. Our method enables users to visualize the tangential deformation of different deformable materials and work with them in real time, independent of the surface size, simply by painting or embedding IR ink on the deformable materials.
</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Deformation estimation technique</h2><div class="c-article-section__content" id="Sec3-content"><p>In this section, we present our tangential deformation estimation method. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig2">2</a> shows the process flow of the proposed technique. The deformable surface with IR markers is captured by an IR camera. The surface detection module finds the target region in the captured image. The motion measurement module calculates feature points and their correspondences between two successive frames. Thereafter, the deformation reconstruction module estimates the deformation of the surface by interpolating the measured displacement information using MLS and adds the estimation result to that from the previous frame.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Process flow framework: <b>a</b> surface detection, <b>b</b> motion measurement, and <b>c</b> deformation reconstruction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec4">Surface detection</h3><p>In this research, we assume that the background of our system, as well as the IR ink (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec7">4</a>), absorb IR light. Therefore, only the surface of the deformable object exhibits high intensity under uniform IR illumination in a captured IR image. We detect the surface region of the deformable object by finding the largest bright region in each captured image. We define a square region containing the surface boundary as a region of interest (ROI). Rather than using the entire captured image, the ROI is used as a working area in the following process to make it faster and ensure robustness against image noise occurring outside the ROI.
</p><h3 class="c-article__sub-heading" id="Sec5">Motion measurement</h3><p>We propose the within-two-frames feature tracking method to measure spatially varying movements of feature points on a deformable surface. The proposed method is designed to avoid tracking failures caused by occlusions, which are unavoidable when features are tracked through an entire captured sequence. Our method computes feature correspondences between two successive frames. If some feature points are occluded, we ignore them and measure only non-occluded points. Therefore, the tracking errors caused by occlusions are not accumulated.</p><p>For stable motion measurement, feature points should be distributed evenly over the surface. Therefore, we divide the ROI into <span class="mathjax-tex">\(m \times m\)</span> blocks and measure the fixed number of feature points, <i>α</i>, in each block. We define the total number of feature points <i>K</i>, which is then divided by the number of blocks to obtain <i>α</i>. We apply a simple feature detector, i.e., good features to track (Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 593–600" href="/article/10.1007/s10055-014-0256-y#ref-CR29" id="ref-link-section-d16634e623">1994</a>), in our system.</p><p>In the feature tracking process, we find a feature point in the current frame, i.e., the <i>corresponding point</i> that corresponds to a point in the previous frame, i.e., the <i>original point</i>. The corresponding point is determined as the feature point that is the nearest to the original point. The proposed method determines the feature points and their correspondences individually for each block. If a block is completely occluded, it is not possible to determine the feature points and their correspondences. In this case, we ignore that particular block and attempt to measure the surface motion when the surface reappears in the block (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig3">3</a>a). Our method also avoids matching errors by finding unusual correspondences. We consider a feature correspondence as an unusual correspondence when the displacement value <i>d</i> is greater than a threshold value <i>β</i>. A threshold value <i>β</i> is specified in advance based on the assumption that, within 25 frames, the feature point should not move more than 0.3 cm or 10 pixels. We exclude such features and their correspondences from the motion measurement (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig3">3</a>b).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Within-two-frames technique used to measure surface motion. The feature point correspondences are calculated in each block individually. <b>a</b> If a block is occluded, the block is ignored until the surface in the block reappears. <b>b</b> The proposed method avoids matching errors by finding unusual correspondences from the displacement of each feature correspondence</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Specular reflection on a deformable surface is problematic: When some regions of a surface exhibit specular reflection of the illuminated IR light, they exhibit bright spots close to white in a captured image. This causes a matching error and results in a tracking failure. To solve this problem, we exclude the pixels that have brightness values greater than a threshold value <i>γ</i> from the motion measurement. A threshold value <i>γ</i> is calculated by (maximum brightness value—minimum brightness value) × 0.8 based on a method proposed by Chang and Tseng (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Chang RC, Tseng FC (2010) Automatic detection and correction for glossy reflections in digital photograph. In: 3rd IEEE international conference on Ubi-media computing (U-Media), pp 44–49" href="/article/10.1007/s10055-014-0256-y#ref-CR6" id="ref-link-section-d16634e685">2010</a>).</p><h3 class="c-article__sub-heading" id="Sec6">Deformation reconstruction</h3><p>In general, dense feature tracking realizes an accurate measurement of the surface motion; however, it requires a rich texture covering the entire surface. Such rich textures do not always appear on a deformed surface because the texture moves unevenly according to the deformation and may be occluded by user interaction. Therefore, we apply an interpolation technique (i.e., MLS) to the motion measurement result to estimate the final deformation.</p><p>MLS was originally proposed to generate in real-time realistic deformations of a graphic character using only a small number of control points. Smooth deformations are produced from the displacement of each control point in each frame. We apply this method to the feature tracking result rather than using the movement of predefined control points.</p><p>Unlike previous MLS approaches, we are concerned with the ROI rather than the entire image. Because the size of the ROI changes according to the surface deformation, we geometrically transform the feature tracking results in such a way that the ROI of each frame corresponds to a reference, i.e., the ROI of the first frame. The transformed ROI is referred to as the <i>operational space</i> in the following. Estimation of the deformation at every location in an operational space requires significant computational resources. Therefore, we calculate the deformation at sparse but uniformly distributed points in the space. We assign a uniform grid to the operational space, and the MLS computation is performed at the grid vertices. The number of uniform grids is related to the deformation quality and computational resources. We assign a number of uniform grids based on the size of the deformable object. Our preliminary experiment shows that a 6 × 6 uniform grid per square centimeter (cm<sup>2</sup>) provides optimal quality and computational resources.</p><p>First, we geometrically transform a set of feature points and their correspondences from each ROI to the operational space. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig4">4</a>, let <span class="mathjax-tex">\(p_t^{i}\)</span> and <span class="mathjax-tex">\(q_t^i\)</span> be the original points and their correspondences, respectively, at time <i>t</i>, where <i>i</i> indicates the index of each point. Then, we define the deformation at each grid vertex <span class="mathjax-tex">\(v_n\)</span> (<span class="mathjax-tex">\(n = 1,\ldots , N\)</span>) by solving a transformation <i>M</i> that maps <span class="mathjax-tex">\(p_t^i\)</span> to <span class="mathjax-tex">\(q_t^i\)</span> between each of the two successive frames (see the solving formulation of <i>M</i> in Schaefer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Schaefer S, McPhail T, Warren J (2006) Image deformation using moving least squares. ACM Trans Graph 25(3):533–540" href="/article/10.1007/s10055-014-0256-y#ref-CR28" id="ref-link-section-d16634e888">2006</a>):</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} f_t(v_n) = \min _{M} \sum _{i} w_t^i(v_n){\left|M\left(p_t^i - q_t^i\right)\right|^2}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where position <span class="mathjax-tex">\(f_t(v_n)\)</span> is the estimation result of <span class="mathjax-tex">\(v_n\)</span> at time <i>t</i> and <span class="mathjax-tex">\(w_t^i(v_n)\)</span> is the weight function of the distance between <span class="mathjax-tex">\(v_n\)</span> and <span class="mathjax-tex">\(p_t^i\)</span> defined as</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$w_t^i(v_n) = \frac{1}{\left|p_t^i - v_n\right|^{2}}.$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>
                          <b>a</b> Transformation matrix <i>M</i> maps between detected feature points <span class="mathjax-tex">\(p_i\)</span>, <b>b</b> feature correspondences <span class="mathjax-tex">\(q_i\)</span>, and <b>c</b> texture image mapped to the deformation estimation result at each vertex <span class="mathjax-tex">\(f_t(v_n)\)</span>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The sum of the estimation result from each of the two successive frames will yield the total deformation estimation result. Consequently, the total deformation result at <span class="mathjax-tex">\(v_n\)</span> from <i>t</i> = 1 to <i>T</i> is computed as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} F_T(v_n) = f_T(v_n) + \sum _{t=1,\ldots , T - 1} f_t(v_n). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                <p>To visualize the deformation estimation result, we geometrically transform the operational space to the projector coordinate. Subsequently, we map projection graphics to the deformation estimation result. The deformed graphics are projected on the surface using a projector.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Deformable objects</h2><div class="c-article-section__content" id="Sec7-content"><p>Our system can use materials that have various deformation properties, such as viscoelastic or elastic materials. Several examples of deformable objects are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig5">5</a>a, including slime,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> elastic clay (NENDO Studio clay), a polyurethane-based gel sheet (EXSEAL Hitohada), and silly putty (Crazy Aaron Enterprises). Viscoelastic materials exhibit a time-dependent strain when the deformation is performed. Elastic materials change shape under force and recover their shapes when the force is released. These materials allow for several interactions, such as touching, pinching, squeezing, kneading, and pulling or stretching.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Human eye view (<i>left</i>) and IR camera view (<i>right</i>) of deformable objects painted or embedded with invisible ink-painted fabrics: <b>a</b> commercial silly putty, <b>b</b> slime (sodium borate + PVA) embedded with invisible particles, <b>c</b> elastic clay, and <b>d</b> polyurethane-based gel sheet painted with invisible ink</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Deformable materials for our application must have uniformly white, or at least non-textured surfaces for projector illumination. This is difficult considering that our deformation estimation method requires a surface texture which can be used for vision-based feature detection. To make a non-textured deformable surface applicable for our deformation estimation method, we use spectral selective ink, called <i>IR ink</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig6">6</a>a). The IR ink is invisible to the human eye, having high absorption only in a near-IR wavelength (between 793 and 820 nm). We use two approaches for applying IR ink to make a deformable material that has a non-textured surface applicable for our deformation estimation method.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>
                        <b>a</b> Invisible ink with high absorption in the IR region, and <b>b</b> spectral selective particles, i.e., IR invisible ink painted on non-woven fabrics</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>First, we paint the IR ink directly on the deformable surface. The IR ink is oil-based; thus, it is durable after application to the surface. In particular, this approach is appropriate for matte materials, such as polyurethane-based gel sheets and elastic clay.</p><p>Second, we embed pieces of fabric soaked in IR ink into a deformable object approximately 0.1–0.2 cm under the surface (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig6">6</a>b). This approach is particularly effective for objects having glossy surfaces, such as slime and silly putty, which are inappropriate for the painting approach. Owing to the IR wavelength being wider than visible wavelength, the embedded particles are still visible in an image captured by an IR camera. The fabric used in our system is non-woven and is matte and durable when soaked in ink. The fabric is particularly suitable because it does not disperse IR ink into the deformable objects.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig5">5</a>b shows the IR image of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig5">5</a>a.
</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Experiments</h2><div class="c-article-section__content" id="Sec8-content"><p>We conducted both objective and subjective experiments to evaluate the proposed method.</p><h3 class="c-article__sub-heading" id="Sec9">System configuration and demonstration</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig7">7</a> shows the hardware configuration of our prototype. We place a projector (ACER K10) and a camera (PointGrey Flea3 FL3-U3-13S2M-CS) so that they share the same optical axis using a beam splitter (Edmund Optical VIS Plate, 1.25 cm). The camera is equipped with an IR-pass filter (Edmund Optical Cast Plastic IR 700 nm Long-pass Filter). A visible-IR projector (PLUS V-1100) with an IR-pass filter (same as the camera’s filter) is used as a spatially uniform infrared light source.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Experimental setup. The visible-IR projector with IR-pass filter is used as an IR light source. <b>a</b> The projector and camera are placed in the same plane using a beam splitter, <b>b</b> slime (viscoelastic material) with embedded IR-based particles is used as the deformable object in our experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The images of the deformable object are acquired at a resolution of 640 × 480 pixels. The image acquisition process, motion measurement, and deformation estimation run on the same PC (CPU: Intel Core i7 950 3.07 GHz, RAM: 6 GB, GPU: NVIDIA GeForce GT520 2 GB). The within-two-frames feature tracking method is implemented on the GPU. We divide the surface ROI into 3 × 3 blocks (i.e., <i>m</i> = 3, in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec5">3.2</a>). The camera and projector share an optical axis; thus, we use homography for the coordinate transformation between the camera and projector.</p><p>We use an approximately 36 cm<sup>2</sup> piece of slime (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig7">7</a>b) as a deformable material in all experiments. The material is made by mixing sodium borate with polyvinyl alcohol. It exhibits both viscosity and elasticity and provides global and local deformations. The material flows like a liquid for a long period, but it has a short period of rigidity. We embed the IR ink-soaked fabric particles into the deformable object’s surface, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec7">4</a>.
</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig8">8</a>a shows the self-dilatation deformation of the surface. The projected graphics become larger according to the self-dilatation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig8">8</a>b, c shows the deformation of the projected results when a user interacts with the surface. The projected graphics deform according to the user’s tangential deformation manipulation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Preliminary results of the implementation. The different sets of selected frames show the visualization of the surface self-dilatation deformation (<b>a</b>) and user interaction (<b>b</b>, <b>c</b>). Users can interact with a deformable surface by kneading, touching, pulling (<b>b</b>), and stretching (<b>c</b>). The projected graphics were deformed as closely as possible to the deformed surface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec10">Accuracy evaluation</h3><p>As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec3">3</a>, the proposed deformation estimation method is designed to avoid tracking errors that result from occluded features. We evaluate how accurately the proposed method estimates the deformation when feature points are sometimes occluded in a sequence by comparing our results with a reference.</p><p>The reference is obtained by manual feature tracking through a whole sequence using the procedure described below. The experimenter selects feature points in the first captured frame and begins tracking them in the next captured frame. If there are occluded features, the experimenter does not attempt to track them in that frame. When such features reappear, the experimenter recovers these previously occluded features and continues to track them. The feature tracking results are then processed using the MLS method to estimate the deformation, which is the same as the proposed method.</p><p>In the experiment, we used 1.25 features per cm<sup>2</sup> or a total 45 feature points for both the proposed and reference methods. The experimenter caused deformations by pushing the surface. A video sequence was recorded by the system’s IR camera. The same sequence was used for both the proposed and reference methods.
</p><p>The estimated positions of grid vertices used in the MLS method
 are plotted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig9">9</a>. The proposed method (blue) is compared with the reference (orange) at 225 vertices. From the figure, we confirm that the trajectories in our method and the reference method look similar. On average, the Euclidean distance between the estimation results of our method and those of the reference is only 5.4 pixels, which corresponds to approximately 0.14 cm on the surface. Moreover, the correlation coefficient analysis shows that the results of our method and the reference are similar (<span class="mathjax-tex">\(\rho = 0.98\)</span>). Consequently, we can confirm that our method performed the surface deformation estimation with comparable accuracy to the reference method using manual feature tracking.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Estimation results for our method (<i>blue</i>) compared with those of the reference method (<i>orange</i>) for 225 vertices (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec11">User test</h3><p>Our system is designed to be used by a user; thus, we must assess the system from the user’s perspective. Here, we show the results from our user study, in which participants subjectively evaluate how realistic the visualization results of our proposed system are.
</p><p>In the experiment, each participant is asked to interact with the deformable surface under three conditions, i.e., 0.42, 0.83, and 1.25 features per cm<sup>2</sup> or a total 15, 30, and 45 feature points, respectively, are used for the estimation. For each condition, participants perform four types of interactions: pinching, kneading, pushing, and shrinking (using their hand, or using plastic sticks) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig10">10</a>). The order of the specified interactions is random for each participant. After the experiment, we ask each participant to rate the deformation visualization result subjectively. In particular, we ask them the following question: “How realistic do you think the graphics projected on the deformable surface were?” The question is asked for each experimental condition. The participants answer the question using a 5-point Likert scale, where 1 is rather unrealistic and 5 is very realistic.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>User experiment interactions: <b>a</b> pushing, <b>b</b> touching, <b>c</b> pulling, and <b>d</b> pinching</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Eleven subjects (nine males, two females) between the ages of 22 and 25 participated in the experiment. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0256-y#Tab1">1</a> shows the average subjective ratings. For 0.83 or more features per cm<sup>2</sup>, the participants found the deformation results to be satisfactory (average &gt; 4.2). However, 0.42 features per cm<sup>2</sup> were less effective in the estimation of the surface deformation (average &gt; 3.1). The Likert scale rating was analyzed using the Friedman test. There was a statistically significant difference in the realistic deformation estimation depending on the number of feature points per cm<sup>2</sup>. Post hoc pairwise comparisons were conducted using the Wilcoxon signed-rank test. For post hoc analyses, we applied the Bonferroni correction for multiple comparisons to a significance level of 0.017 (i.e., <i>p</i> = 0.05/3 multiple comparisons). There were statistically significant differences in the realistic deformation estimation between 0.42 and 0.83 features per cm<sup>2</sup>
                  <span class="mathjax-tex">\((Z = -2.76, p &lt; 0.01)\)</span> and between 0.42 and 1.25 features per cm<sup>2</sup>
                  <span class="mathjax-tex">\((Z = -2.89, p &lt; 0.01)\)</span>. However, there was no significant difference between 0.83 and 1.25 features per cm<sup>2</sup>
                  <span class="mathjax-tex">\((Z = -0.71, p = 0.48)\)</span>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Results of the user test</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0256-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In summary, the experimental results prove that the number of features per cm<sup>2</sup> has a significant effect on the realistic deformation estimation result. Increasing the number of feature points improves the realistic estimation results. In particular, 0.83 and 1.25 features per cm<sup>2</sup> clearly provide a more realistic deformation visualization than 0.42 features per cm<sup>2</sup>.</p><h3 class="c-article__sub-heading" id="Sec12">Optimal number of features</h3><p>Although the number of feature points has significant effect on the realistic deformation visualization result, as confirmed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec11">5.3</a>, increasing the number of feature points requires a longer computational time. We aim to apply our method to interactive applications; thus, we must consider the trade-off between real-time performance and the number of feature points necessary for realistic deformation estimation.</p><p>We conducted an experiment to find the appropriate balance between realistic deformation visualization and real-time performance. The computation time of the system was evaluated by increasing the number of feature points. We tried to find the optimal number of feature points for realistic deformation estimation within real-time performance.
</p><p>The average processing times for 300 frame iterations with 13 different feature density per cm<sup>2</sup> (0.42, 0.83, 1.25, 1.66, 2.03, 2.43, 2.84, 3.24, 3.65, 4.05, 4.46, 5.00, and 5.56) are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0256-y#Fig11">11</a>. The processing time exceeded real-time performance requirements (at 25 frames/s) when the feature density was &gt;2.03. The deformation estimation result was sufficiently realistic to satisfy users when the feature density was 0.83 and above (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0256-y#Sec11">5.3</a>). In other words, the optimal number of feature points that balances realistic deformation visualization within real-time performance was 0.83–2.03 in this prototype system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0256-y/MediaObjects/10055_2014_256_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Average processing times using 0.42–5.56 feature points per square centimeter</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0256-y/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>Our technique allows an intuitive MR experience in projection-based technologies using a simple hardware setup that includes a projector, an IR camera, and an IR light source. The proposed method is not limited by the characteristics of deformable materials (e.g., elastic, viscous, or viscoelastic). Such limitations can be overcome by painting the object’s surface with IR ink or by embedding pieces of fabric soaked in the ink into the surface. Owing to a state-of-the-art deformation estimation technique (i.e., MLS), our method uses a relatively small number of feature points to approximate deformations for large-scale surfaces. Here, we discuss the potential applications and limitations of the proposed system.</p><h3 class="c-article__sub-heading" id="Sec14">Potential applications</h3><p>Adding a deformable projected graphics to physical clay materials has many potential educational applications. Clay is often used as an early childhood educational tool. Mixing the advantages of computer-supported education and our deformed projected graphics using a projector–camera system, we can facilitate new interactive educational activities. For example, teachers and students can experience the deformation of physical objects visualized by projection during fluid dynamics experiments. It can also support graphical designers in creating deformable graphics. They can explore their designs easily by deforming clays or other deformable materials on which the deformation information is projected. As another application, we can apply the proposed system to toys for small children; they can, for example, enjoy deforming preferred graphic images by changing the form of the clay.</p><h3 class="c-article__sub-heading" id="Sec15">Limitations</h3><p>We have proposed a within-two-frames feature tracking method with the MLS deformation reconstruction technique to estimate the deformation of a non-textured surface for projection-based MR technologies. With the proposed technique, the projection graphics can deform to match the real object, even for self-dilating objects made of viscoelastic materials. However, there are currently some limitations. First, although our method can estimate surface deformation without any mechanical devices, artificial textures, which are either painted on or embedded into the physical object’s surface, are required. However, we do not believe that this leads to any critical problems. The textures are invisible to the human eye; thus, they do not degrade the image quality of the augmented graphics. Second, the proposed method shares a limitation regarding deformation estimation with other vision-based methods. As with other methods, estimating twist deformation is not possible because the motion detected by feature tracking does not include rotational information. Although this could be solved by applying a more complex marker, e.g., an identifiable 2D visual marker, such a marker would disturb the estimation of any deformation smaller than the employed marker. While the dense optical flow technique (Alvarez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Alvarez L, Weickert J, Snchez J (2000) Reliable estimation of dense optical flow fields with large displacements. Int J Comput Vis 39(1):41–56" href="/article/10.1007/s10055-014-0256-y#ref-CR1" id="ref-link-section-d16634e2220">2000</a>) might include rotational information, it is limited by the fact that computational resources depend on the size of the surface material for real-time processing. Moreover, the deformation result from interpolated flow can sometimes suffer from tracking noise, leading to lack of realism. In contrast, our sparse tracking method allows size-independent real-time processing. Our method also minimizes the lack of realism by reducing the number of feature points and using interpolated deformation flows, instead.</p><p>In this paper, we proposed combining a sparse tracking method with MLS interpolation to realize robust and real-time tangential deformation estimation. We designed our method so that any sparse tracking methods can be used in combination with the MLS. Investigating which sparse tracking method provides the best performance is beyond the scope of this paper and will be tackled in our future work.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions</h2><div class="c-article-section__content" id="Sec16-content"><p>To the best of our knowledge, no empirical work has addressed projection-based augmentation of nonrigid surfaces based on tangential deformation estimation. This study has described a new technique that can estimate the deformation of a non-textured surface successfully. The system combines the within-two-frames motion measurement and the MLS method for tangential deformation estimation. The results show the effectiveness of our deformation estimation method compared with manual feature tracking. Determining the optimal number of feature points indicated that it is possible to balance realistic deformation visualization and real-time performance. In addition, we presented a range of applications which can benefit from the projection-based technology developed by us. In the future, we will extend the proposed method, which currently only supports 2D deformation, to augment 3D surface deformation by applying multiple cameras and projectors.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>See <a href="http://en.wikipedia.org/wiki/Flubber_(material)">http://en.wikipedia.org/wiki/Flubber_(material)</a> for more information.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Alvarez, J. Weickert, J. Snchez, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Alvarez L, Weickert J, Snchez J (2000) Reliable estimation of dense optical flow fields with large displacemen" /><p class="c-article-references__text" id="ref-CR1">Alvarez L, Weickert J, Snchez J (2000) Reliable estimation of dense optical flow fields with large displacements. Int J Comput Vis 39(1):41–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1008170101536" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1060.68635" aria-label="View reference 1 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reliable%20estimation%20of%20dense%20optical%20flow%20fields%20with%20large%20displacements&amp;journal=Int%20J%20Comput%20Vis&amp;volume=39&amp;issue=1&amp;pages=41-56&amp;publication_year=2000&amp;author=Alvarez%2CL&amp;author=Weickert%2CJ&amp;author=Snchez%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR2">Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: Proceedings of the IEEE/ACM international symposium on augmented reality, pp 207–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="O. Bimber, R. Raskar, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., USA" /><p class="c-article-references__text" id="ref-CR3">Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., USA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20augmented%20reality%3A%20merging%20real%20and%20virtual%20worlds&amp;publication_year=2005&amp;author=Bimber%2CO&amp;author=Raskar%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="O. Bimber, D. Iwai, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bimber O, Iwai D (2008) Superimposing dynamic range. ACM Trans Graph 27(5):15:1–15:8" /><p class="c-article-references__text" id="ref-CR4">Bimber O, Iwai D (2008) Superimposing dynamic range. ACM Trans Graph 27(5):15:1–15:8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1409060.1409103" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Superimposing%20dynamic%20range&amp;journal=ACM%20Trans%20Graph&amp;volume=27&amp;issue=5&amp;pages=15%3A1-15%3A8&amp;publication_year=2008&amp;author=Bimber%2CO&amp;author=Iwai%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bluteau J, Kitahara I, Kameda Y, Noma H, Kogure K, Ohta Y (2005) Visual support for medical communication by u" /><p class="c-article-references__text" id="ref-CR5">Bluteau J, Kitahara I, Kameda Y, Noma H, Kogure K, Ohta Y (2005) Visual support for medical communication by using projector-based augmented reality and thermal markers. In: Proceedings of the international conference on artificial reality and telexistence, pp 98–105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chang RC, Tseng FC (2010) Automatic detection and correction for glossy reflections in digital photograph. In:" /><p class="c-article-references__text" id="ref-CR6">Chang RC, Tseng FC (2010) Automatic detection and correction for glossy reflections in digital photograph. In: 3rd IEEE international conference on Ubi-media computing (U-Media), pp 44–49</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Follmer S, Johnson M, Adelson E, Ishii H (2011) deform: An interactive malleable surface for capturing 2.5d ar" /><p class="c-article-references__text" id="ref-CR7">Follmer S, Johnson M, Adelson E, Ishii H (2011) deform: An interactive malleable surface for capturing 2.5d arbitrary objects, tools and touch. In: Proceedings of the 24th annual ACM symposium on user interface software and technology. ACM, New York, UIST’11, pp 527–536</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Fujimoto, R. Smith, T. Taketomi, G. Yamamoto, J. Miyazaki, H. Kato, B. Thomas, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Fujimoto Y, Smith R, Taketomi T, Yamamoto G, Miyazaki J, Kato H, Thomas B (2014) Geometrically-correct project" /><p class="c-article-references__text" id="ref-CR8">Fujimoto Y, Smith R, Taketomi T, Yamamoto G, Miyazaki J, Kato H, Thomas B (2014) Geometrically-correct projection-based texture mapping onto a deformable object. IEEE Trans Vis Comput Graph 20(4):540–549</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2014.25" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometrically-correct%20projection-based%20texture%20mapping%20onto%20a%20deformable%20object&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=20&amp;issue=4&amp;pages=540-549&amp;publication_year=2014&amp;author=Fujimoto%2CY&amp;author=Smith%2CR&amp;author=Taketomi%2CT&amp;author=Yamamoto%2CG&amp;author=Miyazaki%2CJ&amp;author=Kato%2CH&amp;author=Thomas%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haouchine N, Dequidt J, Kerrien E, Berger MO, Cotin S (2012) Physics-based augmented reality for 3d deformable" /><p class="c-article-references__text" id="ref-CR9">Haouchine N, Dequidt J, Kerrien E, Berger MO, Cotin S (2012) Physics-based augmented reality for 3d deformable object. In: Workshop on virtual reality interaction and physical simulation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heo S, Lee G (2013) Indirect shear force estimation for multi-point shear force operations. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR10">Heo S, Lee G (2013) Indirect shear force estimation for multi-point shear force operations. In: Proceedings of the SIGCHI conference on human factors in computing systems. ACM, New York, CHI’13, pp 281–284</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hisada M, Yamamoto K, Kanaya I, Sato K (2006) Free-form shape design system using stereoscopic projector—hyper" /><p class="c-article-references__text" id="ref-CR11">Hisada M, Yamamoto K, Kanaya I, Sato K (2006) Free-form shape design system using stereoscopic projector—hyperreal 2.0. In: SICE-ICASE international joint conference, pp 4832–4835</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ito Y, Kim Y, Obinata G (2014) Acquisition of contact force and slippage using a vision-based tactile sensor w" /><p class="c-article-references__text" id="ref-CR12">Ito Y, Kim Y, Obinata G (2014) Acquisition of contact force and slippage using a vision-based tactile sensor with a fluid-type touchpad for the dexterous handling of robots. Adv Robot Autom 3(116)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Iwai, K. Sato, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Iwai D, Sato K (2010) Document search support by making physical documents transparent in projection-based mix" /><p class="c-article-references__text" id="ref-CR13">Iwai D, Sato K (2010) Document search support by making physical documents transparent in projection-based mixed reality. Virtual Real 15(2–3):147–160</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Document%20search%20support%20by%20making%20physical%20documents%20transparent%20in%20projection-based%20mixed%20reality&amp;journal=Virtual%20Real&amp;volume=15&amp;issue=2%E2%80%933&amp;pages=147-160&amp;publication_year=2010&amp;author=Iwai%2CD&amp;author=Sato%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MK. Johnson, F. Cole, A. Raj, EH. Adelson, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Johnson MK, Cole F, Raj A, Adelson EH (2011) Microgeometry capture using an elastomeric sensor. ACM Trans Grap" /><p class="c-article-references__text" id="ref-CR14">Johnson MK, Cole F, Raj A, Adelson EH (2011) Microgeometry capture using an elastomeric sensor. ACM Trans Graph 30(4):46:1–46:8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2010324.1964941" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Microgeometry%20capture%20using%20an%20elastomeric%20sensor&amp;journal=ACM%20Trans%20Graph&amp;volume=30&amp;issue=4&amp;pages=46%3A1-46%3A8&amp;publication_year=2011&amp;author=Johnson%2CMK&amp;author=Cole%2CF&amp;author=Raj%2CA&amp;author=Adelson%2CEH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jones BR, Benko H, Ofek E, Wilson AD (2013) Illumiroom: peripheral projected illusions for interactive experie" /><p class="c-article-references__text" id="ref-CR15">Jones BR, Benko H, Ofek E, Wilson AD (2013) Illumiroom: peripheral projected illusions for interactive experiences. In: Proceedings of the ACM annual conference on human factors in computing systems, pp 869–878</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Kamiyama, K. Vlack, T. Mizota, H. Kajimoto, N. Kawakami, S. Tachi, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Kamiyama K, Vlack K, Mizota T, Kajimoto H, Kawakami N, Tachi S (2005) Vision-based sensor for real-time measur" /><p class="c-article-references__text" id="ref-CR16">Kamiyama K, Vlack K, Mizota T, Kajimoto H, Kawakami N, Tachi S (2005) Vision-based sensor for real-time measuring of surface traction fields. IEEE Comput Graph Appl 25(1):68–75</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2005.27" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Vision-based%20sensor%20for%20real-time%20measuring%20of%20surface%20traction%20fields&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=25&amp;issue=1&amp;pages=68-75&amp;publication_year=2005&amp;author=Kamiyama%2CK&amp;author=Vlack%2CK&amp;author=Mizota%2CT&amp;author=Kajimoto%2CH&amp;author=Kawakami%2CN&amp;author=Tachi%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Kocev, F. Ritter, L. Linsen, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Kocev B, Ritter F, Linsen L (2013) Projector-based surgeon–computer interaction on deformable surfaces. Int J " /><p class="c-article-references__text" id="ref-CR17">Kocev B, Ritter F, Linsen L (2013) Projector-based surgeon–computer interaction on deformable surfaces. Int J Comput Assist Radiol Surg 8(6):1015–1025</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11548-013-0897-4" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Projector-based%20surgeon%E2%80%93computer%20interaction%20on%20deformable%20surfaces&amp;journal=Int%20J%20Comput%20Assist%20Radiol%20Surg&amp;volume=8&amp;issue=6&amp;pages=1015-1025&amp;publication_year=2013&amp;author=Kocev%2CB&amp;author=Ritter%2CF&amp;author=Linsen%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Matoba Y, Sato T, Takahashi N, Koike H (2012) Claytricsurface: an interactive surface with dynamic softness co" /><p class="c-article-references__text" id="ref-CR18">Matoba Y, Sato T, Takahashi N, Koike H (2012) Claytricsurface: an interactive surface with dynamic softness control capability. In: ACM SIGGRAPH emerging technologies, p 6:1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Mine, J. Baar, A. Grundhofer, D. Rose, B. Yang, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Mine M, van Baar J, Grundhofer A, Rose D, Yang B (2012) Projection-based augmented reality in disney theme par" /><p class="c-article-references__text" id="ref-CR19">Mine M, van Baar J, Grundhofer A, Rose D, Yang B (2012) Projection-based augmented reality in disney theme parks. IEEE Comput 45(7):32–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2012.154" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Projection-based%20augmented%20reality%20in%20disney%20theme%20parks&amp;journal=IEEE%20Comput&amp;volume=45&amp;issue=7&amp;pages=32-40&amp;publication_year=2012&amp;author=Mine%2CM&amp;author=Baar%2CJ&amp;author=Grundhofer%2CA&amp;author=Rose%2CD&amp;author=Yang%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ni T, Karlson AK, Wigdor D (2011) AnatOnMe: facilitating doctor-patient communication using a projection-based" /><p class="c-article-references__text" id="ref-CR20">Ni T, Karlson AK, Wigdor D (2011) AnatOnMe: facilitating doctor-patient communication using a projection-based handheld device. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 3333–3342</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Pilet, V. Lepetit, P. Fua, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Pilet J, Lepetit V, Fua P (2008) Fast non-rigid surface detection, registration and realistic augmentation. In" /><p class="c-article-references__text" id="ref-CR21">Pilet J, Lepetit V, Fua P (2008) Fast non-rigid surface detection, registration and realistic augmentation. Int J Comput Vis 76(2):109–122</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-006-0017-9" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%20non-rigid%20surface%20detection%2C%20registration%20and%20realistic%20augmentation&amp;journal=Int%20J%20Comput%20Vis&amp;volume=76&amp;issue=2&amp;pages=109-122&amp;publication_year=2008&amp;author=Pilet%2CJ&amp;author=Lepetit%2CV&amp;author=Fua%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-d tangible interface for landscape analysis. In: Proce" /><p class="c-article-references__text" id="ref-CR22">Piper B, Ratti C, Ishii H (2002) Illuminating clay: a 3-d tangible interface for landscape analysis. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 355–362</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumi" /><p class="c-article-references__text" id="ref-CR23">Raskar R, Welch G, Low KL, Bandyopadhyay D (2001) Shader lamps: animating real objects with image-based illumination. In: Proceedings of the eurographics workshop on rendering, pp 89–102</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Ziegler R, Willwacher T (2006) Cartoon dioramas in motion. In: Proceedings of the international symp" /><p class="c-article-references__text" id="ref-CR24">Raskar R, Ziegler R, Willwacher T (2006) Cartoon dioramas in motion. In: Proceedings of the international symposium on non-photorealistic animation and rendering, pp 7–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Rivers, A. Adams, F. Durand, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Rivers A, Adams A, Durand F (2012) Sculpting by numbers. ACM Trans Graph 31(6):157:1–157:7" /><p class="c-article-references__text" id="ref-CR25">Rivers A, Adams A, Durand F (2012) Sculpting by numbers. ACM Trans Graph 31(6):157:1–157:7</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2366145.2366176" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sculpting%20by%20numbers&amp;journal=ACM%20Trans%20Graph&amp;volume=31&amp;issue=6&amp;pages=157%3A1-157%3A7&amp;publication_year=2012&amp;author=Rivers%2CA&amp;author=Adams%2CA&amp;author=Durand%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saakes D, Chiu K, Hutchison T, Buczyk BM, Koizumi N, Inami M, Raskar R (2010) Slow display. In: ACM SIGGRAPH e" /><p class="c-article-references__text" id="ref-CR26">Saakes D, Chiu K, Hutchison T, Buczyk BM, Koizumi N, Inami M, Raskar R (2010) Slow display. In: ACM SIGGRAPH emerging technologies, p 22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato T, Mamiya H, Koike H, Fukuchi K (2009) PhotoelasticTouch: transparent rubbery tangible interface using an" /><p class="c-article-references__text" id="ref-CR27">Sato T, Mamiya H, Koike H, Fukuchi K (2009) PhotoelasticTouch: transparent rubbery tangible interface using an LCD and photoelasticity. In: Proceedings of the 22nd annual ACM symposium on User interface software and technology—UIST ’09. ACM Press, New York, pp 43–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Schaefer, T. McPhail, J. Warren, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Schaefer S, McPhail T, Warren J (2006) Image deformation using moving least squares. ACM Trans Graph 25(3):533" /><p class="c-article-references__text" id="ref-CR28">Schaefer S, McPhail T, Warren J (2006) Image deformation using moving least squares. ACM Trans Graph 25(3):533–540</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1141911.1141920" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20deformation%20using%20moving%20least%20squares&amp;journal=ACM%20Trans%20Graph&amp;volume=25&amp;issue=3&amp;pages=533-540&amp;publication_year=2006&amp;author=Schaefer%2CS&amp;author=McPhail%2CT&amp;author=Warren%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE conference on computer vision and patte" /><p class="c-article-references__text" id="ref-CR29">Shi J, Tomasi C (1994) Good features to track. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shimazu S, Iwai D, Sato K (2011) 3d high dynamic range display system. In: Proceedings of the 10th IEEE/ACM in" /><p class="c-article-references__text" id="ref-CR30">Shimazu S, Iwai D, Sato K (2011) 3d high dynamic range display system. In: Proceedings of the 10th IEEE/ACM international symposium on mixed and augmented reality, pp 235–236</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shimizu N, Yoshida T, Hayashi T, de Sorbier F, Saito H (2013) Non-rigid surface tracking for virtual fitting s" /><p class="c-article-references__text" id="ref-CR31">Shimizu N, Yoshida T, Hayashi T, de Sorbier F, Saito H (2013) Non-rigid surface tracking for virtual fitting system. In: International conference on computer vision theory and applications, pp 1–7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Steimle J, Jordt A, Maes P (2013) Flexpad: highly flexible bending interactions for projected handheld display" /><p class="c-article-references__text" id="ref-CR32">Steimle J, Jordt A, Maes P (2013) Flexpad: highly flexible bending interactions for projected handheld displays. In: Proceedings of ACM SIGCHI conference on human factors in computing systems, pp 237–246</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Marchand E (2011) Deformable random dot markers. In: Proceedings of IEEE international symposium o" /><p class="c-article-references__text" id="ref-CR33">Uchiyama H, Marchand E (2011) Deformable random dot markers. In: Proceedings of IEEE international symposium on mixed and augmented reality, pp 237–238</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Saito H (2011) Random dot markers. In: 2014 IEEE virtual reality (VR), pp 35–38" /><p class="c-article-references__text" id="ref-CR34">Uchiyama H, Saito H (2011) Random dot markers. In: 2014 IEEE virtual reality (VR), pp 35–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vlack K, Mizota T, Kawakami N, Kamiyama K, Kajimoto H, Tachi S (2005) GelForce: a vision-based traction field " /><p class="c-article-references__text" id="ref-CR35">Vlack K, Mizota T, Kawakami N, Kamiyama K, Kajimoto H, Tachi S (2005) GelForce: a vision-based traction field computer interface. In: CHI ’05 extended abstracts on human factors in computing systems. ACM, New York, pp 1154–1155</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HY. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, W. Freeman, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Wu HY, Rubinstein M, Shih E, Guttag J, Durand F, Freeman W (2012) Eulerian video magnification for revealing s" /><p class="c-article-references__text" id="ref-CR36">Wu HY, Rubinstein M, Shih E, Guttag J, Durand F, Freeman W (2012) Eulerian video magnification for revealing subtle changes in the world. ACM Trans Graph 31(4):65–72</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F2185520.2185561" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Eulerian%20video%20magnification%20for%20revealing%20subtle%20changes%20in%20the%20world&amp;journal=ACM%20Trans%20Graph&amp;volume=31&amp;issue=4&amp;pages=65-72&amp;publication_year=2012&amp;author=Wu%2CHY&amp;author=Rubinstein%2CM&amp;author=Shih%2CE&amp;author=Guttag%2CJ&amp;author=Durand%2CF&amp;author=Freeman%2CW">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0256-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Osaka University, 1-3 Machikaneyama, Toyonaka, Osaka, 560-8531, Japan</p><p class="c-article-author-affiliation__authors-list">Parinya Punpongsanon, Daisuke Iwai &amp; Kosuke Sato</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Parinya-Punpongsanon"><span class="c-article-authors-search__title u-h3 js-search-name">Parinya Punpongsanon</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Parinya+Punpongsanon&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Parinya+Punpongsanon" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Parinya+Punpongsanon%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Daisuke-Iwai"><span class="c-article-authors-search__title u-h3 js-search-name">Daisuke Iwai</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Daisuke+Iwai&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Daisuke+Iwai" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Daisuke+Iwai%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kosuke-Sato"><span class="c-article-authors-search__title u-h3 js-search-name">Kosuke Sato</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kosuke+Sato&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kosuke+Sato" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kosuke+Sato%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0256-y/email/correspondent/c1/new">Parinya Punpongsanon</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Projection-based%20visualization%20of%20tangential%20deformation%20of%20nonrigid%20surface%20by%20deformation%20estimation%20using%20infrared%20texture&amp;author=Parinya%20Punpongsanon%20et%20al&amp;contentID=10.1007%2Fs10055-014-0256-y&amp;publication=1359-4338&amp;publicationDate=2014-12-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0256-y" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0256-y" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Punpongsanon, P., Iwai, D. &amp; Sato, K. Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture.
                    <i>Virtual Reality</i> <b>19, </b>45–56 (2015). https://doi.org/10.1007/s10055-014-0256-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0256-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-01-15">15 January 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-10-30">30 October 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-12-11">11 December 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-03">March 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0256-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0256-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Projection-based mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Deformable surface</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0256-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=256;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

