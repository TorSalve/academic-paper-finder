<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Semantic framework for interactive animation generation and its applic"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of functional requirements (e.g. graphics,..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/22/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Semantic framework for interactive animation generation and its application in virtual shadow play performance"/>

    <meta name="dc.source" content="Virtual Reality 2018 22:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-01-21"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 The Author(s)"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of functional requirements (e.g. graphics, physics, AI, multimodal inputs and outputs, and massive data assets management). In this paper, a semantic framework is proposed to model the construction of interactive animation and promote animation assets reuse in a systematic and standardized way. As its ontological implementation, two domain-specific ontologies for the hand-gesture-based interaction and animation data repository have been developed in the context of Chinese traditional shadow play art. Finally, prototype of interactive Chinese shadow play performance system using deep motion sensor device is presented as the usage example."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-01-21"/>

    <meta name="prism.volume" content="22"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="149"/>

    <meta name="prism.endingPage" content="165"/>

    <meta name="prism.copyright" content="2018 The Author(s)"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0333-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0333-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0333-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0333-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Semantic framework for interactive animation generation and its application in virtual shadow play performance"/>

    <meta name="citation_volume" content="22"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2018/06"/>

    <meta name="citation_online_date" content="2018/01/21"/>

    <meta name="citation_firstpage" content="149"/>

    <meta name="citation_lastpage" content="165"/>

    <meta name="citation_article_type" content="S.I. : VR and AR Serious Games"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0333-8"/>

    <meta name="DOI" content="10.1007/s10055-018-0333-8"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0333-8"/>

    <meta name="description" content="Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of funct"/>

    <meta name="dc.creator" content="Hui Liang"/>

    <meta name="dc.creator" content="Shujie Deng"/>

    <meta name="dc.creator" content="Jian Chang"/>

    <meta name="dc.creator" content="Jian Jun Zhang"/>

    <meta name="dc.creator" content="Can Chen"/>

    <meta name="dc.creator" content="Ruofeng Tong"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="A free, open-source ontology editor and framework for building intelligent systems. 
                    http://protege.stanford.edu/
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="Anthony M, Chandler T, Heiser E (2009) A framework for promoting high cognitive performance in instructional games. In: Interservice/industry training simulation and education conference (I/ITSEC), Orlando, FL"/>

    <meta name="citation_reference" content="Apache Jena. 
                    https://jena.apache.org/
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="Arndt R, Troncy R, Staab S, Hardman L (2009) Comm: a core ontology for multimediaannotation. In: Handbook on ontologies. Springer, Berlin, Heidelberg, pp 403&#8211;421"/>

    <meta name="citation_reference" content="Bilasco IM, Gensel J, Villanova-Oliver M, Martin H (2005) On indexing of 3D scenes using MPEG-7. In: Proceedings of the 13th annual ACM international conference on multimedia. ACM, pp 471&#8211;474"/>

    <meta name="citation_reference" content="BinSubaih A, Maddock S, Romano D (2005) Game logic portability. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, pp 458&#8211;461"/>

    <meta name="citation_reference" content="Borst WN (1997) Construction of engineering ontologies for knowledge sharing and reuse, Universiteit Twente"/>

    <meta name="citation_reference" content="Chang PHM, Chien YH, Kao ECC, Soo VW (2005) A knowledge based scenario framework to support intelligent planning characters. In: Intelligent virtual agents. Springer, Berlin, Heidelberg, pp 134&#8211;145"/>

    <meta name="citation_reference" content="Chen F (2015) Visions for the masses: Chinese shadow plays from shaanxi and shanxi. Pacific Science 201"/>

    <meta name="citation_reference" content="De Boeck J, Raymaekers C, Coninx K (2006) Comparing NiMMiT and data-driven notations for describing multimodal interaction. In: Task models and diagrams for users interface design. Springer, Berlin, Heidelberg, pp 217&#8211;229"/>

    <meta name="citation_reference" content="Dubin D, Jett J (2015) An ontological framework for describing games. In: Proceedings of the 15th ACM/IEEE-CE on joint conference on digital libraries. ACM, pp 165&#8211;168"/>

    <meta name="citation_reference" content="Falcidieno B (2004) Aim@ shape project presentation. In: Shape modeling applications, proceedings. IEEE, p 329"/>

    <meta name="citation_reference" content="Falcidieno B, Spagnuolo M, Alliez P, Quak E, Vavalis M, Houstis C (2004) Towards the semantics of digital shapes: the AIM@SHAPE approach. In: EWIMT"/>

    <meta name="citation_reference" content="Floty&#324;ski J, Walczak K (2013a) Conceptual semantic representation of 3d content. In: Business information systems workshops. Springer, Berlin, Heidelberg, pp 244&#8211;257"/>

    <meta name="citation_reference" content="Floty&#324;ski J, Walczak K (2013b) Semantic modelling of interactive 3d content. In: Proceedings of the 5th joint virtual reality conference. Eurographics association, pp 41&#8211;48"/>

    <meta name="citation_reference" content="Floty&#324;ski J, Walczak K (2013c) Semantic multi-layered design of interactive 3d presentations. In: 2013 federated conference on computer science and information systems (FedCSIS). IEEE, pp 541&#8211;548"/>

    <meta name="citation_reference" content="Floty&#324;ski J, Walczak K (2014) Multi-platform semantic representation of interactive 3d content. In: Technological innovation for collective awareness systems. Springer, Berlin, Heidelberg, pp 63&#8211;72"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph (TOG); citation_title=A search engine for 3D models; citation_author=T Funkhouser, P Min, M Kazhdan, J Chen, A Halderman, D Dobkin; citation_volume=22; citation_issue=1; citation_publication_date=2003; citation_pages=83-105; citation_doi=10.1145/588272.588279; citation_id=CR18"/>

    <meta name="citation_reference" content="Ghannem A (2014) Characterization of serious games guided by the educational objectives. In: Proceedings of the second international conference on technological ecosystems for enhancing multiculturality. ACM, pp. 227&#8211;233"/>

    <meta name="citation_reference" content="Google 3D Warehouse. 
                    https://3dwarehouse.sketchup.com/
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="citation_journal_title=Knowl Acquis; citation_title=A translation approach to portable ontology specifications; citation_author=TR Gruber; citation_volume=5; citation_issue=2; citation_publication_date=1993; citation_pages=199-220; citation_doi=10.1006/knac.1993.1008; citation_id=CR21"/>

    <meta name="citation_reference" content="Gr&#252;ninger M, Fox MS (1995) Methodology for the design and evaluation of ontologies, Workshop on Basic Ontological Issues in Knowledge Sharing"/>

    <meta name="citation_reference" content="Gutierrez M, Thalmann D, Vexo F (2005) Semantic virtual environments with adaptive multimodal interfaces. In: Multimedia modelling conference, 2005. MMM 2005. Proceedings of the 11th international. IEEE, pp 277&#8211;283"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=An ontology of virtual humans: incorporating semantics into human shapes; citation_author=M Guti&#233;rrez, A Garc&#237;a-Rojas, D Thalmann, F Vexo, L Moccozet, N Magnenat-Thalmann, M Mortara, M Spagnuolo; citation_volume=23; citation_issue=3; citation_publication_date=2007; citation_pages=207-218; citation_doi=10.1007/s00371-006-0093-4; citation_id=CR24"/>

    <meta name="citation_reference" content="Kazhdan M, Funkhouser T, Rusinkiewicz S (2003) Rotation invariant spherical harmonics representation of 3D shape descriptors. In: Symposium on geometry processing, vol 6, pp 156&#8211;164"/>

    <meta name="citation_reference" content="Leino OT (2010) What erotic tetris has to teach serious games about being serious? Design implications of an experiential ontology of game content. In: Meaningful play 2010 conference proceedings, USA"/>

    <meta name="citation_reference" content="citation_journal_title=AI EDAM Artif Intell Eng Des Anal Manuf; citation_title=Ontology-based design information extraction and retrieval; citation_author=Z Li, K Karthik; citation_volume=21; citation_issue=2; citation_publication_date=2007; citation_pages=137-154; citation_id=CR27"/>

    <meta name="citation_reference" content="Liang H, Chang J, Kazmi IK, Zhang JJ, Jiao P (2015a) Puppet narrator: utilizing motion sensing technology in storytelling for young children. In: 2015 7th international conference on games and virtual worlds for serious applications (VS-Games). IEEE, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Liang H, Chang J, Deng S, Chen C, Tong R, Zhang JJ (2015b) Exploitation of novel multiplayer gesture-based interaction and virtual puppetry for digital storytelling to develop children&#8217;s narrative skills. In: Proceeding VRCAI &#8216;15 proceedings of the 14th ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp 63&#8211;72"/>

    <meta name="citation_reference" content="Mansouri H (2005) Using semantic descriptions for building and querying virtual environments. Licentiate&#8217;s thesis, Vrije Universiteit Brussel"/>

    <meta name="citation_reference" content="Mirbakhsh N, Didandeh A, Afsharchi M (2010) Concept learning games: the game of query and response. In: 2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology (WI-IAT), vol 2. IEEE, pp 234&#8211;238"/>

    <meta name="citation_reference" content="Ohbuchi R, Yamamoto A, Kobayashi J (2007) Learning semantic categories for 3D model retrieval. In: Proceedings of the international workshop on workshop on multimedia information retrieval. ACM, pp 31&#8211;40"/>

    <meta name="citation_reference" content="Ontology for Media Resources 1.0. 
                    http://www.w3.org/TR/mediaont-10/
                    
                  . Retrieved 18 March 2016"/>

    <meta name="citation_reference" content="Otto KA (2005) The semantics of multi-user virtual environments. In: Proceedings of the workshop towards semantic virtual environments"/>

    <meta name="citation_reference" content="Parkkila J, Hynninen T, Ikonen J, Porras J, Radulovic F (2015) Towards interoperability in video games. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, pp 26&#8211;29"/>

    <meta name="citation_reference" content="Ramaprasad A, Papagari SS (2009) Ontological design. In: Proceedings of the 4th international conference on design science research in information systems and technology. ACM, p 5"/>

    <meta name="citation_reference" content="Ruminski D, Walczak K (2014) Semantic contextual augmented reality environments. In: 2014 IEEE international symposium on mixed and augmented reality (ISMAR). IEEE, pp 401&#8211;404"/>

    <meta name="citation_reference" content="Shadow play. 
                    http://www.chinatraveldesigner.com/travel-guide/culture/chinese-opera/shadow-play.htm
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="SPARQL Query Language for RDF. 
                    https://www.w3.org/TR/rdf-sparql-query//
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="citation_journal_title=Data Knowl Eng; citation_title=Knowledge engineering: principles and methods; citation_author=R Studer, VR Benjamins, D Fensel; citation_volume=25; citation_issue=1; citation_publication_date=1998; citation_pages=161-197; citation_doi=10.1016/S0169-023X(97)00056-6; citation_id=CR40"/>

    <meta name="citation_reference" content="SWRL: a semantic web rule language. 
                    https://www.w3.org/Submission/SWRL/
                    
                  . Retrieved 21 March 2016"/>

    <meta name="citation_reference" content="citation_journal_title=Multimed Tools Appl; citation_title=A survey of content based 3D shape retrieval methods; citation_author=JW Tangelder, RC Veltkamp; citation_volume=39; citation_issue=3; citation_publication_date=2008; citation_pages=441-471; citation_doi=10.1007/s11042-007-0181-0; citation_id=CR42"/>

    <meta name="citation_reference" content="Teixeira JSF, S&#225; EDJV, Fernandes CT (2008) A taxonomy of educational games compatible with the LOM-IEEE data model. In: Proceedings of interdisciplinary studies in computer science SCIENTIA, pp 44&#8211;59"/>

    <meta name="citation_reference" content="Thalmann D, Farenc N, Boulic R (1999) Virtual human life simulation and database: why and how. In: 1999 International symposium on database applications in non-traditional environments, 1999. (DANTE&#8217;99) Proceedings. IEEE, pp 471&#8211;479"/>

    <meta name="citation_reference" content="Tutenel T, Smelik RM, Bidarra R, de Kraker KJ (2009) Using semantics to improve the design of game worlds. In: AIIDE"/>

    <meta name="citation_reference" content="World Wide Web Consortium (2012) OWL 2 web ontology language document overview"/>

    <meta name="citation_author" content="Hui Liang"/>

    <meta name="citation_author_institution" content="Zhengzhou University of Light Industry, Zhengzhou, China"/>

    <meta name="citation_author" content="Shujie Deng"/>

    <meta name="citation_author_institution" content="National Centre for Computer Animation, Poole, UK"/>

    <meta name="citation_author" content="Jian Chang"/>

    <meta name="citation_author_email" content="jchang@bournemouth.ac.uk"/>

    <meta name="citation_author_institution" content="National Centre for Computer Animation, Poole, UK"/>

    <meta name="citation_author" content="Jian Jun Zhang"/>

    <meta name="citation_author_institution" content="National Centre for Computer Animation, Poole, UK"/>

    <meta name="citation_author" content="Can Chen"/>

    <meta name="citation_author_institution" content="Changzhou University, Changzhou, China"/>

    <meta name="citation_author" content="Ruofeng Tong"/>

    <meta name="citation_author_institution" content="Zhejiang University, Hangzhou, China"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0333-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2018/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0333-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Semantic framework for interactive animation generation and its application in virtual shadow play performance"/>
        <meta property="og:description" content="Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of functional requirements (e.g. graphics, physics, AI, multimodal inputs and outputs, and massive data assets management). In this paper, a semantic framework is proposed to model the construction of interactive animation and promote animation assets reuse in a systematic and standardized way. As its ontological implementation, two domain-specific ontologies for the hand-gesture-based interaction and animation data repository have been developed in the context of Chinese traditional shadow play art. Finally, prototype of interactive Chinese shadow play performance system using deep motion sensor device is presented as the usage example."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Semantic framework for interactive animation generation and its application in virtual shadow play performance | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0333-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Semantic framework, Virtual interactive, Animation generation, Ontology, Hand-gesture-based interaction, Animation data management, Chinese shadow play","kwrd":["Semantic_framework","Virtual_interactive","Animation_generation","Ontology","Hand-gesture-based_interaction","Animation_data_management","Chinese_shadow_play"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"vgzm.415900-10.1007-s10055-018-0333-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0333-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=333;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0333-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Semantic framework for interactive animation generation and its application in virtual shadow play performance
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0333-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0333-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I. : VR and AR Serious Games</li>
    
    
        <li class="c-article-identifiers__item">
            <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
        </li>
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-01-21" itemprop="datePublished">21 January 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Semantic framework for interactive animation generation and its application in virtual shadow play performance</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hui-Liang" data-author-popup="auth-Hui-Liang">Hui Liang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhengzhou University of Light Industry" /><meta itemprop="address" content="0000 0001 0476 2801, grid.413080.e, Zhengzhou University of Light Industry, Zhengzhou, China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shujie-Deng" data-author-popup="auth-Shujie-Deng">Shujie Deng</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National Centre for Computer Animation" /><meta itemprop="address" content="National Centre for Computer Animation, Poole, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jian-Chang" data-author-popup="auth-Jian-Chang" data-corresp-id="c1">Jian Chang<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National Centre for Computer Animation" /><meta itemprop="address" content="National Centre for Computer Animation, Poole, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jian_Jun-Zhang" data-author-popup="auth-Jian_Jun-Zhang">Jian Jun Zhang</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="National Centre for Computer Animation" /><meta itemprop="address" content="National Centre for Computer Animation, Poole, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Can-Chen" data-author-popup="auth-Can-Chen">Can Chen</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Changzhou University" /><meta itemprop="address" content="grid.440673.2, Changzhou University, Changzhou, China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ruofeng-Tong" data-author-popup="auth-Ruofeng-Tong">Ruofeng Tong</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="0000 0004 1759 700X, grid.13402.34, Zhejiang University, Hangzhou, China" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 22</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">149</span>–<span itemprop="pageEnd">165</span>(<span data-test="article-publication-year">2018</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2448 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0333-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of functional requirements (e.g. graphics, physics, AI, multimodal inputs and outputs, and massive data assets management). In this paper, a semantic framework is proposed to model the construction of interactive animation and promote animation assets reuse in a systematic and standardized way. As its ontological implementation, two domain-specific ontologies for the hand-gesture-based interaction and animation data repository have been developed in the context of Chinese traditional shadow play art. Finally, prototype of interactive Chinese shadow play performance system using deep motion sensor device is presented as the usage example.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>As one of the most famous folk arts, Chinese traditional shadow play has a long history and contains rich cultural elements. Mainly made of donkey’s hide, flat shadow puppet consists of several parts and its joints are connected by threads. When playing, puppeteers use their hands to manipulate the flat shadow puppets through sticks attached on the puppets, and the shadows are projected on a simple rear-illuminated cloth screen to create moving pictures (Chen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chen F (2015) Visions for the masses: Chinese shadow plays from shaanxi and shanxi. Pacific Science 201" href="/article/10.1007/s10055-018-0333-8#ref-CR9" id="ref-link-section-d61655e447">2015</a>), as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig1">1</a>. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Chinese traditional shadow play performance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>However, this intricate operating skill is a higher entry barrier for performers, which requires long-time special training. This valuable and fascinating art form is becoming less known to the public, or even dying nowadays. New technologies are required urgently to give new life to the historic art forms.</p><p>Our motivation is to develop a novel method to generate interactive shadow puppetry animation for the purpose of cultural heritage preservation and dissemination. The ease of use and user-friendly virtual interaction would attract more people’s interest and engage young generations to generate shadow play animation without special trainings.</p><p>Developed for decades, as an essential technology, interactive animation is wildly used in the fields of 3D games and virtual reality, which offers users an immersive experience by providing natural and intuitive interactions with novel input/output devices. However, even though advanced theories, techniques and devices help a lot for generating animation, interactive animation production is still a tedious work and labour-intensive. Its inherent features of complexity—having to handle various aspects of functional requirements (e.g. graphics, physics, artificial intelligence, engineering, multimodal inputs and outputs)—throw out challenges to the current research.</p><p>One appealing solution is leveraging ontology to construct a systematic and standardized framework at a highly abstract and semantic level to provide a full view and understanding of the complex systematic procedure. Using structured terminology, ontological analysis could capture the core logic of complex system with natural language descriptions (Ramaprasad and Papagari <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ramaprasad A, Papagari SS (2009) Ontological design. In: Proceedings of the 4th international conference on design science research in information systems and technology. ACM, p 5" href="/article/10.1007/s10055-018-0333-8#ref-CR36" id="ref-link-section-d61655e483">2009</a>). In recent years, as a high-level conceptual specification, the notion of ontologies has become increasingly important in the computer animation-related domains, such as semantic 3D content representation (Flotyński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013a" title="Flotyński J, Walczak K (2013a) Conceptual semantic representation of 3d content. In: Business information systems workshops. Springer, Berlin, Heidelberg, pp 244–257" href="/article/10.1007/s10055-018-0333-8#ref-CR14" id="ref-link-section-d61655e486">2013a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Flotyński J, Walczak K (2013b) Semantic modelling of interactive 3d content. In: Proceedings of the 5th joint virtual reality conference. Eurographics association, pp 41–48" href="/article/10.1007/s10055-018-0333-8#ref-CR15" id="ref-link-section-d61655e489">b</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference c" title="Flotyński J, Walczak K (2013c) Semantic multi-layered design of interactive 3d presentations. In: 2013 federated conference on computer science and information systems (FedCSIS). IEEE, pp 541–548" href="/article/10.1007/s10055-018-0333-8#ref-CR16" id="ref-link-section-d61655e492">c</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Flotyński J, Walczak K (2014) Multi-platform semantic representation of interactive 3d content. In: Technological innovation for collective awareness systems. Springer, Berlin, Heidelberg, pp 63–72" href="/article/10.1007/s10055-018-0333-8#ref-CR17" id="ref-link-section-d61655e495">2014</a>), ontology-based 3D model retrieval (Li and Karthik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Li Z, Karthik K (2007) Ontology-based design information extraction and retrieval. AI EDAM Artif Intell Eng Des Anal Manuf 21(2):137–154" href="/article/10.1007/s10055-018-0333-8#ref-CR27" id="ref-link-section-d61655e499">2007</a>; Ohbuchi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ohbuchi R, Yamamoto A, Kobayashi J (2007) Learning semantic categories for 3D model retrieval. In: Proceedings of the international workshop on workshop on multimedia information retrieval. ACM, pp 31–40" href="/article/10.1007/s10055-018-0333-8#ref-CR32" id="ref-link-section-d61655e502">2007</a>) and their usage in virtual scene and game environment (e.g. modelling the semantic information of the virtual game environment).</p><p>Additionally, animation data management has become a focal attention to animation production and a popular issue of interests including animation data archiving and reuse in a more efficient and user-friendly way. Because of the phenomenal growth of the animation data, efficient data management, such as structured data presentation and searching for particular information, has become a daunting task. Traditional text-based or content-based media data retrieval introduces problems of the semantic gap between the low-level description and the high-level semantic interpretation of multimedia object. Ontology-based semantic retrieval, as an appropriate way to represent structured knowledge bases, enables data sharing, reuse and inference, and also advances in bridging the semantic gap (Li and Karthik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Li Z, Karthik K (2007) Ontology-based design information extraction and retrieval. AI EDAM Artif Intell Eng Des Anal Manuf 21(2):137–154" href="/article/10.1007/s10055-018-0333-8#ref-CR27" id="ref-link-section-d61655e508">2007</a>; Ohbuchi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ohbuchi R, Yamamoto A, Kobayashi J (2007) Learning semantic categories for 3D model retrieval. In: Proceedings of the international workshop on workshop on multimedia information retrieval. ACM, pp 31–40" href="/article/10.1007/s10055-018-0333-8#ref-CR32" id="ref-link-section-d61655e511">2007</a>). It provides us suitable technique on the animation assets management to facilitate data storage, organization, retrieval, reuse and repurposing.</p><p>For the purposes of heritage preservation, and also as a usage example, a prototype of hand-gesture-based interactive Chinese shadow play animation is generated based on the proposed framework and ontologies. It is expected that our method can provide guidance for building and rapid prototyping of diverse interactive animation/game applications.</p><p>Our work has the following contributions:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Semantic framework for interactive animation generation.</p>
                  </li>
                  <li>
                    <p>Domain-specific ontologies for interactive animation generation, including ontologies for hand-gesture-based interaction and animation data assets management.</p>
                  </li>
                  <li>
                    <p>Prototype of interactive Chinese shadow play performance system.</p>
                  </li>
                </ul>
<p>The rest of this paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec2">2</a> reviews the related works briefly. Semantic framework for interactive animation generation is proposed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec3">3</a>. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec6">4</a>, two domain-specific ontologies are implemented based on the framework in the context of the interactive Chinese shadow play performance. Prototype of virtual interactive shadow play performance system involving hand-gesture-based interaction and ontology-based animation data management is implemented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec9">5</a>. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec18">6</a> concludes the paper.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>According to Gruber (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Gruber TR (1993) A translation approach to portable ontology specifications. Knowl Acquis 5(2):199–220" href="/article/10.1007/s10055-018-0333-8#ref-CR21" id="ref-link-section-d61655e567">1993</a>), ontology is an explicit formal specification of a conceptualization, which is an effective tool to describe general concepts of entities as well as their properties, relationships and constraints (Grüninger and Fox <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Grüninger M, Fox MS (1995) Methodology for the design and evaluation of ontologies, Workshop on Basic Ontological Issues in Knowledge Sharing" href="/article/10.1007/s10055-018-0333-8#ref-CR22" id="ref-link-section-d61655e570">1995</a>). Also as a practical application in information science and technology, ontology provides the vocabulary to define terminology and the constraints required by different applications that cover various fields, from knowledge engineering to software engineering, taking the advantage of the establishment of common vocabulary and semantic term interpretation (Borst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Borst WN (1997) Construction of engineering ontologies for knowledge sharing and reuse, Universiteit Twente" href="/article/10.1007/s10055-018-0333-8#ref-CR7" id="ref-link-section-d61655e573">1997</a>; Studer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Studer R, Benjamins VR, Fensel D (1998) Knowledge engineering: principles and methods. Data Knowl Eng 25(1):161–197" href="/article/10.1007/s10055-018-0333-8#ref-CR40" id="ref-link-section-d61655e576">1998</a>).</p><p>Traditional model retrieval is based on keywords or the shape properties similarity (e.g. geometry and topology) (Tangelder and Veltkamp <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Tangelder JW, Veltkamp RC (2008) A survey of content based 3D shape retrieval methods. Multimed Tools Appl 39(3):441–471" href="/article/10.1007/s10055-018-0333-8#ref-CR42" id="ref-link-section-d61655e582">2008</a>; Kazhdan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kazhdan M, Funkhouser T, Rusinkiewicz S (2003) Rotation invariant spherical harmonics representation of 3D shape descriptors. In: Symposium on geometry processing, vol 6, pp 156–164" href="/article/10.1007/s10055-018-0333-8#ref-CR25" id="ref-link-section-d61655e585">2003</a>), in which only the textual or physical information is utilized. The semantic information is ignored, and the efficacy of retrieval has not been satisfactory, however. AIM@SHAPE Network of Excellence (Falcidieno <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Falcidieno B (2004) Aim@ shape project presentation. In: Shape modeling applications, proceedings. IEEE, p 329" href="/article/10.1007/s10055-018-0333-8#ref-CR12" id="ref-link-section-d61655e588">2004</a>) made first step of using semantics approach to describe and search 3D models to facilitate reuse of the animation contents (Falcidieno et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Falcidieno B, Spagnuolo M, Alliez P, Quak E, Vavalis M, Houstis C (2004) Towards the semantics of digital shapes: the AIM@SHAPE approach. In: EWIMT" href="/article/10.1007/s10055-018-0333-8#ref-CR13" id="ref-link-section-d61655e591">2004</a>). There are similar semantic search engines including 3D model Search Engine (Princeton University) (Funkhouser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Funkhouser T, Min P, Kazhdan M, Chen J, Halderman A, Dobkin D et al (2003) A search engine for 3D models. ACM Trans Graph (TOG) 22(1):83–105" href="/article/10.1007/s10055-018-0333-8#ref-CR18" id="ref-link-section-d61655e594">2003</a>) and Google 3D Warehouse (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Google 3D Warehouse. &#xA;                    https://3dwarehouse.sketchup.com/&#xA;                    &#xA;                  . Retrieved 21 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR20" id="ref-link-section-d61655e598">2016</a>). To describe media data such as images, audio, video and 3D objects, ontological solutions, such as the Ontology for Media Resources (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Ontology for Media Resources 1.0. &#xA;                    http://www.w3.org/TR/mediaont-10/&#xA;                    &#xA;                  . Retrieved 18 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR33" id="ref-link-section-d61655e601">2016</a>) and the Core Ontology for Multimedia (COMM) (Arndt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Arndt R, Troncy R, Staab S, Hardman L (2009) Comm: a core ontology for multimediaannotation. In: Handbook on ontologies. Springer, Berlin, Heidelberg, pp 403–421" href="/article/10.1007/s10055-018-0333-8#ref-CR4" id="ref-link-section-d61655e604">2009</a>), are devised on the basis of standard models for data interchange on the web (e.g. RDF, RDFS, OWL and MPEG-7). To facilitate 3D content accessing, ontology-based approaches are used for semantic modelling of the different 3D models (e.g. geometry, appearance and behaviour). Ontology for virtual human was proposed in Gutiérrez et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Gutiérrez M, García-Rojas A, Thalmann D, Vexo F, Moccozet L, Magnenat-Thalmann N, Mortara M, Spagnuolo M (2007) An ontology of virtual humans: incorporating semantics into human shapes. Vis Comput 23(3):207–218" href="/article/10.1007/s10055-018-0333-8#ref-CR24" id="ref-link-section-d61655e607">2007</a>), which incorporated semantics into the description of human shapes.</p><p>Another application is to model semantic information of virtual environment. Unlike traditional virtual environments design, semantic virtual environments maintain richer semantic information, in which the ontological model provides an abstract and semantic description that is adequate for computer processing (Otto <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Otto KA (2005) The semantics of multi-user virtual environments. In: Proceedings of the workshop towards semantic virtual environments" href="/article/10.1007/s10055-018-0333-8#ref-CR34" id="ref-link-section-d61655e613">2005</a>). Different aspects of the conceptual representation of the virtual world are modelled by ontologies as high-level and semantic description, including the environment structures, entities’ behaviour and domain knowledge (Gutierrez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gutierrez M, Thalmann D, Vexo F (2005) Semantic virtual environments with adaptive multimodal interfaces. In: Multimedia modelling conference, 2005. MMM 2005. Proceedings of the 11th international. IEEE, pp 277–283" href="/article/10.1007/s10055-018-0333-8#ref-CR23" id="ref-link-section-d61655e616">2005</a>). The concept of annotated environment together with structured representations of its contents and purposes was firstly proposed by Thalmann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Thalmann D, Farenc N, Boulic R (1999) Virtual human life simulation and database: why and how. In: 1999 International symposium on database applications in non-traditional environments, 1999. (DANTE’99) Proceedings. IEEE, pp 471–479" href="/article/10.1007/s10055-018-0333-8#ref-CR44" id="ref-link-section-d61655e619">1999</a>). An ontology-based framework with a cognitive middle layer and environment managed semantic concepts were presented by Chang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Chang PHM, Chien YH, Kao ECC, Soo VW (2005) A knowledge based scenario framework to support intelligent planning characters. In: Intelligent virtual agents. Springer, Berlin, Heidelberg, pp 134–145" href="/article/10.1007/s10055-018-0333-8#ref-CR8" id="ref-link-section-d61655e622">2005</a>). Studies (Bilasco et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bilasco IM, Gensel J, Villanova-Oliver M, Martin H (2005) On indexing of 3D scenes using MPEG-7. In: Proceedings of the 13th annual ACM international conference on multimedia. ACM, pp 471–474" href="/article/10.1007/s10055-018-0333-8#ref-CR5" id="ref-link-section-d61655e625">2005</a>; Mansouri <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Mansouri H (2005) Using semantic descriptions for building and querying virtual environments. Licentiate’s thesis, Vrije Universiteit Brussel" href="/article/10.1007/s10055-018-0333-8#ref-CR30" id="ref-link-section-d61655e629">2005</a>) described the semantics of a 3D scene focusing on the high-level description of objects or the composition of existing objects. There are also some ontological research works using semantic information to model the interaction of virtual environments, for example NiMMiT (Notation for MultiModal interaction Techniques), which is a diagram-based notation describing multimodal interaction (De Boeck et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="De Boeck J, Raymaekers C, Coninx K (2006) Comparing NiMMiT and data-driven notations for describing multimodal interaction. In: Task models and diagrams for users interface design. Springer, Berlin, Heidelberg, pp 217–229" href="/article/10.1007/s10055-018-0333-8#ref-CR10" id="ref-link-section-d61655e632">2006</a>). A contextual augmented reality environment was proposed in Ruminski and Walczak (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ruminski D, Walczak K (2014) Semantic contextual augmented reality environments. In: 2014 IEEE international symposium on mixed and augmented reality (ISMAR). IEEE, pp 401–404" href="/article/10.1007/s10055-018-0333-8#ref-CR37" id="ref-link-section-d61655e635">2014</a>), which consists of three elements: the trackables, content objects and interface. With the model of SCM (Semantic Content Model) designed for semantic representation of interactive 3D contents, environments were represented at different levels of abstraction: conceptual level, concrete level and platform level (Flotyński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Flotyński J, Walczak K (2013a) Conceptual semantic representation of 3d content. In: Business information systems workshops. Springer, Berlin, Heidelberg, pp 244–257" href="/article/10.1007/s10055-018-0333-8#ref-CR14" id="ref-link-section-d61655e638">2013</a>).</p><p>There are some ontological applications in the domain of computer games: for example, a game ontology that was developed based on game theory to facilitate interactions in multiagent system (Mirbakhsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Mirbakhsh N, Didandeh A, Afsharchi M (2010) Concept learning games: the game of query and response. In: 2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology (WI-IAT), vol 2. IEEE, pp 234–238" href="/article/10.1007/s10055-018-0333-8#ref-CR31" id="ref-link-section-d61655e644">2010</a>). In Semantic Class Library designed for semantic 3D game virtual environment (Tutenel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Tutenel T, Smelik RM, Bidarra R, de Kraker KJ (2009) Using semantics to improve the design of game worlds. In: AIIDE" href="/article/10.1007/s10055-018-0333-8#ref-CR45" id="ref-link-section-d61655e647">2009</a>), 3D model can be classified and provide with additional semantic information, such as physical attributes, functional information besides 3D representations. Many research works have been conducted to develop game-related ontologies, e.g. developing game content ontology to describe games characteristics, properties and design process (Leino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Leino OT (2010) What erotic tetris has to teach serious games about being serious? Design implications of an experiential ontology of game content. In: Meaningful play 2010 conference proceedings, USA" href="/article/10.1007/s10055-018-0333-8#ref-CR26" id="ref-link-section-d61655e650">2010</a>; Teixeira et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Teixeira JSF, Sá EDJV, Fernandes CT (2008) A taxonomy of educational games compatible with the LOM-IEEE data model. In: Proceedings of interdisciplinary studies in computer science SCIENTIA, pp 44–59" href="/article/10.1007/s10055-018-0333-8#ref-CR43" id="ref-link-section-d61655e653">2008</a>; Dubin and Jett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Dubin D, Jett J (2015) An ontological framework for describing games. In: Proceedings of the 15th ACM/IEEE-CE on joint conference on digital libraries. ACM, pp 165–168" href="/article/10.1007/s10055-018-0333-8#ref-CR11" id="ref-link-section-d61655e656">2015</a>), using ontological framework as the guide to develop and employ game-based training (Anthony et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Anthony M, Chandler T, Heiser E (2009) A framework for promoting high cognitive performance in instructional games. In: Interservice/industry training simulation and education conference (I/ITSEC), Orlando, FL" href="/article/10.1007/s10055-018-0333-8#ref-CR2" id="ref-link-section-d61655e660">2009</a>), using an ontology and a set of rules to represent game logics (BinSubaih et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="BinSubaih A, Maddock S, Romano D (2005) Game logic portability. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, pp 458–461" href="/article/10.1007/s10055-018-0333-8#ref-CR6" id="ref-link-section-d61655e663">2005</a>), using ontologies as a solution to enable transfer of meaningful game information between different video games (Parkkila et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Parkkila J, Hynninen T, Ikonen J, Porras J, Radulovic F (2015) Towards interoperability in video games. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, pp 26–29" href="/article/10.1007/s10055-018-0333-8#ref-CR35" id="ref-link-section-d61655e666">2015</a>), using defined learning game ontology as evaluation methodology of serious games for teachers/trainers to choose and retrieve learning games (Ghannem <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Ghannem A (2014) Characterization of serious games guided by the educational objectives. In: Proceedings of the second international conference on technological ecosystems for enhancing multiculturality. ACM, pp. 227–233" href="/article/10.1007/s10055-018-0333-8#ref-CR19" id="ref-link-section-d61655e669">2014</a>).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Semantic framework for interactive animation</h2><div class="c-article-section__content" id="Sec3-content"><p>As a complex system, interactive animation production involves various functional requirements, including interaction methods and animation data management. To provide a clear vision of this complex process, a three-layer semantic framework is proposed as a formal semantic foundation as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig2">2</a>. Constructed at a highly abstracted level, this semantic framework can be easily applied to different VR applications for various purposes and is expected to facilitate the conceptual design process.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Architecture of semantic framework for interactive animation generation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>At the top abstract layer, the semantic framework provides a systematic and intuitive description with larger granularity for the design of animation generation. As a complex system, multimodal function combinations are involved in the animation procedure, such as UI, animation data management, game design, story scripts and system integration. In this research, two key components—UI and data repository, are our main concerns, instead of including all the related techniques. These components involve two most important aspects of interactive animation generation: the virtual interaction and animation data assets accessing. The domain layer is the ontological implementation of the upper abstract layer, at which the domain-specific ontologies are further defined to provide knowledge support for the development of particular applications. And then, using the knowledge provided by the developed domain-specific ontologies, various applications, such as interactive animations, games and animation database, could be finally developed at the generation layer.</p><p>In the component of UI, player interacts with computer system, which involves the use of various input/output devices and interactive modes, such as monitor screen, keyboard, mouse, touch screen, haptic device, motion sensor and tracking device. Animation data repository component provides data support, in which various animation data resource is maintained and managed for the purpose of enhancing reusability. In Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec6">4</a>, we will discuss its implementation by developing two domain-specific ontologies, which is mapped to the domain layer, and how the proposed framework can be employed to generate interactive Chinese shadow play animation, which is mapped to the generation layer.</p><h3 class="c-article__sub-heading" id="Sec4">UI component</h3><p>As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig3">3</a>, the UI component contains a set of structural concepts, which could be decomposed into three levels of semantic abstraction. Due to the multidisciplinary nature of UI, the interaction between the user and the computer system involves the use of various input/output devices and interactive modes, e.g. touch screen, haptic device and head-mounted devices. The method for designing and implementing novel computer interface and function usability finally provided to users is crucial to facilitate the development of virtual reality.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>UI component</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec5">Animation data repository component</h3><p>As the data resource for animation creation, the digital assets are multimodal involving audio, video, 2D image/textual, 3D models, motion files, scene files, etc., and they vary a lot. Targeting different topics and depending on the specific application domains, the contents of the digital data repositories are also different. From a semantic perspective, the repository could be abstracted and analysed with several sublayers. Taking 3D contents for example, they are interpreted with four layers in this paper: geometry, structure, appearance and logic, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig4">4</a>a. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig4">4</a>b presents an example of animation content, which is a digital character of the Chinese traditional shadow puppetry. As a higher level of expressiveness, it represents a brave warrior in the play of “The Emperor and the Assassin” from the aspect of the logic layer. The shape can be considered as a structure composed by different parts of shadow puppet presenting geometric information. Added with the appearance information, such as colour, pattern or accessories, a complex character is created carrying rich culture contents.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Animation data repository component</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>The conceptual framework can systemize and standardize the procedure of interactive game/animation synthesis and promote system integration in a semantic representation. The two components are abstract concepts of interactive animation generation. Hence, the framework enables conceptual design for various interactive animation applications.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Development of domain-specific ontologies</h2><div class="c-article-section__content" id="Sec6-content"><p>Our framework sets a semantic foundation to formally model the interactive animation production from the abstract high level. This section focuses on investigating the cases in which we apply this semantic framework to guide the implementation of the domain-specified ontologies. As a practical example, our method is presented in the context of a 2D Chinese traditional shadow puppetry performance scenario, in which players manipulate virtual characters to produce animation with hand gestures utilizing depth motion sensing device—the leap motion controller.</p><p>Using the Web Ontology Language (OWL <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="World Wide Web Consortium (2012) OWL 2 web ontology language document overview" href="/article/10.1007/s10055-018-0333-8#ref-CR46" id="ref-link-section-d61655e786">2012</a>) together with the ontology editor and knowledge framework-Protégé (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="A free, open-source ontology editor and framework for building intelligent systems. &#xA;                    http://protege.stanford.edu/&#xA;                    &#xA;                  . Retrieved 21 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR1" id="ref-link-section-d61655e789">2016</a>), the proposed semantic framework has been implemented as two domain-specified ontologies: Hand-Gesture-Based Interaction Ontology (HGBIO) mapped to the UI component, and Digital Chinese Shadow Puppetry Assets Ontology (DCSPAO) mapped to the animation data repository component. We take advantage of the OWL ontology structure in order to incorporate SWRL (semantic web rule language) (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="SWRL: a semantic web rule language. &#xA;                    https://www.w3.org/Submission/SWRL/&#xA;                    &#xA;                  . Retrieved 21 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR41" id="ref-link-section-d61655e792">2016</a>) rules, which can be visualized using SPARQL (a recursive acronym for SPARQL protocol and RDF query language) (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="SPARQL Query Language for RDF. &#xA;                    https://www.w3.org/TR/rdf-sparql-query//&#xA;                    &#xA;                  . Retrieved 21 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR39" id="ref-link-section-d61655e795">2016</a>) queries for knowledge accessing.</p><h3 class="c-article__sub-heading" id="Sec7">Hand-Gesture-Based Interaction Ontology (HGBIO)</h3><p>Chinese traditional shadow puppet is made of several flat parts. Each part is linked together by joints. Thin sticks are attached to the key parts separately for controlling. Holding sticks in hands, puppeteer uses hand gestures to control puppet’s movement and performs the play.</p><p>In our method, we adopt the traditional puppet design mode. Each digital puppet consists of ten parts linked by nine joints. Digital puppet’s motion is controlled by player’s hand movement and gesture using motion sensor device. During playing, player’s hand position is mapped to the position of puppet in virtual environment through motion sensor after mapping. The animation is generated through a hand-gesture-based interaction. The detail of the motion control is described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0333-8#Sec9">5</a> “Implementation”.</p><p>The diagram of a part of the HGBIO is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig5">5</a>a. The ontology defined beneath the root of every OWL ontology “owl:Thing” has three subclasses: “Human”, “Device” and “Method”. Each subclass (ancestor) contains various subclasses (descendant) of their own. Each one of these descendants contains individuals for the assignment of characteristics. A number of object properties are designed to give the ontology fundamental relational functionalities. The set of data type properties are provided to link OWL individuals with typical data values. The structure of hand gesture ontology is organized in a hierarchy consisting of three levels. The lower level presents motion tracking data accessed from the leap motion sensor controller including information of frames, list of detected hands and list of pointables (such as fingers or finger-like tools). Each frame contains the measured positions and the following information of the object detected by the controller. The device provides model of hands with five fingers of bone structures, and each finger is made of four bone joints. By using the positions and rotations information of each finger bone, we could design specific interactions. At the middle level, by calculating hands movement speed, the angle of movement and the orientation angle, we can define simple hand gestures and describe the hand movement. For example, “Leftward” represents “Fly/run to the left”, “Rightward” represents “Fly/run to the right”, “Upward” represents “Fly/jump up”, “Downward” represents “Fly/crouch down”, and “Stay still” represents “Hover in the air/stay unmoved”. At the top level, we can develop more complex hand gestures which are the combination of simple hand gestures. These compounded gestures could meet player’s further needs by depicting sophisticated interactive semantic meaning. For example, “Hand open to close” represents “Grasp”, “Hand close to open” represents “Drop”, and “rapid multiple finger taps” represents “Swipe wings”.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Diagram of Hand-Gesture-Based Interaction Ontology (HGBIO)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig5">5</a>b shows a usage example of the defined HGBIO. The visualization displays the relationships among the selected class and other classes in its neighbourhood.</p><p>“Player” is a subclass of “Human”, which has an instance a young boy, who is using his hand gesture to control the movement of an avatar in the virtual environment through leap motion controller. Through the object property “is operated by”, the leap motion controller is connected with player to deliver the semantic information: “Device is operated by play for interaction”.</p><p>There are a range of interactive devices that enable user interactions using hand gestures, such as leap motion and kinect. In this diagram, leap motion is an instance of class “Motion Sensor” which is a subclass of “Device”. The essential attribute of leap motion is to provide data source to the class “Method”, which is described by the object property “is data source of” that links two classes.</p><p>The sensor data provided by leap motion contain a diversity of information about hands and fingers including id, direction, position, orientation, relative information and status. “Method” is the core of the system, which has two subclasses in our case: “Tracking” and “Recognizing”. Hand Movement is an instance of “Tracking”, which is responsible for processing the player’s hand movement and mapping the relative hand position to the movement of the avatar in virtual world. Recognizing function is in charge of identifying the pattern of hand movement and gestures implicating player’s intention. Hand gesture is an instance of “Recognizing”, which provides a natural and intuitive gestural interaction manner. Once a recognizable hand gesture is detected, recognizing function will trigger a pre-recorded animation or event (e.g. avatar grips a pebble or flaps wings).</p><h3 class="c-article__sub-heading" id="Sec8">Digital Chinese Shadow Puppetry Assets Ontology (DCSPAO)</h3><p>Traditional Chinese shadow play has distinctive folk style and artistic characteristics. As the implementation of the domain layer of the proposed semantic framework, a domain-specified ontology is developed for traditional Chinese shadow puppetry; furthermore, a shadow puppetry animation data repository is also used as the digital assets for the animation generation.</p><p>The diagram of a part of the DCSPAO is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig6">6</a>a. Defined beneath the root of every OWL ontology “owl:Thing”, DCSPAO has several subclasses including “Role”, “Music”, “Prop” and “Scene”. And each of these subclasses has the superclass at the same time, also its descendant. For example, the human characters in traditional Chinese shadow puppetry play fall into four major roles—Sheng (main male role), Dan (female role), Jing (painted-face and forceful male role) and Chou (clown male role), which leads to four subclasses of the class “Role” correspondingly (Shadow Play <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Shadow play. &#xA;                    http://www.chinatraveldesigner.com/travel-guide/culture/chinese-opera/shadow-play.htm&#xA;                    &#xA;                  . Retrieved 21 March 2016" href="/article/10.1007/s10055-018-0333-8#ref-CR38" id="ref-link-section-d61655e865">2016</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Diagram of Digital Chinese Shadow Puppetry Assets Ontology (DCSPAO)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>A set of object properties are added to create relationship among classes by linking relevant objects, such as “has Prop of”, “has Scene of” and “has Music of”. We also propose data properties which provide attribute descriptions for classes in the DCSPAO, including “name”, “age”, “rank” and “personality”.</p><p>As an example, an ontological description of the character “Jing Ke” is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig6">6</a>b, who is one of the most famous generals in ancient China and widely respected as a symbolic cultural icon of loyalty and righteousness in general worship. “Laosheng” (decent middle-/old-aged man with beard) is a subclass of “Sheng” (main male role). As its instance, “Jing Ke” has a set of inherited properties including name, age and personality. As a most famous character in shadow play, “Jing Ke” has unique personality and character symbolism, which is normally presented by particular props (e.g. his trademark weapon) and other dramatic elements. This specified relation between the class “Role” and the class “Prop” is determined by the object property defined in the domain ontology. For instance, through the object property “has Prop of”, “Jing Ke” and his pole weapon named “Yu Chang” (an instance of the subclass of “Weapon”) are connected, which is the same as the relationship between “Jing Ke” and his war horse (an instance of the subclass of “Animal” which belongs to other characters except human roles).</p><p>Since “Jing Ke” is a significant role in the classic shadow play “The Emperor and the Assassin”, object property “has Scene of” can assign a particular instance (“Xian Yan Palace”) of the class “Scene” to him, which implicates the significant battlefield in the play. As a key role in shadow play, music creates particular atmosphere and shape characters. Object property “has Music of” can be used to reflect the bravery personality by connecting the unique melody “Splendour Allegro” with “Jing Ke”.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Implementation of the prototype of virtual interactive Chinese shadow play performance system</h2><div class="c-article-section__content" id="Sec9-content"><p>To demonstrate how the proposed semantic framework and developed ontologies can be employed to fertilize the design of interactive animation generation, the prototype of an interactive Chinese traditional shadow play performance system is generated, which enables players to interact with virtual shadow puppetry using hand gestural control intuitively.</p><p>As the ontological implementation of UI component in the semantic framework, HGBIO has been used as a guidance to develop a novel hand-gesture-based interaction method. By utilizing depth motion sensing technology, the method tracks and recognizes user’s hand and finger motions as input, which enables the users to use hand gestures to control the movement and manipulate animation of the digital shadow puppetry with natural interaction/control and immersive experience. DCSPAO has been used to implement the animation data repository component of the semantic framework. Using domain-specific information extraction, inference and rules to exploit the semantic metadata, the repository supports ontology-based retrieval, which improves searching performance by recognizing the animator’s intent and contextual meaning of the digital assets in the context of Chinese traditional shadow play. More relevant results can be generated to meet the user’s needs and also provide the capability of data reuse. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig7">7</a> gives an overview of the prototype architecture which has the following components.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Prototype architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec10">Hand-gesture-based interaction</h3><p>To provide natural interactive experience for the players, we design and develop a novel hand-gesture-based interaction by utilizing depth motion sensing technology as the interaction method, which allows them to use hand gestures to play with virtual shadow puppetries and manipulate them to interact with virtual items in virtual environment. Leap motion controller is used in our system as the interaction device to track hand gestures, which can provide a high-fidelity finger tracking through an infrared depth sensor. We utilize the leap motion SDK provided by the Leap Motion Co. as the API to access the motion data of hands and fingers from the device. UI module mainly includes three submodules: input data processing, movement control and output. For more details of hands movement control mechanism and gesture recognizing, please refer (Liang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015a" title="Liang H, Chang J, Kazmi IK, Zhang JJ, Jiao P (2015a) Puppet narrator: utilizing motion sensing technology in storytelling for young children. In: 2015 7th international conference on games and virtual worlds for serious applications (VS-Games). IEEE, pp 1–8" href="/article/10.1007/s10055-018-0333-8#ref-CR28" id="ref-link-section-d61655e939">2015a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Liang H, Chang J, Deng S, Chen C, Tong R, Zhang JJ (2015b) Exploitation of novel multiplayer gesture-based interaction and virtual puppetry for digital storytelling to develop children’s narrative skills. In: Proceeding VRCAI ‘15 proceedings of the 14th ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp 63–72" href="/article/10.1007/s10055-018-0333-8#ref-CR29" id="ref-link-section-d61655e942">b</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Input data processing</h4><p>As illustrated by the UI component in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig7">7</a>, once the connection between the leap motion controller and our system is established, the deep images of user’s hand will be achieved. The sensor data provided by leap motion controller contain a diversity of information about hands and pointables [such as fingers or finger-like tools defined by Leap Motion Co. (Gutierrez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gutierrez M, Thalmann D, Vexo F (2005) Semantic virtual environments with adaptive multimodal interfaces. In: Multimedia modelling conference, 2005. MMM 2005. Proceedings of the 11th international. IEEE, pp 277–283" href="/article/10.1007/s10055-018-0333-8#ref-CR23" id="ref-link-section-d61655e955">2005</a>)], in the virtual scene, which is updated by frame and can be represented as follows:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$SensorData = \left\langle {FR, H, P, T} \right\rangle$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(FR\)</span> is frame rate; <span class="mathjax-tex">\(H\)</span> represents the set of hands detected and <span class="mathjax-tex">\(P\)</span> represents the set of pointables; and <span class="mathjax-tex">\(T\)</span> is timestamp. Hand data <i>H</i> mainly contain the hand id, direction and different value about the palm position and status:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$H = \left\langle {id,dir,palmInf} \right\rangle$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>And pointables data <i>P</i> include its id, direction and position information relative to the hand:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$${\text{P}} = \left\langle {id,dir,handid,positionInf} \right\rangle$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Motion control</h4><p>To control the movement of the shadow puppetry, the most direct way is converting the translation of their hands position in leap motion coordinate system to the position of puppet in virtual environment. Considering the different scale between these two workspaces, a proper transformation matrix should be involved as a scaling into the coordinate translation. Once players’ hands is not recognized by the leap motion device, puppet will keep its position in the last frame and then resume the movement immediately when hands can be detected again in the following frames. The virtual puppet’s position in virtual workspace will move accordingly as shown in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0333-8#Equ4">4</a>):</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$C_{avatar} = T \cdot S \cdot R_{hand} + C_{avatar}^{{\prime }}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <span class="mathjax-tex">\(C_{avatar}\)</span> represents the virtual puppet’s position coordinate in current frame, <span class="mathjax-tex">\(R_{hand}\)</span> is the player’s relative hand movement in real world, i.e. the displacement of hand positions in current frame and the previous frame, which can be obtained in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0333-8#Equ1">1</a>), <span class="mathjax-tex">\(T\)</span> is the transformation matrix between the player’s workspace and the avatar’s coordinate system, <span class="mathjax-tex">\(S\)</span> is the scaling matrix, and <span class="mathjax-tex">\(C_{avatar}^{{\prime }}\)</span> represents avatar’s position in the previous frame.</p><p>How to design hand gestures for the purposes of ease of use and intuition is of particular importance for interaction experience, which involves various considerations, such as the natural language, players’ physical and memorial limitation, recognition accuracy of the input device and the assumption of the task in storyline. To provide a natural and intuitive interaction, we have designed our hand gestures vocabulary elaborately. There are in total two sets of hand gestures (“Steering” gestures and “Trigger” gestures), and each set is attached with one hand: “Steering” gestures are assigned for navigation tasks, and “Trigger” gestures are allotted for action performance.</p><p>To control virtual puppets’ movement in the virtual environment, players just need to move their stretched palms intuitively. The virtual puppet’s position in virtual workspace will move accordingly as shown in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0333-8#Equ4">4</a>). This task is designed to be accomplished by a set of “Steering” gestures performed by one hand (the red hand), as illustrated in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0333-8#Tab1">1</a>. </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Pre-defined “Steering” hand gestures</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0333-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Besides the navigation, virtual puppets’ actions are controlled by another set of pre-defined “Trigger” gestures performed by another hand (the blue hand), as illustrated in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0333-8#Tab2">2</a>. We have designed characteristic actions for the Chinese traditional shadow puppetries, including walking, running and singing, and these actions can be triggered and performed corresponding to players’ hand gestures. To provide real-time response and live visualization, these actions are integrated as a set of pre-recorded animation clips. Furthermore, these pre-recorded animations associated with puppets’ particular actions can only be triggered by specific events of hand movement and hand gestures.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Part of pre-defined “Trigger” hand gestures</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0333-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec13">Ontology-based animation data assets retrieval</h3><p>As one of the most important aspects of the interactive animation generation, animation data assets is a mixed-type animation database of digitalized traditional Chinese shadow puppets, which involves domain-specified knowledge including classification of performers and roles, visual/aural performance elements and repertoire, and depicts model’s high-level concept. Two key submodules: Assets repository and retrieval systems are involved to provide animation content support. Using domain-specific information extraction to exploit the semantic metadata, the data repository supports data retrieval in a more intelligent way. Through automated reasoning and inferencing with ontology-based annotations and rules, ontology-based retrieval makes possible an intelligent and productive manipulation of the digital assets in the context of Chinese traditional shadow play, which improves the retrieval effectiveness by recognizing the animator’s semantic intent and contextual meaning.</p><p>Let us take the shadow puppet character “Jing Ke” as an instance to show all his relationship to other entities. “Jing Ke” is the hero of a famous Chinese traditional shadow play titled “The Emperor and the Assassin”. As a brave fighter, “Jing Ke” attempted to assassinate the king of Kingdom Qin-“Qin Shi Huang” to avert the imminent conquest of his home country by Kingdom Qin. After intense fighting in the palace, however, the assassination attempt failed, and “Jing Ke” was killed on the spot.</p><p>Let us take the retrieval of the character “Jing Ke” for example. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig8">8</a>a, using traditional keywords marching search method, only the character “Jing Ke” itself was retrieved. However, for the character designer of the play, there is plenty of animation data existing to be referred to and reused as digital assets in the shadow puppet repository. If he wants to review all the subjects and objects that are related to the character “Jing Ke”, through the object properties which are used to link to classes or individuals of another, such as “has Prop of”, “has Scene of” and “has Music of”, related entities will be retrieved after referring and reasoning, e.g. his weapon and the scene of the play. As the result, these retrieved related objects could provide more reasonable relevant feedback and wider choice to animators. From the result of ontological retrieval, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig8">8</a>b, we can find that not only the digital character “Jing Ke” itself is retrieved, other useful assets related to the hero are also found from the knowledge base, which may satisfy the artist’s potential requirements, such as the props affiliated to the play and his weapon used in the story.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Interface of retrieval system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Concept process of semantic reasoning is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig9">9</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Sematic reasoning</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h3 class="c-article__sub-heading" id="Sec14">Result</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">System prototype</h4><p>Finally, the prototype of the interactive traditional Chinese shadow play performance system has been generated. Shadow puppets’ intensive motion is animated by player’s hand movement and gestures through motion sensor device, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig10">10</a>a. The performance involves a set of hand gestures, which can only be successfully performed by specific hand movement and touch off puppets’ related actions. Then, the pre-defined animations associated with puppets’ particular actions can be successfully triggered. This prototype of a novel interactive Chinese traditional shadow play performance system provides players natural interaction/control and immersive experience.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Scenario of digital shadow play “The Emperor and the Assassin”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<p>Screenshots of generated digital shadow plays “The Emperor and the Assassin” and “Journey to the West” performed by an 8-year-old boy are shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig10">10</a>b and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig11">11</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Scenario of digital shadow play “Journey to the West”</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
<h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">User test</h4><p>To get qualitative feedback of our method, the feasibility verification has been conducted including both the user experience test of the ontology-based animation assets retrieval and interaction comfort test of the prototype of the interactive shadow play system. We invited 7 target users to join the tests involving three animation designers and four 7–9-year-old young children (mean 8 years and 3 months old) with permission from their parents who were informed about the nature of the study and its purpose. Animation designers have good experience with computer animation design but little cognition of semantics or ontology knowledge. Seven participants were divided into two groups to have two qualitative tests: the first group has 3 ontology-based assets retrieval tester (animation designers), and 4 young children took part in the second group for the interaction comfort test. After detailing the operation of system and 15-min training, two tests began. Qualitative tests were designed as follows:</p><p>Qualitative Test 1: It describes the feasibility of the ontology-based digital assets retrieval. We provided three participants with a list of animation data including different type of digital assets in the context of Chinese traditional shadow play, e.g. “Role”, “Music”, “Prop” and “Scene”. Each of the animation designers was asked to use ontology-based retrieval to search 5 assets randomly and then make comparison with traditional keywords marching search method.</p><p>Qualitative Test 2: After the learning process, following the provided key story points, four children got engaged with the digital shadow puppets with great interests. Qualitative test 2 achieves testers’ intuitive feelings on the interactive shadow play system through questionnaires. There are five criterions given—ease of use, handiness, naturalness, freedom of movement and effectiveness to describe the feelings with a scale of 1–5, indicating “Poor satisfaction” to “Excellently satisfied”, and the higher the scores are, the more positive the feedback is.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Qualitative feedback</h4><p>Qualitative Test 1: The three animation designers have input totally 15 different keywords to test the ontology-based retrieval and made comparison with traditional keyword-based marching method. All the three testers provided rather positive feedback. Not only the target data itself were retrieved, other useful assets related to the target were also found by conducting semantic reasoning. The relative wealth of retrieved material animation resources greatly satisfied the designers’ potential creation requirements and also gave designers more creative inspiration.</p><p>Qualitative Test 2: Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0333-8#Fig12">12</a> shows the mean score of each criterion about the interaction comfort. The ease of use, naturalness, freedom of movement, handiness illustrated positive feedback with score around four, especially the ease of use and naturalness. This is possibly due to the natural hand-gesture-based interaction which was elaborately designed for young players and provided intuitive interactive experience. However, the criterion of effectiveness received the lowest score which possibly was induced by the limitation of the device recognition accuracy. For example, folding the fingers or overlapping the hands could lead to depth sensor’s false detection. Additionally, player’s hand position relative to the location of the device is also crucial. Any less perfect position may affect the accuracy of hand tracking. From children’s feedback, we also have observed that it is relatively easy for the player to use one hand to control one character. The novel interaction provides player a more immersive and interactive experience and enable him perform the story in intuitive and natural way. But manipulating two puppets simultaneously, each by one hand, are rather challenging. This kind of operation demands bimanual coordination skill. We hope this appealing interactive animation generation method may help people become intimate with their cultural heritage and promote traditional arts and culture among the young generation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0333-8/MediaObjects/10055_2018_333_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Mean scores for each criterion of interaction comfort test (1 Poor, 2 Fair, 3 Average, 4 Good, 5 Excellent)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0333-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
</div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Discussion and conclusion</h2><div class="c-article-section__content" id="Sec18-content"><p>The construction process of interactive animation is complex because there are too many low-level details involved, for instance multimedia animation data management and data reuse, user-friendly interaction and integration. To provide a systematic and standardized semantic description, a semantic framework is constructed at an abstract high level in this paper. The ontological implementation based on the framework defined two domain-specified ontologies (Hand-Gesture-Based Interaction Ontology and Digital Chinese Shadow Puppetry Assets Ontology) to formalize the multimodal interaction method and the construction of the animation data assets repository, which finally leads to an interactive puppetry animation as a usage example.</p><p>Ontology is the core element of our research, by using which we provided a concept depiction for the complex animation production process, derived a semantic representation providing a more profound understanding of interaction and presented the specific domain knowledge of traditional Chinese shadow play art. Ontology is a high-level conceptual representation of the components in the animation production. The defined class hierarchy, object properties and data properties revealed how the low-level details involved in animation production are related to each other and how they collaborated and interacted with each other as well as the players and devices.</p><p>Our main goal is to utilize semantic/ontology concept to improve the reusability, extensibility and modularity of the interactive animation production and facilitate the development process. The proposed framework is flexible and extendable for many applications. In the usage example, we mainly worked with hand-gesture-based interactive digital shadow play, which provides a novel method to generate stylized traditional shadow play animation. This will contribute to the preservation of this cultural treasure of art. Based on the semantic framework, researcher can define various domain-specified ontologies and construct animation asset repositories depending on the context and application.</p><p>One limitation of the user interface comes from the leap motion sensor device, which can only detect hands and fingers within limited tracking area. Performance system will easily lose track if player’s hand moves outside. To keep the consistency of the movement of the shadow puppetry, the puppet is kept still and stays where it is until the hand is detected again by leap motion when moving back. The user evaluation of our method is very brief and preliminary examined in the test. We plan to examine more cases to affirm the conclusion at the next step. And also, a detailed evaluation of the semantic retrieval system will be carried out to illustrate the benefit of this data management approach further.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="A free, open-source ontology editor and framework for building intelligent systems. http://protege.stanford.ed" /><p class="c-article-references__text" id="ref-CR1">A free, open-source ontology editor and framework for building intelligent systems. <a href="http://protege.stanford.edu/">http://protege.stanford.edu/</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Anthony M, Chandler T, Heiser E (2009) A framework for promoting high cognitive performance in instructional g" /><p class="c-article-references__text" id="ref-CR2">Anthony M, Chandler T, Heiser E (2009) A framework for promoting high cognitive performance in instructional games. In: Interservice/industry training simulation and education conference (I/ITSEC), Orlando, FL</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Apache Jena. https://jena.apache.org/. Retrieved 21 March 2016" /><p class="c-article-references__text" id="ref-CR3">Apache Jena. <a href="https://jena.apache.org/">https://jena.apache.org/</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Arndt R, Troncy R, Staab S, Hardman L (2009) Comm: a core ontology for multimediaannotation. In: Handbook on o" /><p class="c-article-references__text" id="ref-CR4">Arndt R, Troncy R, Staab S, Hardman L (2009) Comm: a core ontology for multimediaannotation. In: Handbook on ontologies. Springer, Berlin, Heidelberg, pp 403–421</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bilasco IM, Gensel J, Villanova-Oliver M, Martin H (2005) On indexing of 3D scenes using MPEG-7. In: Proceedin" /><p class="c-article-references__text" id="ref-CR5">Bilasco IM, Gensel J, Villanova-Oliver M, Martin H (2005) On indexing of 3D scenes using MPEG-7. In: Proceedings of the 13th annual ACM international conference on multimedia. ACM, pp 471–474</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="BinSubaih A, Maddock S, Romano D (2005) Game logic portability. In: Proceedings of the 2005 ACM SIGCHI interna" /><p class="c-article-references__text" id="ref-CR6">BinSubaih A, Maddock S, Romano D (2005) Game logic portability. In: Proceedings of the 2005 ACM SIGCHI international conference on advances in computer entertainment technology. ACM, pp 458–461</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Borst WN (1997) Construction of engineering ontologies for knowledge sharing and reuse, Universiteit Twente" /><p class="c-article-references__text" id="ref-CR7">Borst WN (1997) Construction of engineering ontologies for knowledge sharing and reuse, Universiteit Twente</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chang PHM, Chien YH, Kao ECC, Soo VW (2005) A knowledge based scenario framework to support intelligent planni" /><p class="c-article-references__text" id="ref-CR8">Chang PHM, Chien YH, Kao ECC, Soo VW (2005) A knowledge based scenario framework to support intelligent planning characters. In: Intelligent virtual agents. Springer, Berlin, Heidelberg, pp 134–145</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen F (2015) Visions for the masses: Chinese shadow plays from shaanxi and shanxi. Pacific Science 201" /><p class="c-article-references__text" id="ref-CR9">Chen F (2015) Visions for the masses: Chinese shadow plays from shaanxi and shanxi. Pacific Science 201</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Boeck J, Raymaekers C, Coninx K (2006) Comparing NiMMiT and data-driven notations for describing multimodal" /><p class="c-article-references__text" id="ref-CR10">De Boeck J, Raymaekers C, Coninx K (2006) Comparing NiMMiT and data-driven notations for describing multimodal interaction. In: Task models and diagrams for users interface design. Springer, Berlin, Heidelberg, pp 217–229</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dubin D, Jett J (2015) An ontological framework for describing games. In: Proceedings of the 15th ACM/IEEE-CE " /><p class="c-article-references__text" id="ref-CR11">Dubin D, Jett J (2015) An ontological framework for describing games. In: Proceedings of the 15th ACM/IEEE-CE on joint conference on digital libraries. ACM, pp 165–168</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Falcidieno B (2004) Aim@ shape project presentation. In: Shape modeling applications, proceedings. IEEE, p 329" /><p class="c-article-references__text" id="ref-CR12">Falcidieno B (2004) Aim@ shape project presentation. In: Shape modeling applications, proceedings. IEEE, p 329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Falcidieno B, Spagnuolo M, Alliez P, Quak E, Vavalis M, Houstis C (2004) Towards the semantics of digital shap" /><p class="c-article-references__text" id="ref-CR13">Falcidieno B, Spagnuolo M, Alliez P, Quak E, Vavalis M, Houstis C (2004) Towards the semantics of digital shapes: the AIM@SHAPE approach. In: EWIMT</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Flotyński J, Walczak K (2013a) Conceptual semantic representation of 3d content. In: Business information syst" /><p class="c-article-references__text" id="ref-CR14">Flotyński J, Walczak K (2013a) Conceptual semantic representation of 3d content. In: Business information systems workshops. Springer, Berlin, Heidelberg, pp 244–257</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Flotyński J, Walczak K (2013b) Semantic modelling of interactive 3d content. In: Proceedings of the 5th joint " /><p class="c-article-references__text" id="ref-CR15">Flotyński J, Walczak K (2013b) Semantic modelling of interactive 3d content. In: Proceedings of the 5th joint virtual reality conference. Eurographics association, pp 41–48</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Flotyński J, Walczak K (2013c) Semantic multi-layered design of interactive 3d presentations. In: 2013 federat" /><p class="c-article-references__text" id="ref-CR16">Flotyński J, Walczak K (2013c) Semantic multi-layered design of interactive 3d presentations. In: 2013 federated conference on computer science and information systems (FedCSIS). IEEE, pp 541–548</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Flotyński J, Walczak K (2014) Multi-platform semantic representation of interactive 3d content. In: Technologi" /><p class="c-article-references__text" id="ref-CR17">Flotyński J, Walczak K (2014) Multi-platform semantic representation of interactive 3d content. In: Technological innovation for collective awareness systems. Springer, Berlin, Heidelberg, pp 63–72</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Funkhouser, P. Min, M. Kazhdan, J. Chen, A. Halderman, D. Dobkin, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Funkhouser T, Min P, Kazhdan M, Chen J, Halderman A, Dobkin D et al (2003) A search engine for 3D models. ACM " /><p class="c-article-references__text" id="ref-CR18">Funkhouser T, Min P, Kazhdan M, Chen J, Halderman A, Dobkin D et al (2003) A search engine for 3D models. ACM Trans Graph (TOG) 22(1):83–105</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F588272.588279" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20search%20engine%20for%203D%20models&amp;journal=ACM%20Trans%20Graph%20%28TOG%29&amp;volume=22&amp;issue=1&amp;pages=83-105&amp;publication_year=2003&amp;author=Funkhouser%2CT&amp;author=Min%2CP&amp;author=Kazhdan%2CM&amp;author=Chen%2CJ&amp;author=Halderman%2CA&amp;author=Dobkin%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ghannem A (2014) Characterization of serious games guided by the educational objectives. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR19">Ghannem A (2014) Characterization of serious games guided by the educational objectives. In: Proceedings of the second international conference on technological ecosystems for enhancing multiculturality. ACM, pp. 227–233</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Google 3D Warehouse. https://3dwarehouse.sketchup.com/. Retrieved 21 March 2016" /><p class="c-article-references__text" id="ref-CR20">Google 3D Warehouse. <a href="https://3dwarehouse.sketchup.com/">https://3dwarehouse.sketchup.com/</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TR. Gruber, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Gruber TR (1993) A translation approach to portable ontology specifications. Knowl Acquis 5(2):199–220" /><p class="c-article-references__text" id="ref-CR21">Gruber TR (1993) A translation approach to portable ontology specifications. Knowl Acquis 5(2):199–220</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fknac.1993.1008" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20translation%20approach%20to%20portable%20ontology%20specifications&amp;journal=Knowl%20Acquis&amp;volume=5&amp;issue=2&amp;pages=199-220&amp;publication_year=1993&amp;author=Gruber%2CTR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grüninger M, Fox MS (1995) Methodology for the design and evaluation of ontologies, Workshop on Basic Ontologi" /><p class="c-article-references__text" id="ref-CR22">Grüninger M, Fox MS (1995) Methodology for the design and evaluation of ontologies, Workshop on Basic Ontological Issues in Knowledge Sharing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutierrez M, Thalmann D, Vexo F (2005) Semantic virtual environments with adaptive multimodal interfaces. In: " /><p class="c-article-references__text" id="ref-CR23">Gutierrez M, Thalmann D, Vexo F (2005) Semantic virtual environments with adaptive multimodal interfaces. In: Multimedia modelling conference, 2005. MMM 2005. Proceedings of the 11th international. IEEE, pp 277–283</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Gutiérrez, A. García-Rojas, D. Thalmann, F. Vexo, L. Moccozet, N. Magnenat-Thalmann, M. Mortara, M. Spagnuolo, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Gutiérrez M, García-Rojas A, Thalmann D, Vexo F, Moccozet L, Magnenat-Thalmann N, Mortara M, Spagnuolo M (2007" /><p class="c-article-references__text" id="ref-CR24">Gutiérrez M, García-Rojas A, Thalmann D, Vexo F, Moccozet L, Magnenat-Thalmann N, Mortara M, Spagnuolo M (2007) An ontology of virtual humans: incorporating semantics into human shapes. Vis Comput 23(3):207–218</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-006-0093-4" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20ontology%20of%20virtual%20humans%3A%20incorporating%20semantics%20into%20human%20shapes&amp;journal=Vis%20Comput&amp;volume=23&amp;issue=3&amp;pages=207-218&amp;publication_year=2007&amp;author=Guti%C3%A9rrez%2CM&amp;author=Garc%C3%ADa-Rojas%2CA&amp;author=Thalmann%2CD&amp;author=Vexo%2CF&amp;author=Moccozet%2CL&amp;author=Magnenat-Thalmann%2CN&amp;author=Mortara%2CM&amp;author=Spagnuolo%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kazhdan M, Funkhouser T, Rusinkiewicz S (2003) Rotation invariant spherical harmonics representation of 3D sha" /><p class="c-article-references__text" id="ref-CR25">Kazhdan M, Funkhouser T, Rusinkiewicz S (2003) Rotation invariant spherical harmonics representation of 3D shape descriptors. In: Symposium on geometry processing, vol 6, pp 156–164</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leino OT (2010) What erotic tetris has to teach serious games about being serious? Design implications of an e" /><p class="c-article-references__text" id="ref-CR26">Leino OT (2010) What erotic tetris has to teach serious games about being serious? Design implications of an experiential ontology of game content. In: Meaningful play 2010 conference proceedings, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Li, K. Karthik, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Li Z, Karthik K (2007) Ontology-based design information extraction and retrieval. AI EDAM Artif Intell Eng De" /><p class="c-article-references__text" id="ref-CR27">Li Z, Karthik K (2007) Ontology-based design information extraction and retrieval. AI EDAM Artif Intell Eng Des Anal Manuf 21(2):137–154</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ontology-based%20design%20information%20extraction%20and%20retrieval&amp;journal=AI%20EDAM%20Artif%20Intell%20Eng%20Des%20Anal%20Manuf&amp;volume=21&amp;issue=2&amp;pages=137-154&amp;publication_year=2007&amp;author=Li%2CZ&amp;author=Karthik%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liang H, Chang J, Kazmi IK, Zhang JJ, Jiao P (2015a) Puppet narrator: utilizing motion sensing technology in s" /><p class="c-article-references__text" id="ref-CR28">Liang H, Chang J, Kazmi IK, Zhang JJ, Jiao P (2015a) Puppet narrator: utilizing motion sensing technology in storytelling for young children. In: 2015 7th international conference on games and virtual worlds for serious applications (VS-Games). IEEE, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liang H, Chang J, Deng S, Chen C, Tong R, Zhang JJ (2015b) Exploitation of novel multiplayer gesture-based int" /><p class="c-article-references__text" id="ref-CR29">Liang H, Chang J, Deng S, Chen C, Tong R, Zhang JJ (2015b) Exploitation of novel multiplayer gesture-based interaction and virtual puppetry for digital storytelling to develop children’s narrative skills. In: Proceeding VRCAI ‘15 proceedings of the 14th ACM SIGGRAPH international conference on virtual reality continuum and its applications in industry. ACM, pp 63–72</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mansouri H (2005) Using semantic descriptions for building and querying virtual environments. Licentiate’s the" /><p class="c-article-references__text" id="ref-CR30">Mansouri H (2005) Using semantic descriptions for building and querying virtual environments. Licentiate’s thesis, Vrije Universiteit Brussel</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mirbakhsh N, Didandeh A, Afsharchi M (2010) Concept learning games: the game of query and response. In: 2010 I" /><p class="c-article-references__text" id="ref-CR31">Mirbakhsh N, Didandeh A, Afsharchi M (2010) Concept learning games: the game of query and response. In: 2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology (WI-IAT), vol 2. IEEE, pp 234–238</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ohbuchi R, Yamamoto A, Kobayashi J (2007) Learning semantic categories for 3D model retrieval. In: Proceedings" /><p class="c-article-references__text" id="ref-CR32">Ohbuchi R, Yamamoto A, Kobayashi J (2007) Learning semantic categories for 3D model retrieval. In: Proceedings of the international workshop on workshop on multimedia information retrieval. ACM, pp 31–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ontology for Media Resources 1.0. http://www.w3.org/TR/mediaont-10/. Retrieved 18 March 2016" /><p class="c-article-references__text" id="ref-CR33">Ontology for Media Resources 1.0. <a href="http://www.w3.org/TR/mediaont-10/">http://www.w3.org/TR/mediaont-10/</a>. Retrieved 18 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Otto KA (2005) The semantics of multi-user virtual environments. In: Proceedings of the workshop towards seman" /><p class="c-article-references__text" id="ref-CR34">Otto KA (2005) The semantics of multi-user virtual environments. In: Proceedings of the workshop towards semantic virtual environments</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parkkila J, Hynninen T, Ikonen J, Porras J, Radulovic F (2015) Towards interoperability in video games. In: Pr" /><p class="c-article-references__text" id="ref-CR35">Parkkila J, Hynninen T, Ikonen J, Porras J, Radulovic F (2015) Towards interoperability in video games. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, pp 26–29</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ramaprasad A, Papagari SS (2009) Ontological design. In: Proceedings of the 4th international conference on de" /><p class="c-article-references__text" id="ref-CR36">Ramaprasad A, Papagari SS (2009) Ontological design. In: Proceedings of the 4th international conference on design science research in information systems and technology. ACM, p 5</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruminski D, Walczak K (2014) Semantic contextual augmented reality environments. In: 2014 IEEE international s" /><p class="c-article-references__text" id="ref-CR37">Ruminski D, Walczak K (2014) Semantic contextual augmented reality environments. In: 2014 IEEE international symposium on mixed and augmented reality (ISMAR). IEEE, pp 401–404</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shadow play. http://www.chinatraveldesigner.com/travel-guide/culture/chinese-opera/shadow-play.htm. Retrieved " /><p class="c-article-references__text" id="ref-CR38">Shadow play. <a href="http://www.chinatraveldesigner.com/travel-guide/culture/chinese-opera/shadow-play.htm">http://www.chinatraveldesigner.com/travel-guide/culture/chinese-opera/shadow-play.htm</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SPARQL Query Language for RDF. https://www.w3.org/TR/rdf-sparql-query//. Retrieved 21 March 2016" /><p class="c-article-references__text" id="ref-CR39">SPARQL Query Language for RDF. <a href="https://www.w3.org/TR/rdf-sparql-query//">https://www.w3.org/TR/rdf-sparql-query//</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Studer, VR. Benjamins, D. Fensel, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Studer R, Benjamins VR, Fensel D (1998) Knowledge engineering: principles and methods. Data Knowl Eng 25(1):16" /><p class="c-article-references__text" id="ref-CR40">Studer R, Benjamins VR, Fensel D (1998) Knowledge engineering: principles and methods. Data Knowl Eng 25(1):161–197</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0169-023X%2897%2900056-6" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0896.68114" aria-label="View reference 40 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Knowledge%20engineering%3A%20principles%20and%20methods&amp;journal=Data%20Knowl%20Eng&amp;volume=25&amp;issue=1&amp;pages=161-197&amp;publication_year=1998&amp;author=Studer%2CR&amp;author=Benjamins%2CVR&amp;author=Fensel%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SWRL: a semantic web rule language. https://www.w3.org/Submission/SWRL/. Retrieved 21 March 2016" /><p class="c-article-references__text" id="ref-CR41">SWRL: a semantic web rule language. <a href="https://www.w3.org/Submission/SWRL/">https://www.w3.org/Submission/SWRL/</a>. Retrieved 21 March 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Tangelder, RC. Veltkamp, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Tangelder JW, Veltkamp RC (2008) A survey of content based 3D shape retrieval methods. Multimed Tools Appl 39(" /><p class="c-article-references__text" id="ref-CR42">Tangelder JW, Veltkamp RC (2008) A survey of content based 3D shape retrieval methods. Multimed Tools Appl 39(3):441–471</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11042-007-0181-0" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20content%20based%203D%20shape%20retrieval%20methods&amp;journal=Multimed%20Tools%20Appl&amp;volume=39&amp;issue=3&amp;pages=441-471&amp;publication_year=2008&amp;author=Tangelder%2CJW&amp;author=Veltkamp%2CRC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Teixeira JSF, Sá EDJV, Fernandes CT (2008) A taxonomy of educational games compatible with the LOM-IEEE data m" /><p class="c-article-references__text" id="ref-CR43">Teixeira JSF, Sá EDJV, Fernandes CT (2008) A taxonomy of educational games compatible with the LOM-IEEE data model. In: Proceedings of interdisciplinary studies in computer science SCIENTIA, pp 44–59</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thalmann D, Farenc N, Boulic R (1999) Virtual human life simulation and database: why and how. In: 1999 Intern" /><p class="c-article-references__text" id="ref-CR44">Thalmann D, Farenc N, Boulic R (1999) Virtual human life simulation and database: why and how. In: 1999 International symposium on database applications in non-traditional environments, 1999. (DANTE’99) Proceedings. IEEE, pp 471–479</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tutenel T, Smelik RM, Bidarra R, de Kraker KJ (2009) Using semantics to improve the design of game worlds. In:" /><p class="c-article-references__text" id="ref-CR45">Tutenel T, Smelik RM, Bidarra R, de Kraker KJ (2009) Using semantics to improve the design of game worlds. In: AIIDE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="World Wide Web Consortium (2012) OWL 2 web ontology language document overview" /><p class="c-article-references__text" id="ref-CR46">World Wide Web Consortium (2012) OWL 2 web ontology language document overview</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0333-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The research leading to these results has received funding from the People Programme (Marie Curie Actions) of the European Union’s Seventh Framework Programme FP7/2007–2013/under REA Grant Agreement No [623883]—“AniM”. The authors acknowledge partial support from project Dr Inventor (FP7-ICT-611383). The authors acknowledge partial support from project AniNex (FP7-IRSES-612627).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Zhengzhou University of Light Industry, Zhengzhou, China</p><p class="c-article-author-affiliation__authors-list">Hui Liang</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">National Centre for Computer Animation, Poole, UK</p><p class="c-article-author-affiliation__authors-list">Shujie Deng, Jian Chang &amp; Jian Jun Zhang</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Changzhou University, Changzhou, China</p><p class="c-article-author-affiliation__authors-list">Can Chen</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Zhejiang University, Hangzhou, China</p><p class="c-article-author-affiliation__authors-list">Ruofeng Tong</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Hui-Liang"><span class="c-article-authors-search__title u-h3 js-search-name">Hui Liang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hui+Liang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hui+Liang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hui+Liang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Shujie-Deng"><span class="c-article-authors-search__title u-h3 js-search-name">Shujie Deng</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Shujie+Deng&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shujie+Deng" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shujie+Deng%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jian-Chang"><span class="c-article-authors-search__title u-h3 js-search-name">Jian Chang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jian+Chang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jian+Chang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jian+Chang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jian_Jun-Zhang"><span class="c-article-authors-search__title u-h3 js-search-name">Jian Jun Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jian Jun+Zhang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jian Jun+Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jian Jun+Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Can-Chen"><span class="c-article-authors-search__title u-h3 js-search-name">Can Chen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Can+Chen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Can+Chen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Can+Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ruofeng-Tong"><span class="c-article-authors-search__title u-h3 js-search-name">Ruofeng Tong</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ruofeng+Tong&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ruofeng+Tong" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ruofeng+Tong%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0333-8/email/correspondent/c1/new">Jian Chang</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" rel="license" itemprop="license">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Semantic%20framework%20for%20interactive%20animation%20generation%20and%20its%20application%20in%20virtual%20shadow%20play%20performance&amp;author=Hui%20Liang%20et%20al&amp;contentID=10.1007%2Fs10055-018-0333-8&amp;publication=1359-4338&amp;publicationDate=2018-01-21&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0333-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0333-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Liang, H., Deng, S., Chang, J. <i>et al.</i> Semantic framework for interactive animation generation and its application in virtual shadow play performance.
                    <i>Virtual Reality</i> <b>22, </b>149–165 (2018). https://doi.org/10.1007/s10055-018-0333-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0333-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-12-07">07 December 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-01-14">14 January 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-01-21">21 January 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-06">June 2018</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0333-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0333-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Semantic framework</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual interactive</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Animation generation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Ontology</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hand-gesture-based interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Animation data management</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Chinese shadow play</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0333-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=333;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

