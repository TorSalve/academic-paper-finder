<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Context-driven interaction in immersive virtual environments"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they are often performed under different..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/14/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Context-driven interaction in immersive virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2010 14:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-11-16"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they are often performed under different contexts. For each task and context, specialized interaction techniques can be developed. We present the context-driven interaction model: a design pattern that represents contextual information as a first-class, quantifiable component within a user interface and supports the development of context-sensitive applications by decoupling context recognition, context representation, and interaction technique development. As a primary contribution, this model provides an enumeration of important representations of contextual information gathered from across the literature and describes how these representations can effect the selection of an appropriate interaction technique. We also identify how several popular 3D interaction techniques adhere to this design pattern and describe how the pattern itself can lead to a more focused development of effective interfaces. We have constructed a formalized programming toolkit and runtime system that serves as a reference implementation of the context-driven model and a discussion is provided explaining how the toolkit can be used to implement a collection of representative 3D interaction interfaces."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-11-16"/>

    <meta name="prism.volume" content="14"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="277"/>

    <meta name="prism.endingPage" content="290"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0178-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0178-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0178-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0178-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Context-driven interaction in immersive virtual environments"/>

    <meta name="citation_volume" content="14"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2010/12"/>

    <meta name="citation_online_date" content="2010/11/16"/>

    <meta name="citation_firstpage" content="277"/>

    <meta name="citation_lastpage" content="290"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0178-2"/>

    <meta name="DOI" content="10.1007/s10055-010-0178-2"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0178-2"/>

    <meta name="description" content="There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they "/>

    <meta name="dc.creator" content="Scott Frees"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Albinsson PA, Zhai S (2003) High precision touch screen interaction. In: Proceedings of SIGCHI conference on human factors in computing systems, pp 105&#8211;112"/>

    <meta name="citation_reference" content="Beir EA (1990) Snap-dragging in three dimensions. In: Proceedings of the ACM symposium on interactive 3D graphics, 24(2):193&#8211;204"/>

    <meta name="citation_reference" content="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 35&#8211;38"/>

    <meta name="citation_reference" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Proceedings of the virtual reality annual international symposium, pp 45&#8211;52"/>

    <meta name="citation_reference" content="Bowman DA, Johnson D, Hodges LF (1999) Testbed evaluation of V.E. interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 26&#8211;33"/>

    <meta name="citation_reference" content="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. Proc HCI Int:629&#8211;633"/>

    <meta name="citation_reference" content="Bowman D, Kruijff E, LaViola J, Poupyrev I (2001b) An introduction to 3D user interface design. Presence Teleoper Virtual Environ 10(1):96&#8211;108"/>

    <meta name="citation_reference" content="Bowman D, Badillo B, Manek D (2007) Evaluating the need for display-specific and device-specific 3D interaction techniques. In: Proceedings of virtual reality international conference (in Lecture notes in computer science, vol 4563, pp 195&#8211;204)"/>

    <meta name="citation_reference" content="Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 131&#8211;138"/>

    <meta name="citation_reference" content="CDI Toolkit Web Site: 
                    http://give.ramapo.edu/lab.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Chen J, Bowman D (2006) Evaluation of the effectiveness of cloning techniques for architectural virtual environment. In: Proceedings of IEEE virtual reality. Alexandria, VA, pp 103&#8211;110"/>

    <meta name="citation_reference" content="Feiner S, Macintyre B, Haupt M, Solomon E (1993) Windows on the world: 2D windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, pp 145&#8211;155"/>

    <meta name="citation_reference" content="Forsberg A, Herndon K, Zelesnik R (1996) Aperture based selection for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 95&#8211;96"/>

    <meta name="citation_reference" content="Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh University, Department of Computer Science, 266 p. Available from Dissertations and Theses database. (UMI/AAT No. 3215837)"/>

    <meta name="citation_reference" content="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=MASSIVE: a collaborative virtual environment for teleconferencing; citation_author=C Greenhalgh, S Benford; citation_volume=2; citation_issue=3; citation_publication_date=1995; citation_pages=239-261; citation_doi=10.1145/210079.210088; citation_id=CR15"/>

    <meta name="citation_reference" content="Grosjean J, Coquillart S (2001) Command &amp; control cube: a shortcut paradigm for virtual environments. In: Immersive projection technology and virtual environments 2001 proceedings, pp 1&#8211;12"/>

    <meta name="citation_reference" content="Kessler GD (1999) A framework for interactors in immersive virtual environments. Proc IEEE Virtual Real, pp 190&#8211;197"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The simple virtual environment library, and extensible framework for building VE applications; citation_author=GD Kessler, DA Bowman, LF Hodges; citation_volume=9; citation_issue=2; citation_publication_date=2000; citation_pages=187-208; citation_id=CR18"/>

    <meta name="citation_reference" content="Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 81&#8211;82"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoper Virtual Environ; citation_title=A two-handed interface for object manipulation in virtual environments; citation_author=DP Mapes, JM Moshell; citation_volume=4; citation_issue=4; citation_publication_date=1995; citation_pages=403-416; citation_id=CR20"/>

    <meta name="citation_reference" content="Nardi BA (ed) (1996) Context and consciousness&#8212;activity theory and human computer interaction. MIT Press, Cambridge"/>

    <meta name="citation_reference" content="Pierce J, Pausch R (2004) Navigation with place representations and visible landmarks. In: Proc IEEE Virtual Real, pp 173&#8211;180"/>

    <meta name="citation_reference" content="Pierce J, Forsberg A, Conway M, Hong S, Zeleznik R, Mine M (1997) Image plane interaction techniques in 3D immersive environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 39&#8211;40"/>

    <meta name="citation_reference" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the ACM symposium on user interface software and technology, pp 79&#8211;80"/>

    <meta name="citation_reference" content="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1997) A framework and testbed for studying manipulation techniques for immersive VR. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 21&#8211;28"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques; citation_author=I Poupyrev, S Weghorst, M Billinghurst, T Ichikawa; citation_volume=17; citation_issue=3; citation_publication_date=1998; citation_pages=41-52; citation_id=CR26"/>

    <meta name="citation_reference" content="Ray A, Bowman D (2007) Towards a system for reusable 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 187&#8211;190"/>

    <meta name="citation_reference" content="Ruddle RA, Jones DM (2001) Movement in cluttered virtual environments. Presence: Teleoper Virtual Environ 10:511&#8211;524"/>

    <meta name="citation_reference" content="Stoakley R, Conway M, Paush R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of the ACM conference on human factors in computing systems, pp 265&#8211;272"/>

    <meta name="citation_reference" content="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. Proc IEEE Virtual Real, pp 251&#8211;258"/>

    <meta name="citation_reference" content="Tan DS, Robertson GG, Czerwinski M (2001) Exploring 3D navigation: combining speed-coupled flying with orbiting. In: Proceedings of the ACM conference on human factors in computing systems, pp 418&#8211;424"/>

    <meta name="citation_reference" content="Tanriverdi V, Jacob RJ (2001) VRID: a design model and methodology for developing virtual reality interfaces. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, pp 175&#8211;182"/>

    <meta name="citation_reference" content="Usoh M, Arthur K, Whitton MC, Bastos R, Steed A, Slater M (1999) Walking &gt; walking-in-place &gt; flying, in Virtual Environments. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques, pp 259&#8211;264"/>

    <meta name="citation_reference" content="Ware C, Arsenault R (2004) Frames of reference in virtual object rotation. In: Proceedings of the 1st symposium on applied perception in graphics and visualization. Los Angeles, California, 7&#8211;8 Aug 2004"/>

    <meta name="citation_reference" content="Wesche G (2003) The toolfinger: supporting complex direct manipulation in virtual environments. ACM international conference proceedings series&#8212;proceedings of the workshop on virtual environments, pp 39&#8211;45"/>

    <meta name="citation_reference" content="Wingrave C, Bowman D (2008) Tiered developer-centric representations for 3D interfaces: concept-oriented design in chasm. IEEE Virtual Real"/>

    <meta name="citation_author" content="Scott Frees"/>

    <meta name="citation_author_email" content="sfrees@ramapo.edu"/>

    <meta name="citation_author_institution" content="Ramapo College of New Jersey, Mahwah, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0178-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2010/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0178-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Context-driven interaction in immersive virtual environments"/>
        <meta property="og:description" content="There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they are often performed under different contexts. For each task and context, specialized interaction techniques can be developed. We present the context-driven interaction model: a design pattern that represents contextual information as a first-class, quantifiable component within a user interface and supports the development of context-sensitive applications by decoupling context recognition, context representation, and interaction technique development. As a primary contribution, this model provides an enumeration of important representations of contextual information gathered from across the literature and describes how these representations can effect the selection of an appropriate interaction technique. We also identify how several popular 3D interaction techniques adhere to this design pattern and describe how the pattern itself can lead to a more focused development of effective interfaces. We have constructed a formalized programming toolkit and runtime system that serves as a reference implementation of the context-driven model and a discussion is provided explaining how the toolkit can be used to implement a collection of representative 3D interaction interfaces."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Context-driven interaction in immersive virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0178-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Human–computer interaction, Context-sensitive interaction, Virtual reality, Virtual environments, 3DUI, interaction techniques","kwrd":["Human–computer_interaction","Context-sensitive_interaction","Virtual_reality","Virtual_environments,_3DUI,_interaction_techniques"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0178-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0178-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=178;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0178-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Context-driven interaction in immersive virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0178-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0178-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-11-16" itemprop="datePublished">16 November 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Context-driven interaction in immersive virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Scott-Frees" data-author-popup="auth-Scott-Frees" data-corresp-id="c1">Scott Frees<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Ramapo College of New Jersey" /><meta itemprop="address" content="grid.418922.4, Ramapo College of New Jersey, Mahwah, NJ, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 14</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">277</span>–<span itemprop="pageEnd">290</span>(<span data-test="article-publication-year">2010</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">290 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0178-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>There are many interaction tasks a user may wish to accomplish in an immersive virtual environment. A careful examination of these tasks reveals that they are often performed under different contexts. For each task and context, specialized interaction techniques can be developed. We present the context-driven interaction model: a design pattern that represents contextual information as a first-class, quantifiable component within a user interface and supports the development of context-sensitive applications by decoupling context recognition, context representation, and interaction technique development. As a primary contribution, this model provides an enumeration of important representations of contextual information gathered from across the literature and describes how these representations can effect the selection of an appropriate interaction technique. We also identify how several popular 3D interaction techniques adhere to this design pattern and describe how the pattern itself can lead to a more focused development of effective interfaces. We have constructed a formalized programming toolkit and runtime system that serves as a reference implementation of the context-driven model and a discussion is provided explaining how the toolkit can be used to implement a collection of representative 3D interaction interfaces.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>A recognized interaction problem in immersive virtual environments is that users have difficulty manipulating virtual objects precisely. There are good solutions to this precision problem, many of which place constraints on the movement of objects (limiting the speed of motion, snapping to grid points, etc.). Of course, if the user does not desire precision, these same constraints can inhibit interaction and become a nuisance. Put simply, there are two <i>separate and competing</i> contexts in which users interact with virtual objects—one requiring free flowing, rapid manipulation, and another requiring a constraint to support precision.</p><p>This issue is not unique to object manipulation. Selection of proximate objects is typically straightforward; the user intersects the cursor or stylus with a virtual object and performs some action (perhaps a button press) to gain control of the object. Difficulties arise when the user wants to select objects outside arms’ reach. Instead of forcing the user to navigate to distant objects, selection techniques exist that allow users to specify an object from a distance (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 35–38" href="/article/10.1007/s10055-010-0178-2#ref-CR3" id="ref-link-section-d39171e280">1997</a>; Forsberg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Forsberg A, Herndon K, Zelesnik R (1996) Aperture based selection for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 95–96" href="/article/10.1007/s10055-010-0178-2#ref-CR12" id="ref-link-section-d39171e283">1996</a>; Pierce et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Pierce J, Forsberg A, Conway M, Hong S, Zeleznik R, Mine M (1997) Image plane interaction techniques in 3D immersive environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 39–40" href="/article/10.1007/s10055-010-0178-2#ref-CR23" id="ref-link-section-d39171e286">1997</a>; Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the ACM symposium on user interface software and technology, pp 79–80" href="/article/10.1007/s10055-010-0178-2#ref-CR24" id="ref-link-section-d39171e289">1996</a>). These techniques all share a common characteristic; they <i>add</i> to the interface a specialized method of selection suitable for selecting <i>distant</i> objects and, in most cases, provide the more natural direct-touch method when selecting near objects. Once again, there are two <i>separate and competing contexts</i> under which users select objects: one where objects are within arms’ reach and one where objects are far from the user.</p><p>When confronted with this situation, developers are forced to make a decision; they can (1) implement an interaction technique that attempts to support both contexts or (2) implement two interaction techniques and force the user to choose between them while interacting in the environment. Implementing an interaction technique that supports both contexts may seem ideal; however, this solution often leads to a monolithic technique, which supports neither context very well, degrading to a least common denominator solution. The second option has its own drawback, as explicit mode switching between interaction techniques disrupts natural interaction.</p><p>Missing from the current model of interaction is a formalized method of representing the current <i>context</i> under which interaction tasks are performed. Given a sufficient model, developers would be better equipped to create sophisticated context recognition (perhaps limiting the need for explicit system control) and specialized interaction techniques while maximizing code reuse and portability. This paper presents such a model. We present several context components as a means of fully specifying the current interaction context. The idea of using separate, independent context recognition mechanisms, both implicit and explicit, is introduced. Finally, a design pattern for developing context-sensitive user interfaces is defined and a programming toolkit built on top of SVE (Kessler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kessler GD, Bowman DA, Hodges LF (2000) The simple virtual environment library, and extensible framework for building VE applications. Presence Teleoper Virtual Environ 9(2):187–208" href="/article/10.1007/s10055-010-0178-2#ref-CR18" id="ref-link-section-d39171e310">2000</a>) is presented. The result is a model that encourages developers to focus on creating highly effective and specialized interaction techniques for specific contexts rather than attempting to adequately support all contexts with one monolithic technique.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The notion that context, or the “current situation”, affects the way we perform tasks (and consequently, our choice of interaction technique) is neither new, nor groundbreaking. In the real world, we constantly make these decisions—while both a bicycle and automobile can take you from place to place, contextual information (weather, time constraints, and distance to be traveled) usually makes the choice obvious. This principle holds in virtual worlds as well and has long been implicitly recognized by 3D user interface researchers (as well as for general UI design, as suggested by Nardi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Nardi BA (ed) (1996) Context and consciousness—activity theory and human computer interaction. MIT Press, Cambridge" href="/article/10.1007/s10055-010-0178-2#ref-CR21" id="ref-link-section-d39171e321">1996</a>). Early in virtual reality research, it was understood that selecting objects from a distance requires a fundamentally different approach than when selecting a nearby object—and thus the development of many interaction techniques to fill that void (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 35–38" href="/article/10.1007/s10055-010-0178-2#ref-CR3" id="ref-link-section-d39171e324">1997</a>; Forsberg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Forsberg A, Herndon K, Zelesnik R (1996) Aperture based selection for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 95–96" href="/article/10.1007/s10055-010-0178-2#ref-CR12" id="ref-link-section-d39171e327">1996</a>; Pierce et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Pierce J, Forsberg A, Conway M, Hong S, Zeleznik R, Mine M (1997) Image plane interaction techniques in 3D immersive environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 39–40" href="/article/10.1007/s10055-010-0178-2#ref-CR23" id="ref-link-section-d39171e330">1997</a>). Likewise, radically different interaction techniques have been developed to support rapid object manipulation (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the ACM symposium on user interface software and technology, pp 79–80" href="/article/10.1007/s10055-010-0178-2#ref-CR24" id="ref-link-section-d39171e333">1996</a>; Stoakley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Stoakley R, Conway M, Paush R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of the ACM conference on human factors in computing systems, pp 265–272" href="/article/10.1007/s10055-010-0178-2#ref-CR29" id="ref-link-section-d39171e337">1995</a>) versus precision techniques (Beir <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Beir EA (1990) Snap-dragging in three dimensions. In: Proceedings of the ACM symposium on interactive 3D graphics, 24(2):193–204" href="/article/10.1007/s10055-010-0178-2#ref-CR2" id="ref-link-section-d39171e340">1990</a>; Frees et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)" href="/article/10.1007/s10055-010-0178-2#ref-CR14" id="ref-link-section-d39171e343">2007</a>; Albinsson and Zhai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Albinsson PA, Zhai S (2003) High precision touch screen interaction. In: Proceedings of SIGCHI conference on human factors in computing systems, pp 105–112" href="/article/10.1007/s10055-010-0178-2#ref-CR1" id="ref-link-section-d39171e346">2003</a>; Ruddle and Jones <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Ruddle RA, Jones DM (2001) Movement in cluttered virtual environments. Presence: Teleoper Virtual Environ 10:511–524" href="/article/10.1007/s10055-010-0178-2#ref-CR28" id="ref-link-section-d39171e349">2001</a>). A similar variety of interaction techniques have been created for navigation (Pierce and Pausch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Pierce J, Pausch R (2004) Navigation with place representations and visible landmarks. In: Proc IEEE Virtual Real, pp 173–180" href="/article/10.1007/s10055-010-0178-2#ref-CR22" id="ref-link-section-d39171e352">2004</a>; Tan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Tan DS, Robertson GG, Czerwinski M (2001) Exploring 3D navigation: combining speed-coupled flying with orbiting. In: Proceedings of the ACM conference on human factors in computing systems, pp 418–424" href="/article/10.1007/s10055-010-0178-2#ref-CR31" id="ref-link-section-d39171e356">2001</a>; Usoh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Usoh M, Arthur K, Whitton MC, Bastos R, Steed A, Slater M (1999) Walking &gt; walking-in-place &gt; flying, in Virtual Environments. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques, pp 259–264" href="/article/10.1007/s10055-010-0178-2#ref-CR33" id="ref-link-section-d39171e359">1999</a>) and other interaction tasks. The key theme throughout the literature is that <i>specific</i> interaction techniques lend themselves well to <i>specific</i> situations. One of the principal objectives of this paper is to enumerate what those <i>specific</i> situations are, and how they can be modeled and acted upon.</p><p>Examples of contextual information that affects interaction techniques can be found throughout the literature. Perhaps the most studied is the idea of workspace. The workspace or “area of interest” is related to the idea of focus and nimbus (Greenhalgh and Benford <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Greenhalgh C, Benford S (1995) MASSIVE: a collaborative virtual environment for teleconferencing. ACM Trans Comput Hum Interact 2(3):239–261" href="/article/10.1007/s10055-010-0178-2#ref-CR15" id="ref-link-section-d39171e374">1995</a>) used in the MASSIVE system. Poupyrev et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1998) Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques. Comput Graph Forum 17(3):41–52" href="/article/10.1007/s10055-010-0178-2#ref-CR26" id="ref-link-section-d39171e377">1998</a>) have shown that (direct) virtual hand placement is the best manipulation technique when dealing with objects at close range and that ray casting outperforms Go–Go for selection at long distances. For manipulation at larger distances, Go–Go was shown to be significantly better than ray casting.</p><p>Other contextual attributes (or context components) found in the literature include object groupings (Bukowski and Sequin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 131–138" href="/article/10.1007/s10055-010-0178-2#ref-CR9" id="ref-link-section-d39171e383">1995</a>; Stuerzlinger and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. Proc IEEE Virtual Real, pp 251–258" href="/article/10.1007/s10055-010-0178-2#ref-CR30" id="ref-link-section-d39171e386">2002</a>), frame of reference (Ware and Arsenault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ware C, Arsenault R (2004) Frames of reference in virtual object rotation. In: Proceedings of the 1st symposium on applied perception in graphics and visualization. Los Angeles, California, 7–8 Aug 2004" href="/article/10.1007/s10055-010-0178-2#ref-CR34" id="ref-link-section-d39171e389">2004</a>), and constraints (Mapes and Moshell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Mapes DP, Moshell JM (1995) A two-handed interface for object manipulation in virtual environments. Presence: Teleoper Virtual Environ 4(4):403–416" href="/article/10.1007/s10055-010-0178-2#ref-CR20" id="ref-link-section-d39171e392">1995</a>). Contextual information that does not fall into these categories may also exist, especially in highly specialized and domain-specific applications (Chen and Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Chen J, Bowman D (2006) Evaluation of the effectiveness of cloning techniques for architectural virtual environment. In: Proceedings of IEEE virtual reality. Alexandria, VA, pp 103–110" href="/article/10.1007/s10055-010-0178-2#ref-CR10" id="ref-link-section-d39171e395">2006</a>). Furthermore, other issues play a role in the selection of interaction techniques—such as hardware availability (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman D, Badillo B, Manek D (2007) Evaluating the need for display-specific and device-specific 3D interaction techniques. In: Proceedings of virtual reality international conference (in Lecture notes in computer science, vol 4563, pp 195–204)" href="/article/10.1007/s10055-010-0178-2#ref-CR8" id="ref-link-section-d39171e399">2007</a>).</p><p>Several interaction techniques have been developed that can be described as composite techniques, delivering different types of interaction depending on the current context. Go–Go (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1998) Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques. Comput Graph Forum 17(3):41–52" href="/article/10.1007/s10055-010-0178-2#ref-CR26" id="ref-link-section-d39171e405">1998</a>) is perhaps the most well known of the approaches. Go–Go delivers direct manipulation when the user is interested in objects within arms’ reach and provides an amplification of the hand movement to allow the user to extend their reach as they interact with distant objects. Go–Go <i>recognizes</i> the interaction context using a very simple mechanism—the distance between the user’s hand and body. As the user reaches further, Go–Go automatically delivers more amplification. PRISM (Frees et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)" href="/article/10.1007/s10055-010-0178-2#ref-CR14" id="ref-link-section-d39171e411">2007</a>) takes a similar approach; however, rather than adding amplification, it scales or dampens the user’s hand movements to provide enhanced levels of precision when the user is working closely with an object. Just like Go–Go, it also includes an automatic <i>recognition</i> mechanism—inferring that the slowing down of the user’s hand speed indicates they require more precision (and thus scaling). Go–Go and PRISM are specific examples of a generalized model, which uses context to switch between interaction techniques.</p><p>The implementation of interaction techniques (let alone context-driven techniques) can be difficult and is in need of standardized toolkits similar to those previously developed for 2D windowing systems. This issue has long been recognized, but the goal has not yet been realized. Early 3D interaction toolkits, such as VRID (Tanriverdi and Jacob <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Tanriverdi V, Jacob RJ (2001) VRID: a design model and methodology for developing virtual reality interfaces. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, pp 175–182" href="/article/10.1007/s10055-010-0178-2#ref-CR32" id="ref-link-section-d39171e421">2001</a>) and SVIFT (Kessler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kessler GD (1999) A framework for interactors in immersive virtual environments. Proc IEEE Virtual Real, pp 190–197" href="/article/10.1007/s10055-010-0178-2#ref-CR17" id="ref-link-section-d39171e424">1999</a>), have been followed by higher-level and portable implementations such as CHASM (Wingrave and Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wingrave C, Bowman D (2008) Tiered developer-centric representations for 3D interfaces: concept-oriented design in chasm. IEEE Virtual Real" href="/article/10.1007/s10055-010-0178-2#ref-CR36" id="ref-link-section-d39171e427">2008</a>). A recent trend in 3D interaction toolkit development is the leveraging of task decomposition (Ray and Bowman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ray A, Bowman D (2007) Towards a system for reusable 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 187–190" href="/article/10.1007/s10055-010-0178-2#ref-CR27" id="ref-link-section-d39171e430">2007</a>) (such as those found in (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Proceedings of the virtual reality annual international symposium, pp 45–52" href="/article/10.1007/s10055-010-0178-2#ref-CR4" id="ref-link-section-d39171e433">1997</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman DA, Johnson D, Hodges LF (1999) Testbed evaluation of V.E. interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 26–33" href="/article/10.1007/s10055-010-0178-2#ref-CR5" id="ref-link-section-d39171e437">1999</a>; Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1997) A framework and testbed for studying manipulation techniques for immersive VR. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 21–28" href="/article/10.1007/s10055-010-0178-2#ref-CR25" id="ref-link-section-d39171e440">1997</a>)). Although our toolkit provides a similar approach toward interaction technique development, this paper focuses on the toolkit’s support for contextual information as a first-class entity in a user interface toolkit—a feature we do not believe has been adequately addressed yet in the literature.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">The context-driven interaction model</h2><div class="c-article-section__content" id="Sec3-content"><p>The goal of this research is to develop a programming and conceptual model supporting context-sensitive user interfaces with the following features:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">a)</span>
                    
                      <p>A quantifiable model of the information that makes up the current “interaction context”: information that can be used to decide which interaction technique will best suit the user’s current needs.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">b)</span>
                    
                      <p>A decoupling of the way in which context is recognized (either explicitly through modal commands or implicitly by observing user behavior) from the representation of context and implementation of the interaction techniques themselves.</p>
                    
                  </li>
                </ol>
                     <p>The product of this research is not only a model; it is also a toolkit and runtime system that serves as a reference implementation. The Context-Driven Interaction (CDI) toolkit is built on top of the Simple Virtual Environment (SVE)<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> library (Kessler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kessler GD, Bowman DA, Hodges LF (2000) The simple virtual environment library, and extensible framework for building VE applications. Presence Teleoper Virtual Environ 9(2):187–208" href="/article/10.1007/s10055-010-0178-2#ref-CR18" id="ref-link-section-d39171e490">2000</a>) and consists of a set of C++ abstract base classes that developers extend to implement interaction techniques, context types, and context recognition mechanisms (CRM). A large set of concrete implementations of common interaction techniques and CRMs are included. The toolkit can be broken into two conceptual layers, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig1">1</a>. The first layer is the API, which allows developers to create, register, and activate context recognition mechanisms and context switch callbacks. The second layer is the internal runtime, which manages context components, updates context via registered CRMs, and invokes the context switch callbacks when necessary.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>CDI programming toolkit organization</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec4">Context components</h3><p>Interaction context represents the current state of the user, including position, interaction history, and intentions or objectives while performing a task. This definition is imprecise by design; context in fact represents <i>all</i> knowledge the system has about the user’s objectives—the sum of many small bits of information. These “bits” of information can be thought of as answers to questions—questions the system must “ask” in order to provide the most suitable interaction technique for the user’s current task. Some information is easily acquired through simple observation of the current state of the virtual world. This includes the orientation and position of the viewpoint, speed of the user’s hand movement, etc. Other information, such as the object the user’s hand is “touching”, can also be easily determined and is unambiguous. On the other hand, higher-level characterizations of context are also (and often more) valuable when deciding upon interaction techniques.</p><p>Part of the challenge of developing the CDI model was the identification of the most commonly used high-level characterizations of context—which we call <i>context components</i>. To establish a list of context components, we performed a literature review of the most commonly used and successful 3D interaction techniques. For each interaction technique, we asked the question: “Under what circumstance is this technique designed to be more effective than its competitors?” For example, when looking at a viewing technique like orbital viewing (Koller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 81–82" href="/article/10.1007/s10055-010-0178-2#ref-CR19" id="ref-link-section-d39171e530">1996</a>), its clear the technique attempts to add value in situations where the user is focused on a specific area/workspace, while a technique such as landmarks (Pierce and Pausch <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Pierce J, Pausch R (2004) Navigation with place representations and visible landmarks. In: Proc IEEE Virtual Real, pp 173–180" href="/article/10.1007/s10055-010-0178-2#ref-CR22" id="ref-link-section-d39171e533">2004</a>) is for a broad overview (large workspaces). Looking at the literature as a whole, five distinct categories of “context” began to emerge: level of control, workspace, frame of reference, constraints, and object groupings. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab1">1</a> presents these context components and summarizes how our model quantifies them. For each of these components, non-trivial procedures must be developed to recognize the current context “value”; these procedures are referred to as CRMs.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Inferred context components</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0178-2/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>While we believe these five context components cover most of the more common scenarios in a typical application, we do not claim that all contextual information can be distilled into five categories. There is little doubt that unique applications may or may not require specific context components. Our programming toolkit accommodates the creation of new context components by the application developer for use in the system.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Level of control</h4><p>Level of control refers to the granularity or speed in which we manipulate controlled objects or navigate the world. Consider object translation: often users wish to move objects quickly over large distances. Go–Go (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the ACM symposium on user interface software and technology, pp 79–80" href="/article/10.1007/s10055-010-0178-2#ref-CR24" id="ref-link-section-d39171e733">1996</a>) translation provides this by amplifying the movement of the user’s hand and thus the movement of the controlled object. This allows the user to move objects quickly over large distances, but sacrifices accuracy as small changes in the position of the user’s hand results in even larger movements in the controlled object. This is an example of a translation technique useful in situations where a user requires a <i>low</i> level of control. In contrast, scaled manipulation can be used to enhance accuracy. Scaled translation moves a controlled object more slowly than the hand, dampening the effects of unintended movement. Scaled manipulation is most applicable when a user requires a <i>high</i> level of control. In between these extremes lies the normal 1:1 mapping of the physical and virtual hand.</p><p>The appropriate interaction technique can only be selected after the current level of control (LOC) is determined. The current LOC cannot be described by a set of discrete values, rather it can be any value lying along a continuum between “Low” and “High.” In our model, LOC is defined by a real number between zero and three. A value of zero indicates an extremely low level of control is desired. More specifically, a value close to zero indicates an interaction technique providing rapid, broad reaching manipulation and navigation is most suitable. A value closer to three indicates the user requires precision to accomplish their task. The choice to represent LOC as a value between zero and three is arbitrary; any set of numerical values would have been sufficient.</p><p>Being a real number, there are an infinite number of values for LOC; however, there is a finite set of interaction techniques available to the developer. Typically an interaction technique will be suited for a <i>range</i> of LOC values, not one value in particular. Our model explicitly supports three ranges: 0–1 is considered “low”, 1–2 is “normal”, and 2–3 is “high.” These threshold values are again arbitrary; they are defined merely as a standard so values of the LOC can be interpreted regardless of the method used calculate it. As an example, PRISM interaction (Frees et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)" href="/article/10.1007/s10055-010-0178-2#ref-CR14" id="ref-link-section-d39171e750">2007</a>) uses hand speed to calculate the <i>level of control</i>. Slow hand speeds indicate high LOC; however, the definition of “slow” hand speeds can vary (depending on the user or implementation). The context recognition mechanism responsible for observing hand speed converts speed values into the standard LOC notation so the runtime/developer need not be concerned about what hand speeds are considered “low” in order to decide when to provide scaled manipulation (for high LOC) instead of direct or amplified manipulation (for normal and low LOC).<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> This decoupling allows for alternative methods of determining LOC to be added to the system without modifying the implementation of the interaction techniques. For example, rather than simply observing hand speed, a CRM could be designed to detect physical or cognitive limitations of specific users and adjust LOC as necessary—all without changing the implementation of the scaled manipulation.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Workspace</h4><p>Knowing which part of the world the user is currently interested in (their workspace) is also an important factor when deciding between interaction techniques. Although the workspace is almost always within view of the user, its exact location and size must be determined, possibly by observing the user’s recent actions.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>
                           </p><p>The current workspace (WKSP) is defined as a volume, with a precise center position and dimensions. As with LOC, this definition implies that the current WKSP can be any sized volume—it is not a discrete value. When the current WKSP is large, rapid navigation techniques such as “flying” should be made available. A large or distant WKSP also suggests larger scale selection techniques such as ray casting and broad-reaching manipulation techniques such as Go–Go. In contrast, a small WKSP might indicate that viewpoint control techniques such as orbital viewing (Koller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 81–82" href="/article/10.1007/s10055-010-0178-2#ref-CR19" id="ref-link-section-d39171e778">1996</a>) should be used, as they allow the user to inspect smaller regions of the world rather than gain an overview. The <i>position</i> of the workspace provides additional information. For instance, a distant yet small WKSP might indicate scaled manipulation, but it also indicates that arm extension techniques should be used—perhaps a selection/manipulation technique such as HOMER (Bowman and Hodges <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 35–38" href="/article/10.1007/s10055-010-0178-2#ref-CR3" id="ref-link-section-d39171e784">1997</a>).</p><p>Our model defines three <i>size ranges</i> for the current WKSP: small, medium, and large. Like LOC, these ranges are described by using threshold values; however, the mapping is made in a slightly different manner. LOC is described by an abstraction (a real number between 0 and 3), and a context recognition mechanism is free to map any information to the level of control. In contrast, the WKSP is described by a very real value, its physical dimensions. In order to give WKSP recognition mechanisms the flexibility to define what sizes are to be considered “small” or “large”, the CRM defines customizable threshold values to indicate small, normal, or large volumes. The distance between the user and the workspace is represented by a similar set of threshold values (near, medium, or far).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Frame of reference</h4><p>Many interactions are implicitly made with the world or user as the reference point (e.g. navigating to a specific place in the world). Often however, interactions can be made with other objects as the <i>frame of reference</i> (FOR). For example, users might want to navigate relative to another object or to move an object such that it is at a specific position relative to another object in the world (Ware and Arsenault <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ware C, Arsenault R (2004) Frames of reference in virtual object rotation. In: Proceedings of the 1st symposium on applied perception in graphics and visualization. Los Angeles, California, 7–8 Aug 2004" href="/article/10.1007/s10055-010-0178-2#ref-CR34" id="ref-link-section-d39171e804">2004</a>).</p><p>Unlike LOC and WKSP, the FOR can indeed be defined by a discrete value, namely the object in the world that is currently the frame of reference. This object can be <i>any</i> object within the world; however, with respect to selecting the most appropriate interaction technique, the main concern is generally whether the reference object is the world itself, the user, or any other object. This suggests three ranges: world, user, and “other.” Unlike the previous context components, the assignment of the range is trivial, as it is a straightforward classification of the object determined to be the frame of reference. As any object can be the current frame of reference, the developer may create arbitrary objects (such as lines or vectors) to serve as a frame of reference or extend the range to include pseudo-objects such as the viewport to aid in supporting interaction techniques utilizing view-fixed semantics as described in (Feiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Feiner S, Macintyre B, Haupt M, Solomon E (1993) Windows on the world: 2D windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, pp 145–155" href="/article/10.1007/s10055-010-0178-2#ref-CR11" id="ref-link-section-d39171e813">1993</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Object grouping</h4><p>It is common for an object to be part of a larger group or association of objects (Bukowski and Sequin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 131–138" href="/article/10.1007/s10055-010-0178-2#ref-CR9" id="ref-link-section-d39171e824">1995</a>). Depending on the current situation, the user may intend to treat the object as an individual (independent) object or may want to operate on either a subgroup or the entire group of objects.</p><p>Object groups can be static, such as the association between a table and chairs. In other situations, object groupings can be dynamic, such as an association created when two objects have been placed in close proximity to each other. Typically the underlying VR system will contain a scene graph, defining parent/child relationships between objects. While a parent child relationship could be expressed through the scene graph, object associations are often dynamic and are semantic in nature. It is also possible that objects can be in more than one group at any given time. These properties make the scene graph a poor method of representation. Our model provides the developer an additional construct, <i>group definition</i> (GD). The GD contains a <i>root</i> object and a set of one or more <i>child</i> objects. This is a purely semantic construct; creating and destroying a GD have no effect on the underlying structure of the VR system’s scene graph. When any manipulation or selection is performed on the <i>root</i> of a <i>group definition</i>, the same, or similar, operation can be applied to the <i>children</i>. Note—to implement pure sets, where actions on any object within the set are replicated on all other members, one must define several GD relationships.</p><p>The <i>grouping</i> context type (GRP) is defined as the set of all GDs relevant to the current interaction task—as determined by a grouping context recognition mechanism. The presence of object groups may or may not be relevant for deciding which interaction technique to use, but it is very likely to affect how the interaction technique works. Consider manipulating an object within a group; the decision of which manipulation technique to activate does not normally depend on whether the object is part of a group—the grouping only determines the <i>number</i> of objects affected by the manipulation. Our toolkit allows interaction techniques to access group information and behave accordingly.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Constraints</h4><p>Often either application semantics or the need to reduce complexity requires interaction to be constrained in some way. Constraints can be defined by the application, or they might be created when user behavior indicates they are appropriate. As with GRP, the <i>constraints</i> context type (CNSTR) is defined as the set of constraints <i>relevant</i> to the current interaction task. A constraints recognition mechanism is responsible for defining which constraints are relevant to the current interaction task. In addition, a constraint recognition mechanism may also create new constraints dynamically based on observation.</p><p>The presence of constraints may or may not affect the choice of an interaction technique. For manipulation and navigation tasks, constraints often limit the position of an object or the user. This constraint could be maintained by a specialized interaction technique; however, it is often sufficient to allow the interaction technique to perform the task and apply constraints separately. A constraint that requires the user to stay on the “ground” need not be supported by a special navigation technique; if stylus/cursor directed navigation is active then after the techniques moves the user by some distance the “ground constraint” can be applied independently.</p><p>From an implementation standpoint, constraints are supported in a very simple fashion. An abstract Constraint class is provided which defines one particular method—applyConstraints that must be implemented. When a constraint recognition mechanism provides a list of relevant constraint objects, the runtime can automatically invoke each of them by calling applyConstraints. Thus, to implement arbitrary constraints, a developer need only extend the base Constraint class.</p><h3 class="c-article__sub-heading" id="Sec10">Task/context pairs</h3><p>The purpose of modeling contextual information is to allow a developer to select the best interaction technique for the user. Thus, the combination of an interaction task (e.g. “translate object”) and a context component (e.g. high or low LOC) dictates the selection of an individual interaction technique (e.g. PRISM or Go–Go). This concept is referred to as “task/context pairs.”</p><p>Combining common interaction tasks and the context components described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab1">1</a> leads us to a grid mapping out the possible task/context pairs, which is presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab2">2</a>. Common interaction <i>tasks</i> (based on those identified in Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Proceedings of the virtual reality annual international symposium, pp 45–52" href="/article/10.1007/s10055-010-0178-2#ref-CR4" id="ref-link-section-d39171e896">1997</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman DA, Johnson D, Hodges LF (1999) Testbed evaluation of V.E. interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 26–33" href="/article/10.1007/s10055-010-0178-2#ref-CR5" id="ref-link-section-d39171e899">1999</a>) are listed as rows, and the context components defined above are listed as columns. Each cell in the table represents a <i>series</i> of interaction techniques; each specialized at providing an interface for the task and a specific value/range of the context component. For example, the cell in the table corresponding to Selection/Workspace might consist of “Touch Selection” when working in a small workspace and “Ray Casting” when working in a large workspace. The purpose of this table is to map where context-based decisions are often applicable in 3D interaction. For each marked cell, a UI designer must choose a method of recognizing the context value/range (using a CRM) and the best interaction technique for the current context value.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Common task/context pairs</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0178-2/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Of course, not all task/context pairs are as significant as others; while object translation is very dependent on the level of control desired, the user’s frame of reference is not particularly relevant to the method in which the user places/releases the object after translation. The most relevant task/context pairs in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab2">2</a> are marked with an X (this does not imply cells without a mark do not have interesting features of their own, however). This table is the product of in-depth studies of current interaction techniques and VR application domains. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec19">4</a> provides several concrete examples where task/context pairs are implemented by specific interaction techniques. A broader discussion, along with a review of the different interaction techniques in the literature that support each cell, can be found in (Frees <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh University, Department of Computer Science, 266 p. Available from Dissertations and Theses database. (UMI/AAT No. 3215837)" href="/article/10.1007/s10055-010-0178-2#ref-CR13" id="ref-link-section-d39171e1226">2006</a>).</p><h3 class="c-article__sub-heading" id="Sec11">Implementation of context model</h3><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig1">1</a>, the Context object is the central repository for contextual information and logic within the runtime. This object is populated with a set of TaskContext objects—representing the task context pairs outlined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec10">3.2</a>. A TaskContext object in turn holds a reference to two types of objects: (1) a CRM to implement the recognition mechanism for the context type, and (2) a ContextComponent, which defines the current context type. The developer indirectly deals with TaskContext objects by registering CRMs and context switch callbacks with the Context object. While the runtime is active, the Context object is updated before each frame drawn by the underlying VR system. The principal job of the update function is to use the registered CRMs (for each Task/Context pair) to determine whether a context switch has occurred and then to notify the application of such a switch via registered callbacks. The details of CRM objects will be discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec14">3.4</a>. The remainder of this section is dedicated to the implementation of context components and context switch callbacks.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Implementation of context components</h4><p>The base class of all context components is the ContextComponent object, an abstract class defining the interface that all components must implement. The interface class defines several utility methods, but the most important is the isContextSwitch method, called by the Context object to trigger context switches and callback functions.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figa_HTML.gif" alt="figurea" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div>
                           <p>This method should return “true” if the given ContextComponent (old context) is <i>fundamentally</i> different from itself. A context component is defined as <i>fundamentally</i> different from another when its “value” is in a different <i>range.</i> The predefined implementations of ContextComponent objects (the toolkit includes implementations of level of control, workspace, frame of reference, object groupings, and constraints) define context switches according to the ranges as described in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab1">1</a>. The application developer is free to extend the predefined context component objects to override this method.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Context switch callbacks</h4><p>Context switches are fundamental in the development of context-driven user interfaces—they indicate a new interaction technique is likely necessary. Simply recognizing when context switches occur is not sufficient, the system must have a way to notify the application that this event has occurred. To facilitate this, the Context object allows developers to register callback functions. The developer can register a callback for a specific task context pair, all pairs associated with a specified task, or even all pairs.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figb_HTML.gif?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figb_HTML.gif" alt="figureb" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div>
                           <p>For each of these methods, the developer specifies the task and/or context type that the callback will be registered to (each type of task and context component are enumerated by integer constants within the toolkit). The last parameter in each of these methods is of type ContextSwitchCallback, which is a typedef specifying the format of the function.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figc_HTML.gif?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figc_HTML.gif" alt="figurec" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div>
                           <p>The first parameter is simply a reference to the context object provided for convenience. The second parameter is of type ContextSwitch. As the name implies, this object describes the nature of the context switch that has occurred.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-d"><figure><div class="c-article-section__figure-content" id="Figd"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figd_HTML.gif?as=webp"></source><img aria-describedby="figure-d-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figd_HTML.gif" alt="figured" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-d-desc"></div></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec14">Context recognition</h3><p>Using a combination of specialized and effective interaction techniques has been recognized as an important principle in the development of immersive user interfaces (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001b" title="Bowman D, Kruijff E, LaViola J, Poupyrev I (2001b) An introduction to 3D user interface design. Presence Teleoper Virtual Environ 10(1):96–108" href="/article/10.1007/s10055-010-0178-2#ref-CR7" id="ref-link-section-d39171e1324">2001b</a>). Providing many different interaction techniques for each task is not a panacea; however, the user/system must have some method of choosing the interaction technique depending on the current context—a CRM is needed.</p><p>The most common way of communicating a discrete decision in an immersive system is through an explicit modal command—such as a menu, button click, or more sophisticated interfaces as found in Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference (2001)" title="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. Proc HCI Int:629–633" href="/article/10.1007/s10055-010-0178-2#ref-CR6" id="ref-link-section-d39171e1330">(2001)</a>, Grosjean and Coquillart (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Grosjean J, Coquillart S (2001) Command &amp; control cube: a shortcut paradigm for virtual environments. In: Immersive projection technology and virtual environments 2001 proceedings, pp 1–12" href="/article/10.1007/s10055-010-0178-2#ref-CR16" id="ref-link-section-d39171e1333">2001</a>), and Wesche (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wesche G (2003) The toolfinger: supporting complex direct manipulation in virtual environments. ACM international conference proceedings series—proceedings of the workshop on virtual environments, pp 39–45" href="/article/10.1007/s10055-010-0178-2#ref-CR35" id="ref-link-section-d39171e1336">2003</a>). When the number of interaction techniques available to the user is low, this form of <i>explicit</i> context recognition may be adequate—the user can explicitly indicate the context (and thus select the interaction technique they desire). The drawback of an <i>explicit</i> command is that system control widgets take up valuable screen space and often require the use of a keyboard, stylus button, or more specialized input device (scarce resources in an immersive environment).</p><p>Fortunately, context can often be recognized without an explicit modal command. Perhaps the most well-known example is Go–Go manipulation, which combines direct and amplified translation by selecting the active interaction technique based on the distance the arm has been extended. This can be thought of as a form of simple context recognition, the user signals to the system the need for low level of control by reaching out their arm.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Supporting the development of implicit CRMs</h4><p>Implicit CRMs vary significantly with respect to their complexity, and developing quality techniques can be challenging. Worse yet, poorly developed CRMs can profoundly confuse the user and acting on faulty contextual information to select interaction techniques leads to an extremely frustrating experience. Ultimately, implicit context recognition requires extensive user studies in order to determine what contextual information can be gleaned from user behavior and how this information can be acted upon.<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup>
                           </p><p>With these challenges in mind, we set out to develop a programming model that promotes code reuse, modularity, and good software design principles while developing implicit CRMs. Our solution focuses on the decoupling of implementation of implicit CRMs, interaction techniques, and the mechanisms providing automatic interaction technique activation (based on context). The following text explains, by demonstration, why this decoupling is advantageous and how it has been achieved.</p><p>Let us first consider Go–Go’s arm extension CRM from an implementation perspective. To determine the current distance between the user’s hand and body, a trivial calculation is performed based on tracking data. This data is unambiguous and can be mapped directly to a level of control (LOC) value. This value can then be used to control a scaling (or amplification) coefficient to provide 1:1 or 1:N mappings between physical and virtual hand movements. These mappings are so trivial that one may argue that a formal separation of the context recognition mechanism (mapping arm extension to LOC) and interaction technique (LOC to scaling coefficient) is in fact unnecessary; a monolithic approach that directly maps arm distance to the scaling coefficient is adequate.</p><p>Now let us consider a more complex scenario, one that focuses on using context to choose between two view control techniques: egocentric view control (analogous to the real world) and object-centered orbital viewing (Koller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 81–82" href="/article/10.1007/s10055-010-0178-2#ref-CR19" id="ref-link-section-d39171e1366">1996</a>). An implicit CRM for deciding between egocentric and orbital viewing might require the system to determine if a user is focused on a particular object or volume in the world (<i>frame of reference</i>). This could be achieved by observing the user’s interaction history and developing heuristics to identify their current frame of reference. The implementation of such a CRM would be complex and no doubt requires significant development time and testing. A monolithic interaction technique incorporating the frame of reference heuristics and both types of view control techniques could be developed nonetheless.</p><p>The software engineering problem with the monolithic approach occurs when the developer later wishes to create another user interface that also considers frame of reference, but perhaps for some other interaction task (e.g. object manipulation) and with other interaction techniques. In this situation, a developer has two options, (1) reimplement the heuristics for implicitly determining frame of reference or (2) devise a way to share the information already produced by the existing code.</p><p>This shared contextual information is precisely what our model provides. The model isolates the implementation of the CRM from the implementation of the interaction techniques by representing contextual information in a standard notation. CRMs communicate the current context through this standard (the Context object), and any interaction techniques can then make use of the information. Different CRMs can be swapped within an application, while the interaction techniques can remain the same—and vice versa.</p><p>This decoupling aids in the implementation of interaction techniques as well, as they can be implemented with the assumption that a specific context holds. Instead of embedding conditional code within interaction techniques to support changes to the context, the developer can focus on creating <i>individual and specialized</i> interaction techniques. The fact that context is represented in a standard form allows a generic runtime to be built that can simply activate the appropriate specialized interaction technique whenever a change in context occurs.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">CRM implementation</h4><p>CRMs are implemented through an abstract CRM class. CRM objects are registered to a specific task/context pair through the Context object. Once registered, the Context object uses the CRM to determine the current ContextComponent corresponding to the task/context pair. Below is the public interface for the CRM class:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-e"><figure><div class="c-article-section__figure-content" id="Fige"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fige_HTML.gif?as=webp"></source><img aria-describedby="figure-e-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fige_HTML.gif" alt="figuree" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-e-desc"></div></div></figure></div>
                           <p>The most important method in a CRM is the GetCurrentContext method. The runtime calls this method before each frame to obtain a current ContextComponent for the task/context pair. A CRM can produce any type of ContextComponent (typically Level of Control, Grouping, Frame of Reference, Workspace, or Constraints). The method in which the CRM determines the context is independent from the runtime (GetCurrentContext is the runtime’s only way of obtaining contextual information).</p><p>The runtime does help a CRM decide the current context by ensuring that its Update method is called before each frame is drawn (as long as the CRM has been registered for a task/context pair). This method allows the CRM to monitor all activity in the world on a regular basis. Special caching facilities are also implemented in the base CRM implementation to make the sharing of a single CRM instance between multiple task/context pairs more efficient. All CRM objects must also implement the Reset method, which clears (historical) data being collected to determine context. This method is not called by the runtime, rather by the user/application.</p><p>A developer creates new context recognition mechanisms by extending the CRM class. When creating a CRM to handle one of the predefined context components, a developer rarely inherits CRM directly. Instead, the developer would extend LevelOfControlCRM, WorkspaceCRM, FrameOfReferenceCRM, GroupingCRM, or ConstraintsCRM, respectively. These objects contain functionality more specific to the types of context components they produce, especially concerning the caching of context. Much more detail concerning the use of these objects is found in Frees (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh University, Department of Computer Science, 266 p. Available from Dissertations and Theses database. (UMI/AAT No. 3215837)" href="/article/10.1007/s10055-010-0178-2#ref-CR13" id="ref-link-section-d39171e1407">2006</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Example CRM object</h4><p>As an example, this section describes the implementation of a CRM designed to implicitly determine the LOC based on hand speed. This CRM is used to implement PRISM. HandSpeedCRM extends the LevelOfControlCRM. Below is the (simplified) class definition:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-f"><figure><div class="c-article-section__figure-content" id="Figf"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figf_HTML.gif?as=webp"></source><img aria-describedby="figure-f-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Figf_HTML.gif" alt="figuref" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-f-desc"></div></div></figure></div>
                           <p>HandSpeedCRM’s Update method is called by the runtime before each frame is drawn. This method records the position of an object (the hand) and calculates its speed. The speed is mapped into a value between 0 and 3 (level of control) and stored it in a private member called currentLOC. This translation is done through linear interpolation between the threshold values set by the SetMinThreshold, SetMaxTreshold, and SetPrecisionCuttoff.</p><p>After calling Update, the Context object will call GetCurrentContext, to determine if a context switch has occurred. This method simply wraps the currentLOC in a LevelOfControl context component object, which can then be used by the Context object to determine if a switch has indeed occurred. Note, while the Context object always calls the CRM’s Update and GetCurrentContext in sequence, they are implemented separately so external application code can obtain the current context from the CRM at any time with less overhead.</p><p>The toolkit provides other CRM objects that use hand position or speed to determine context, including CRMs to implement Go–Go’s hand distance methods and another to calculate level of control based on the hand’s rotational speed. In addition, CRM objects may use a more explicit method of determining context such as a menu system or widget (see examples in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec19">4</a>).</p><h3 class="c-article__sub-heading" id="Sec18">Modeling interaction tasks and techniques</h3><p>The preceding sections describe how our model achieves the two research goals outlined in the beginning of this section: (a) a quantifiable model of interaction context (Context and ContextComponent objects) and (b) a decoupling of context recognition from interaction technique (and application) development—achieved through CRM objects and context switch callbacks. A secondary goal of the reference implementation was a model and API for developing interaction techniques. Although the details of the API are beyond the scope of this paper, we elaborate some of the design features here in order to make the examples in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec19">4</a> more clear.</p><p>All interaction techniques implemented with the toolkit are managed by the runtime itself. Each interaction technique object is composed of a number of other object/components; decomposed according to the task decomposition outlined in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab2">2</a>. These components contain logic (and any necessary virtual objects/visualizations) that implements specific techniques for accomplishing an interaction task. These include methods in which a user indicates an action (such as “select”, or “stop manipulating”), the method in which a user indicates an object to select, the way in which a user manipulates an object, and the method used to indicate direction of navigation, etc. Interaction technique objects serve as containers for these components and for managing communication between them (ex. passing a selected object to a manipulation technique). Each of these components can be swapped in and out at runtime by calling the appropriate methods on the interaction technique object. It is by switching these components a developer can respond to context changes. Further detail of our interaction technique API can be found in Frees (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh University, Department of Computer Science, 266 p. Available from Dissertations and Theses database. (UMI/AAT No. 3215837)" href="/article/10.1007/s10055-010-0178-2#ref-CR13" id="ref-link-section-d39171e1454">2006</a>), and <a href="http://give.ramapo.edu/lab.html">http://give.ramapo.edu/lab.html</a>.</p><p>It is important to note that while CRMs and the firing of context switch events are completely managed by the runtime, what occurs in response to a context switch is entirely up to the application developer. Typically a context switch indicates that the interaction technique should be changed to something more applicable. This change could be automatic, especially in circumstances where the interaction techniques blend together well. For example, direct manipulation can transition naturally into scaled interaction without creating an abrupt change in the interface. In other circumstances the two interaction techniques may be quite different—and automatically transitioning between them could lead to a jarring experience for the user. In this situation a context switch may simply be used to recommend a different technique to the user (with explicit confirmation). Often an application developer may choose the set of interaction techniques to be provided based on how well they can be blended together and how seamlessly the user can be switched between them; however, our framework does not require this.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Example implementations</h2><div class="c-article-section__content" id="Sec19-content"><p>The decoupling of context, CRM, and interaction technique allows developers to reuse and combine components to create new user interfaces techniques quickly. Below we present an outline of how the model supports a PRISM hybrid manipulation technique using hand speed (level of control) or workspace to determine the appropriate translation technique. The toolkit’s reliance on well-defined abstract classes allows significant code reuse between the two interfaces, with the only difference being the implementation of the CRM and registering to different task/context pairs. Later in the section we also show how the toolkit is reconfigured to support other interaction techniques from the literature. While by no means a formal proof of the toolkit, each of the examples that follow have been successfully implemented and have served as an initial evaluation of the effectiveness and flexibility of the model.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig2">2</a> illustrates a default implementation of PRISM translation. An object translation technique consists of three distinct parts—a way in which the user translates the object, a way in which the user indicates that the translation is complete (e.g. a button press, voice command, etc.), and finally a way in which the object is placed (perhaps gravity is applied, etc.). We focus here on the way in which an object is translated, and how level of control dictates the proper technique. In PRISM, scaled translation (implemented by the ScaledTranslator object) is active when the level of control is <i>high</i> and direct translation (implemented by the DirectTranslator object) is active when the LOC value is normal or low. Using context switch callbacks, PRISM can easily be implemented by switching between the component translator objects based on LOC. The current LOC value is determined by the same HandSpeedCRM object (registered to the translation/level of control task/context pair) described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0178-2#Sec17">3.4.3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>PRISM translation implementation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig3">3</a> illustrates a significantly different user interface, but one that reuses most of the same programming code. In this example, PRISM translation is being implemented using all of the same translation code (DirectTranslator and ScaledTranslator), however the techniques are being activated based on the current <i>workspace</i>—not level of control. In addition, a new CRM is introduced (ExplicitWorkspaceCRM), which calculates the current workspace using a simple widget (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig4">4</a>). In this case, the DirectTranslator could be activated when the workspace is large and the ScaledTranslator would be activated when the workspace is small. More detailed user analysis may also reveal a more implicit method of recognizing the current workspace size, at which time the new CRM could be substituted into the interface.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>PRISM translation based on workspace</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Explicit workspace CRM visualization—the user explicitly resizes and moves the outlined volume to indicate their current workspace</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The example above demonstrates how different CRMs can be plugged into the runtime to fundamentally change the interface without changing the code/implementation of the manipulation technique itself. Similarly, one can reuse the same CRM across different tasks. For example, a HandRotationSpeedCRM has been developed that maps the rotational speed of a user’s hand to the level of control context component (slow hand speed maps to high level of control). This determination can be used across interaction tasks to choose between object rotation techniques or selection techniques. The effectiveness of this CRM was demonstrated with PRISM scaled rotation and ray casting (Frees et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)" href="/article/10.1007/s10055-010-0178-2#ref-CR14" id="ref-link-section-d39171e1555">2007</a>). As shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig5">5</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig6">6</a>, the implementation of these techniques differ only in the interaction task and techniques activated—the CRM is completely reused.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>PRISM rotation implementation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>PRISM ray-casting implementation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The interface in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig5">5</a> implements object rotation in much the same way as the PRISM translation example in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig2">2</a>. Using the same HandRotationSpeedCRM, one can also build PRISM ray casting as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig6">6</a>. This user interface activates normal ray casting for normal and low LOC values and scaled ray casting for high LOC values (i.e. when a high degree of control/accuracy is needed). Each of these techniques are again implemented in their own objects (DirectRayCaster and ScaledRayCaster) and extend the base Selector class, which dictates basic functionality required in order to allow the runtime system (and SelectionTechnique object) to work with them.</p><p>Other established user interface techniques are also supported by our model and programming toolkit. Go–Go object manipulation can be implemented in a similar fashion as the PRISM manipulation technique shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig2">2</a>; however, in this implementation a hand position CRM could be used to drive the level of control, and an amplified translator could be used for low levels of control (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Go–go interaction implementation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Moving out of the realm of object selection/manipulation, the framework can also be used to work with viewpoint control. One implementation might offer the user the ability to switch between normal viewing and orbital viewing when the workspace becomes focused and small (as orbital viewing may be more helpful when concentrating on a small area). In this situation, one could reuse the explicit workspace constraint from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig3">3</a> but now use the workspace value to switch between viewpoint control techniques (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig8">8</a>).<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup>
                        </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Egocentric and orbital viewing based on workspace</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Of course, more than one context component can work to drive the user interface. Our final example combines Go–Go manipulation with an alignment constraint that is automatically created as the object being manipulated comes to rest near another object. This type of user interface resembles the object associations described in Bukowski and Sequin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 131–138" href="/article/10.1007/s10055-010-0178-2#ref-CR9" id="ref-link-section-d39171e1675">1995</a>), where a virtual object such as a picture frame may be constrained to snap against a virtual wall depending on its proximity. In this implementation, an implicit constraint recognition mechanism is driving the creation of an alignment constraint and the object placer is using this constraint to move the object to its final resting location (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0178-2#Fig9">9</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0178-2/MediaObjects/10055_2010_178_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Go–Go with association constraints</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0178-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>This example is of particular note, as the addition of such a constraint is being applied at the time the object is released, meaning the actual selection of the interaction technique is unaffected and somewhat irrelevant. Alternatively, if the translator object implementing the object translation technique were to also take into account current constraints, the user could be provided with instant feedback by constraining the object while it was being manipulated.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusion</h2><div class="c-article-section__content" id="Sec20-content"><p>This research consists of both abstract and concrete contributions. We first outline the important context types involved in interaction in immersive virtual environments and describe the importance of maintaining a separation between the implementation of the context recognition mechanism and interaction technique. We have shown how several successful interaction techniques fit within this model (Go–Go, PRISM). We believe this model also suggests a general strategy toward the development of context-sensitive user interfaces.</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>Select relevant Task/Context pairs for the particular application: <i>Each application requires users to perform some set of interaction tasks. In addition, most applications incorporate a number of interaction contexts. Identifying these pairs is the first step towards developing a context</i>-<i>sensitive user interface</i>.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Select a Context Recognition Mechanism (CRM) to calculate each Task/Context pair: <i>The selected CRM may be implicit (context is deduced from observed behavior) or explicit (such as a menu system). Extensive domain knowledge cannot be substituted</i>—<i>for each task/context pair, the development and selection of appropriate CRMs requires significant thought, development time, and evaluation</i>.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Select interaction techniques to support each range associated with each Task/Context pair: <i>The selection of which technique to use for each context should be informed by the literature and usability studies.</i>
                                 </p>
                    
                  </li>
                </ol>
                     <p>To aid in the development of context-sensitive user interfaces, we have developed a programming toolkit that follows the context model. We have presented the most important aspects of the toolkit, specifically its treatment of context and the method in which developers can reuse and combine interaction techniques and context recognition techniques to implement user interfaces. We have used the toolkit to implement user interfaces dealing with each of the context components identified in this paper, and the toolkit as proven to be flexible and fairly easy to work with—as demonstrated by the fact that undergraduate students have been able to use the toolkit to implement the examples provided in the previous section of this paper. We have also found that using the toolkit works well for rapid prototype development. This is due to the decoupling of interaction techniques and CRMs, which allows the developer to create interaction techniques with a simple menu-based CRM and then once the interaction techniques have been perfected, turn their attention to creating the implicit or more sophisticated CRM.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Future work</h2><div class="c-article-section__content" id="Sec21-content"><p>To a large extent, the goal of this research was the facilitation of future work. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0178-2#Tab2">2</a> identifies task/context pairs that should be considered when designing a user interface and serves as a roadmap for the future development of interaction techniques; each task/context pair represents a relatively independent path of research. The future development of effective and modular context recognition mechanisms also provides a type of feedback loop. As more reusable CRMs and interaction techniques are developed, these components can be combined in different ways without requiring extensive development work. It is hoped that this multiplicative effect can eventually make developing context-sensitive user interface less of a programming task and more a careful decision process focusing on assigning the best CRMs and interaction techniques for the task/context pairs relevant to the target application.</p><p>Part of our future efforts will also be to enhance and extend our programming toolkit. The toolkit itself is merely a reference implementation and as more user interfaces are implemented with it there will undoubtedly be a need for more functionality to be added. It is also currently closely integrated with the SVE virtual reality library; although SVE is freely available and works with a large variety of VR platforms, we are interested in creating an abstract layer between our toolkit and the underlying VR library (scene graph, tracking, etc.) so our implementation can be useful to a wider audience.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>The SVE toolkit is freely available for academic use at <a href="http://give.ramapo.edu/lab.html">http://give.ramapo.edu/lab.html</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>The true value of the LOC is always available to the developer. The low, normal, and high ranges are defined simply for convenience and as a guideline.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>The user could also explicitly indicate their workspace—perhaps by entering coordinates into the application.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>We do not claim that our model replaces the thorough usability analysis involved in CRM development—it only aids in their implementation.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>Switching viewpoint control techniques certainly has the possibility of disorienting the user, and thus this is a prime example of a situation where explicit confirmation might be used so the user is not “surprised” by the change.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Albinsson PA, Zhai S (2003) High precision touch screen interaction. In: Proceedings of SIGCHI conference on h" /><p class="c-article-references__text" id="ref-CR1">Albinsson PA, Zhai S (2003) High precision touch screen interaction. In: Proceedings of SIGCHI conference on human factors in computing systems, pp 105–112</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Beir EA (1990) Snap-dragging in three dimensions. In: Proceedings of the ACM symposium on interactive 3D graph" /><p class="c-article-references__text" id="ref-CR2">Beir EA (1990) Snap-dragging in three dimensions. In: Proceedings of the ACM symposium on interactive 3D graphics, 24(2):193–204</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immers" /><p class="c-article-references__text" id="ref-CR3">Bowman DA, Hodges LF (1997) An evaluation of techniques for grabbing and manipulating remote objects in immersive virtual environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 35–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint mot" /><p class="c-article-references__text" id="ref-CR4">Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques. In: Proceedings of the virtual reality annual international symposium, pp 45–52</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Johnson D, Hodges LF (1999) Testbed evaluation of V.E. interaction techniques. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR5">Bowman DA, Johnson D, Hodges LF (1999) Testbed evaluation of V.E. interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 26–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction tec" /><p class="c-article-references__text" id="ref-CR6">Bowman D, Wingrave C, Campbell J, Ly V (2001) Using pinch gloves for both natural and abstract interaction techniques in virtual environments. Proc HCI Int:629–633</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Kruijff E, LaViola J, Poupyrev I (2001b) An introduction to 3D user interface design. Presence Teleo" /><p class="c-article-references__text" id="ref-CR7">Bowman D, Kruijff E, LaViola J, Poupyrev I (2001b) An introduction to 3D user interface design. Presence Teleoper Virtual Environ 10(1):96–108</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman D, Badillo B, Manek D (2007) Evaluating the need for display-specific and device-specific 3D interactio" /><p class="c-article-references__text" id="ref-CR8">Bowman D, Badillo B, Manek D (2007) Evaluating the need for display-specific and device-specific 3D interaction techniques. In: Proceedings of virtual reality international conference (in Lecture notes in computer science, vol 4563, pp 195–204)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation." /><p class="c-article-references__text" id="ref-CR9">Bukowski RW, Sequin CH (1995) Object associations: a simple and practical approach to virtual 3D manipulation. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 131–138</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="CDI Toolkit Web Site: http://give.ramapo.edu/lab.html&#xA;                        " /><p class="c-article-references__text" id="ref-CR37">CDI Toolkit Web Site: <a href="http://give.ramapo.edu/lab.html">http://give.ramapo.edu/lab.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen J, Bowman D (2006) Evaluation of the effectiveness of cloning techniques for architectural virtual enviro" /><p class="c-article-references__text" id="ref-CR10">Chen J, Bowman D (2006) Evaluation of the effectiveness of cloning techniques for architectural virtual environment. In: Proceedings of IEEE virtual reality. Alexandria, VA, pp 103–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Feiner S, Macintyre B, Haupt M, Solomon E (1993) Windows on the world: 2D windows for 3D augmented reality. In" /><p class="c-article-references__text" id="ref-CR11">Feiner S, Macintyre B, Haupt M, Solomon E (1993) Windows on the world: 2D windows for 3D augmented reality. In: Proceedings of the ACM symposium on user interface software and technology, pp 145–155</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Forsberg A, Herndon K, Zelesnik R (1996) Aperture based selection for immersive virtual environments. In: Proc" /><p class="c-article-references__text" id="ref-CR12">Forsberg A, Herndon K, Zelesnik R (1996) Aperture based selection for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 95–96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh Uni" /><p class="c-article-references__text" id="ref-CR13">Frees S (2006) Intent driven interaction in immersive virtual environments (Doctoral Dissertation). Lehigh University, Department of Computer Science, 266 p. Available from Dissertations and Theses database. (UMI/AAT No. 3215837)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. A" /><p class="c-article-references__text" id="ref-CR14">Frees S, Kessler GD, Kay E (2007) PRISM interaction for enhancing control in immersive virtual environments. ACM Trans Comput Hum Interact 14(1)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Greenhalgh, S. Benford, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Greenhalgh C, Benford S (1995) MASSIVE: a collaborative virtual environment for teleconferencing. ACM Trans Co" /><p class="c-article-references__text" id="ref-CR15">Greenhalgh C, Benford S (1995) MASSIVE: a collaborative virtual environment for teleconferencing. ACM Trans Comput Hum Interact 2(3):239–261</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F210079.210088" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=MASSIVE%3A%20a%20collaborative%20virtual%20environment%20for%20teleconferencing&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=2&amp;issue=3&amp;pages=239-261&amp;publication_year=1995&amp;author=Greenhalgh%2CC&amp;author=Benford%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grosjean J, Coquillart S (2001) Command &amp; control cube: a shortcut paradigm for virtual environments. In: Imme" /><p class="c-article-references__text" id="ref-CR16">Grosjean J, Coquillart S (2001) Command &amp; control cube: a shortcut paradigm for virtual environments. In: Immersive projection technology and virtual environments 2001 proceedings, pp 1–12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kessler GD (1999) A framework for interactors in immersive virtual environments. Proc IEEE Virtual Real, pp 19" /><p class="c-article-references__text" id="ref-CR17">Kessler GD (1999) A framework for interactors in immersive virtual environments. Proc IEEE Virtual Real, pp 190–197</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GD. Kessler, DA. Bowman, LF. Hodges, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Kessler GD, Bowman DA, Hodges LF (2000) The simple virtual environment library, and extensible framework for b" /><p class="c-article-references__text" id="ref-CR18">Kessler GD, Bowman DA, Hodges LF (2000) The simple virtual environment library, and extensible framework for building VE applications. Presence Teleoper Virtual Environ 9(2):187–208</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20simple%20virtual%20environment%20library%2C%20and%20extensible%20framework%20for%20building%20VE%20applications&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=9&amp;issue=2&amp;pages=187-208&amp;publication_year=2000&amp;author=Kessler%2CGD&amp;author=Bowman%2CDA&amp;author=Hodges%2CLF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual" /><p class="c-article-references__text" id="ref-CR19">Koller D, Mine M, Hudson S (1996) Head-tracked orbital viewing: an interaction technique for immersive virtual environments. In: Proceedings of the ACM symposium on user interface software and technology, pp 81–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DP. Mapes, JM. Moshell, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Mapes DP, Moshell JM (1995) A two-handed interface for object manipulation in virtual environments. Presence: " /><p class="c-article-references__text" id="ref-CR20">Mapes DP, Moshell JM (1995) A two-handed interface for object manipulation in virtual environments. Presence: Teleoper Virtual Environ 4(4):403–416</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20two-handed%20interface%20for%20object%20manipulation%20in%20virtual%20environments&amp;journal=Presence%3A%20Teleoper%20Virtual%20Environ&amp;volume=4&amp;issue=4&amp;pages=403-416&amp;publication_year=1995&amp;author=Mapes%2CDP&amp;author=Moshell%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nardi BA (ed) (1996) Context and consciousness—activity theory and human computer interaction. MIT Press, Camb" /><p class="c-article-references__text" id="ref-CR21">Nardi BA (ed) (1996) Context and consciousness—activity theory and human computer interaction. MIT Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pierce J, Pausch R (2004) Navigation with place representations and visible landmarks. In: Proc IEEE Virtual R" /><p class="c-article-references__text" id="ref-CR22">Pierce J, Pausch R (2004) Navigation with place representations and visible landmarks. In: Proc IEEE Virtual Real, pp 173–180</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pierce J, Forsberg A, Conway M, Hong S, Zeleznik R, Mine M (1997) Image plane interaction techniques in 3D imm" /><p class="c-article-references__text" id="ref-CR23">Pierce J, Forsberg A, Conway M, Hong S, Zeleznik R, Mine M (1997) Image plane interaction techniques in 3D immersive environments. In: Proceedings of the ACM symposium on interactive 3D graphics, pp 39–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping " /><p class="c-article-references__text" id="ref-CR24">Poupyrev I, Billinghurst M, Weghorst S, Ichikawa T (1996) The go-go interaction technique: non-linear mapping for direct manipulation in VR. In: Proceedings of the ACM symposium on user interface software and technology, pp 79–80</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1997) A framework and testbed for studying manipulation te" /><p class="c-article-references__text" id="ref-CR25">Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1997) A framework and testbed for studying manipulation techniques for immersive VR. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 21–28</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, S. Weghorst, M. Billinghurst, T. Ichikawa, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1998) Egocentric object manipulation in virtual environmen" /><p class="c-article-references__text" id="ref-CR26">Poupyrev I, Weghorst S, Billinghurst M, Ichikawa T (1998) Egocentric object manipulation in virtual environments: empirical evaluation of interaction techniques. Comput Graph Forum 17(3):41–52</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Egocentric%20object%20manipulation%20in%20virtual%20environments%3A%20empirical%20evaluation%20of%20interaction%20techniques&amp;journal=Comput%20Graph%20Forum&amp;volume=17&amp;issue=3&amp;pages=41-52&amp;publication_year=1998&amp;author=Poupyrev%2CI&amp;author=Weghorst%2CS&amp;author=Billinghurst%2CM&amp;author=Ichikawa%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ray A, Bowman D (2007) Towards a system for reusable 3D interaction techniques. In: Proceedings of the ACM sym" /><p class="c-article-references__text" id="ref-CR27">Ray A, Bowman D (2007) Towards a system for reusable 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, pp 187–190</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ruddle RA, Jones DM (2001) Movement in cluttered virtual environments. Presence: Teleoper Virtual Environ 10:5" /><p class="c-article-references__text" id="ref-CR28">Ruddle RA, Jones DM (2001) Movement in cluttered virtual environments. Presence: Teleoper Virtual Environ 10:511–524</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stoakley R, Conway M, Paush R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceeding" /><p class="c-article-references__text" id="ref-CR29">Stoakley R, Conway M, Paush R (1995) Virtual reality on a WIM: interactive worlds in miniature. In: Proceedings of the ACM conference on human factors in computing systems, pp 265–272</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. Proc IEEE Virt" /><p class="c-article-references__text" id="ref-CR30">Stuerzlinger W, Smith G (2002) Efficient manipulation of object groups in virtual environments. Proc IEEE Virtual Real, pp 251–258</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tan DS, Robertson GG, Czerwinski M (2001) Exploring 3D navigation: combining speed-coupled flying with orbitin" /><p class="c-article-references__text" id="ref-CR31">Tan DS, Robertson GG, Czerwinski M (2001) Exploring 3D navigation: combining speed-coupled flying with orbiting. In: Proceedings of the ACM conference on human factors in computing systems, pp 418–424</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tanriverdi V, Jacob RJ (2001) VRID: a design model and methodology for developing virtual reality interfaces. " /><p class="c-article-references__text" id="ref-CR32">Tanriverdi V, Jacob RJ (2001) VRID: a design model and methodology for developing virtual reality interfaces. Proceedings of the ACM Symposium on Virtual Reality Software and Technology, pp 175–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Usoh M, Arthur K, Whitton MC, Bastos R, Steed A, Slater M (1999) Walking &gt; walking-in-place &gt; flying, in Virtu" /><p class="c-article-references__text" id="ref-CR33">Usoh M, Arthur K, Whitton MC, Bastos R, Steed A, Slater M (1999) Walking &gt; walking-in-place &gt; flying, in Virtual Environments. In: Proceedings of the 26th annual conference on computer graphics and interactive techniques, pp 259–264</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ware C, Arsenault R (2004) Frames of reference in virtual object rotation. In: Proceedings of the 1st symposiu" /><p class="c-article-references__text" id="ref-CR34">Ware C, Arsenault R (2004) Frames of reference in virtual object rotation. In: Proceedings of the 1st symposium on applied perception in graphics and visualization. Los Angeles, California, 7–8 Aug 2004</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wesche G (2003) The toolfinger: supporting complex direct manipulation in virtual environments. ACM internatio" /><p class="c-article-references__text" id="ref-CR35">Wesche G (2003) The toolfinger: supporting complex direct manipulation in virtual environments. ACM international conference proceedings series—proceedings of the workshop on virtual environments, pp 39–45</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wingrave C, Bowman D (2008) Tiered developer-centric representations for 3D interfaces: concept-oriented desig" /><p class="c-article-references__text" id="ref-CR36">Wingrave C, Bowman D (2008) Tiered developer-centric representations for 3D interfaces: concept-oriented design in chasm. IEEE Virtual Real</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0178-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Ramapo College of New Jersey, Mahwah, NJ, USA</p><p class="c-article-author-affiliation__authors-list">Scott Frees</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Scott-Frees"><span class="c-article-authors-search__title u-h3 js-search-name">Scott Frees</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Scott+Frees&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Scott+Frees" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Scott+Frees%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0178-2/email/correspondent/c1/new">Scott Frees</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Context-driven%20interaction%20in%20immersive%20virtual%20environments&amp;author=Scott%20Frees&amp;contentID=10.1007%2Fs10055-010-0178-2&amp;publication=1359-4338&amp;publicationDate=2010-11-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Frees, S. Context-driven interaction in immersive virtual environments.
                    <i>Virtual Reality</i> <b>14, </b>277–290 (2010). https://doi.org/10.1007/s10055-010-0178-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0178-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-08-06">06 August 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-10-28">28 October 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-16">16 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-12">December 2010</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0178-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0178-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Context-sensitive interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments, 3DUI, interaction techniques</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0178-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=178;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

