<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Haptic interpersonal communication: improvement of actions coordinatio"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This article explores the use of haptic feedback for interpersonal communication in collaborative virtual environments. After a detailed presentation of all communication mechanisms involved, we..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/16/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2011 16:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-11-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This article explores the use of haptic feedback for interpersonal communication in collaborative virtual environments. After a detailed presentation of all communication mechanisms involved, we propose the investigation of a low-level communication approach through the feedthrough mechanism. This channel is used to communicate kinematic information about a partner&#8217;s gestures during closely coupled collaboration. Several communication metaphors, with complementary behaviors, were investigated to improve the coordination between two partners during an assembly task. The results clearly show the role of communication strategies for the improvement of gesture coordination and highlight the correlation between applied force and the level of efficiency."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-11-23"/>

    <meta name="prism.volume" content="16"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="173"/>

    <meta name="prism.endingPage" content="186"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0201-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0201-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0201-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0201-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments"/>

    <meta name="citation_volume" content="16"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2012/09"/>

    <meta name="citation_online_date" content="2011/11/23"/>

    <meta name="citation_firstpage" content="173"/>

    <meta name="citation_lastpage" content="186"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0201-2"/>

    <meta name="DOI" content="10.1007/s10055-011-0201-2"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0201-2"/>

    <meta name="description" content="This article explores the use of haptic feedback for interpersonal communication in collaborative virtual environments. After a detailed presentation of al"/>

    <meta name="dc.creator" content="Jean Simard"/>

    <meta name="dc.creator" content="Mehdi Ammi"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. ICRA 454&#8211;460. 
                    http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4209133
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=An experimental study on the role of touch in shared virtual environments; citation_author=C Basdogan, C-h Ho, MA Srinivasan, M Slater; citation_volume=7; citation_publication_date=2000; citation_pages=443-460; citation_doi=10.1145/365058.365082; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Media spaces: bringing people together in a video, audio, and computing environment; citation_author=SA Bly, SR Harrison, S Irwin; citation_volume=36; citation_issue=1; citation_publication_date=1993; citation_pages=28-47; citation_doi=10.1145/151233.151235; citation_id=CR3"/>

    <meta name="citation_reference" content="Catlin T, Bush P, Yankelovich N (1989) InterNote: extending a hypermedia framework to support annotative collaboration. In: Proceeding of hypertext&#8217;89 ACM conference, November 1989, pp 365&#8211;378"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Multimed Comput Commun Appl (TOMCCAP); citation_title=Touchable 3D video system; citation_author=J Cha, MA Eid, A El-Saddik; citation_volume=5; citation_issue=4; citation_publication_date=2009; citation_pages=1-25; citation_doi=10.1145/1596990.1596993; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Designing haptic icons to support collaborative turn-taking; citation_author=A Chan, KE MacLean, J McGrenere; citation_volume=66; citation_publication_date=2008; citation_pages=333-355; citation_doi=10.1016/j.ijhcs.2007.11.002; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=An investigation of groupware support for collaborative awareness through distortion-oriented views; citation_author=A Cockburn, P Weir; citation_volume=11; citation_issue=3; citation_publication_date=1999; citation_pages=231-255; citation_doi=10.1207/S15327590IJHC1103_3; citation_id=CR9"/>

    <meta name="citation_reference" content="Codella CF, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: CHI 1992, pp 329&#8211;334"/>

    <meta name="citation_reference" content="Collier G (1985) Emotional expression. Lawrence Erlbaum Associates, Hillsdale, NJ. 
                    http://www.questia.com/PM.qst?a=o&amp;d=28053073
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Comput Supported Coop Work; citation_title=Challenges for cooperative work on the web: an analytical approach; citation_author=AJ Dix; citation_volume=6; citation_issue=2/3; citation_publication_date=1997; citation_pages=135-156; citation_doi=10.1023/A:1008635907287; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Groupware: some issues and experiences; citation_author=C Ellis, SJ Gibbs, GL Rein; citation_volume=34; citation_issue=1; citation_publication_date=1991; citation_pages=38-58; citation_doi=10.1145/99977.99987; citation_id=CR13"/>

    <meta name="citation_reference" content="Enriquez M, MacLean KE et al (2006) Haptic phonemes: basic building blocks of haptic communication. In: Proceedings of 8th int&#8217;l conf. on multimodal interfaces (ICMI &#8216;06). Banff, Canada, pp 302&#8211;309"/>

    <meta name="citation_reference" content="Gentry S (2005) Dancing cheek to cheek: haptic communication between partner dancers and swing as a finite state machine. Ph.D thesis, Massachusetts Institute of Technology"/>

    <meta name="citation_reference" content="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of the 45th annual meeting of the human factors and ergonomics society, 8&#8211;12 October 2001, Minneapolis/St. Paul, MN, pp 1911&#8211;1915"/>

    <meta name="citation_reference" content="Grasset R (2004) Environnement de r&#233;alit&#233; augment&#233;e 3D cooperative: approche colocalis&#233;e sur table. ARTIS-GRAVIR/IMAG-INRIA"/>

    <meta name="citation_reference" content="Grasset R, Lamb P, Billinghurst M (2005) Evaluation of mixed-space collaboration. In: ISMAR &#8216;05: Proceedings of the 4th IEEE/ACM international symposium on mixed and augmented reality, pp 90&#8211;99"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=The effects of workspace awareness support on the usability of real-time distributed groupware; citation_author=C Gutwin, S Greenberg; citation_volume=6; citation_issue=3; citation_publication_date=1999; citation_pages=243-281; citation_doi=10.1145/329693.329696; citation_id=CR19"/>

    <meta name="citation_reference" content="Gutwin C, Greenberg S (2000) The mechanics of collaboration: developing low cost usability evaluation methods for shared workspaces. In: Proceedings of the 9th IEEE international workshops on enabling technologies: infrastructure for collaborative enterprises, 4&#8211;16 June 2000. WETICE. IEEE Computer Society, Washington, DC, pp 98&#8211;103"/>

    <meta name="citation_reference" content="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: GROUP&#8217;03: Proceedings of the 2003 international ACM SIGGROUP conference on supporting group work. Sanibel Island, Florida, USA, pp 258&#8211;267"/>

    <meta name="citation_reference" content="Kim Y-S, Ryu J-H (2009) Performance analysis of teleoperation systems with different haptic and video time-delay. In: Proceedings of the ICROS-SICE international joint conference 2009. Fukuoka, Japan, pp 3371&#8211;3375"/>

    <meta name="citation_reference" content="Kj&#246;lberg J, Salln&#228;s E-L (2002) Supporting object handling and hand over tasks in haptic Collaborative Virtual Environments. In: Wall S, Riedel B, Crossan A, McGee MR (eds) Proceedings of Eurohaptics&#8242;02, May 2002, pp 71&#8211;76"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=A network architecture supporting consistent rich behavior in collaborative interactive applications; citation_author=J Marsh, M Glencross, S Pettifer, RJ Hubbold; citation_volume=12; citation_issue=3; citation_publication_date=2006; citation_pages=405-416; citation_doi=10.1109/TVCG.2006.40; citation_id=CR24"/>

    <meta name="citation_reference" content="Pe&#241;a P&#233;rez Negr&#243;n A, de Antonio Jim&#233;nez A (2009) Using avatar&#8217;s nonverbal communication to monitor collaboration in a task-oriented learning situation in a CVE. Workshop on intelligent and innovative support for collaborative learning activities. Rhodes, Greece, 8&#8211;13 June 2009, pp 19&#8211;26"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Task analysis for groupware usability evaluation: modeling shared-workspace tasks with the mechanics of collaboration; citation_author=D Pinelle, C Gutwin, S Greenberg; citation_volume=10; citation_issue=4; citation_publication_date=2003; citation_pages=281-311; citation_doi=10.1145/966930.966932; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=J Inf Technol Educ; citation_title=Collaborative virtual environments to support communication and community in internet-based distance education; citation_author=S Redfern, N Naughton; citation_volume=1; citation_issue=3; citation_publication_date=2002; citation_pages=201-211; citation_id=CR28"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Haptics; citation_title=Physical collaboration of human-human and human-robot teams; citation_author=KB Reed, MA Peshkin; citation_volume=1; citation_issue=2; citation_publication_date=2008; citation_pages=108-120; citation_doi=10.1109/TOH.2008.13; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Comput Hum Interact; citation_title=Supporting presence in collaborative environments by haptic force feedback; citation_author=E Sallnas, K Rassmus-grohn, C Sjostrom; citation_volume=7; citation_issue=4; citation_publication_date=2000; citation_pages=461-476; citation_doi=10.1145/365058.365086; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Two eyes for an eye: the neuroscience of force escalation; citation_author=SS Shergill, PM Bays, CD Frith, DM Wolpert; citation_volume=301; citation_issue=5630; citation_publication_date=2003; citation_pages=187; citation_doi=10.1126/science.1085327; citation_id=CR31"/>

    <meta name="citation_reference" content="Simard J, Ammi M, Auvray M (2010) Closely coupled collaboration for search tasks. In: Proceedings of the 17th ACM symposium on Virtual Reallity Software and Technology (VRST), vol 1. pp 181&#8211;182"/>

    <meta name="citation_reference" content="Takemura H, Kishino F (1992) Cooperative work environment using virtual workspace. In: CSCW &#8216;92 proceedings, Toronto, Canada, pp 226&#8211;232"/>

    <meta name="citation_reference" content="Varadaradjou ES, Dax P, Grumbach A (2006) Improved communication in virtual worlds. In: Virtual reality international conference. Rocquencourt, Avril, pp 103&#8211;111"/>

    <meta name="citation_reference" content="citation_journal_title=IPSJ SIG Notes; citation_title=Collaboration in virtual environment with force feedback; citation_author=H Yano, H Iwata; citation_volume=94; citation_issue=59; citation_publication_date=1994; citation_pages=31-34; citation_id=CR35"/>

    <meta name="citation_author" content="Jean Simard"/>

    <meta name="citation_author_email" content="Jean.Simard@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, University of Paris-Sud, Orsay, France"/>

    <meta name="citation_author" content="Mehdi Ammi"/>

    <meta name="citation_author_email" content="Mehdi.Ammi@limsi.fr"/>

    <meta name="citation_author_institution" content="LIMSI-CNRS, University of Paris-Sud, Orsay, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0201-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2012/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0201-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments"/>
        <meta property="og:description" content="This article explores the use of haptic feedback for interpersonal communication in collaborative virtual environments. After a detailed presentation of all communication mechanisms involved, we propose the investigation of a low-level communication approach through the feedthrough mechanism. This channel is used to communicate kinematic information about a partner’s gestures during closely coupled collaboration. Several communication metaphors, with complementary behaviors, were investigated to improve the coordination between two partners during an assembly task. The results clearly show the role of communication strategies for the improvement of gesture coordination and highlight the correlation between applied force and the level of efficiency."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0201-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Collaborative virtual environments, Haptics, Awareness, Sensorial communication, Communication metaphors","kwrd":["Collaborative_virtual_environments","Haptics","Awareness","Sensorial_communication","Communication_metaphors"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0201-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0201-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=201;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0201-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0201-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0201-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-11-23" itemprop="datePublished">23 November 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jean-Simard" data-author-popup="auth-Jean-Simard">Jean Simard</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Paris-Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, University of Paris-Sud, Orsay, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mehdi-Ammi" data-author-popup="auth-Mehdi-Ammi" data-corresp-id="c1">Mehdi Ammi<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Paris-Sud" /><meta itemprop="address" content="grid.5842.b, 0000000121712558, LIMSI-CNRS, University of Paris-Sud, Orsay, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 16</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">173</span>–<span itemprop="pageEnd">186</span>(<span data-test="article-publication-year">2012</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">595 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0201-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This article explores the use of haptic feedback for interpersonal communication in collaborative virtual environments. After a detailed presentation of all communication mechanisms involved, we propose the investigation of a low-level communication approach through the feedthrough mechanism. This channel is used to communicate kinematic information about a partner’s gestures during closely coupled collaboration. Several communication metaphors, with complementary behaviors, were investigated to improve the coordination between two partners during an assembly task. The results clearly show the role of communication strategies for the improvement of gesture coordination and highlight the correlation between applied force and the level of efficiency.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Collaborative virtual environments (CVE) are emerging as a key research focus, at the convergence of human–computer interfaces (HCI), information and communication technologies (ICT), and computer supported cooperative work approaches (CSCW). These environments have the potential to change the way teams create, exchange, manipulate, and disseminate information in collaborative projects (distant and colocated). The emergence of these platforms is induced by the development of communication networks, especially the emergence of the Internet, in the early ‘90s.</p><p>First, collaborative systems enable several users to edit and manipulate text and graph documents through the Internet and local networks (Catlin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Catlin T, Bush P, Yankelovich N (1989) InterNote: extending a hypermedia framework to support annotative collaboration. In: Proceeding of hypertext’89 ACM conference, November 1989, pp 365–378" href="/article/10.1007/s10055-011-0201-2#ref-CR6" id="ref-link-section-d42103e297">1989</a>). In order to improve the communication between remote sites, videoconferencing systems were developed to allow direct communication between collaborators (Bly et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing environment. Commun ACM 36(1):28–47" href="/article/10.1007/s10055-011-0201-2#ref-CR3" id="ref-link-section-d42103e300">1993</a>). At the same time, virtual reality environments (VRE), through 3D immersive and interactive approaches, have been explored to set up new sensorial communication and exchange approaches. First, platforms integrate elementary simulations with multisensory user interfaces such as hand motion and gestures, speech input and output, sound output, 3D stereoscopic graphics, and head-motion parallax (Codella et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Codella CF, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: CHI 1992, pp 329–334" href="/article/10.1007/s10055-011-0201-2#ref-CR10" id="ref-link-section-d42103e303">1992</a>). Moreover, several works rely on the integration of virtual avatars to improve communication between remote partners (Peña Pérez Negrón and de Antonio Jiménez <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Peña Pérez Negrón A, de Antonio Jiménez A (2009) Using avatar’s nonverbal communication to monitor collaboration in a task-oriented learning situation in a CVE. Workshop on intelligent and innovative support for collaborative learning activities. Rhodes, Greece, 8–13 June 2009, pp 19–26" href="/article/10.1007/s10055-011-0201-2#ref-CR26" id="ref-link-section-d42103e306">2009</a>). Takemura and Kishino in (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Takemura H, Kishino F (1992) Cooperative work environment using virtual workspace. In: CSCW ‘92 proceedings, Toronto, Canada, pp 226–232" href="/article/10.1007/s10055-011-0201-2#ref-CR33" id="ref-link-section-d42103e309">1992</a>) and Yano and Iwata in (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Yano H, Iwata H (1994) Collaboration in virtual environment with force feedback. IPSJ SIG Notes 94(59):31–34" href="/article/10.1007/s10055-011-0201-2#ref-CR35" id="ref-link-section-d42103e313">1994</a>) are among the first to successfully introduce haptic feedback for elementary collaborative tasks. However, the latency of networks introduces instabilities and makes haptic feedback inappropriate for long distance collaboration. Therefore, several propositions were made to improve existing systems with the time delay constraints (Kim and Ryu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kim Y-S, Ryu J-H (2009) Performance analysis of teleoperation systems with different haptic and video time-delay. In: Proceedings of the ICROS-SICE international joint conference 2009. Fukuoka, Japan, pp 3371–3375" href="/article/10.1007/s10055-011-0201-2#ref-CR23" id="ref-link-section-d42103e316">2009</a>).</p><p>Nowadays, with the development of CVEs in new applications involving a close collaboration between distant partners (collaborative design and assembly, military training, e-learning, remote surgical operations, etc.), new problems and constraints arise. One of the most critical is the limit of communications between partners during closely coupled tasks like manipulation of shared objects and collaborative selection (Redfern and Naughton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Redfern S, Naughton N (2002) Collaborative virtual environments to support communication and community in internet-based distance education. J Inf Technol Educ 1(3):201–211" href="/article/10.1007/s10055-011-0201-2#ref-CR28" id="ref-link-section-d42103e322">2002</a>). In fact, communication plays a strategic role during partners’ exchanges. For instance, it supports explicit and implicit direct exchanges between partners (e.g., oral dialog, gestural, and emotional communication). At a higher level, it supports the awareness of the presence of other participants and the understanding of their activities (Cockburn and Weir <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion-oriented views. Int J Hum Comput Interact 11(3):231–255" href="/article/10.1007/s10055-011-0201-2#ref-CR9" id="ref-link-section-d42103e325">1999</a>). All these exchanges allow the establishment and the maintaining of a shared background of understanding, which we call common ground (Dix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dix AJ (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6(2/3):135–156" href="/article/10.1007/s10055-011-0201-2#ref-CR12" id="ref-link-section-d42103e328">1997</a>). Moreover, they improve the coordination of gestures and actions of partners during closely coupled collaborations (e.g., manipulation of shared objects, and overlapped tasks). Thus, the inhibition of some channels and components of communication has a significant impact on the efficiency of collaboration and thus on the relevance of CVEs.</p><p>The present paper focuses on the action awareness between partners during closely coupled collaborations. In fact, during collaborative tasks, people have to establish and maintain awareness of one another’s intentions, actions, and results during the manipulation of shared artifacts. Beyond existing communication and notification strategies in CVEs (3D avatar, gestural guidance through virtual fixtures, static notification, etc.), we propose exploration of new approaches to extend natural exchanges between partners through collaborative sensorial communication metaphors. These metaphors will support some components of the activity awareness and extend the standard process with no accessible nor abstract information to improve gesture coordination during 3D assembly tasks (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig1">1</a>). The proposed approach uses the haptic feedback through the feedthrough communication channel (Grasset et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Grasset R, Lamb P, Billinghurst M (2005) Evaluation of mixed-space collaboration. In: ISMAR ‘05: Proceedings of the 4th IEEE/ACM international symposium on mixed and augmented reality, pp 90–99" href="/article/10.1007/s10055-011-0201-2#ref-CR18" id="ref-link-section-d42103e337">2005</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Investigated collaborative virtual assembly task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec2">2</a> presents a detailed review about the role of haptic channel for interpersonal communication in different collaborative environments. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec3">3</a> describes all involved communication mechanisms during collaborative tasks. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec6">4</a> presents the proposed concept for the improvement of communication and awareness. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec9">5</a> develops the proposed communication metaphors by presenting the communicated information and its corresponding sensorial display. Finally, Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec15">6</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec18">7</a> present the experiments and discuss the results.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">State of the art</h2><div class="c-article-section__content" id="Sec2-content"><p>During human–human interaction, several levels of communication can take place according to the task. These exchanges vary from high-level communication, like written and spoken language, to the most elementary and implicit communication, like facial and gestural expression. Although these different ways of communication are the subject of work in several fields of research, haptic communication has not received much attention. In fact, in common tasks, the haptic channel conveys many social messages like hostility, level of intimacy, sexual exchanges, nurturance, or dependence (Collier <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Collier G (1985) Emotional expression. Lawrence Erlbaum Associates, Hillsdale, NJ. &#xA;                    http://www.questia.com/PM.qst?a=o&amp;d=28053073&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0201-2#ref-CR11" id="ref-link-section-d42103e389">1985</a>). Thereby, the haptic channel plays a strategic role in interpersonal communication and supports the sense of presence and co-presence in different environments (Cha et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Cha J, Eid MA, El-Saddik A (2009) Touchable 3D video system. ACM Trans Multimed Comput Commun Appl (TOMCCAP) 5(4):1–25" href="/article/10.1007/s10055-011-0201-2#ref-CR7" id="ref-link-section-d42103e392">2009</a>).</p><p>Early work focused on how two partners physically cooperate in common tasks like lifting and moving a bulky object, teaching manual skills, dancing, or handing off a baton or a drinking glass (Reed and Peshkin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Reed KB, Peshkin MA (2008) Physical collaboration of human-human and human-robot teams. IEEE Trans Haptics 1(2):108–120" href="/article/10.1007/s10055-011-0201-2#ref-CR29" id="ref-link-section-d42103e398">2008</a>). Several mechanisms of anticipation, coordination, and reaction to each other’s forces were carried out. Shergill et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Shergill SS, Bays PM, Frith CD, Wolpert DM (2003) Two eyes for an eye: the neuroscience of force escalation. Science 301(5630):187" href="/article/10.1007/s10055-011-0201-2#ref-CR31" id="ref-link-section-d42103e401">2003</a>) highlight the relationship between the perceived force and the level of communication among partners. They observe that when the transfer of forces is hindered, communication can be significantly diminished.</p><p>Reed and Peshkin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Reed KB, Peshkin MA (2008) Physical collaboration of human-human and human-robot teams. IEEE Trans Haptics 1(2):108–120" href="/article/10.1007/s10055-011-0201-2#ref-CR29" id="ref-link-section-d42103e407">2008</a>) identify two types of forces involved during collaboration. The first one is in opposition of partner’s gesture (dyadic-contraction), and the second one is in cooperation. The dyadic-contraction has two roles. On the one hand, it enables the stabilization of the interaction during the collaboration, and on the other hand, it enables a communication between the two partners through a shared artifact (tools, manipulated objects, etc.). This research highlights the difficulty of understanding how the force and the motion of two people combine during common collaborative tasks. Thereafter, Glynn et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of the 45th annual meeting of the human factors and ergonomics society, 8–12 October 2001, Minneapolis/St. Paul, MN, pp 1911–1915" href="/article/10.1007/s10055-011-0201-2#ref-CR16" id="ref-link-section-d42103e410">2001</a>) carry out experiments to understand the type of information that can be communicated through the haptic channel. These experiments show that the physical interaction between partners enables the communication of force and position simultaneously and without ambiguity, since the position information does not overlap the force information.</p><p>Gentry (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gentry S (2005) Dancing cheek to cheek: haptic communication between partner dancers and swing as a finite state machine. Ph.D thesis, Massachusetts Institute of Technology" href="/article/10.1007/s10055-011-0201-2#ref-CR15" id="ref-link-section-d42103e416">2005</a>) study how haptic interaction works between dancers. The physical connection in dancing is maintained through the follower’s right hand holding onto the leader’s left hand. This physical connection allows the leader to send messages to the follower allowing both partners to exchange energy. A good follower will keep her hand in the same position relative to her body, which enable the leader to communicate. Most of the communication is based on haptic cues even though the dancers can see each other, and it thus allows an efficient coordination and fast coordination of movements. Based on this observation, Sallnas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sallnas E, Rassmus-grohn K, Sjostrom C (2000) Supporting presence in collaborative environments by haptic force feedback. ACM Trans Comput Hum Interact 7(4):461–476" href="/article/10.1007/s10055-011-0201-2#ref-CR30" id="ref-link-section-d42103e419">2000</a>) and Basdogan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Basdogan C, Ho C-h, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput Hum Interact 7:443–460" href="/article/10.1007/s10055-011-0201-2#ref-CR2" id="ref-link-section-d42103e422">2000</a>) demonstrate the role of kinesthetic feedback in the representation of a partner’s movements for virtual assembly and manipulation tasks. Beyond the improvement of communication between partners, the haptic channel brings social presence into virtual and remote environments.</p><p>Chan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chan A, MacLean KE, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355" href="/article/10.1007/s10055-011-0201-2#ref-CR8" id="ref-link-section-d42103e429">2008</a>) propose a set of vibrotactile perceptions to support turn-taking communication. This high-level communication approach enables indicating several states of control on the shared object (who controls the object, releases the object, etc.) to partners. These perceptions concern (1) request for control and loss of control and (2) the gentle and urgent request control. These perceptions concern both users with current floor control and users who have made a request. It consists of elementary haptic icons.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Communication in synchronous CVEs</h2><div class="c-article-section__content" id="Sec3-content"><p>This section gives the required background to understand the role of the proposed methods in the global communication framework. First, we describe the communication mechanisms involved and their role during collaborative tasks. Then, we explain the limits and the constraints of communication and awareness in CVEs.</p><h3 class="c-article__sub-heading" id="Sec4">Communication mechanisms</h3><p>In general, collaborative systems can be classified according to the following dimensions (Ellis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Ellis C, Gibbs SJ, Rein GL (1991) Groupware: some issues and experiences. Commun ACM 34(1):38–58" href="/article/10.1007/s10055-011-0201-2#ref-CR13" id="ref-link-section-d42103e446">1991</a>) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig2">2</a>):</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Ellis matrix, space–time taxonomy (Ellis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Ellis C, Gibbs SJ, Rein GL (1991) Groupware: some issues and experiences. Commun ACM 34(1):38–58" href="/article/10.1007/s10055-011-0201-2#ref-CR13" id="ref-link-section-d42103e462">1991</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Spatial axis</b>: The distance between partners. In fact, the collaboration may be either colocalized or distant.</p>
                    </li>
                    <li>
                      <p>
                        <b>Temporal axis</b>: The time interval separating the actions of participants. Thus, the collaboration may be synchronous or asynchronous.</p>
                    </li>
                  </ul>
                <p>From these configurations, we identify two main components: participants and artifacts of work.</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b><i>Participants</i></b>
                        <b>(or partners)</b>: person who achieves the task either directly or indirectly.</p>
                    </li>
                    <li>
                      <p>
                        <b><i>Artifacts of work</i></b>: components on/with which the collaborative work is carried out. Artifacts are controlled and acted on by the participants who are also able to perceive their states. Artifacts are not necessarily shared between all partners and can be <i>hard</i> or <i>soft</i> (Dix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dix AJ (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6(2/3):135–156" href="/article/10.1007/s10055-011-0201-2#ref-CR12" id="ref-link-section-d42103e525">1997</a>). A hard artifact is a physical object: for example, a computer or a document. Soft artifact refers to intellectual ware like the knowledge of each participant.</p>
                    </li>
                  </ul>
                <p>Based on these two components, several exchanges can take place between partners in synchronous collaborative spaces. We can classify them according to four levels (Grasset <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Grasset R (2004) Environnement de réalité augmentée 3D cooperative: approche colocalisée sur table. ARTIS-GRAVIR/IMAG-INRIA" href="/article/10.1007/s10055-011-0201-2#ref-CR17" id="ref-link-section-d42103e534">2004</a>) (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig3">3</a>):</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Communications and exchanges in synchronous collaborative space (Grasset <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Grasset R (2004) Environnement de réalité augmentée 3D cooperative: approche colocalisée sur table. ARTIS-GRAVIR/IMAG-INRIA" href="/article/10.1007/s10055-011-0201-2#ref-CR17" id="ref-link-section-d42103e550">2004</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1)</span>
                      
                        <p>
                          <b><i>Direct communication</i></b>
                          <i>:</i> it is the most natural way to communicate. It occurs between participants and can be conscious or unconscious (Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Gutwin C, Greenberg S (2000) The mechanics of collaboration: developing low cost usability evaluation methods for shared workspaces. In: Proceedings of the 9th IEEE international workshops on enabling technologies: infrastructure for collaborative enterprises, 4–16 June 2000. WETICE. IEEE Computer Society, Washington, DC, pp 98–103" href="/article/10.1007/s10055-011-0201-2#ref-CR20" id="ref-link-section-d42103e578">2000</a>). We can distinguish two levels of <i>direct communication</i>:</p><ol class="u-list-style-none">
                            <li>
                              <span class="u-custom-list-number">a.</span>
                              
                                <p>
                                  <b><i>Explicit communication</i></b>: it addresses information produced by one user and explicitly intended to be received by other users (oral and verbal communication).</p>
                              
                            </li>
                            <li>
                              <span class="u-custom-list-number">b.</span>
                              
                                <p>
                                  <b><i>Back</i></b>
                                  <b>-</b>
                                  <b><i>channel feedback communication</i></b>: it concerns unintentional information initiated by one user and directed toward another user to facilitate communication (gesture, emotion, vocal activities, etc.).</p>
                              
                            </li>
                          </ol>
                        
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2)</span>
                      
                        <p>
                          <b><i>Interaction and feedback</i></b> correspond to whole actions and sensorial feedbacks that can occur between participants and artifacts. This low-level exchange enables the control and the observation of the states of the artifacts involved (e.g., tools and manipulated object).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3)</span>
                      
                        <p>
                          <b><i>Feedthrough</i></b> is an indirect communication channel. It concerns implicit information delivered to several users reporting actions executed by one user (Hill and Gutwin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: GROUP’03: Proceedings of the 2003 international ACM SIGGROUP conference on supporting group work. Sanibel Island, Florida, USA, pp 258–267" href="/article/10.1007/s10055-011-0201-2#ref-CR22" id="ref-link-section-d42103e649">2003</a>). This communication occurs between participants through shared artifacts. In fact, each action or manipulation of artifacts implicitly informs partners about the evolution and modification of the environment. Thus, artifacts are not only a tool or a support, but also a mediator for communication. Feedthrough is essential to provide group awareness and to build meaningful contexts for collaboration (Sallnas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sallnas E, Rassmus-grohn K, Sjostrom C (2000) Supporting presence in collaborative environments by haptic force feedback. ACM Trans Comput Hum Interact 7(4):461–476" href="/article/10.1007/s10055-011-0201-2#ref-CR30" id="ref-link-section-d42103e652">2000</a>). A very simple way to generate feedthrough consists of multiplexing feedback information to several users. Sophisticated schemes consider delivering less information by manipulating the granularity and timing associated with the operations executed by the group (Gutwin and Greenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput Hum Interact 6(3):243–281" href="/article/10.1007/s10055-011-0201-2#ref-CR19" id="ref-link-section-d42103e655">1999</a>).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4)</span>
                      
                        <p>
                          <b><i>Understanding</i></b>
                          <b><i>in collaborative work</i></b>: It is obvious that each participant must understand actions of other participant, even if it is only a part of the environment. In some cases, the understanding could be the goal of the task like in academic work. Understanding is considered as a soft artifact (Dix <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dix AJ (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6(2/3):135–156" href="/article/10.1007/s10055-011-0201-2#ref-CR12" id="ref-link-section-d42103e675">1997</a>).</p>
                      
                    </li>
                  </ol>
                <h3 class="c-article__sub-heading" id="Sec5">Limits of awareness and communication in CVEs</h3><p>As discussed above, each collaborative work uses different levels of communication channels and awareness mechanisms. However, the use of a CVE introduces some limitations on natural communication processes and inhibits conscious and unconscious exchange. We identify two main categories of constraints related to CVEs and VR.</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Distance between partners</b>: Common close collaboration involves the simultaneous presence of several partners in the same physical environment, which enables the establishment of a natural communication process. However, CVEs can have real (physical) and/or virtual distances between partners. The real distance comes from the non-colocated collaboration when partners work in different physical environments (i.e., geographical distance). The virtual distance concerns the collaborations occurring in large and/or complex virtual environments. In fact, applications like molecular manipulation or computational fluid dynamics involve complex environments with both large dataflow to analyze and multiple Degrees of Freedom (DoFs) to manipulate. Thus, several potential collaborative tasks (deformation of molecules, assembly of molecules) may involve simultaneous manipulations of large artifacts (e.g., manipulation of large molecular structures and manipulation of two molecules) or require an important focus of the users’ resources on the current activity and action (e.g., perception of the complex environment and control of several DoFs) with less resources for communication between partners (direct communication, awareness).</p>
                    </li>
                    <li>
                      <p>
                        <b>Distance to virtual environments</b>: this level of constraint concerns the distance between the users and the virtual environment. In fact, natural interaction on real artifacts involves a geometrical superimposition between gestural interaction and the corresponding visual feedback. However, usual VR technologies create a distance between users (e.g., real end-effectors, haptic arm, and hand) and manipulated artifacts (e.g., virtual end-effectors, and virtual artifacts), which constrain or inhibit several inter-referential communication and awareness mechanisms (e.g., designation/indication of a region of interest and collaborative selection). In addition to this constraint, we identify other VR limitations like time delays between actions and corresponding feedbacks (visual/audio/haptic updates) and the limits of rendering metaphors (i.e., efficiency of collision detection, limits, and constraints of stereovision rendering).</p>
                    </li>
                  </ul>
                <p>Thus, these two levels of constraints have a direct impact not only on the implicit and explicit communication process (e.g., gestural communication, emotion on face, and feedthrough communication) but also on several levels of awareness (static and dynamic components). The consciousness of the presence of other partners and the understanding of their activities become very difficult tasks. This has a direct consequence on the grounding and understanding processes, and on the coordination of actions and gestures during a closely coupled collaboration (Kjölberg and Sallnäs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kjölberg J, Sallnäs E-L (2002) Supporting object handling and hand over tasks in haptic Collaborative Virtual Environments. In: Wall S, Riedel B, Crossan A, McGee MR (eds) Proceedings of Eurohaptics′02, May 2002, pp 71–76" href="/article/10.1007/s10055-011-0201-2#ref-CR25" id="ref-link-section-d42103e714">2002</a>). The efficiency of the collaborative work in a CVE decreases significantly.</p><p>Beyond available tangible information in common real collaborative tasks, efficient collaborations in CVEs require an adapted communication framework suitable for fulfilling the task, either by: (1) filtering the existing communication, in order to improve the focus on individual tasks or by (2) augmenting the existing communication framework with abstract and/or inaccessible information that is important for efficiency in the collaborative tasks (Varadaradjou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Varadaradjou ES, Dax P, Grumbach A (2006) Improved communication in virtual worlds. In: Virtual reality international conference. Rocquencourt, Avril, pp 103–111" href="/article/10.1007/s10055-011-0201-2#ref-CR34" id="ref-link-section-d42103e720">2006</a>).</p><p>All these constraints together with the potential of virtual reality lead us to propose a new approach to improve collaboration and coordination between partners in CVEs through adapted communication metaphors. Unlike virtual reality metaphors that concern information about the environment and the constraints related to the fulfillment of the tasks (e.g., sensorial metaphors and virtual fixtures), the proposed concept introduces an intuitive representation of a novel class of information related to partners’ actions, activities, and states.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Proposed concept for the improvement of awareness and communication</h2><div class="c-article-section__content" id="Sec6-content"><h3 class="c-article__sub-heading" id="Sec7">Feedthrough’s features</h3><p>From previous definitions, we identify two main levels of communication: (1) direct communication and (2) feedthrough. During common collaborative tasks, the direct communication uses natural communication languages and conveys complex information with a high level of abstraction. The required perception, interpretation, and motor reaction mechanisms are complex. Moreover, this communication involves important cognitive efforts and processing time delays (Pinelle et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Pinelle D, Gutwin C, Greenberg S (2003) Task analysis for groupware usability evaluation: modeling shared-workspace tasks with the mechanics of collaboration. ACM Trans Comput Hum Interact 10(4):281–311" href="/article/10.1007/s10055-011-0201-2#ref-CR27" id="ref-link-section-d42103e738">2003</a>). Furthermore, a direct information transfer characterizes this level of communication without physical medium between partners.</p><p>On the contrary, the feedthrough communication channel is a part of the forces involved during collaboration, and more precisely, of <i>dyadic</i>-<i>contraction</i> component (Reed and Peshkin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Reed KB, Peshkin MA (2008) Physical collaboration of human-human and human-robot teams. IEEE Trans Haptics 1(2):108–120" href="/article/10.1007/s10055-011-0201-2#ref-CR29" id="ref-link-section-d42103e750">2008</a>). In fact, beyond the stabilization of the interaction between partners, this effort component is the main channel that carries all the tangible information exchanges. This communication channel requires a hard medium and supports simple physical information with a low level of abstraction like geometric, kinematic, or physical information. The feedthrough mechanism uses the haptic channel through the tactile and kinesthetic perception. The interpretation and reaction processes for feedthrough events and information are the same as for the haptic channel (Chan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chan A, MacLean KE, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355" href="/article/10.1007/s10055-011-0201-2#ref-CR8" id="ref-link-section-d42103e753">2008</a>). In fact, feedthrough inherits all the psychophysical behaviors of the standard haptic interaction (e.g., reactivity, local perception, and temporal integration of information). Thus, in addition to the perception of environment, feedthrough enhances the haptic channel with the perception of the actions and the activities of partners.</p><p>Considering all the information supported by the haptic channel, we can summarize the haptic perceptive flow during the collaborative work in the two following points:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Feedback from environment</b>: Standard perceptions resulting from the current interactions with environment and artifacts.</p>
                    </li>
                    <li>
                      <p>
                        <b>Feedthrough between partners</b>: All exchanges related to actions, activities, or other partners’ states suppurated by the feedthrough channel. It concerns both the direct communication and the awareness processes.</p>
                    </li>
                  </ul>
                <p>According to the complex role of haptic channel, it is necessary to take into consideration this dual function (perception/communication) for all the modifications or the increase in haptic channel.</p><h3 class="c-article__sub-heading" id="Sec8">Improvement of feedthrough</h3><p>Even though feedthrough is an important channel for indirect communication in usual collaborative environments, it conveys and concerns a limited class of elementary tangible information about partners’ activity (e.g., direction of movement, position, velocity, and applied force). Moreover, the standard rendering mechanism of the feedthrough may not be optimal to some collaborative actions. For instance, several collaborative manipulations can require orthogonal actions of partners: a vertical movement for the first partner and a horizontal movement for the second partner.</p><p>Virtual environments allow us to go beyond these constraints (i.e., limited information, not adapted rendering). In fact, beyond conventional and accessible information, it would be interesting to convey more information about the current actions and the activity between partners. This information can be standard, abstract, or not directly accessible (change of direction, delay between actions, distance, acceleration, etc.). Moreover, the rendering of this information can use an efficient representation by considering the cognitive workload. Psychophysical studies enable the design of efficient rendering by considering the correlation between the perceptions involved. Thereby, the role of this additional information is to improve the interaction through an enhance perception of the artifact’s states.</p><p>We can summarize the objectives of the improvement of feedthrough in the two following points:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Increase in the standard communications with new and additional information. This information can be standard, abstract, or inaccessible data.</p>
                    </li>
                    <li>
                      <p>Improvement of the rendering of existing and added information, with efficient communication metaphors, through psychophysical study.</p>
                    </li>
                  </ul>
                </div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Augmented information and corresponding communication metaphors</h2><div class="c-article-section__content" id="Sec9-content"><p>The context of the proposed work concerns the manipulation of molecules. This environment is characterized with a large dataflow to analyze and with multiple DoFs to simultaneously manipulate. The CVE could play a strategic role by enabling the problem’s complexity to be shared between the partners. This could result in a significant decrease in the cognitive workload for each participant, as well as making the process more robust and reliable by introducing an additional expertise and knowledge to the same problem-solving process (Simard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simard J, Ammi M, Auvray M (2010) Closely coupled collaboration for search tasks. In: Proceedings of the 17th ACM symposium on Virtual Reallity Software and Technology (VRST), vol 1. pp 181–182" href="/article/10.1007/s10055-011-0201-2#ref-CR32" id="ref-link-section-d42103e816">2010</a>).</p><p>Among the different tasks involved during molecular docking (e.g., selection of residues and deformation of structures), we propose the investigation of structures assembly. This task requires the moving of ligand structures and to dock them on the receptor molecule. However, due to the complexity of the molecular structures, the awareness about the partners’ actions becomes very difficult, which limits the gesture coordination.</p><p>This work focuses on the assembly of one molecular structure on the receptor molecule. We propose the investigation of this task in a simplified 1D and 3D physical environment. We develop the experimental conditions and procedures in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0201-2#Sec15">6</a>.</p><h3 class="c-article__sub-heading" id="Sec10">Augmented information</h3><p>To improve the coordination between the two partners, we will act on the kinematic information, namely: (1) position and (2) velocity. Thus, the gestures’ coordination reduces the position and velocity differences between the two partners. We propose the following strategies:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Position</b>: Communication about the difference of positions between the assistant (follower) and the main expert. When the distance between the two partners reaches a certain value, some qualitative information is sent to the follower (important difference, weak difference, etc.).</p>
                    </li>
                    <li>
                      <p>
                        <b>Velocity</b>: Communication about the difference of velocity between the two partners. When the velocity of the main expert exceeds that of the assistant, the expert is informed about a qualitative difference between the two velocities.</p>
                      <p>Communication about the change of the movement’s direction. When the expert changes the direction of his movement, a quick signal informs the assistant (follower) about the switch of direction.</p>
                    </li>
                  </ul>
                <h3 class="c-article__sub-heading" id="Sec11">Communication metaphors</h3><p>The increase in feedthrough with this new kinematic information must be enhanced by suitable and intuitive rendering metaphors. Different strategies presenting several level of abstraction can be used. Chan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chan A, MacLean KE, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355" href="/article/10.1007/s10055-011-0201-2#ref-CR8" id="ref-link-section-d42103e866">2008</a>) propose in the use of a set of haptic icons to request and indicate to partners several states of control. This level of communication presents an important vocabulary and therefore can be used to render a lot of information. However, this level of communication requires an important learning step before an efficient understanding and use. This communication strategy requires also important cognitive processing that reduces the reactivity of users to some fast events or information (Enriquez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Enriquez M, MacLean KE et al (2006) Haptic phonemes: basic building blocks of haptic communication. In: Proceedings of 8th int’l conf. on multimodal interfaces (ICMI ‘06). Banff, Canada, pp 302–309" href="/article/10.1007/s10055-011-0201-2#ref-CR14" id="ref-link-section-d42103e869">2006</a>). Thus, this communication strategy is more adapted to render the information presenting limited static states and a very low frequency bandwidth (dialog, states, etc.).</p><p>For tasks requiring important dynamic with fast gestures, a greater reactivity is required. Therefore, it is necessary to use elementary and intuitive haptic representations. This haptic feedback should not require important cognitive process and can even make use of reflex mechanisms. We can summarize the requirements for haptic stimuli in the following points (Enriquez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Enriquez M, MacLean KE et al (2006) Haptic phonemes: basic building blocks of haptic communication. In: Proceedings of 8th int’l conf. on multimodal interfaces (ICMI ‘06). Banff, Canada, pp 302–309" href="/article/10.1007/s10055-011-0201-2#ref-CR14" id="ref-link-section-d42103e875">2006</a>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Differentiable</b>: All stimuli must be distinguishable from one to another when presented either alone or in any used combinations.</p>
                    </li>
                    <li>
                      <p>
                        <b>Identifiable</b>: Once a meaning has been associated with a stimulus, it must be easy to remember.</p>
                    </li>
                    <li>
                      <p>
                        <b>Learnable</b>: The associations between meanings and stimuli should be intuitive and easy to learn.</p>
                    </li>
                    <li>
                      <p>
                        <b>Reactivity</b>: Stimuli must allow a great reactivity and present a low cognitive load for understanding.</p>
                    </li>
                  </ul>
                <p>Among several haptic rendering, the perceptions based on elementary physical forces (friction, viscosity, etc.) are good candidates to address these constraints. In fact, human users are accustomed to interacting intuitively, and with a good reactivity, with environments presenting physical forces from everyday. Moreover, the understanding of these perceptions requires only a short period of learning, mainly for the association between these physical representations and the corresponding events and gestures (meaning) (Ammi and Ferreira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. ICRA 454–460. &#xA;                    http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4209133&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0201-2#ref-CR1" id="ref-link-section-d42103e920">2007</a>). We develop in the following sections the proposed haptic rendering (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Proposed communication metaphors: <b>a</b> spring, <b>b</b> viscosity, and <b>c</b> vibration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Communication of the difference in positions (distance): spring model</h4><p>The most intuitive model to map the distance information onto a force model is the spring model. In fact, the rendered force through this model is directly proportional to the elongation of the spring (distance between the two ends). The general model of a non-linear spring, based on Hooke formula, can be expressed with the following equation:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ F_{\text{s}} = k*f(x) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <i>F</i>
                          <sub>s</sub>: is the force of the spring (in N)</p>
                      </li>
                      <li>
                        <p>
                          <i>k</i>: is the spring constant (in N/m)</p>
                      </li>
                      <li>
                        <p>
                          <i>f</i>(<i>x</i>): is a non-linear function (<i>x</i>
                          <sup>2</sup> and <i>x</i>
                          <sup>3</sup>
                          <i>,</i>) of deformation. The more the degree of the function is important, the more the attraction is strong (in meters).</p>
                      </li>
                    </ul>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Communication of the difference in velocity (relative velocity): viscosity model</h4><p>If the spring model expresses a direct relationship between the distance information and the resulting force, the viscosity model produces a proportional force to the gesture’s velocity. The generated force is opposed to motion that tends to slow the movement. Thus, this model is more adapted to render the relative velocity. Moreover, it plays the role of a dynamic virtual fixture, limiting the relative velocity of the main expert.</p><p>The viscosity force has several linear and non-linear models. However, the most understandable and stable model is the basic direct linear model:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ F_{v} = - B*v $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <i>F</i>
                          <sub>
                            <i>v</i>
                          </sub>: the viscosity force (in N)</p>
                      </li>
                      <li>
                        <p>
                          <i>B</i>: the damping constant (in N s/m)</p>
                      </li>
                      <li>
                        <p>
                          <i>v</i>: the system velocity (in m/s).</p>
                      </li>
                    </ul>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Communication of the change of direction: vibration</h4><p>In a real environment, a haptic warning is usually the result of a physical impact with the environment or with another partner. Beyond this representation, and for technical constraints and ergonomic recommendations (e.g., intrusive interfaces and actuators technology), the haptic warning is displayed with success through vibrotactile signals in the virtual environment (collision, forbidden regions, etc.).</p><p>The vibration feedback consists to generate quick and small oscillations without a kinesthetic effect (tactile only). This model can support several signal forms (square, rectangular, sawtooth, trapezoidal, etc.) and can be a binary perception (active or not active for warning) or a modulated signal (frequency modulation, amplitude modulation, etc.).</p><p>Since the tactile perception is based on a temporal integration mechanism, it is not possible to use the amplitude and frequency modulation for the quantitative perception of the information. However, it presents a very good background and redundant qualitative information during interaction (Ammi and Ferreira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. ICRA 454–460. &#xA;                    http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4209133&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0201-2#ref-CR1" id="ref-link-section-d42103e1095">2007</a>).</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Experimental study</h2><div class="c-article-section__content" id="Sec15-content"><p>We performed two main experimentations to evaluate the contribution and the complementarity between the different communication metaphors for the improvement of the 1D and 3D assembly tasks. The first experiment concerns a 1 DoF assembly task and consists to present to the participants the different haptic metaphors. This experiment aims to study the behaviors of proposed approaches in a simplified environment before the generalization to the 3D space. The second experiment concerns the 3 DoFs assembly task (<i>X</i>–<i>Y</i>–<i>Z</i>). It consists to compare a native configuration (i.e., tension force transfer only) with two configurations presenting all communication metaphors with different levels of coupling.</p><h3 class="c-article__sub-heading" id="Sec16">Hardware and software setup</h3><p>The collaborative platform is based on a client–server architecture (Marsh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Marsh J, Glencross M, Pettifer S, Hubbold RJ (2006) A network architecture supporting consistent rich behavior in collaborative interactive applications. IEEE Trans Vis Comput Graph 12(3):405–416" href="/article/10.1007/s10055-011-0201-2#ref-CR24" id="ref-link-section-d42103e1125">2006</a>). This configuration enables a flexible investigation of several scenarios (e.g., different number of partners and distant/collocated collaboration) without important software and hardware modifications. The server node supports the main physical simulation module and the haptic calculation module. Client nodes support the graphic and the haptic rendering modules. These nodes are connected through a local network (time delay &lt;50 ms). The physical calculation, in the simulation module, is based on the open dynamics engine (ODE). This software is an open source high-performance library for simulating 3D rigid body dynamics (<a href="http://www.ode.org/">http://www.ode.org/</a>). The haptic calculation module generates the communication metaphors by combining the information coming from the 3D physics engine and from the partners’ actions (positions, velocity, etc.). The client nodes support the display of the 3D scene on desktop screens (24 inch) with an OpenGL-based graphic module and the render of the communication metaphors with Omni haptic arms (SensAble) through OpenHaptics library.</p><h3 class="c-article__sub-heading" id="Sec17">Experimental protocol</h3><p>
                  <b>Collaborative tasks</b>:</p>
                  <h3 class="c-article__sub-heading">A<i>. 1D manipulation tasks</i>:</h3>
                  <p>The investigated assembly task in a 1D space includes two components (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig5">5</a>): (1) a mobile component and (2) a fixed component. The mobile component is modeled by a rigid link (line). It is connected to the two virtual proxies (expert and follower) through a spring model. The mobile component supports the common haptic interaction and the feedthrough between partners. The base (fixed component) is modeled by a static line on which the partners will set the mobile component. These two components have a width of 40 cm. The task is carried out in a 1D space (<i>Y</i>), and the movement is constrained according to the “<i>Y</i>” axis through a virtual fixture.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Experimented 1 DoF assembly task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  
                <p>During 30s, the expert participant is asked to reach a series of 12 sequential targets aligned with the “<i>Y</i>” axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig5">5</a>). The follower partner must follow and coordinate his movement with the expert to minimize the position and the velocity differences. The follower and the expert participants use the visual feedback (e.g., artifact’s tilt) and the native or augmented feedthrough (i.e., tension force or communication metaphors). Four conditions were experimented (<b>independent variable</b>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Condition A.1</b>: No metaphor configuration. It is the native configuration without the increase in the feedthrough. Only the tension force is transferred between partners.</p>
                    </li>
                    <li>
                      <p>
                        <b>Condition A.2</b>: Spring metaphor configuration. It is the communication of the difference in positions through a spring metaphor. This information concerns the follower partner.</p>
                    </li>
                    <li>
                      <p>
                        <b>Condition A.3</b>: Viscosity metaphor configuration. It is the communication of the difference in velocity through a viscosity model (in addition to the spring metaphor). This information concerns the expert partner.</p>
                    </li>
                    <li>
                      <p>
                        <b>Condition A.4</b>: Vibration metaphor configuration. It is the communication of the change of direction of the main expert. This information is communicated to the follower partner through a vibration metaphor (in addition to the spring and the viscosity metaphors). This information concerns the follower partner.</p>
                    </li>
                  </ul>
                
                  <h3 class="c-article__sub-heading">B<i>. 3D manipulation task</i>:</h3>
                  <p>For the 3 DoFs assembly task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig6">6</a>), the static component is modeled by two rectangular pillars separated by a 30 cm distance. The mobile component (40 cm width) is model by a big box presenting two holes. The distance between holes corresponds to the distance between pillars.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Experimented 3 DoFs assembly task: the two participants grab the mobile component from the virtual proxies (<i>expert and follower virtual proxy</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  
                <p>The task consists to assemble the two 3D models by entering the holes in corresponding pillars. The two partners grab and manipulate the mobile components from the two ends (virtual proxy). The expert partner leads the task (leader). The physics engine calculates the collision detection between the 3D models and control the 3D physical link (manipulated box) between the partners.</p><p>For this experiment, we compare the native configuration with configurations presenting all communication metaphors. Two levels of coupling were investigated for communication metaphors: strong and soft configurations. The strong constants (<i>B</i> = 0.9 N s/m, <i>k</i> = 0.7 N/m) present a high coupling between partners and the soft constants (<i>B</i> = 0.4 N s/m, <i>k</i> = 0.35 N/m) present a low coupling between partners. According the level of coupling, three conditions were experimented (<b>independent variable</b>):</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <b>Condition B.1</b>: No coupling configuration. No communication metaphors are presented. Only the tension force is transferred between partners.</p>
                    </li>
                    <li>
                      <p>
                        <b>Condition B.2</b>: High coupling configuration. The communication metaphors use strong constants (<i>B</i> = 0.9 N s/m, <i>k</i> = 0.7 N/m).</p>
                    </li>
                    <li>
                      <p>
                        <b>Condition B.3</b>: Low coupling configuration. The communication metaphors use soft constants (<i>B</i> = 0.4 N s/m, <i>k</i> = 0.35 N/m).</p>
                    </li>
                  </ul>
                
                  <h3 class="c-article__sub-heading">
                    <b>Communication between partners</b>:</h3>
                  <p>During experiments (1D &amp; 3D), the two participants were located in the same room but were unable to see each other (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig1">1</a>). Only verbal communication was allowed between them. No gestural or emotional communications were possible. The two partners did not meet, speak to or see each other prior to the experiment. Each subject had a personal haptic interface and an LCD monitor. The proposed scenario enables the collaboration between expert and beginner users. The expert knows the exact movement to assemble the two components. The beginner follows the movements of the main expert.</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>Measures</b>:</h3>
                  <p>We collected several measures including the execution time, the position error, and the sum of applied force on master and slave arms. These measures were carried out for the expert partner (master arm) and for the follower partner (slave arm). We can summarize the dependent variables for the 1 DoF and 3 DoFs assembly tasks in the following points:</p>
                
                  <h3 class="c-article__sub-heading">
                    <b>Dependent variables for 1</b> <b>DoF assembly task</b>:</h3>
                  
                    <ul class="u-list-style-bullet">
                      <li>
                        <p>Position difference (error in position)</p>
                      </li>
                      <li>
                        <p>Applied force on the master arm</p>
                      </li>
                      <li>
                        <p>Applied force on the slave arm</p>
                      </li>
                    </ul>
                  
                
                  <h3 class="c-article__sub-heading">
                    <b>Dependent variables for 3</b> <b>DoF assembly task</b>:</h3>
                  
                    <ul class="u-list-style-bullet">
                      <li>
                        <p>Position difference (error in position)</p>
                      </li>
                      <li>
                        <p>Mean collision number</p>
                      </li>
                      <li>
                        <p>Mean execution time</p>
                      </li>
                    </ul>
                  
                <p>In addition to these objective measures, we propose during the last experiment (3D task) a questionnaire about the global appreciation of the proposed approach (5-points Likert’s scale). The questionnaire concerns both the expert and the follower partners.</p>
                  <h3 class="c-article__sub-heading">
                    <b>The questionnaire</b>:</h3>
                  
                    <ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <b>Q1</b>: “Do you perceive a better gesture coordination?”</p>
                      </li>
                      <li>
                        <p>
                          <b>Q2</b>: “Do you think that you are faster with the communication metaphors?”</p>
                      </li>
                      <li>
                        <p>
                          <b>Q3</b>: “Do you have a better understanding about the actions of the partner?”</p>
                      </li>
                      <li>
                        <p>
                          <b>Q4</b>: “Do you perceive gesture guidance or a communication of information?”</p>
                      </li>
                    </ul>
                  
                
                  <h3 class="c-article__sub-heading">
                    <b>Participants</b>:</h3>
                  <p>Fourteen couples (28 participants) with 20 men and 8 women (age range from 20 to 45 with median 27) took part in this study. Most were university graduate students in Computer Science. Most had at least an experience with a haptic displays and a CVE. Each experimental configuration was executed in a block of 10 trials (two subjects for each configuration).</p>
                </div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Experimental results and discussion</h2><div class="c-article-section__content" id="Sec18-content"><h3 class="c-article__sub-heading" id="Sec19">1D Assembly tasks</h3><p>We ran an analysis of variance (ANOVA) for the four configurations (native configuration and for the three communication metaphors) according to three factors: (1) position error, (2) the applied force on the master arm, and (3) the applied force on the slave arm. The analysis revealed a significant effect of the spring (Condition A.2: decrease of 38% with <i>p</i> = 0.011, <i>p</i> &lt; 0.05) and the vibration (Condition 4: decrease of 25% with <i>p</i> = 0.03, <i>p</i> &lt; 0.05) metaphors for the position error factor. There was no significant effect of the viscosity metaphor on the position error factor (Condition A.3: decrease of 13% with <i>p</i> &gt; 0.05). For the applied force factor, the ANOVA analysis reveals a significant effect of the viscosity metaphor (Condition A.3: increase of 50% with <i>p</i> = 0.017, <i>p</i> &lt; 0.05) on the applied force on the master arm and a significant effect of the spring metaphor (Condition A.2: decrease of 15% with <i>p</i> = 0.035, <i>p</i> &lt; 0.05) on the applied force on the slave arm. There was not significant effect of the other metaphors on the applied force on the master and on the slave arms (<i>p</i> &gt; 0.05). We develop these results in the following sections.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Condition A.1: native configuration</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a> shows the result of the first experiment when no communication metaphors are displayed. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a>a and b present the force applied on the master and on the slave arms. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a>c shows the error (differences in position) between the two arms. The mean position error between the two partners is 7.3 cm. This error increases when the expert partner changes the direction of movement (until 20 cm). During this short period, we observe a fluctuation of the movements (i.e., small gestural oscillations, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a>c). In fact, the decrease in the tension force between the two proxies during this period inhibits the dyadic-contraction, which do not allow informing the follower partner about the movement direction of the main expert (“continue the movement?” and “Stop the movement?”). This hesitation leads to the observed fluctuations. Shergill et al. observe similar behaviors in (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Shergill SS, Bays PM, Frith CD, Wolpert DM (2003) Two eyes for an eye: the neuroscience of force escalation. Science 301(5630):187" href="/article/10.1007/s10055-011-0201-2#ref-CR31" id="ref-link-section-d42103e1557">2003</a>) and highlight the relationship between the perceived force and the level of communication among the partners.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Applied force and position error between the master and the slave arms without communication metaphors (Condition 1): <b>a</b> applied force on the master arm, <b>b</b> applied force on the slave arm, <b>c</b> position error between the master and the slave arms</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>We observe also that the mean force applied on the slave arm (12.2 N) is greater than the force applied on the master arm (7.9 N). The force applied on the master arm presents short peaks when the expert changes the direction of the movement (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a>a). The corresponding reactions of the follower partner correspond to peaks of force with a small amplitude and a greater duration (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig7">7</a>b).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Condition A.2: communication of positions difference</h4><p>The difference between the positions of the two partners is communicated to the follower partner through the spring metaphor. In this experiment, the expert partner perceives only the dyadic-contraction. Several spring models were experimented (linear and non-linear). The best result was obtained with a non-linear model (<b>constant</b> stiffness):</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ F_{\text{s}} = k*x^{2} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                  <p>The mean position error between the two partners decreases until 4.5 cm. We always observe an increase in the position error and the fluctuations of partner’s movement when the expert partner changes the movement direction. The mean force applied on the master arm slightly decreases at 7.3 N. Moreover, the mean force applied on the slave arm significantly decreases at a mean value of 10.3 N. The force profile of the slave arm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig8">8</a>) highlights the decrease in the duration of the peaks in comparison with previous configuration. The force profile of the master arm presents a similar behavior than the previous experiment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Applied force on the slave arm (Condition 2)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Condition A.3: communication of the difference in velocity</h4><p>Two communication metaphors were used. The first metaphor, like the previous experiment, concerns the communication of the distance to the follower partner (spring model). The second metaphor concerns the communication of the relative velocity to the expert partner through a viscosity model.</p><p>The mean position error decreases in comparison with previous configurations (3.9 cm). The mean force applied on the slave arm is very slightly below the value of preceding configuration (10.0 N). The mean force applied to the master arm increases significantly (11.1 N). This force tends to slow the gesture of the expert partner. The force profile on the slave arm presents a similar behavior than the previous experiment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Condition A.4: communication of the change of direction</h4><p>Previous configuration is added to the communication of the direction change of the main expert. This information is communicated to the follower partner through a vibration metaphor.</p><p>The mean error between the two arms decreases at a value of 2.9 cm. We observe in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig9">9</a>c a very high attenuation of fluctuations when the expert changes its direction. The mean force applied on the master arm remains the same in comparison with the previous configuration (11.1 N). The mean force applied on the slave arm slightly decreases (9.8 N). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig9">9</a>b shows that the duration of force peaks on the slave arm is similar than for the master arm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig9">9</a>a).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Applied force and position error between the master and the slave for Condition 4: <b>a</b> applied force on the master arm, <b>b</b> applied force on the slave arm, <b>c</b> position error between master and slave arms</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig10">10</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig11">11</a>, and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig12">12</a> summarize the results of 1D experiment. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig10">10</a> shows a clear decrease in position errors between the two partners according to the several communication metaphors. We observe that the spring metaphor (Condition A.2) introduces an important reduction in the position error. In fact, the spring component plays the role of a guidance tool that helps the follower partner to easily reach the configuration of the expert partner. The viscosity and the vibration metaphors (Conditions A.3 and A.4) introduce an error reduction in comparison with the Condition A.2. The viscosity metaphors improve the position error by slowing the gesture of the expert partner, which enable the second partner to follow more easily the assembly movement. The vibration metaphor acts mainly during the direction changes. It reduces the delay of the follower reaction for the direction change events.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Position error between the master and slave arms for the four experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Applied force on the master arm for the four experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Applied force on the slave arm for the four experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig11">11</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig12">12</a> show the evolution of the mean force applied on the master and slave arms according to the different communication metaphors. We observe that the spring metaphor greatly reduces the force applied on the slave arm and slightly less on the master arm. In fact, the improvement of the coordination between the two partners reduces the stabilization component of the dyadic-contraction. The viscosity metaphor greatly increases the force applied on the master arm. In fact, the viscosity force has an opposite direction from the master movement what slow the gesture and adds an additional component (force) to the current applied force. Finally, the vibration metaphor has no important impact on the force applied on the slave and master arms. This metaphor has only a tactile effect without kinesthetic components.</p><p>Based on these results, we can conclude that proposing communication metaphors greatly improves the coordination during closely coupled collaborations requiring a dynamic and continuous communication of kinematic information. These metaphors act on the coordination of position and velocity, and on the applied force by the involved partners. Beyond the improvement of the coordination, it is necessary to take into consideration the applied force factor. In fact, long collaborative tasks can be penalized by the partner fatigue. For these configurations, we propose to only use a spring and a vibration metaphor that slightly reduce the applied force on master and slave arms.</p><h3 class="c-article__sub-heading" id="Sec24">3D Assembly tasks</h3><p>This experimentation uses all the developed communication metaphors according to the three dimensions (X, Y, and Z).</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig13">13</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig14">14</a>, and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig15">15</a> show the results of the 3D assembly task for the three conditions: Conditions B.1, B.2, and B.3. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig13">13</a> is the mean position error according to the three axes. It clearly shows the improvement of coordination between the two partners with the communication metaphors. The coordination is better with strong constants (<i>p</i> = 0.002, <i>p</i> &lt; 0.05). In fact, this configuration provides an efficient approach for the mutual gesture guidance between the two partners. The soft factors provide intermediate performances by enabling a greater freedom of the movements (<i>p</i> = 0.009, <i>p</i> &lt; 0.05).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Mean position error for the 3D assembly task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Mean collisions number with the environment for the 3D assembly task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0201-2/MediaObjects/10055_2011_201_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Mean execution time for the 3D assembly task</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0201-2/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig14">14</a> highlights the role of communication metaphors for the reduction in collisions with the 3D environment and confirms the previous results. In fact, the level of coordination allows avoiding the obstacles.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig15">15</a> shows a significant increase in the execution time with the communication metaphors presenting strong constants (<i>p</i> = 0.01, <i>p</i> &lt; 0.05). In fact, important viscosity greatly slows the movement of the expert partner. There is no significant effect of the soft constants (<i>p</i> = 0.1, <i>p</i> &gt; 0.05). However, the mean value is slightly more important.</p><p>Finally, we observe that the applied force on the master and the slave (13 and 10 N respectively) are close to that obtained for the 1D experiment (see Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig11">11</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0201-2#Fig12">12</a>).</p><p>The qualitative results from the questionnaire revealed that the majority of participants perceive a real improvement of gesture coordination with the communication metaphors for both strong and soft constants (Q1: mean score on the 5-points Likert scale of 4.8/5.0 ± 0.5). However, the participants perceive an increase in the execution time with the communication metaphors presenting the strong constants (Q2: mean score of 2.1/5.0 ± 0.8). The questionnaire shows that communication metaphors enable a better understanding of the actions and the gestures of partners (Q3: mean score of 4.1/5.0 ± 0.6). Finally, the role of communication metaphors (guidance vs. communication) was considered as guidance when the force factors of the viscosity and the spring models were strong (mean score of 3.9/5.0 ± 0.5) and a communication mechanism when these factors were soft (mean score of 3.6/5.0 ± 0:4). Thereby, the applied force factors (spring and viscosity) have a direct impact on the perceived role of the collaborative metaphors. Strong constants enable gestures guidance with constrained movements. Conversely, low constants enable more freedom of movements with a flexible exchange of information between the partners.</p></div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusion</h2><div class="c-article-section__content" id="Sec25-content"><p>This paper explores an active communication approach through the feedthrough mechanism. The different proposed communication metaphors highlight the potential of feedthrough to convey kinematic information about the partner gestures. The spring metaphor acts on the coordination of the partners’ positions. The viscosity metaphor improves the coordination of partners’ velocity, and finally, the vibration metaphor plays an important role during the change of direction between the gestures of the two partners. Furthermore, the results highlight the impact of these communication metaphors on the applied force by the two partners. If the spring metaphor reduces the applied force on the slave arm, the viscosity metaphor substantially increases the involved force on the master arm. Thus, according to the required level of coordination and the specifications of the collaborative tasks (accuracy, fatigue, tasks durations, etc.), it is necessary to integrate the most suitable combination of communication metaphors.</p><p>Our future investigation will concern the application of the proposed concept for real docking problems. In addition to the coordination of positions, this context requires strategies to support the coordination of orientations during a complex manipulation.</p><p>Beyond 3D assembly, the proposed communication metaphors could be applied in a collaborative gestural learning for different applications (e.g., surgery, design, and art). The proposed approaches provide several coupling strategies acting on the different kinematic parameters. These features could provide a flexible tool for the different learning strategies (i.e., close monitoring, trajectory learning, gestural/velocity learning).</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. ICRA" /><p class="c-article-references__text" id="ref-CR1">Ammi M, Ferreira A (2007) Robotic assisted micromanipulation system using virtual fixtures and metaphors. ICRA 454–460. <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4209133">http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4209133</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Basdogan, C-h. Ho, MA. Srinivasan, M. Slater, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Basdogan C, Ho C-h, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtua" /><p class="c-article-references__text" id="ref-CR2">Basdogan C, Ho C-h, Srinivasan MA, Slater M (2000) An experimental study on the role of touch in shared virtual environments. ACM Trans Comput Hum Interact 7:443–460</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365082" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20study%20on%20the%20role%20of%20touch%20in%20shared%20virtual%20environments&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=7&amp;pages=443-460&amp;publication_year=2000&amp;author=Basdogan%2CC&amp;author=Ho%2CC-h&amp;author=Srinivasan%2CMA&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SA. Bly, SR. Harrison, S. Irwin, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing en" /><p class="c-article-references__text" id="ref-CR3">Bly SA, Harrison SR, Irwin S (1993) Media spaces: bringing people together in a video, audio, and computing environment. Commun ACM 36(1):28–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F151233.151235" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Media%20spaces%3A%20bringing%20people%20together%20in%20a%20video%2C%20audio%2C%20and%20computing%20environment&amp;journal=Commun%20ACM&amp;volume=36&amp;issue=1&amp;pages=28-47&amp;publication_year=1993&amp;author=Bly%2CSA&amp;author=Harrison%2CSR&amp;author=Irwin%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Catlin T, Bush P, Yankelovich N (1989) InterNote: extending a hypermedia framework to support annotative colla" /><p class="c-article-references__text" id="ref-CR6">Catlin T, Bush P, Yankelovich N (1989) InterNote: extending a hypermedia framework to support annotative collaboration. In: Proceeding of hypertext’89 ACM conference, November 1989, pp 365–378</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Cha, MA. Eid, A. El-Saddik, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Cha J, Eid MA, El-Saddik A (2009) Touchable 3D video system. ACM Trans Multimed Comput Commun Appl (TOMCCAP) 5" /><p class="c-article-references__text" id="ref-CR7">Cha J, Eid MA, El-Saddik A (2009) Touchable 3D video system. ACM Trans Multimed Comput Commun Appl (TOMCCAP) 5(4):1–25</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1596990.1596993" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Touchable%203D%20video%20system&amp;journal=ACM%20Trans%20Multimed%20Comput%20Commun%20Appl%20%28TOMCCAP%29&amp;volume=5&amp;issue=4&amp;pages=1-25&amp;publication_year=2009&amp;author=Cha%2CJ&amp;author=Eid%2CMA&amp;author=El-Saddik%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Chan, KE. MacLean, J. McGrenere, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chan A, MacLean KE, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum " /><p class="c-article-references__text" id="ref-CR8">Chan A, MacLean KE, McGrenere J (2008) Designing haptic icons to support collaborative turn-taking. Int J Hum Comput Stud 66:333–355</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2007.11.002" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Designing%20haptic%20icons%20to%20support%20collaborative%20turn-taking&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=66&amp;pages=333-355&amp;publication_year=2008&amp;author=Chan%2CA&amp;author=MacLean%2CKE&amp;author=McGrenere%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Cockburn, P. Weir, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion" /><p class="c-article-references__text" id="ref-CR9">Cockburn A, Weir P (1999) An investigation of groupware support for collaborative awareness through distortion-oriented views. Int J Hum Comput Interact 11(3):231–255</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15327590IJHC1103_3" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20investigation%20of%20groupware%20support%20for%20collaborative%20awareness%20through%20distortion-oriented%20views&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;volume=11&amp;issue=3&amp;pages=231-255&amp;publication_year=1999&amp;author=Cockburn%2CA&amp;author=Weir%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Codella CF, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Tu" /><p class="c-article-references__text" id="ref-CR10">Codella CF, Jalili R, Koved L, Lewis JB, Ling DT, Lipscomb JS, Rabenhorst DA, Wang CP, Norton A, Sweeney P, Turk G (1992) Interactive simulation in a multi-person virtual world. In: CHI 1992, pp 329–334</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Collier G (1985) Emotional expression. Lawrence Erlbaum Associates, Hillsdale, NJ. http://www.questia.com/PM.q" /><p class="c-article-references__text" id="ref-CR11">Collier G (1985) Emotional expression. Lawrence Erlbaum Associates, Hillsdale, NJ. <a href="http://www.questia.com/PM.qst?a=o&amp;d=28053073">http://www.questia.com/PM.qst?a=o&amp;d=28053073</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Dix, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Dix AJ (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6" /><p class="c-article-references__text" id="ref-CR12">Dix AJ (1997) Challenges for cooperative work on the web: an analytical approach. Comput Supported Coop Work 6(2/3):135–156</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1489284" aria-label="View reference 10 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1008635907287" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Challenges%20for%20cooperative%20work%20on%20the%20web%3A%20an%20analytical%20approach&amp;journal=Comput%20Supported%20Coop%20Work&amp;volume=6&amp;issue=2%2F3&amp;pages=135-156&amp;publication_year=1997&amp;author=Dix%2CAJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Ellis, SJ. Gibbs, GL. Rein, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Ellis C, Gibbs SJ, Rein GL (1991) Groupware: some issues and experiences. Commun ACM 34(1):38–58" /><p class="c-article-references__text" id="ref-CR13">Ellis C, Gibbs SJ, Rein GL (1991) Groupware: some issues and experiences. Commun ACM 34(1):38–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F99977.99987" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Groupware%3A%20some%20issues%20and%20experiences&amp;journal=Commun%20ACM&amp;volume=34&amp;issue=1&amp;pages=38-58&amp;publication_year=1991&amp;author=Ellis%2CC&amp;author=Gibbs%2CSJ&amp;author=Rein%2CGL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Enriquez M, MacLean KE et al (2006) Haptic phonemes: basic building blocks of haptic communication. In: Procee" /><p class="c-article-references__text" id="ref-CR14">Enriquez M, MacLean KE et al (2006) Haptic phonemes: basic building blocks of haptic communication. In: Proceedings of 8th int’l conf. on multimodal interfaces (ICMI ‘06). Banff, Canada, pp 302–309</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gentry S (2005) Dancing cheek to cheek: haptic communication between partner dancers and swing as a finite sta" /><p class="c-article-references__text" id="ref-CR15">Gentry S (2005) Dancing cheek to cheek: haptic communication between partner dancers and swing as a finite state machine. Ph.D thesis, Massachusetts Institute of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleopera" /><p class="c-article-references__text" id="ref-CR16">Glynn S, Fekieta R, Henning RA (2001) Use of force-feedback joysticks to promote teamwork in virtual teleoperation. In: Proceedings of the 45th annual meeting of the human factors and ergonomics society, 8–12 October 2001, Minneapolis/St. Paul, MN, pp 1911–1915</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R (2004) Environnement de réalité augmentée 3D cooperative: approche colocalisée sur table. ARTIS-GRAV" /><p class="c-article-references__text" id="ref-CR17">Grasset R (2004) Environnement de réalité augmentée 3D cooperative: approche colocalisée sur table. ARTIS-GRAVIR/IMAG-INRIA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R, Lamb P, Billinghurst M (2005) Evaluation of mixed-space collaboration. In: ISMAR ‘05: Proceedings o" /><p class="c-article-references__text" id="ref-CR18">Grasset R, Lamb P, Billinghurst M (2005) Evaluation of mixed-space collaboration. In: ISMAR ‘05: Proceedings of the 4th IEEE/ACM international symposium on mixed and augmented reality, pp 90–99</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Gutwin, S. Greenberg, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distribu" /><p class="c-article-references__text" id="ref-CR19">Gutwin C, Greenberg S (1999) The effects of workspace awareness support on the usability of real-time distributed groupware. ACM Trans Comput Hum Interact 6(3):243–281</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F329693.329696" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20workspace%20awareness%20support%20on%20the%20usability%20of%20real-time%20distributed%20groupware&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=6&amp;issue=3&amp;pages=243-281&amp;publication_year=1999&amp;author=Gutwin%2CC&amp;author=Greenberg%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutwin C, Greenberg S (2000) The mechanics of collaboration: developing low cost usability evaluation methods " /><p class="c-article-references__text" id="ref-CR20">Gutwin C, Greenberg S (2000) The mechanics of collaboration: developing low cost usability evaluation methods for shared workspaces. In: Proceedings of the 9th IEEE international workshops on enabling technologies: infrastructure for collaborative enterprises, 4–16 June 2000. WETICE. IEEE Computer Society, Washington, DC, pp 98–103</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: GROUP’03: Proceedings of the 2003" /><p class="c-article-references__text" id="ref-CR22">Hill J, Gutwin C (2003) Awareness support in a groupware widget toolkit. In: GROUP’03: Proceedings of the 2003 international ACM SIGGROUP conference on supporting group work. Sanibel Island, Florida, USA, pp 258–267</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim Y-S, Ryu J-H (2009) Performance analysis of teleoperation systems with different haptic and video time-del" /><p class="c-article-references__text" id="ref-CR23">Kim Y-S, Ryu J-H (2009) Performance analysis of teleoperation systems with different haptic and video time-delay. In: Proceedings of the ICROS-SICE international joint conference 2009. Fukuoka, Japan, pp 3371–3375</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kjölberg J, Sallnäs E-L (2002) Supporting object handling and hand over tasks in haptic Collaborative Virtual " /><p class="c-article-references__text" id="ref-CR25">Kjölberg J, Sallnäs E-L (2002) Supporting object handling and hand over tasks in haptic Collaborative Virtual Environments. In: Wall S, Riedel B, Crossan A, McGee MR (eds) Proceedings of Eurohaptics′02, May 2002, pp 71–76</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Marsh, M. Glencross, S. Pettifer, RJ. Hubbold, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Marsh J, Glencross M, Pettifer S, Hubbold RJ (2006) A network architecture supporting consistent rich behavior" /><p class="c-article-references__text" id="ref-CR24">Marsh J, Glencross M, Pettifer S, Hubbold RJ (2006) A network architecture supporting consistent rich behavior in collaborative interactive applications. IEEE Trans Vis Comput Graph 12(3):405–416</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2006.40" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20network%20architecture%20supporting%20consistent%20rich%20behavior%20in%20collaborative%20interactive%20applications&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=12&amp;issue=3&amp;pages=405-416&amp;publication_year=2006&amp;author=Marsh%2CJ&amp;author=Glencross%2CM&amp;author=Pettifer%2CS&amp;author=Hubbold%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Peña Pérez Negrón A, de Antonio Jiménez A (2009) Using avatar’s nonverbal communication to monitor collaborati" /><p class="c-article-references__text" id="ref-CR26">Peña Pérez Negrón A, de Antonio Jiménez A (2009) Using avatar’s nonverbal communication to monitor collaboration in a task-oriented learning situation in a CVE. Workshop on intelligent and innovative support for collaborative learning activities. Rhodes, Greece, 8–13 June 2009, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Pinelle, C. Gutwin, S. Greenberg, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Pinelle D, Gutwin C, Greenberg S (2003) Task analysis for groupware usability evaluation: modeling shared-work" /><p class="c-article-references__text" id="ref-CR27">Pinelle D, Gutwin C, Greenberg S (2003) Task analysis for groupware usability evaluation: modeling shared-workspace tasks with the mechanics of collaboration. ACM Trans Comput Hum Interact 10(4):281–311</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F966930.966932" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Task%20analysis%20for%20groupware%20usability%20evaluation%3A%20modeling%20shared-workspace%20tasks%20with%20the%20mechanics%20of%20collaboration&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=10&amp;issue=4&amp;pages=281-311&amp;publication_year=2003&amp;author=Pinelle%2CD&amp;author=Gutwin%2CC&amp;author=Greenberg%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Redfern, N. Naughton, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Redfern S, Naughton N (2002) Collaborative virtual environments to support communication and community in inte" /><p class="c-article-references__text" id="ref-CR28">Redfern S, Naughton N (2002) Collaborative virtual environments to support communication and community in internet-based distance education. J Inf Technol Educ 1(3):201–211</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20virtual%20environments%20to%20support%20communication%20and%20community%20in%20internet-based%20distance%20education&amp;journal=J%20Inf%20Technol%20Educ&amp;volume=1&amp;issue=3&amp;pages=201-211&amp;publication_year=2002&amp;author=Redfern%2CS&amp;author=Naughton%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KB. Reed, MA. Peshkin, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Reed KB, Peshkin MA (2008) Physical collaboration of human-human and human-robot teams. IEEE Trans Haptics 1(2" /><p class="c-article-references__text" id="ref-CR29">Reed KB, Peshkin MA (2008) Physical collaboration of human-human and human-robot teams. IEEE Trans Haptics 1(2):108–120</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTOH.2008.13" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physical%20collaboration%20of%20human-human%20and%20human-robot%20teams&amp;journal=IEEE%20Trans%20Haptics&amp;volume=1&amp;issue=2&amp;pages=108-120&amp;publication_year=2008&amp;author=Reed%2CKB&amp;author=Peshkin%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Sallnas, K. Rassmus-grohn, C. Sjostrom, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sallnas E, Rassmus-grohn K, Sjostrom C (2000) Supporting presence in collaborative environments by haptic forc" /><p class="c-article-references__text" id="ref-CR30">Sallnas E, Rassmus-grohn K, Sjostrom C (2000) Supporting presence in collaborative environments by haptic force feedback. ACM Trans Comput Hum Interact 7(4):461–476</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F365058.365086" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20presence%20in%20collaborative%20environments%20by%20haptic%20force%20feedback&amp;journal=ACM%20Trans%20Comput%20Hum%20Interact&amp;volume=7&amp;issue=4&amp;pages=461-476&amp;publication_year=2000&amp;author=Sallnas%2CE&amp;author=Rassmus-grohn%2CK&amp;author=Sjostrom%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Shergill, PM. Bays, CD. Frith, DM. Wolpert, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Shergill SS, Bays PM, Frith CD, Wolpert DM (2003) Two eyes for an eye: the neuroscience of force escalation. S" /><p class="c-article-references__text" id="ref-CR31">Shergill SS, Bays PM, Frith CD, Wolpert DM (2003) Two eyes for an eye: the neuroscience of force escalation. Science 301(5630):187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1126%2Fscience.1085327" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two%20eyes%20for%20an%20eye%3A%20the%20neuroscience%20of%20force%20escalation&amp;journal=Science&amp;volume=301&amp;issue=5630&amp;publication_year=2003&amp;author=Shergill%2CSS&amp;author=Bays%2CPM&amp;author=Frith%2CCD&amp;author=Wolpert%2CDM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simard J, Ammi M, Auvray M (2010) Closely coupled collaboration for search tasks. In: Proceedings of the 17th " /><p class="c-article-references__text" id="ref-CR32">Simard J, Ammi M, Auvray M (2010) Closely coupled collaboration for search tasks. In: Proceedings of the 17th ACM symposium on Virtual Reallity Software and Technology (VRST), vol 1. pp 181–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Takemura H, Kishino F (1992) Cooperative work environment using virtual workspace. In: CSCW ‘92 proceedings, T" /><p class="c-article-references__text" id="ref-CR33">Takemura H, Kishino F (1992) Cooperative work environment using virtual workspace. In: CSCW ‘92 proceedings, Toronto, Canada, pp 226–232</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Varadaradjou ES, Dax P, Grumbach A (2006) Improved communication in virtual worlds. In: Virtual reality intern" /><p class="c-article-references__text" id="ref-CR34">Varadaradjou ES, Dax P, Grumbach A (2006) Improved communication in virtual worlds. In: Virtual reality international conference. Rocquencourt, Avril, pp 103–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Yano, H. Iwata, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Yano H, Iwata H (1994) Collaboration in virtual environment with force feedback. IPSJ SIG Notes 94(59):31–34" /><p class="c-article-references__text" id="ref-CR35">Yano H, Iwata H (1994) Collaboration in virtual environment with force feedback. IPSJ SIG Notes 94(59):31–34</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaboration%20in%20virtual%20environment%20with%20force%20feedback&amp;journal=IPSJ%20SIG%20Notes&amp;volume=94&amp;issue=59&amp;pages=31-34&amp;publication_year=1994&amp;author=Yano%2CH&amp;author=Iwata%2CH">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0201-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">LIMSI-CNRS, University of Paris-Sud, Orsay, France</p><p class="c-article-author-affiliation__authors-list">Jean Simard &amp; Mehdi Ammi</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jean-Simard"><span class="c-article-authors-search__title u-h3 js-search-name">Jean Simard</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jean+Simard&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jean+Simard" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jean+Simard%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mehdi-Ammi"><span class="c-article-authors-search__title u-h3 js-search-name">Mehdi Ammi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mehdi+Ammi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mehdi+Ammi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mehdi+Ammi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0201-2/email/correspondent/c1/new">Mehdi Ammi</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Haptic%20interpersonal%20communication%3A%20improvement%20of%20actions%20coordination%20in%20collaborative%20virtual%20environments&amp;author=Jean%20Simard%20et%20al&amp;contentID=10.1007%2Fs10055-011-0201-2&amp;publication=1359-4338&amp;publicationDate=2011-11-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Simard, J., Ammi, M. Haptic interpersonal communication: improvement of actions coordination in collaborative virtual environments.
                    <i>Virtual Reality</i> <b>16, </b>173–186 (2012). https://doi.org/10.1007/s10055-011-0201-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0201-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-08-02">02 August 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11-01">01 November 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11-23">23 November 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-09">September 2012</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0201-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0201-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Awareness</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sensorial communication</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Communication metaphors</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0201-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=201;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

