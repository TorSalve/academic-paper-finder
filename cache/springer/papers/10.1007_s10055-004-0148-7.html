<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Intelligent virtual agents keeping watch in the battlefield"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="One of the first areas where virtual reality found a practical application was military training. Two fairly obvious reasons have driven the military to explore and employ this kind of technique in..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Intelligent virtual agents keeping watch in the battlefield"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-02-23"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="One of the first areas where virtual reality found a practical application was military training. Two fairly obvious reasons have driven the military to explore and employ this kind of technique in their training; to reduce exposure to hazards and to increase stealth. Many aspects of combat operations are very hazardous, and they become even more dangerous if the combatant seeks to improve his performance. Some smart weapons are autonomous, while others are remotely controlled after they are launched. This allows the shooter and weapon controller to launch the weapon and immediately seek cover, thus decreasing his exposure to return fire. Before launching a weapon, the person who controls that weapon must acquire/perceive as much information as he can, not only from its environment, but also from the people who inhabits that environment. Intelligent virtual agents (IVAs) are used in a wide variety of simulation environments, especially in order to simulate realistic situations as, for example, high fidelity virtual environment (VE) for military training that allows thousands of agents to interact in battlefield scenarios. In this paper, we propose a perceptual model, which seeks to introduce more coherence between IVA perception and human being perception, increasing the psychological &#8220;coherence&#8221; between the real life and the VE experience. Agents lacking this perceptual model could react in a non-realistic way, hearing or seeing things that are too far away or hidden behind other objects. The perceptual model, we propose in this paper introduces human limitations inside the agent&#8217;s perceptual model with the aim of reflecting human perception."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-02-23"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="185"/>

    <meta name="prism.endingPage" content="193"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-004-0148-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-004-0148-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-004-0148-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-004-0148-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Intelligent virtual agents keeping watch in the battlefield"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2005/06"/>

    <meta name="citation_online_date" content="2005/02/23"/>

    <meta name="citation_firstpage" content="185"/>

    <meta name="citation_lastpage" content="193"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-004-0148-7"/>

    <meta name="DOI" content="10.1007/s10055-004-0148-7"/>

    <meta name="citation_doi" content="10.1007/s10055-004-0148-7"/>

    <meta name="description" content="One of the first areas where virtual reality found a practical application was military training. Two fairly obvious reasons have driven the military to ex"/>

    <meta name="dc.creator" content="Pilar Herrero"/>

    <meta name="dc.creator" content="Ang&#233;lica de Antonio"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Benford SD, Fahl&#233;n LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of 3rd European conference on computer supported cooperative work (ECSCW&#8217;93). Kluwer, Milano"/>

    <meta name="citation_reference" content="Blumberg B (1997) Go with the flow: synthetic vision for autonomous animated creatures. In: Proceedings of the 1st international conference on autonomous agents (Agents&#8217;97), Marina del Rey"/>

    <meta name="citation_reference" content="citation_journal_title=Autonomous Agents Multi-agent Syst; citation_title=Where to look? Automating attending behaviors of virtual human characters; citation_author=S Chopra-Khullar, N Badler; citation_volume=4; citation_issue=1/2; citation_publication_date=2001; citation_pages=9-23; citation_doi=10.1023/A:1010010528443; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Toward a theory of situation awareness in dynamic systems; citation_author=M Endsley; citation_volume=37; citation_issue=1; citation_publication_date=1995; citation_pages=65-84; citation_id=CR4"/>

    <meta name="citation_reference" content="Herrero P (2003) A human like perceptual model for intelligent virtual agents. PhD Thesis. Universidad Polit&#233;cnica de Madrid"/>

    <meta name="citation_reference" content="Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in cooperative information systems. In: Proceedings of the 11th international conference on cooperative information systems (CoopIS 2003). Catania, Sicily"/>

    <meta name="citation_reference" content="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne"/>

    <meta name="citation_reference" content="Hill R, Han C, van Lent M (2002) Applying perceptually driven cognitive mapping to virtual urban environments. In: Conference on innovative applications of artificial intelligence (IAAI-2002) in Edmonton"/>

    <meta name="citation_reference" content="Hill R, Han C, van Lent M (2002) Perceptually driven cognitive mapping of urban environments. In: Proceedings of the first international joint conference on autonomous agents and multiagent systems, Bologna"/>

    <meta name="citation_reference" content="citation_title=Contemporary ergonomics 1997; citation_publication_date=1997; citation_id=CR10; citation_author=PA Howarth; citation_author=PJ Costello; citation_publisher=Robertson SA (ed) Taylor and Francis"/>

    <meta name="citation_reference" content="Kendall G (2002) 3D Sound. Center for Music Technology School of Music. Northwestern University, Consulted."/>

    <meta name="citation_reference" content="citation_journal_title=J Vis; citation_title=Suppressive and facilitatory spatial interactions in foveal vision: foveal crowding is simple contrast masking; citation_author=DM Levi, SA Klein, S Hariharan; citation_volume=2; citation_publication_date=2002; citation_pages=140-166; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis; citation_title=Suppressive and facilitatory spatial interactions in peripheral vision: peripheral crowding is neither size invariant nor simple contrast masking; citation_author=DM Levi, S Hariharan, SA Klein; citation_volume=2; citation_publication_date=2002; citation_pages=167-177; citation_id=CR13"/>

    <meta name="citation_reference" content="Noser H (1997) A behavioral animation system based on L-systems and synthetic sensors for actors. PhD Thesis, &#201;cole Polytechnique F&#233;d&#233;rale De Lausanne"/>

    <meta name="citation_reference" content="Shinn-Cunningham, BG (2000) Distance cues for virtual auditory space. In: Proceedings of the IEEE 2000 international symposium on multimedia information processing, Sydney"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Vis Res; citation_title=Animat vision: active vision in artificial animals; citation_author=D Terzopoulos, TF Rabie; citation_volume=1; citation_issue=1; citation_publication_date=1997; citation_pages=2-19; citation_id=CR16"/>

    <meta name="citation_reference" content="Thalmann D (2001) The foundations to build a virtual human society. In: Proceedings of Intelligent Virtual Actors (IVA&#8217;01). Madrid, Spain"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Assessing auditory distance perception using virtual acoustics; citation_author=P Zahorik; citation_volume=111; citation_publication_date=2002; citation_pages=1832-1846; citation_doi=10.1121/1.1458027; citation_id=CR18"/>

    <meta name="citation_author" content="Pilar Herrero"/>

    <meta name="citation_author_email" content="pherrero@fi.upm.es"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_author" content="Ang&#233;lica de Antonio"/>

    <meta name="citation_author_email" content="angelica@fi.upm.es"/>

    <meta name="citation_author_institution" content="Facultad de Inform&#225;tica, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-004-0148-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-004-0148-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Intelligent virtual agents keeping watch in the battlefield"/>
        <meta property="og:description" content="One of the first areas where virtual reality found a practical application was military training. Two fairly obvious reasons have driven the military to explore and employ this kind of technique in their training; to reduce exposure to hazards and to increase stealth. Many aspects of combat operations are very hazardous, and they become even more dangerous if the combatant seeks to improve his performance. Some smart weapons are autonomous, while others are remotely controlled after they are launched. This allows the shooter and weapon controller to launch the weapon and immediately seek cover, thus decreasing his exposure to return fire. Before launching a weapon, the person who controls that weapon must acquire/perceive as much information as he can, not only from its environment, but also from the people who inhabits that environment. Intelligent virtual agents (IVAs) are used in a wide variety of simulation environments, especially in order to simulate realistic situations as, for example, high fidelity virtual environment (VE) for military training that allows thousands of agents to interact in battlefield scenarios. In this paper, we propose a perceptual model, which seeks to introduce more coherence between IVA perception and human being perception, increasing the psychological “coherence” between the real life and the VE experience. Agents lacking this perceptual model could react in a non-realistic way, hearing or seeing things that are too far away or hidden behind other objects. The perceptual model, we propose in this paper introduces human limitations inside the agent’s perceptual model with the aim of reflecting human perception."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Intelligent virtual agents keeping watch in the battlefield | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-004-0148-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Intelligent virtual agents (IVAs), Perception, Awareness, Focus, Nimbus, Human factors","kwrd":["Intelligent_virtual_agents_(IVAs)","Perception","Awareness","Focus","Nimbus","Human_factors"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-004-0148-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-004-0148-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=148;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-004-0148-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Intelligent virtual agents keeping watch in the battlefield
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0148-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0148-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-02-23" itemprop="datePublished">23 February 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Intelligent virtual agents keeping watch in the battlefield</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Pilar-Herrero" data-author-popup="auth-Pilar-Herrero" data-corresp-id="c1">Pilar Herrero<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo S/N. 28660 Boadilla del Monte, Madrid, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ang_lica_de-Antonio" data-author-popup="auth-Ang_lica_de-Antonio">Angélica de Antonio</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000000121512978, Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo S/N. 28660 Boadilla del Monte, Madrid, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">185</span>–<span itemprop="pageEnd">193</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">129 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-004-0148-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>One of the first areas where virtual reality found a practical application was military training. Two fairly obvious reasons have driven the military to explore and employ this kind of technique in their training; to reduce exposure to hazards and to increase stealth. Many aspects of combat operations are very hazardous, and they become even more dangerous if the combatant seeks to improve his performance. Some smart weapons are autonomous, while others are remotely controlled after they are launched. This allows the shooter and weapon controller to launch the weapon and immediately seek cover, thus decreasing his exposure to return fire. Before launching a weapon, the person who controls that weapon must acquire/perceive as much information as he can, not only from its environment, but also from the people who inhabits that environment. Intelligent virtual agents (IVAs) are used in a wide variety of simulation environments, especially in order to simulate realistic situations as, for example, high fidelity virtual environment (VE) for military training that allows thousands of agents to interact in battlefield scenarios. In this paper, we propose a perceptual model, which seeks to introduce more coherence between IVA perception and human being perception, increasing the psychological “coherence” between the real life and the VE experience. Agents lacking this perceptual model could react in a non-realistic way, hearing or seeing things that are too far away or hidden behind other objects. The perceptual model, we propose in this paper introduces human limitations inside the agent’s perceptual model with the aim of reflecting human perception.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Imagine yourself as a young soldier of the Army on your first risky mission. You are in the battlefield exposed to the hostile fire. You have already used up all your ammunition and you just have a hand grenade on your stores. You need to calculate pretty well at which distance you are going to launch the weapon.</p><p>You know that you are very far away from the enemy and, at that position, probably you will fail. You must be closer before launching the grenade but you do not want to be detected by the enemy, so you decide to drag yourself on the ground. While you are moving you cannot raise your head and therefore you can look up from the ground. You try to guide yourself by ear. You start hearing the start of an engine and you run towards a dense shrub, which is covering that area.</p><p>You have not a good field of vision as to perceive what is happening there but you start hearing a group of people chatting each other. They are pretty far and you cannot distinguish what they are speaking about but following their voices you can move along the dense shrub.</p><p>One of them starts shouting and they move forward, approaching a truck. Thanks to their voices you have managed to locate yourself in a strategy position where although you have a very reduce field of view but you have a pretty good visibility.</p><p>In that moment you think that it could be a good idea to wait until all of them are inside the truck to launch the grenade, and then you start preparing your hand grenade to be launched. However, before doing it, you hear a couple of people approaching you. The dense shrub and the distance make the perception more difficult but while, they approach you hear the word “prisoners” and then, as the distance decreases, the clarity of visual and hearing perception increases, to come just at the right moment when you are able to distinguish that there are some allied, captured by the enemy, with them, who are going to be gotten into the truck.</p><p>At this moment one should react quickly but, from all the possible reactions, which should be the more appropriated for a realistic and useful training?</p><p>Our goal is to enrich virtual worlds with more human-like intelligent virtual agents (IVAs). An IVA is an autonomous embodied agent in a usually 3D interactive graphical environment or virtual environment (VE), which draws on Artificial Intelligence (AI) and Artificial Life technology so as to interact intelligently with its environment and with human users.</p><p>An IVA may simply evolve in its environment or it may interact with this environment or even communicate with other IVAs or humans but, in order to make this interaction possible, an IVA has to be aware of its environment. This awareness is gathered through the agent’s perception.</p><p>The research that we present in this paper is precisely oriented towards endowing IVAs with perceptual mechanisms that allow them to be “realistically <i>aware</i>” of their surroundings. We propose a perceptual model, which seeks to introduce more coherence between IVA perception and human being perception. This will increment the psychological “<i>coherence</i>” between the real life and the VE experience. This coherence is especially important in order to simulate realistic situations as, for example, the scenarios described above. A useful training would involve endowing soldier agents with a human-like perceptual model, so that they would react to the same stimuli as a human soldier. Agents lacking this perceptual model could react in a non-realistic way, hearing or seeing things that are too far away or hidden behind other objects. The perceptual model, we propose in this paper, introduces human limitations inside the agent’s perceptual model with the aim of reflecting human perception.</p><p>In this paper, we first give an overview of how we have designed and formalised our perceptual model: analysing the factors that can make the perceptual model more realistic; redefining and reinterpreting the set of key concepts introduced by the Spatial Model of Interaction; and introducing a set of mathematical functions to describe the agents’ perception. We also explain how this model of perception has been implemented and we describe some scenarios where the introduction of this perceptual model could be interesting.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Designing and formalising our perceptual model</h2><div class="c-article-section__content" id="Sec2-content"><p>Many approaches have been employed to implement the visual process of perception in IVAs, oriented to different kind of applications, such as artificial creatures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Blumberg B (1997) Go with the flow: synthetic vision for autonomous animated creatures. In: Proceedings of the 1st international conference on autonomous agents (Agents’97), Marina del Rey" href="/article/10.1007/s10055-004-0148-7#ref-CR2" id="ref-link-section-d39051e308">2</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Terzopoulos D, Rabie TF (1997) Animat vision: active vision in artificial animals. J Comput Vis Res 1(1):2–19" href="/article/10.1007/s10055-004-0148-7#ref-CR16" id="ref-link-section-d39051e311">16</a>] or virtual humans [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Chopra-Khullar S, Badler N (2001) Where to look? Automating attending behaviors of virtual human characters. Autonomous Agents Multi-agent Syst 4(1/2):9–23" href="/article/10.1007/s10055-004-0148-7#ref-CR3" id="ref-link-section-d39051e314">3</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Hill R, Han C, van Lent M (2002) Applying perceptually driven cognitive mapping to virtual urban environments. In: Conference on innovative applications of artificial intelligence (IAAI-2002) in Edmonton" href="/article/10.1007/s10055-004-0148-7#ref-CR8" id="ref-link-section-d39051e317">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Hill R, Han C, van Lent M (2002) Perceptually driven cognitive mapping of urban environments. In: Proceedings of the first international joint conference on autonomous agents and multiagent systems, Bologna" href="/article/10.1007/s10055-004-0148-7#ref-CR9" id="ref-link-section-d39051e320">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Noser H (1997) A behavioral animation system based on L-systems and synthetic sensors for actors. PhD Thesis, École Polytechnique Fédérale De Lausanne" href="/article/10.1007/s10055-004-0148-7#ref-CR14" id="ref-link-section-d39051e324">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Thalmann D (2001) The foundations to build a virtual human society. In: Proceedings of Intelligent Virtual Actors (IVA’01). Madrid, Spain" href="/article/10.1007/s10055-004-0148-7#ref-CR17" id="ref-link-section-d39051e327">17</a>]. Perception in those agents has been modelled in diverse ways, depending on what they were designed for. Basically, the implementation of perception can be focussed on the processing of sensory inputs [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Terzopoulos D, Rabie TF (1997) Animat vision: active vision in artificial animals. J Comput Vis Res 1(1):2–19" href="/article/10.1007/s10055-004-0148-7#ref-CR16" id="ref-link-section-d39051e330">16</a>] or on the cognitive process of perception [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Hill R, Han C, van Lent M (2002) Applying perceptually driven cognitive mapping to virtual urban environments. In: Conference on innovative applications of artificial intelligence (IAAI-2002) in Edmonton" href="/article/10.1007/s10055-004-0148-7#ref-CR8" id="ref-link-section-d39051e333">8</a>]. In this paper, we have focussed on the sensory inputs of the perceptual model. A classification of current approaches can be found in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Herrero P (2003) A human like perceptual model for intelligent virtual agents. PhD Thesis. Universidad Politécnica de Madrid" href="/article/10.1007/s10055-004-0148-7#ref-CR5" id="ref-link-section-d39051e336">5</a>].</p><p>Perception can be understood as the first level of a situational awareness model. Endsley defines situational awareness—or situational assessment—as “The perception of the elements in the environment within a volume of space and time, the comprehension of their meaning, the projection of their status into the near future, and the prediction of how various actions will affect the fulfilment of one’s goals” [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Endsley M (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37(1):65–84" href="/article/10.1007/s10055-004-0148-7#ref-CR4" id="ref-link-section-d39051e342">4</a>]. So, the critical factors in the process of situation assessment are (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig1">1</a>): Perception of elements in current situation; comprehension of current situations, and projection of future.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb1.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb1.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Situational awareness</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Bearing in mind the previous definition, sensitive perception can be understood as the first level of awareness, and therefore, taking into account our own experience on Computer Supported Collaborative Work (CSCW) applications, we have decided to develop our perceptual model based on one of the most successful CSCW awareness models, known as the “Spatial Model of Interaction” (SMI) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Benford SD, Fahlén LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of 3rd European conference on computer supported cooperative work (ECSCW’93). Kluwer, Milano" href="/article/10.1007/s10055-004-0148-7#ref-CR1" id="ref-link-section-d39051e368">1</a>]. This awareness model introduces a set of key awareness concepts—which have been extended to introduce some human factors—and uses the properties of the space to mediate interaction.</p><p>There are many factors that contribute to our ability as humans to perceive an object, some of which are directly working on the mental processes, being not easily modelled or reproduced in a virtual world.</p><p>In order to carry out this research, we have analysed separately those human factors which are relevant for visual [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne" href="/article/10.1007/s10055-004-0148-7#ref-CR7" id="ref-link-section-d39051e377">7</a>] and auditory [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in cooperative information systems. In: Proceedings of the 11th international conference on cooperative information systems (CoopIS 2003). Catania, Sicily" href="/article/10.1007/s10055-004-0148-7#ref-CR6" id="ref-link-section-d39051e380">6</a>] perception. Then, we have selected some of them to be introduced in our perceptual model:</p><h3 class="c-article__sub-heading" id="Sec3">Sense acuity</h3><p>In a visual medium, it is known as <i>visual acuity</i>. The visual acuity is a measure of the eye’s ability to resolve fine detail and is dependent upon the person itself, the accommodative state of the eye, the illumination level and the contrast between target and background [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Howarth PA, Costello PJ (1997) Contemporary ergonomics 1997. Robertson SA (ed) Taylor and Francis, London, pp 109–116" href="/article/10.1007/s10055-004-0148-7#ref-CR10" id="ref-link-section-d39051e393">10</a>]. Virtual agents that exhibit this property would be able, for instance, to perceive a message, wrote on a notice board, only if the distance from the agent to that notice board is within the visual range of perception. In a hearing medium, it is know as <i>auditory acuity</i>. There are two different kinds of auditory acuity: spatial acuity and frequency acuity. Both acuities are inter-related. Virtual agents that exhibit this property would be able, for instance, to detect a sound only if its frequency is within the usual human audible range, and only if it is not too far.</p><h3 class="c-article__sub-heading" id="Sec4">Sense transition region</h3><p>It is the interval in the space between perfect and null perception. This factor plays an important role in a visual medium where it is known as <i>lateral vision</i>. The lateral vision corresponds to the visual perception towards the extremes of the visual field. Virtual agents should exhibit this characteristic to avoid anomalous behaviours as, for example, those that will happen if a soldier agent A is not aware of and cannot interact with another soldier agent B, who is inside its lateral vision area. In a hearing medium, this concept can be understood as the cone in the space known as <i>cone of confusion</i>. The cone of confusion is a cone extending outwards from each ear. Sound events that originate from a point in this cone are subject to ambiguity, which has been solved by head movements, leaving the pinna effect as a future research line.</p><p>These human factors are strongly related to some <i>physical factors</i> such as the distance between the item (object or sound) and the position of the agent’s sense (<i>d</i><sub>sense-item</sub>) and some <i>item’s factors</i> such as the <i>object’s size</i> (for a visual medium) or the <i>sound intensity</i> (in a hearing medium).</p><p>In a hearing medium, it is also important to take into account some factors associated to the sound source propagation, as the <i>directivity of sound</i>, introducing the directional characteristic of a sound source. If a virtual source is endowed with this property, we will be able to determine how much sound is directed towards a specific area compared to all the sound energy being generated by a source.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Key concepts in the spatial model of interaction</h2><div class="c-article-section__content" id="Sec5-content"><p>As we mentioned in previous sections, the key concepts of our perceptual model are based on the main concepts of a CSCW awareness model known as the spatial model of interaction (SMI) [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Benford SD, Fahlén LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of 3rd European conference on computer supported cooperative work (ECSCW’93). Kluwer, Milano" href="/article/10.1007/s10055-004-0148-7#ref-CR1" id="ref-link-section-d39051e447">1</a>].</p><p>The spatial model, as its name suggests, uses the properties of space as the basis for mediating interaction. It was proposed as a way to control the flow of information of the environment in collaborative VEs (CVEs). It allows objects in a virtual world to govern their interaction through some key concepts: medium, aura, awareness, focus, nimbus, adapters and boundaries.</p><p><i>Aura</i> is the sub-space, which effectively bounds the presence of an object within a given medium and which acts as an enabler of potential interaction. In each particular medium, it is also possible to delimit the observing object’s interest; this area is called <i>focus:</i> “The more an object is within your focus the more aware you are of it”. The focus concept has been implemented in the SMI as a circular sector limited by the object’s aura.</p><p>In the same way, it is possible to represent the observed object’s projection in a particular medium; this area is called <i>nimbus</i>: “The more an object is within your nimbus the more aware it is of you”. The nimbus concept, as it was defined in the Spatial Model of Interaction, has always been implemented as a circumference in a visual medium. The radio of this circumference has an “ideal” infinite value, although in practice, it is limited by the object’s aura.</p><p>The implementations of these concepts—aura, focus and nimbus—in the SMI did not have in mind human aspects. Therefore, if our perceptual model for IVAs had taken these concepts as they were defined, then it would have reduced the level of coherence between the real and the virtual agent behaviour.</p><p>An additional concept was involved in controlling interaction between objects in the SMI, <i>awareness</i>. One object’s awareness of another object quantifies the subjective importance or relevance of that object. The awareness relationship between every pair of objects is achieved on the basis of quantifiable <i>levels</i> of awareness between them and it is unidirectional and specific to each medium. Awareness between objects, in a given medium, is manipulated via <i>focus</i> and <i>nimbus</i>. Moreover, an object’s aura, focus, nimbus, and hence awareness, can be modified through <i>boundaries</i> and some artefacts called <i>adapters</i>.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Reinterpreting the SMI’s key concepts</h2><div class="c-article-section__content" id="Sec6-content"><p>Neither the SMI nor its implementations considered aspects of human perception. Therefore, we decided to introduce into the SMI some factors concerning human perception. In this section, we are going to describe how the key concepts defining the SMI have been modified to introduce these human factors.</p><h3 class="c-article__sub-heading" id="Sec7">Focus</h3><p>In our perceptual model, the focus notion is the area within which the agent perceives the environment.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Visual focus</h4><p>Taking into account two human factors—the <i>visual acuity</i> and the <i>lateral vision</i>—and the <i>object’s size</i>, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne" href="/article/10.1007/s10055-004-0148-7#ref-CR7" id="ref-link-section-d39051e520">7</a>] a new mathematical function has been defined to represent the human-like visual focus as a double cone delimited by two angles: one of them associated to the human foveal field of vision and the other associated to human lateral field of vision (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig2">2</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb2.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb2.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Intelligent virtual agents visual focus in the perceptual model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>An IVA endowed with this perceptual model and a focus, as the showed in the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig2">2</a>, could be able to perceive an object as O1, as well as some details—as for example the movement—of an object as O2 and will not be able to perceive an object as O3.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Hearing focus</h4><p>Taking into account two human perceptual factors—the <i>auditory acuity</i> and the <i>cone of confusion</i>—a new mathematical function has been defined to represent the human-like hearing focus as an sphere, whose centre is located in between both the ears (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb3.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb3.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Intelligent virtual agents hearing focus in the perceptual model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>An IVA endowed with this perceptual model and a focus, as shown in the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig3">3</a>, could be able to detect a sound only if the propagated sound reaches the agent’s ear within its audible range.</p><h3 class="c-article__sub-heading" id="Sec10">Nimbus</h3><p>Just as with the above-mentioned focus concept, the nimbus concept in the spatial model of interaction does not consider any human factors, thus hypothetically reducing the level of coherence between real and virtual agent behaviour.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Visual nimbus</h4><p>Taking into account the object’s physical constraints—such as the object’s shape and size—in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne" href="/article/10.1007/s10055-004-0148-7#ref-CR7" id="ref-link-section-d39051e605">7</a>] some mathematical functions have been defined to represent the object’s nimbus as an ellipsoid or a sphere depending on the conic by which it is circumscribed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Hearing nimbus</h4><p>In a hearing medium, the nimbus delimits the physical area of projection of a sound source. Sound is propagated in the medium by a spherical wavefront, but even if this occurs, it could happen that the sound amplitude, and therefore its intensity, were not the same in all the directions. For this reason, in this model, we interpret the nimbus concept as the region within which the sound source is projected with the same intensity.</p><p>Starting from this interpretation, we have decided to take into account some factors—such as the <i>directivity of sound</i> and the <i>sound intensity</i>—and their influence on nimbus and its representation within an environment, leaving the rest of the factors, as for example, the presence of non-linear effects or the homogeneity of the medium, for future research and extensions to this work.</p><p>Taking into account these factors, in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in cooperative information systems. In: Proceedings of the 11th international conference on cooperative information systems (CoopIS 2003). Catania, Sicily" href="/article/10.1007/s10055-004-0148-7#ref-CR6" id="ref-link-section-d39051e627">6</a>] we have centred our research in the projection of human voice, giving a new mathematical function to represent the human-like hearing nimbus as a cardioid (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7flb4.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7flb4.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Auditory nimbus</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This figure represents the perimeter within which the human being projects its voice (human voice nimbus) with a given intensity. For any other sound source (different to human voice), it would be necessary to calculate the pattern of directivity before formulating nimbus.</p><h3 class="c-article__sub-heading" id="Sec13">Awareness</h3><p>Awareness is a very broad concept with many different meanings in many different areas. In fact, we have reinterpreted the concept of “awareness” introduced by the SMI from the agent perception point of view.</p><p>In our perceptual model <i>awareness</i> represents whether an agent perceive an object or a sound as to be aware of it.</p><p>In this way, in our visual perceptual model, awareness represents the overlap between the focus and the nimbus. If this overlap is not null, it means that the agent is aware of the item’s presence (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb5.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb5.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Visual awareness</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In the same way, in our hearing perceptual model, awareness represents whether the item (the sound in this case) “effectively” projects inside the agent’s focus. If so, it means that the agent is aware of the sound’s projection (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb6.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb6.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Hearing awareness</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We have also introduced an awareness architecture, dividing the process of sensitive perception defined along this paper into two different blocks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig7">7</a>): </p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Perceptual engine</i>: Implements concepts, such as focus, nimbus and awareness for all the agents, objects and sound sources in the environment.</p>
                    </li>
                    <li>
                      <p><i>Agent’s perception</i>: Implements concepts, such as the agent’s clarity of perception (CP), as well as the specific agent’s parameters.</p>
                    </li>
                  </ul> <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7flb7.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7flb7.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Sensitive perception</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The perceptual engine is in charge of getting the physical details, such as position size or sound source intensity, of all the objects/agents (in general we will call them items) that are in the environment and calculating their nimbus. The perceptual engine also asks each agent’s about its perceptual details—such as its sense acuity or which is the angle delimiting its field of vision—calculating the agent’s focus and outputting a list of all those items that can be perceived by this agent according to these physical and perceptual details.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Clarity of perception</h2><div class="c-article-section__content" id="Sec14-content"><p>In the previous sections, we have introduced the key concepts of our perceptual model. In this section, we are going to concentrate on the main function of the agent’s perception.</p><h3 class="c-article__sub-heading" id="Sec15">Visual clarity of perception</h3><p>The process of human visual perception is continuous, and the size of the image on the retina will continuously depend on the distance between the eye and the object to be perceived. Therefore, from the sensorial point of view, if CP is the ability to distinguish what object is being perceived, then it should depend on the object image that we have on the retina. Moreover, as the retinal image decreases continuously with the eye-object distance, then the CP should decrease continuously with the increase in this distance as well. But we are also taking into account the size constancy phenomenon, by means of which the object’s size tends to appear constant in spite of it changing with distance. This factor will imply that the CP will fall still more smoothly. Following the research conducted by Levi et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Levi DM, Klein SA, Hariharan S (2002) Suppressive and facilitatory spatial interactions in foveal vision: foveal crowding is simple contrast masking. J Vis 2:140–166. &#xA;                    http://journalofvision.org/2/2/2/&#xA;                    &#xA;                  " href="/article/10.1007/s10055-004-0148-7#ref-CR12" id="ref-link-section-d39051e772">12</a>], we propose a Gaussian as the function to describe the variation in the CP with the eye-object distance for a fixed object’s size in the foreground region (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ1">1</a>), where <i>d</i><sub>1</sub> represents the minimum distance necessary to have a clear perception of an object and <i>d</i><sub>2</sub> represents the maximum distance at which we can have a clear perception of an object. The level of detail starts decreasing between <i>d</i><sub>2</sub> and <i>d</i><sub>3</sub>, and starting from <i>d</i><sub>4</sub> the eye cannot perceive almost any detail from any object. </p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp;{ 0.0 \leq d \leq d_1 \quad {\text{CP}}(d) = \lambda d }\\ &amp; {d_1 \leq d \leq d_2 \quad {\text{CP}}(d) = {\text{CP}}_{\max } }\\ &amp; {d \geqslant d_2 \quad {\text{CP}}(d) = \frac{1} {{\sigma \times \sqrt {2 \times \pi } }} \times \exp \left\{ { - \frac{{(d - d_2 )^2 }} {{2 \times \sigma ^2 }}} \right\} }\\ \end{aligned} $$ </span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> We also propose another Gaussian function to describe the variation that the CP has with the distance eye-object (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ2">2</a>) in the lateral region. </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp;{ {\text{0}}{\text{.0}} &lt; {\text{ CP}}_{{\text{L max}}} &lt; {\text{CP}}_{\max } }\\ &amp;{ d'_1 &gt; d_1 \;d'_2 &lt; d_2 \;d'_3 &gt; d_3 \;d'_4 \approx d_4 } \\ \end{aligned} $$ </span></div><div class="c-article-equation__number">
                    (2)
                </div></div><h3 class="c-article__sub-heading" id="Sec16">Auditory clarity of perception</h3><p>A human listener can hear a sound arriving to him in two different ways: combining information from the two ears, “<i>binaural hearing”</i>, or taking information from one ear or from each ear independently, “<i>monaural hearing”</i>. Modern psychoacoustic research has turned its attention to binaural hearing [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Kendall G (2002) 3D Sound. Center for Music Technology School of Music. Northwestern University, Consulted." href="/article/10.1007/s10055-004-0148-7#ref-CR11" id="ref-link-section-d39051e838">11</a>] and we are also going to focus on this kind of hearing along this research.</p><p>There are many factors contributing to auditory CP. It is generally accepted that the perception of sound source depends on a variety of acoustic cues, including: intensity or sound pressure level, frequency spectrum and binaural cues. It has also been demonstrated that this perception depends thorough on the distance listener-sound [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Zahorik P (2002) Assessing auditory distance perception using virtual acoustics. J Acoust Soc Am 111:1832–1846" href="/article/10.1007/s10055-004-0148-7#ref-CR18" id="ref-link-section-d39051e844">18</a>].</p><p>Based on Shinn-Cunningham [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Shinn-Cunningham, BG (2000) Distance cues for virtual auditory space. In: Proceedings of the IEEE 2000 international symposium on multimedia information processing, Sydney" href="/article/10.1007/s10055-004-0148-7#ref-CR15" id="ref-link-section-d39051e850">15</a>] studies we can conclude that hearing perception varies continually with the distance to the sound source. We propose to model the auditory CP by two functions to describe the monaural—CP<sub>M</sub>-and binaural—CP<sub>B</sub>—(Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ3">3</a>) CP versus the distance. </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; {{\text{CP}}_{\text{M}} \left( d \right) = {\text{CP}}_0 \left( \theta \right) - 20 \times \log \left( d \right)} \\ &amp; {{\text{CP}}_{\text{B}} \left( d \right) = \frac{r} {d} \times {\text{CP}}_{\text{M}} \left( d \right)\quad {\text{if}}\quad d \in \left[ {0.15,1} \right] }\\ &amp; {{\text{CP}}_{\text{B}} \left( d \right) = \frac{r} {d} \times {\text{CP}}_{\text{M}} \left( d \right)\quad {\text{if}}\quad d \in \left[ {1,d_{\max } } \right]} \\ \end{aligned} $$ </span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> where </p><ul class="u-list-style-dash">
                    <li>
                      <p>CP<sub>0</sub>(<i>d</i>) represents the initial value of hearing clarity (as this value depends on the azimuth angle, we have taken the experimental results obtained by Shinn-Cunningham experiments)</p>
                    </li>
                    <li>
                      <p><i>d</i> represents the distance calculated, in binaural perception, from the centre of the virtual head of the participant.</p>
                    </li>
                    <li>
                      <p><i>r</i> represents the head radius</p>
                    </li>
                    <li>
                      <p><i>d</i><sub>max</sub> represents the maximum distance at which the sound can be perceived with clarity (this value depends on the hearing acuity as for the kind of sound)</p>
                    </li>
                  </ul></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Perceptual model implementation</h2><div class="c-article-section__content" id="Sec17-content"><p>This model has been implemented in Visual C++ as a library. This library has been integrated with MASSIM_AGENT, a prototype system built using the MASSIVE-3 CVE system and the SIM_AGENT toolkit for developing agents, but the design of this library has been done to make it independent on any VE system or agent platform. In fact, our next goal is to integrate it with the system being built under a project called Model for the Application of Intelligent Virtual Environments to Education and Training (MAEVIF). This project is part of the Spanish National R&amp;D plan.</p><p>MASSIM_AGENT is the first prototype resulting of the integration of the MASSIVE-3 system and the SIM_AGENT toolkit. MASSIM_AGENT was the result of a collaboration established between the Mixed Reality Laboratory (MRL) at the University of Nottingham and the Universidad Politécnica de Madrid (UPM). SIM_AGENT has its own methods for the agents to get information from the environment. The main problem in integrating it with MASSIVE-3 has been to over-ride these default methods, obtaining the information from the MASSIVE-3 environment. Once SIM_AGENT has enough information from the 3D environment, this information is used by the SIM_AGENT methods to make their own actions and decisions, updating, subsequently, the MASSIVE-3 environment. Before executing any action, SIM_AGENT has to request permission from MASSIVE-3. If it does not raise any objection (for instance, there is not a collision detected), the action can be executed inside the MASSIVE-3 environment, and once this has be done, MASSIVE-3 will send updated information to SIM_AGENT before starting with the next cycle.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Some scenarios for human-like perception</h2><div class="c-article-section__content" id="Sec18-content"><p>In order to prove the usefulness of the proposed perception model, let us consider that, as it was previously mentioned, mIVAS systems can be used to simulate risky situations, as for example, a world-wide war, where the soldiers’ training plays a very important role.</p><p>In this kind of systems, soldiers can be trained for living and surviving the worse real-life situations. To get a useful training, it is important to endow soldier agents with a human-like perceptual model.</p><p>Different scenarios (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig8">8</a>) and situations can be raised where human-like perception plays a very important role. In this section, we are going to describe some of them.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb8.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb8.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>A fighter plane interfering in the soldiers’ training</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec19">Visual perception</h3><p>Let us imagine that a soldier agent is at the battlefield. He is placed at a physical position given by the co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)= (1,0,0) in the space, in meters, with an orientation of 90° related to the <i>x</i>-axis of co-ordinates. This soldier is endowed with a visual acuity (in Snellen notation) equivalent to 20/20 and his foreground angle of vision is θ=30°, while his lateral angle of vision is θ’=65°.</p><p>Let us also imagine that a fighter plane, a Focke-Wulf BMW, whose size is (length, wide, height)=(20, 15, 7), in meters, appears in the air space.</p><p>Introducing all these values in the implemented visual perceptual model, we get the foreground and lateral soldier’s cone of vision [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne" href="/article/10.1007/s10055-004-0148-7#ref-CR7" id="ref-link-section-d39051e977">7</a>]. In the same way, we get the nimbus geometry associated to the plane, which in this case is an ellipsoid, and the plane’s nimbus, following the set of equations introduced in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne" href="/article/10.1007/s10055-004-0148-7#ref-CR7" id="ref-link-section-d39051e980">7</a>], the perceptual model calculates the maximum distance of resolution (<i>D</i><sub>m</sub>), which in this case is 64.20 m.</p><p>When the plane is placed at co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(1,25,0) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig9">9</a>), in meters, away from the soldier, the visual awareness function [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Herrero P (2003) A human like perceptual model for intelligent virtual agents. PhD Thesis. Universidad Politécnica de Madrid" href="/article/10.1007/s10055-004-0148-7#ref-CR5" id="ref-link-section-d39051e1002">5</a>] determines than there is and overlapping between the visual nimbus of the plane and the agent’s visual cone. Moreover, the visual awareness functions will determine if this overlapping is located at the Foreground Region of the agent’s focus. The visual clearness with which it can perceive the plane would be given by the Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ1">1</a>, being its value (CP<sub>max</sub>) in this scenario maximum (normalised value equal to 1). The soldier can perceive most of the plane’s details.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb9.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb9.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Soldier’s position at (1,0,0) and plane’s position at (1,25,0)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>When the plane is placed at co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(−40,25,25) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig10">10</a>), in meters, away from the soldier, there will still be an overlapping between the lateral area of the agent’s focus and the visual nimbus of the plane. The soldier will be able to perceive the plane in its lateral area of perception. At this distance, the soldier could perceive just a few details of the plane due to its CP–given by the Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ2">2</a>, which will take a value equal to 0.5. The soldier will probably perceive an object but he will not be able to identify its physical details. As the plane is placed in the lateral area of perception, the details that this soldier can get from the plane will be associated to its movement.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb10.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb10.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Soldier’s position (1,0,0) and plane’s position at (−40,25,25)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>When the plane is placed at co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(15,70,25), in meters, away from the soldier, the agent is very far away and the CP in this point—given by the Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ1">1</a>—is very low (0.033). Maybe the soldier can perceive the plane’s shape and its movement, but he can make a mistake confusing the coming plane with a friendly plane instead of recognising it as the hostile plane that it really is.</p><h3 class="c-article__sub-heading" id="Sec20">Auditory perception</h3><p>Starting with the scenario presented at the beginning of this paper, let us imagine that a soldier agent (A) is at the battlefield, keeping guard. He is placed at a position (<i>x</i>,<i>y</i>,<i>z</i>)=(0,0,0), in the space, in metres. The soldier azimuth angle is 80° and his elevation angle is 87.5° (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig11">11</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb11.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb11.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>A fighter plane interfering in the soldier’s training</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>At this moment, a couple of soldiers approach him and one of them starts speaking when at co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(2,2,2). Following the set of equations presented in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in cooperative information systems. In: Proceedings of the 11th international conference on cooperative information systems (CoopIS 2003). Catania, Sicily" href="/article/10.1007/s10055-004-0148-7#ref-CR6" id="ref-link-section-d39051e1130">6</a>], the soldier’s voice will be projected by a set of cardioids and each of them will have an intensity associated. As they are having a moderate conversation, the intensity of the sound that this soldier is emitting is 60 dB.</p><p>While this sound is being propagated, the intensity, associated to each of the cardioid that are taking part in the sound wavefront, will decrease with the increase of the distance, reaching soldier agent A with a value equal to 51.77 dB.</p><p>As the soldier’s voice reaches the agent A’s focus with an intensity in the standard thresholds of hearing, the hearing awareness focus will return the maximum value. The perceptual model will determine the CP with which the agent A can hear the couple conversation by the Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-004-0148-7#Equ3">3</a>, returning a <i>high</i> binaural CP. The soldier A can hear the soldiers’ conversation, and get a clear idea of what they are speaking about. Maybe the couple of soldiers approaching soldier A are enemies and they could be speaking about the next attack. In this situation, the soldier A should try to sharpen his senses in order to get as much information as he can while he passes unnoticed.</p><p>Following the same procedure, if they continue walking under the same conditions, when the speaker reaches co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(3,5,3), the intensity of the sound that is reaching soldier agent A is 42.63 dB and the CP value is <i>Medium</i>. The soldier A could miss some of the details of the couple conversation. In this situation, if the agent A does not want to miss a trick, he should change his position.</p><p>If the same couple continue walking and speaking with the same intensity, when the speaker soldier reaches co-ordinates (<i>x</i>,<i>y</i>,<i>z</i>)=(5,7,5) the intensity of the sound that is reaching soldier agent A is 11.22 dB. In this situation, the sound can be heard by soldier agent A because it is between the standard thresholds of hearing, and our perceptual model returns that this sound can be heard by soldier A with a CP <i>low</i>. Soldier A can hear the conversation as a background noise but he cannot perceive any detail of the couple conversation. Now let us imagine that the same couple stops at that position and stop speaking. If, after a while, one of the two soldiers reduce the intensity of sound with which he is speaking at 30 dB (for example, he starts to whisper to his couple), soldier A’s CP is reduced to <i>very low</i>.</p><p>Let us also imagine that a fighter plane, a Focke–Wulf BMW, whose size is (length, wide, height)=(20, 15, 7), in meters, appears in the air space. The intensity of sound coming from this activity is 120 dB.</p><p>Let us also consider the same soldier agent (A) keeping guard at position (<i>x</i>,<i>y</i>,<i>z</i>)=(0,0,0) with the same azimuth and elevation angles (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-004-0148-7#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb12.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-004-0148-7/MediaObjects/s10055-004-0148-7fhb12.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Fighter plane and soldier agent’s positions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-004-0148-7/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>If the jet is placed at (<i>x</i>,<i>y</i>,<i>z</i>)=(15,70,10), then the intensity that is reaching the soldier is 1.28 dB and the CP with which he perceives the sound is <i>very low</i>. This means that probably the soldier will not appreciate the sound at that distance.</p><p>If the jet is placed at (<i>x</i>,<i>y</i>,<i>z</i>)=(10,10,10), then the intensity of sound that the aircraft is emitting is at least 300 dB (or even greater, depending on the aircraft’s engine) and the intensity of sound that is reaching the soldier is at least 127.636 dB. In this case, the intensity reaching the soldier surpasses the threshold of pain (120 dB) and it is very close to the threshold of feeling (130 dB) and, therefore, it will not be possible to get a clear perception of the sound at that distance, where the CP is <i>very low</i>.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusions</h2><div class="c-article-section__content" id="Sec21-content"><p>We have developed a human-like perceptual model for IVAs based on one of the most successful awareness models in Computer Supported Cooperative Work (CSCW), called the SMI [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Benford SD, Fahlén LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of 3rd European conference on computer supported cooperative work (ECSCW’93). Kluwer, Milano" href="/article/10.1007/s10055-004-0148-7#ref-CR1" id="ref-link-section-d39051e1251">1</a>]. Our perceptual model extends the key concepts of the SMI introducing some factors typical from human being perception—such as sense acuity or sense transition region—as well as it makes a reinterpretation of the key concepts with the aim of using them as the key concepts of an IVA’s human-like perceptual model. We also have introduced a new concept, which we have called CP as a way of having a measurement of the ability to perceive clearly an object or sound inside the agent’s area of perception. The resulting perceptual model allows an IVA to perceive its environment and surrounding objects in real-time, giving it the chance to react to stimuli in its environment, as well as to respond to interactions with the real world, making it more believable.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benford SD, Fahlén LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of " /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Benford SD, Fahlén LE (1993) A spatial model of interaction in large virtual environments. In: Proceedings of 3rd European conference on computer supported cooperative work (ECSCW’93). Kluwer, Milano</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blumberg B (1997) Go with the flow: synthetic vision for autonomous animated creatures. In: Proceedings of the" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Blumberg B (1997) Go with the flow: synthetic vision for autonomous animated creatures. In: Proceedings of the 1st international conference on autonomous agents (Agents’97), Marina del Rey</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Chopra-Khullar, N. Badler, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Chopra-Khullar S, Badler N (2001) Where to look? Automating attending behaviors of virtual human characters. A" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Chopra-Khullar S, Badler N (2001) Where to look? Automating attending behaviors of virtual human characters. Autonomous Agents Multi-agent Syst 4(1/2):9–23</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1010010528443" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Where%20to%20look%3F%20Automating%20attending%20behaviors%20of%20virtual%20human%20characters&amp;journal=Autonomous%20Agents%20Multi-agent%20Syst&amp;volume=4&amp;issue=1%2F2&amp;pages=9-23&amp;publication_year=2001&amp;author=Chopra-Khullar%2CS&amp;author=Badler%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Endsley, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Endsley M (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37(1):65–84" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Endsley M (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37(1):65–84</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20a%20theory%20of%20situation%20awareness%20in%20dynamic%20systems&amp;journal=Hum%20Factors&amp;volume=37&amp;issue=1&amp;pages=65-84&amp;publication_year=1995&amp;author=Endsley%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Herrero P (2003) A human like perceptual model for intelligent virtual agents. PhD Thesis. Universidad Politéc" /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Herrero P (2003) A human like perceptual model for intelligent virtual agents. PhD Thesis. Universidad Politécnica de Madrid</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in c" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Herrero P, De Antonio A (2003) Keeping watch: intelligent virtual agents reflecting human-like perception in cooperative information systems. In: Proceedings of the 11th international conference on cooperative information systems (CoopIS 2003). Catania, Sicily</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual age" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Herrero P, De Antonio A, Benford S, Greenhalgh C (2003) A hearing perceptual model for intelligent virtual agents. In: Proceedings of the 2nd international joint conference on autonomous agents and multiagent systems, Melbourne</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hill R, Han C, van Lent M (2002) Applying perceptually driven cognitive mapping to virtual urban environments." /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Hill R, Han C, van Lent M (2002) Applying perceptually driven cognitive mapping to virtual urban environments. In: Conference on innovative applications of artificial intelligence (IAAI-2002) in Edmonton</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hill R, Han C, van Lent M (2002) Perceptually driven cognitive mapping of urban environments. In: Proceedings " /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Hill R, Han C, van Lent M (2002) Perceptually driven cognitive mapping of urban environments. In: Proceedings of the first international joint conference on autonomous agents and multiagent systems, Bologna</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PA. Howarth, PJ. Costello, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Howarth PA, Costello PJ (1997) Contemporary ergonomics 1997. Robertson SA (ed) Taylor and Francis, London, pp " /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Howarth PA, Costello PJ (1997) Contemporary ergonomics 1997. Robertson SA (ed) Taylor and Francis, London, pp 109–116</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Contemporary%20ergonomics%201997&amp;pages=109-116&amp;publication_year=1997&amp;author=Howarth%2CPA&amp;author=Costello%2CPJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kendall G (2002) 3D Sound. Center for Music Technology School of Music. Northwestern University, Consulted." /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Kendall G (2002) 3D Sound. Center for Music Technology School of Music. Northwestern University, Consulted.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DM. Levi, SA. Klein, S. Hariharan, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Levi DM, Klein SA, Hariharan S (2002) Suppressive and facilitatory spatial interactions in foveal vision: fove" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Levi DM, Klein SA, Hariharan S (2002) Suppressive and facilitatory spatial interactions in foveal vision: foveal crowding is simple contrast masking. J Vis 2:140–166. <a href="http://journalofvision.org/2/2/2/">http://journalofvision.org/2/2/2/</a></p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Suppressive%20and%20facilitatory%20spatial%20interactions%20in%20foveal%20vision%3A%20foveal%20crowding%20is%20simple%20contrast%20masking&amp;journal=J%20Vis&amp;volume=2&amp;pages=140-166&amp;publication_year=2002&amp;author=Levi%2CDM&amp;author=Klein%2CSA&amp;author=Hariharan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DM. Levi, S. Hariharan, SA. Klein, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Levi DM, Hariharan S, Klein SA (2002) Suppressive and facilitatory spatial interactions in peripheral vision: " /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Levi DM, Hariharan S, Klein SA (2002) Suppressive and facilitatory spatial interactions in peripheral vision: peripheral crowding is neither size invariant nor simple contrast masking. J Vis 2:167–177. <a href="http://www.journalofvision.org/2/2/3/">http://www.journalofvision.org/2/2/3/</a></p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Suppressive%20and%20facilitatory%20spatial%20interactions%20in%20peripheral%20vision%3A%20peripheral%20crowding%20is%20neither%20size%20invariant%20nor%20simple%20contrast%20masking&amp;journal=J%20Vis&amp;volume=2&amp;pages=167-177&amp;publication_year=2002&amp;author=Levi%2CDM&amp;author=Hariharan%2CS&amp;author=Klein%2CSA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Noser H (1997) A behavioral animation system based on L-systems and synthetic sensors for actors. PhD Thesis, " /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Noser H (1997) A behavioral animation system based on L-systems and synthetic sensors for actors. PhD Thesis, École Polytechnique Fédérale De Lausanne</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shinn-Cunningham, BG (2000) Distance cues for virtual auditory space. In: Proceedings of the IEEE 2000 interna" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Shinn-Cunningham, BG (2000) Distance cues for virtual auditory space. In: Proceedings of the IEEE 2000 international symposium on multimedia information processing, Sydney</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Terzopoulos, TF. Rabie, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Terzopoulos D, Rabie TF (1997) Animat vision: active vision in artificial animals. J Comput Vis Res 1(1):2–19" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Terzopoulos D, Rabie TF (1997) Animat vision: active vision in artificial animals. J Comput Vis Res 1(1):2–19</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Animat%20vision%3A%20active%20vision%20in%20artificial%20animals&amp;journal=J%20Comput%20Vis%20Res&amp;volume=1&amp;issue=1&amp;pages=2-19&amp;publication_year=1997&amp;author=Terzopoulos%2CD&amp;author=Rabie%2CTF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thalmann D (2001) The foundations to build a virtual human society. In: Proceedings of Intelligent Virtual Act" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Thalmann D (2001) The foundations to build a virtual human society. In: Proceedings of Intelligent Virtual Actors (IVA’01). Madrid, Spain</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Zahorik, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Zahorik P (2002) Assessing auditory distance perception using virtual acoustics. J Acoust Soc Am 111:1832–1846" /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Zahorik P (2002) Assessing auditory distance perception using virtual acoustics. J Acoust Soc Am 111:1832–1846</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1458027" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessing%20auditory%20distance%20perception%20using%20virtual%20acoustics&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=111&amp;pages=1832-1846&amp;publication_year=2002&amp;author=Zahorik%2CP">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-004-0148-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The work presented in this paper was supported by the Communication Research Group (CRG), led by Steve Benford and Chris Greenhalgh at the School of Computer Science and Information Technology in the University of Nottingham, in UK.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Facultad de Informática, Universidad Politécnica de Madrid, Campus de Montegancedo S/N. 28660 Boadilla del Monte, Madrid, Spain</p><p class="c-article-author-affiliation__authors-list">Pilar Herrero &amp; Angélica de Antonio</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Pilar-Herrero"><span class="c-article-authors-search__title u-h3 js-search-name">Pilar Herrero</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Pilar+Herrero&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Pilar+Herrero" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Pilar+Herrero%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ang_lica_de-Antonio"><span class="c-article-authors-search__title u-h3 js-search-name">Angélica de Antonio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ang%C3%A9lica de+Antonio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ang%C3%A9lica de+Antonio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ang%C3%A9lica de+Antonio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-004-0148-7/email/correspondent/c1/new">Pilar Herrero</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Intelligent%20virtual%20agents%20keeping%20watch%20in%20the%20battlefield&amp;author=Pilar%20Herrero%20et%20al&amp;contentID=10.1007%2Fs10055-004-0148-7&amp;publication=1359-4338&amp;publicationDate=2005-02-23&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Herrero, P., Antonio, A.d. Intelligent virtual agents keeping watch in the battlefield.
                    <i>Virtual Reality</i> <b>8, </b>185–193 (2005). https://doi.org/10.1007/s10055-004-0148-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-004-0148-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-02-23">23 February 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-06">June 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-004-0148-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-004-0148-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Intelligent virtual agents (IVAs)</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Perception</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Awareness</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Focus</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Nimbus</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human factors</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-004-0148-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=148;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

