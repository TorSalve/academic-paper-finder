<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="The road to surgical simulation and surgical navigation"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="The recent advantage of the power of graphic workstations has made it possible to handle 3D human structures in an interactive way. Real-time imaging of medical 3D or 4D images can be used not only..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/12/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="The road to surgical simulation and surgical navigation"/>

    <meta name="dc.source" content="Virtual Reality 2008 12:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2008-12-04"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="The recent advantage of the power of graphic workstations has made it possible to handle 3D human structures in an interactive way. Real-time imaging of medical 3D or 4D images can be used not only for diagnosis, but also for various novel medical treatments. By elaborating on the history of the establishment of our laboratory, which focuses on medical virtual reality, we describe our experience of developing surgery simulation and surgery navigation systems according to our research results. In the case of surgical simulation, we mention two kinds of virtual surgery simulators that produce the haptic sensation of surgical maneuvers in the user&#8217;s fingers. Regarding surgical navigation systems, we explain the necessity of the augmented reality function for the encouragement of the ability of robotic surgery and its trial for clinical case."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2008-12-04"/>

    <meta name="prism.volume" content="12"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="281"/>

    <meta name="prism.endingPage" content="291"/>

    <meta name="prism.copyright" content="2008 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-008-0103-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-008-0103-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-008-0103-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-008-0103-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="The road to surgical simulation and surgical navigation"/>

    <meta name="citation_volume" content="12"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2008/12"/>

    <meta name="citation_online_date" content="2008/12/04"/>

    <meta name="citation_firstpage" content="281"/>

    <meta name="citation_lastpage" content="291"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-008-0103-0"/>

    <meta name="DOI" content="10.1007/s10055-008-0103-0"/>

    <meta name="citation_doi" content="10.1007/s10055-008-0103-0"/>

    <meta name="description" content="The recent advantage of the power of graphic workstations has made it possible to handle 3D human structures in an interactive way. Real-time imaging of me"/>

    <meta name="dc.creator" content="Naoki Suzuki"/>

    <meta name="dc.creator" content="Asaki Hattori"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=A head-mounted operating binocular for augmented reality visualization in medicine&#8212;design and initial evaluation; citation_author=W Birkfellner, M Figl, K Huber, F Watzinger, F Wanschitz, J Hummel, R Hanel, W Greimel, P Homolka, R Ewers, H Bergmann; citation_volume=21; citation_issue=8; citation_publication_date=2002; citation_pages=991-997; citation_doi=10.1109/TMI.2002.803099; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Real-time elastic deformations of soft tissues for surgery simulation; citation_author=S Cotin, H Delingette, A Ayache; citation_volume=5; citation_issue=1; citation_publication_date=1999; citation_pages=62-73; citation_doi=10.1109/2945.764872; citation_id=CR2"/>

    <meta name="citation_reference" content="Devernay F, Mourgues F, Coste-Maniere E (2001) Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery. Proc. of the international workshop on medical imaging and augmented reality (MIAR2001); pp 16&#8211;20"/>

    <meta name="citation_reference" content="citation_journal_title=Clin Anat; citation_title=A novel interactive anatomic atlas of the hand; citation_author=S Gehrmann, KH Karl H&#246;hne, W Linhart, B Pflesser, A Pommert, M Riemer, Ulf Tiede, J Windolf, U Schumacher, JM Rueger; citation_volume=19; citation_issue=3; citation_publication_date=2006; citation_pages=258-266; citation_doi=10.1002/ca.20266; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Int J CARS; citation_title=Design and clinical evaluation of an image-guided surgical microscope with an integrated tracking system; citation_author=JG Giraldez, M Caversaccio, I Pappas, J Kowal, U Rohrer, G Marti, C Baur, LP Nolte, MAG Ballester; citation_volume=1; citation_publication_date=2007; citation_pages=253-264; citation_doi=10.1007/s11548-006-0066-0; citation_id=CR5"/>

    <meta name="citation_reference" content="Guthart GS, Salisbury JK (2000). The intuitive telesurgery system: overview and application. Proc. of the IEEE international conference on robotics and automation (ICRA 2000). San Francisco, CA, April 2000"/>

    <meta name="citation_reference" content="citation_journal_title=Medicine Meets Virtual Reality; citation_title=A robotic surgery system (da Vinci) with image-guided function; citation_author=A Hattori, N Suzuki, M Hashizume, T Akahoshi, K Konishi, S Yamaguchi, M Shimada, M Hayashibe; citation_volume=11; citation_publication_date=2003; citation_pages=110-116; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Development of a navigation function for an endoscopic robot surgery system; citation_author=A Hattori, N Suzuki, M Hayashibe, S Suzuki, Y Otake, H Tajiri, S Kobayashi; citation_volume=13; citation_publication_date=2005; citation_pages=167-171; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Nat Med; citation_title=A new representation of knowledge concerning human anatomy and function; citation_author=KH H&#246;hne, B Pflesser, A Pommert, M Riemer, T Schiemann, R Schubert, Ulf Tiede; citation_volume=1; citation_issue=6; citation_publication_date=1995; citation_pages=506-511; citation_doi=10.1038/nm0695-506; citation_id=CR9"/>

    <meta name="citation_reference" content="Mendoza C, Laugier C (2003), Tissue cutting using finite elements and force feedback, lecture notes in computer science 2673, Surgery Simulation and Soft Tissue Modeling, pp 175&#8211;82"/>

    <meta name="citation_reference" content="Nicolau SA, Pennec X, Soler L, Ayache N (2005) A Complete augmented reality guidance system for liver punctures: first clinical evaluation. MICCAI, LNCS 3749:539&#8211;547"/>

    <meta name="citation_reference" content="Petersik A, Pflesser B, Tiede U, H&#246;hne KH, Leuwer R (2003), Realistic haptic interaction in volume sculpting for surgery simulation. In: Ayache N, Delingette H (eds) Surgery simulation and soft tissue modeling, Proc. IS4TM 2003, Lect Notes Comput Sci 2673, Springer-Verlag, Berlin, pp 194&#8211;202"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Aided Surg; citation_title=A computer-based simulation for petrous bone surgery with haptic feedback; citation_author=B Pflesser, R Leuwer, A Petersik, Ulf Tiede, KH H&#246;hne; citation_volume=7; citation_issue=2; citation_publication_date=2002; citation_pages=117; citation_doi=10.3109/10929080209146018; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=J Biomed Mater Res; citation_title=Optimal segmentation of microcomputed tomographic images of porous tissue-engineering scaffolds; citation_author=S Rajagopalan, L Lu, RA Robb, MJ Yaszemski; citation_volume=75A; citation_issue=4; citation_publication_date=2005; citation_pages=877-887; citation_doi=10.1002/jbm.a.30498; citation_id=CR14"/>

    <meta name="citation_reference" content="Robb RA (1971), Computer-aided contour determination and dynamic display of individual cardiac chambers from digitized serial angiocardiographic film. In: Heintzen PH (ed) Roentgen-, cine-, and videodensitometry. Fundamentals and application for blood flow and heart volume determination. Georg Thieme Verlag, Stuttgart, Germany, pp 170&#8211;178"/>

    <meta name="citation_reference" content="Robb RA (1997), Virtual endoscopy: evaluation using the visible human datasets and comparison with real endoscopy in patients. Proceedings of medicine meets virtual reality. In: Morgan KS, Hoffman HM, Stredney D, Weghorst SJ (eds) IOS Press, Netherlands, 39, 195&#8211;206"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graphics; citation_title=Quantitative imaging of dynamic structure and function of the heart, lungs and circulation by computerized reconstruction and subtraction techniques; citation_author=RA Robb, EL Ritman, JF Greenleaf, RE Sturm, HK Liu, PA Chevalier, EH Wood; citation_volume=10; citation_issue=2; citation_publication_date=1976; citation_pages=246-256; citation_doi=10.1145/965143.563318; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Med Imaging Graph; citation_title=ANALYZE: a comprehensive, operator-interactive software package for multidimensional medical image display and analysis; citation_author=RA Robb, DP Hanson, RA Karwoski, AG Larson, EL Workman, MC Stacy; citation_volume=13; citation_issue=6; citation_publication_date=1989; citation_pages=433-454; citation_doi=10.1016/0895-6111(89)90285-1; citation_id=CR18"/>

    <meta name="citation_reference" content="Rosen J, Soltanian H, Redett R et al (1996), Evolution of virtual reality-from planning to performing surgery, IEEE Engineering in Medicine and Biology, pp 16&#8211;22"/>

    <meta name="citation_reference" content="Salisbury JK (1998), The heart of microsurgery. Mechanical Engineering Magazine, ASME Int&#8217;l.; 120(12):47&#8211;51"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Implementation, calibration and accuracy testing of an image-enhanced endoscopy system; citation_author=R Shahidi, MR Bax, CR Maurer, JA Johnson, EP Wilkinson, B Wang, JB West, MJ Citardi, KH Manwaring, R Khadem; citation_volume=21; citation_issue=12; citation_publication_date=2002; citation_pages=1524-1535; citation_doi=10.1109/TMI.2002.806597; citation_id=CR21"/>

    <meta name="citation_reference" content="Suzuki N, Hattori A, Kai S et al (1997) Surgical planning system for soft tissues using virtual reality. Proc. of medicine meets virtual reality 5, 159&#8211;63"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Simulator for virtual surgery using deformable organ model and force feedback system; citation_author=N Suzuki, A Hattori, T Ezumi; citation_volume=6; citation_publication_date=1998; citation_pages=227-233; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Dynamic deformation of elastic organ model and the VR cockpit for virtual surgery and tele-surgery; citation_author=S Suzuki, N Suzuki, A Hattori; citation_volume=11; citation_publication_date=2003; citation_pages=354-356; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Development of dynamic spatial video camera (DSVC) for 4D observation, analysis and modeling of human body locomotion; citation_author=N Suzuki, A Hattori, M Hayashibe, S Suzuki, Y Otake; citation_volume=11; citation_publication_date=2003; citation_pages=346-348; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Development of an endoscopic robotic system with two hands for various gastric tube surgeries; citation_author=N Suzuki, K Sumiyama, A Hattori, K Ikeda, EAY Murakami, S Suzuki, M Hayashibe, Y Otake, H Tajiri; citation_volume=11; citation_publication_date=2003; citation_pages=349-353; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Med Imaging; citation_title=Sphere-filled organ model for virtual surgery system; citation_author=S Suzuki, N Suzuki, A Hattori; citation_volume=23; citation_issue=6; citation_publication_date=2004; citation_pages=714-722; citation_doi=10.1109/TMI.2004.826947; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=MMVR; citation_title=Construction of a high-tech operating room for image-guided surgery using VR; citation_author=N Suzuki, A Hattori, S Suzuki, Y Otake, M Hayashibe, S Kobayashi, T Nezu, H Sakai, Y Umezawa; citation_volume=13; citation_publication_date=2005; citation_pages=538-542; citation_id=CR28"/>

    <meta name="citation_reference" content="Tiede U, Schiemann T, H&#246;hne KH (1998), High quality rendering of attributed volume data. In: David Ebert et al (eds) Proc. IEEE visualization 1998. Research Triangle Park, NC, pp 255&#8211;262"/>

    <meta name="citation_author" content="Naoki Suzuki"/>

    <meta name="citation_author_email" content="nsuzuki@jikei.ac.jp"/>

    <meta name="citation_author_institution" content="Institute for High Dimensional Medical Imaging, The Jikei University School of Medicine, Komae, Japan"/>

    <meta name="citation_author" content="Asaki Hattori"/>

    <meta name="citation_author_institution" content="Institute for High Dimensional Medical Imaging, The Jikei University School of Medicine, Komae, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-008-0103-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2008/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-008-0103-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="The road to surgical simulation and surgical navigation"/>
        <meta property="og:description" content="The recent advantage of the power of graphic workstations has made it possible to handle 3D human structures in an interactive way. Real-time imaging of medical 3D or 4D images can be used not only for diagnosis, but also for various novel medical treatments. By elaborating on the history of the establishment of our laboratory, which focuses on medical virtual reality, we describe our experience of developing surgery simulation and surgery navigation systems according to our research results. In the case of surgical simulation, we mention two kinds of virtual surgery simulators that produce the haptic sensation of surgical maneuvers in the user’s fingers. Regarding surgical navigation systems, we explain the necessity of the augmented reality function for the encouragement of the ability of robotic surgery and its trial for clinical case."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>The road to surgical simulation and surgical navigation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-008-0103-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Medical imaging, Surgical simulation, Virtual reality, Augmented reality, Navigation surgery","kwrd":["Medical_imaging","Surgical_simulation","Virtual_reality","Augmented_reality","Navigation_surgery"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-008-0103-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-008-0103-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=103;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-008-0103-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            The road to surgical simulation and surgical navigation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0103-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0103-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2008-12-04" itemprop="datePublished">04 December 2008</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">The road to surgical simulation and surgical navigation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Naoki-Suzuki" data-author-popup="auth-Naoki-Suzuki" data-corresp-id="c1">Naoki Suzuki<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The Jikei University School of Medicine" /><meta itemprop="address" content="grid.411898.d, 0000000106612073, Institute for High Dimensional Medical Imaging, The Jikei University School of Medicine, 4-11-1 Izumihoncho, Komae, Tokyo, 201-8601, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Asaki-Hattori" data-author-popup="auth-Asaki-Hattori">Asaki Hattori</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The Jikei University School of Medicine" /><meta itemprop="address" content="grid.411898.d, 0000000106612073, Institute for High Dimensional Medical Imaging, The Jikei University School of Medicine, 4-11-1 Izumihoncho, Komae, Tokyo, 201-8601, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 12</b>, Article number: <span data-test="article-number">281</span> (<span data-test="article-publication-year">2008</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">207 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-008-0103-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The recent advantage of the power of graphic workstations has made it possible to handle 3D human structures in an interactive way. Real-time imaging of medical 3D or 4D images can be used not only for diagnosis, but also for various novel medical treatments. By elaborating on the history of the establishment of our laboratory, which focuses on medical virtual reality, we describe our experience of developing surgery simulation and surgery navigation systems according to our research results. In the case of surgical simulation, we mention two kinds of virtual surgery simulators that produce the haptic sensation of surgical maneuvers in the user’s fingers. Regarding surgical navigation systems, we explain the necessity of the augmented reality function for the encouragement of the ability of robotic surgery and its trial for clinical case.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Preface (the beginning)</h2><div class="c-article-section__content" id="Sec1-content"><p>We are sure that many researchers remember the days, more than 20 years ago, when it took an hour to create just one 3D image. In those days, it was common to wait for an hour for another 3D image of a patient’s brain at a slightly different angle. With small incremental steps forward from year to year, the time required to create 3D images has become shorter as the power of the computers used was enhanced. We could then begin to imagine what the world of medical imaging would be like if we could handle real-time 3D images. It was about the same time that 3D images came into use not only for diagnosis but also for application to various medical treatments, and interactive use of 3D images with sufficient information for medical use became possible.</p><p>We started to perform research in this field, because we were very much attracted to the sophisticated 3D images that were announced by Prof. Richard A. Robb (Robb <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Robb RA (1971), Computer-aided contour determination and dynamic display of individual cardiac chambers from digitized serial angiocardiographic film. In: Heintzen PH (ed) Roentgen-, cine-, and videodensitometry. Fundamentals and application for blood flow and heart volume determination. Georg Thieme Verlag, Stuttgart, Germany, pp 170–178" href="/article/10.1007/s10055-008-0103-0#ref-CR15" id="ref-link-section-d74431e302">1971</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Robb RA (1997), Virtual endoscopy: evaluation using the visible human datasets and comparison with real endoscopy in patients. Proceedings of medicine meets virtual reality. In: Morgan KS, Hoffman HM, Stredney D, Weghorst SJ (eds) IOS Press, Netherlands, 39, 195–206" href="/article/10.1007/s10055-008-0103-0#ref-CR16" id="ref-link-section-d74431e305">1997</a>; Robb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1976" title="Robb RA, Ritman EL, Greenleaf JF, Sturm RE, Liu HK, Chevalier PA, Wood EH (1976) Quantitative imaging of dynamic structure and function of the heart, lungs and circulation by computerized reconstruction and subtraction techniques. Comput Graphics 10(2):246–256" href="/article/10.1007/s10055-008-0103-0#ref-CR17" id="ref-link-section-d74431e308">1976</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Robb RA, Hanson DP, Karwoski RA, Larson AG, Workman EL, Stacy MC (1989) ANALYZE: a comprehensive, operator-interactive software package for multidimensional medical image display and analysis. Comput Med Imaging Graph 13(6):433–454" href="/article/10.1007/s10055-008-0103-0#ref-CR18" id="ref-link-section-d74431e311">1989</a>; Rajagopalan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Rajagopalan S, Lu L, Robb RA, Yaszemski MJ (2005) Optimal segmentation of microcomputed tomographic images of porous tissue-engineering scaffolds. J Biomed Mater Res 75A(4):877–887" href="/article/10.1007/s10055-008-0103-0#ref-CR14" id="ref-link-section-d74431e314">2005</a>) and Prof. Karl Heinz Höhne (Gehrmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Gehrmann S, Karl Höhne KH, Linhart W, Pflesser B, Pommert A, Riemer M, Tiede Ulf, Windolf J, Schumacher U, Rueger JM (2006) A novel interactive anatomic atlas of the hand. Clin Anat 19(3):258–266" href="/article/10.1007/s10055-008-0103-0#ref-CR4" id="ref-link-section-d74431e318">2006</a>; Höhne et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Höhne KH, Pflesser B, Pommert A, Riemer M, Schiemann T, Schubert R, Tiede Ulf (1995) A new representation of knowledge concerning human anatomy and function. Nat Med 1(6):506–511" href="/article/10.1007/s10055-008-0103-0#ref-CR9" id="ref-link-section-d74431e321">1995</a>; Tiede et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Tiede U, Schiemann T, Höhne KH (1998), High quality rendering of attributed volume data. In: David Ebert et al (eds) Proc. IEEE visualization 1998. Research Triangle Park, NC, pp 255–262" href="/article/10.1007/s10055-008-0103-0#ref-CR29" id="ref-link-section-d74431e324">1998</a>; Pflesser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Pflesser B, Leuwer R, Petersik A, Tiede Ulf, Höhne KH (2002) A computer-based simulation for petrous bone surgery with haptic feedback. Comput Aided Surg 7(2):117" href="/article/10.1007/s10055-008-0103-0#ref-CR13" id="ref-link-section-d74431e327">2002</a>; Petersik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Petersik A, Pflesser B, Tiede U, Höhne KH, Leuwer R (2003), Realistic haptic interaction in volume sculpting for surgery simulation. In: Ayache N, Delingette H (eds) Surgery simulation and soft tissue modeling, Proc. IS4TM 2003, Lect Notes Comput Sci 2673, Springer-Verlag, Berlin, pp 194–202" href="/article/10.1007/s10055-008-0103-0#ref-CR12" id="ref-link-section-d74431e330">2003</a>), who are pioneers in this field. It is one of our best memories in our research career, as we were able to have discussions with them and were able to work with them; people that we only could meet by reading their papers. In 1987 we had a chance to own a high-speed graphics workstation produced by Silicon Graphics Inc. (SGI). It filled half of our research room at the time. This enabled us to commence research on medical real-time 3D imaging.</p><p>This machine should have been placed in a room that had been built for it, but we could not afford it and we spent several years working in the same room without windows and with its loud noise and heat. We felt like a crew on a submarine. However, we cannot forget how new and eye-opening it was for us to be able to look at 3D images of the head and stomach and freely change the viewpoint and display by using a dial on the workstation. As we became excited by the images, we searched for ways to visualize human inner structures as freely as possible. As with so many other researchers around the world at the time, we were trying to solve this problem by applying virtual reality (VR) technologies. Our university permitted us to establish a research laboratory for this field of research (Institute for High Dimensional Medial Imaging), and we were able to gather some staff and joint researchers. In this text, we would like to describe our development of surgical simulation technology applying VR and surgical navigation technology.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Surgical simulation</h2><div class="c-article-section__content" id="Sec2-content"><p>We believed that surgery simulation would be one of the largest applications of medical virtual reality (Rosen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Rosen J, Soltanian H, Redett R et al (1996), Evolution of virtual reality-from planning to performing surgery, IEEE Engineering in Medicine and Biology, pp 16–22" href="/article/10.1007/s10055-008-0103-0#ref-CR19" id="ref-link-section-d74431e344">1996</a>; Cotin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Cotin S, Delingette H, Ayache A (1999) Real-time elastic deformations of soft tissues for surgery simulation. IEEE Trans Vis Comput Graph 5(1):62–73" href="/article/10.1007/s10055-008-0103-0#ref-CR2" id="ref-link-section-d74431e347">1999</a>; Mendoza and Laugier <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Mendoza C, Laugier C (2003), Tissue cutting using finite elements and force feedback, lecture notes in computer science 2673, Surgery Simulation and Soft Tissue Modeling, pp 175–82" href="/article/10.1007/s10055-008-0103-0#ref-CR10" id="ref-link-section-d74431e350">2003</a>). We have been developing a surgery simulation system that is capable of simulating surgical maneuvers on soft tissue organs since 1993 (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Suzuki N, Hattori A, Kai S et al (1997) Surgical planning system for soft tissues using virtual reality. Proc. of medicine meets virtual reality 5, 159–63" href="/article/10.1007/s10055-008-0103-0#ref-CR22" id="ref-link-section-d74431e353">1997</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Suzuki N, Hattori A, Ezumi T et al (1998) Simulator for virtual surgery using deformable organ model and force feedback system. MMVR 6:227–233" href="/article/10.1007/s10055-008-0103-0#ref-CR23" id="ref-link-section-d74431e356">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003a" title="Suzuki S, Suzuki N, Hattori A et al (2003a) Dynamic deformation of elastic organ model and the VR cockpit for virtual surgery and tele-surgery. MMVR 11:354–356" href="/article/10.1007/s10055-008-0103-0#ref-CR24" id="ref-link-section-d74431e360">2003a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Suzuki S, Suzuki N, Hattori A et al (2004) Sphere-filled organ model for virtual surgery system. IEEE Trans Med Imaging 23(6):714–722" href="/article/10.1007/s10055-008-0103-0#ref-CR27" id="ref-link-section-d74431e363">2004</a>; Devernay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Devernay F, Mourgues F, Coste-Maniere E (2001) Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery. Proc. of the international workshop on medical imaging and augmented reality (MIAR2001); pp 16–20" href="/article/10.1007/s10055-008-0103-0#ref-CR3" id="ref-link-section-d74431e366">2001</a>).</p><p>Soft tissues change shape when touched with the fingertips, as occurs during surgery. We did not want to use a typical model of the tissue in the system as we have in case of conventional simulator for surgery education. We would like to construct a simulator which can perform rehearsal of coming real surgery using patient specific data. We tried to build a way that would instantly reconstruct a patient’s data for the simulation. When our research started to produce some results, we realized that to simulate an operation, we needed to feel with all fingers the softness of the organs.</p><p>In order to achieve this, we constructed an elastic tissue model known as a sphere-filled model (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Suzuki N, Hattori A, Ezumi T et al (1998) Simulator for virtual surgery using deformable organ model and force feedback system. MMVR 6:227–233" href="/article/10.1007/s10055-008-0103-0#ref-CR23" id="ref-link-section-d74431e375">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003a" title="Suzuki S, Suzuki N, Hattori A et al (2003a) Dynamic deformation of elastic organ model and the VR cockpit for virtual surgery and tele-surgery. MMVR 11:354–356" href="/article/10.1007/s10055-008-0103-0#ref-CR24" id="ref-link-section-d74431e378">2003a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Suzuki S, Suzuki N, Hattori A et al (2004) Sphere-filled organ model for virtual surgery system. IEEE Trans Med Imaging 23(6):714–722" href="/article/10.1007/s10055-008-0103-0#ref-CR27" id="ref-link-section-d74431e381">2004</a>). This proposed organ model allowed us to perform surgical maneuvers such as pushing, pinching, incising and resection. In addition, we tried to obtain haptic feedback from the patients organs in a surgery simulation. The developed system made it possible to handle soft tissue organs with two force feedback devices attached to the user’s hands.</p><p>First, we constructed a soft tissue model known as a sphere-filled model for real-time simulation. We gave up on applying the finite element method, because it was difficult to construct a patient’s specific model in a short preparation time. It was also difficult to handle it as a deformable organ model in real-time. The sphere-filled model consists of a group of small spheres inside the organ’s surface and polyhedrons at the surface. Using this model, we could perform surgical maneuvers such as pushing, grasping, incision, and resection with a speed of more than 30 frames per second (fps), including manipulation of vascular structures in the organ. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a> shows a cutting deformation of the liver model. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a>a-1 shows an incision deformation of the surface while Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a>b-1 shows the condition of the internal spheres. The spheres in which the color was given shows the region of divided spheres. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a>a-2 and b-2 show the form of the resection surface after making some incisions. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a>a-3 and b-3 show grasping of the resection region. We could perform these maneuvers with real-time visual feedback in an interactive way. This model is also able to calculate the volume of the resection region by automatically counting the spheres in that region.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The deformation of the liver model. <i>a-1</i> and <i>b-1</i> shows an incision deformation, while <i>b-1</i> and <i>b-2</i> shows the generation of the resection surface. <i>a-3</i> and <i>b-3</i> shows the grasping of the resection region</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Second, we developed a haptic device, which allows the user to experience tactile sensations. We developed a force feedback device for both hands, which possesses 16 degrees-of-freedom (DOF) for manual interactions with virtual environments. The force control manipulator assembly was located on the backs of both hands producing haptic sensation to the thumbs, forefingers and middle fingers. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig2">2</a>a shows a user and the devices attached to both hands. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig2">2</a>b shows the linkage between the device and the real-time image. The user’s hand is perceived as a 3D image in order to identify its location on the liver surface.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A force feedback device. <b>a</b> Shows a demonstration of the force feedback devices for the right and left hands while <b>b</b> shows an experiment to touch the liver model using the manipulator with the 3D image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig1">1</a> shows how we completed the system. We remember taping on video how the device worked until dawn of the day; we were to fly off to Los Angeles to introduce this video at Medicine Meets Virtual Reality (MMVR) on the January 1999. The device was only completed around Christmas of 1998. We used two industrial robots so that we could link the haptic device assembly with the fast movement of both of the surgeon’s hands. Although we installed a safety device on the robots, if they were to malfunction, they would have had the power to tear the user limb from limb.</p><p>In 2001, using this first device as base, we developed a second device that had various possibilities on tele-surgery. At this stage, we gave up on using the head-mounted display that we had persisted with for a long time. We created a curved display that only covered the field vision of the user. It could see the operating field in 3D, and we tried to make the virtual reality world as close to the real world image as possible. This made the size of the device large, but we effectively obtained the function for the “virtual surgery”.</p><p>The user could not immerse him/herself in the virtual surgical space because of the method of presenting simulation results using a conventional computer display. To perform simulated surgery in the same environment as the actual operation according to the user’s viewpoint and the operation, the angle of elevation of this display can be changed from 0° to 45°, and the elevator that equipped the user’s stage with haptic devices (the cockpit) can be shifted up and down or back and forth. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig3">3</a> shows a general view of this cockpit. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig3">3</a>a shows the situation for open surgery simulation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig3">3</a>b shows the initial position of the screen and the elevator, while Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig3">3</a>c shows their maximum positions. A surgeon attaches the force feedback device to both of his hands and stands on the elevator as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig3">3</a>d and e.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A general view of our virtual surgery, tele-surgery cockpit. In this figure, <b>b</b> shows the initial position of the screen and the elevator while <b>c</b> shows the maximum position. A surgeon simulating surgery attaches the force feedback devices to his hands and stands on an elevator (<b>d</b>, <b>e</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>We also mounted functions for the controlling surgical robot applying large display and haptic devices, so that the device could carry out not only simulations, but also tele-surgeries. Hence, we decided to call this device the Tele surgery–Virtual surgery Cockpit (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003a" title="Suzuki S, Suzuki N, Hattori A et al (2003a) Dynamic deformation of elastic organ model and the VR cockpit for virtual surgery and tele-surgery. MMVR 11:354–356" href="/article/10.1007/s10055-008-0103-0#ref-CR24" id="ref-link-section-d74431e538">2003a</a>).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Surgical navigation</h2><div class="c-article-section__content" id="Sec3-content"><h3 class="c-article__sub-heading" id="Sec4">Overlay imaging for whole body actions of a human body</h3><p>The application of real-time imaging using VR holds great promise for techniques other than surgical simulation (Devernay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Devernay F, Mourgues F, Coste-Maniere E (2001) Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery. Proc. of the international workshop on medical imaging and augmented reality (MIAR2001); pp 16–20" href="/article/10.1007/s10055-008-0103-0#ref-CR3" id="ref-link-section-d74431e553">2001</a>; Birkfellner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Birkfellner W, Figl M, Huber K, Watzinger F, Wanschitz F, Hummel J, Hanel R, Greimel W, Homolka P, Ewers R, Bergmann H (2002) A head-mounted operating binocular for augmented reality visualization in medicine—design and initial evaluation. IEEE Trans Med Imaging 21(8):991–997" href="/article/10.1007/s10055-008-0103-0#ref-CR1" id="ref-link-section-d74431e556">2002</a>; Shahidi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Shahidi R, Bax MR, Maurer CR Jr, Johnson JA, Wilkinson EP, Wang B, West JB, Citardi MJ, Manwaring KH, Khadem R (2002) Implementation, calibration and accuracy testing of an image-enhanced endoscopy system. IEEE Trans Med Imaging 21(12):1524–1535" href="/article/10.1007/s10055-008-0103-0#ref-CR21" id="ref-link-section-d74431e559">2002</a>; Nicolau et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Nicolau SA, Pennec X, Soler L, Ayache N (2005) A Complete augmented reality guidance system for liver punctures: first clinical evaluation. MICCAI, LNCS 3749:539–547" href="/article/10.1007/s10055-008-0103-0#ref-CR11" id="ref-link-section-d74431e562">2005</a>; Giraldez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Giraldez JG, Caversaccio M, Pappas I, Kowal J, Rohrer U, Marti G, Baur C, Nolte LP, Ballester MAG (2007) Design and clinical evaluation of an image-guided surgical microscope with an integrated tracking system. Int J CARS 1:253–264" href="/article/10.1007/s10055-008-0103-0#ref-CR5" id="ref-link-section-d74431e565">2007</a>). We have especially put efforts into enabling surgeons to see through patients bodies. We of course put efforts in augmented reality at the operation site, but there was one thing that nagged our minds. It is the visualization of the whole body movement. It bothered us that there was no way to see inside the whole human body when a person was walking or jumping around. In CT and MRI, we tend to think that we are able to look freely inside the human body from head to toe. However, we are only seeing the insides of a human body quietly lying down, face up. To solve this problem, we simultaneously carried out research on developing a navigation system that can visualize the whole structure of the body and the movements of the skeletal system and muscles. Our goal was to produce real-time 4D imaging with movements of the whole body and visualize and analyze them freely in virtual space.</p><p>First, we developed an imaging system for free and quantitative observation of human locomotion in a time-spatial domain by real-time imaging (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003b" title="Suzuki N, Hattori A, Hayashibe M, Suzuki S, Otake Y (2003b) Development of dynamic spatial video camera (DSVC) for 4D observation, analysis and modeling of human body locomotion. MMVR 11:346–348" href="/article/10.1007/s10055-008-0103-0#ref-CR25" id="ref-link-section-d74431e571">2003b</a>). The system is equipped with 65 computer-controlled video cameras to film human locomotion from all angles simultaneously. Video images are installed into the main graphic workstation and translated into a time-spatial matrix. It was able to perform observation of the subject from various directions by selecting the view point from the optimum image sequence in this image matrix. This system also possesses a function to reconstruct 4D models of the subject’s moving human body surface by using images recorded from all directions at a particular time. This system also has the capability to visualize inner structures, such as the skeletal or muscular systems of the subject, by compositing computer graphics reconstructed from the MRI data set. We plan to apply this imaging system to clinical observation in the area of orthopedics, rehabilitation and sports science.</p><p>The system is divided into two large parts. One is the assemblage of computer-controlled video cameras to collect time sequential video images of the subject from the subject’s surrounding positions. The other is a computer system to control the camera array of the assembly and to collect and process video images from multiple cameras systematically. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig4">4</a> shows the appearance of the system. For the camera assembly, we positioned 60 video cameras around the subject and 5 video cameras pointing from upwards. Specifically, we designed a 4-m diameter ring-shaped camera assembly in order to provide a space for the subject to be able to move freely in. The ring port was hung under a motor-controlled crane that is movable along a rail attached to the ceiling of the room. After image data collection, all images are lined up as one huge time-spatial image matrix to enable access of the image freely in time domain and spatial domain. The user is able to change the viewpoint or come and go in the time axis, interactively, for precise observation of the subject’s locomotion. The location and direction of each camera was previously calibrated for the precise reconstruction of a 4D surface. The inner structure of the subject was previously reconstructed from the MRI data set, and this image was superimposed on the live video image in all directions. The user is able to observe the condition of joints of bones or muscle in an interactive way.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The appearance of the constructed dynamic spatial video camera (DSVC) system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig5">5</a> shows the rotating subject’s view while time is stopped. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig6">6</a> shows the reconstructed 4D model from time sequential images using 65 angles. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig7">7</a> shows the superimposed image of the subject’s skeletal and muscular system conditions while the subject is walking. The 4D modeling of the subject’s locomotion that expresses the time sequential change of the shape, position and size of every part and the inner structure of the subject are able to be applied to the quantitative analysis of the human body.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>An example of the free observation in view points during the subject is freezing. In those image, the time is stopped at the point when the position of the body is the highest against the floor, and the viewpoint is moving clockwise to observe its condition from the different directions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The reconstructed 4D model from time sequential images using 65 angles</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Blending image of the DSVC images and 4D model of patient skeletal and muscular model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec5">Navigation system for surgical robot</h3><p>Now we would like to get back to surgical augmented reality and mention the relationship between robotic operation and augmented reality. We thought that robotic surgery, where the patient and the surgeon are separated by some distance and the surgeon has to carry out the operation with endoscopic images, needed an augmented reality that could give more information to the surgeon. First, we succeeded in mounting the augmented reality function on a commercial robot, da Vinci (Intuitive Surgical Inc.) (Salisbury <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Salisbury JK (1998), The heart of microsurgery. Mechanical Engineering Magazine, ASME Int’l.; 120(12):47–51" href="/article/10.1007/s10055-008-0103-0#ref-CR20" id="ref-link-section-d74431e673">1998</a>; Guthart and Salisbury <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Guthart GS, Salisbury JK (2000). The intuitive telesurgery system: overview and application. Proc. of the IEEE international conference on robotics and automation (ICRA 2000). San Francisco, CA, April 2000" href="/article/10.1007/s10055-008-0103-0#ref-CR6" id="ref-link-section-d74431e676">2000</a>), in 2003, and the first clinical application was a cholecystectomy (Hattori et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hattori A, Suzuki N, Hashizume M, Akahoshi T, Konishi K, Yamaguchi S, Shimada M, Hayashibe M (2003) A robotic surgery system (da Vinci) with image-guided function. Medicine Meets Virtual Reality 11:110–116" href="/article/10.1007/s10055-008-0103-0#ref-CR7" id="ref-link-section-d74431e679">2003</a>). At that time, there was hardly any information about the da Vinci system. We had a hard time trying to mount the augmented reality function by covering the new system from around the da Vinci system.</p><p>The system is composed of a robotic surgery system (da Vinci) and an augmented reality system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig8">8</a>). The da Vinci system consists of the surgeon’s console and the patient side cart. The surgeon’s console has a stereo display and handles that manipulate the patient side cart’s arms. The patient side cart has three arms. Two arms allow various surgical maneuvers and the other is attached to the laparoscope. The augmented reality system consists of two devices. One is a graphic workstation (GWS, OCTANE MXE, Silicon Graphics Inc.) that captures the laparoscopic image and superimposes 3D organ models on the image. The superimposed image is outputted to the stereo viewer of the surgeon’s console. The other device is the optical location sensor (POLARIS, Northern Digital, Inc.) that measures the location of the laparoscope. The optical marker is attached away from the laparoscope’s tip so as not to interfere with the laparoscope’s movement. Using data from the location sensor, the GWS transforms the coordinate system of the 3D organ model to the surgical field coordinate system and renders the 3D organ model image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>System configuration</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To fuse the laparoscope image and 3D organ models, laparoscope and organ model coordinate systems have to be transformed to a world coordinate system. After we measured the laparoscope optical parameters and registered the same locations on the organ model and real organ, we calculated the transformation matrix for fusing the images.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig9">9</a> shows a scene from a cholecystectomy. The superimposed image is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig10">10</a>. The gallbladder, common bile duct and hepatic duct model are superimposed onto the laparoscope’s surgical field image. The frame rate at this experiment was around 10 fps. The GUI of the top window is used to change a transparency of each organ. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig11">11</a> shows the surgeon’s console displaying the superimposed stereo image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A scene showing registration during a cholecystectomy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Superimposed images onto the GWS display; the patient’s organ models (gallbladder, common bile duct and hepatic duct) are superimposed onto the surgical field image; <i>top window</i> the left eye’s view, <i>bottom window</i> the right eye’s view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Superimposed image on the surgeon’s console. <b>a</b> A scene of the stereo viewer from surgeon’s console, <b>b</b> a scene from the right eye’s viewer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In this clinical application, the system enables image-guided navigation that followed laparoscope movement with a stereo view. Although the system’s frame rate is under 10 fps, this low frame rate does not affect the use of this system, because this system is used for confirming the internal structure of the organs and the laparoscope does not move in this situation.</p><h3 class="c-article__sub-heading" id="Sec6">Augmented reality for an endoscopic robot</h3><p>We also applied surgical navigation function for another robotic surgery. This system is composed of a developed endoscopic robot system and an augmented reality system (Hattori et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hattori A, Suzuki N, Hayashibe M, Suzuki S, Otake Y, Tajiri H, Kobayashi S (2005) Development of a navigation function for an endoscopic robot surgery system. MMVR 13:167–171" href="/article/10.1007/s10055-008-0103-0#ref-CR8" id="ref-link-section-d74431e802">2005</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig12">12</a> shows the system outline. In our developed endoscopic robot system (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003c" title="Suzuki N, Sumiyama K, Hattori A, Ikeda K, Murakami EAY, Suzuki S, Hayashibe M, Otake Y, Tajiri H (2003c) Development of an endoscopic robotic system with two hands for various gastric tube surgeries. MMVR 11:349–353" href="/article/10.1007/s10055-008-0103-0#ref-CR26" id="ref-link-section-d74431e808">2003c</a>), the distal part of the elastic tube has endoscopic eyes with a pair of small robot arms, one either side of the eye. Its shape is similar to a small scorpion. The distal part of each arm has forceps. A wire drive mechanism is applied to operate the robot arms. The diameter of each robot arm is 3 mm, and the maximum size of the distal part of the robot is 21 mm so that it is able to reach the gastric tube via the esophagus. The component power of three stainless wires enclosed inside a thin elastic tube controls the endo-effecter. An endoscopist operates the endoscope-shaped body of the robot to move it into the gastric tube, and a surgeon beside the endoscopist remotely controls both robot arms using the controller panel.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>System outline</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The augmented reality system consisted of two visual devices. One is a graphic workstation (GWS: OCTANE MXE, Silicon Graphics Inc.) that has a digital video processing board installed in it. The GWS captures the endoscopic video image and superimposes a 3D organ model onto the captured video image. The surgeon is able to watch the superimposed video on the endoscope’s monitor. One is the small magnetic location sensor (mini BIRD, Ascension Technology Co.) that always measures the location and the direction of the endoscopic robot’s tip. The sensor is fixed to the tip of the robot (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig13">13</a>). Using the updated positional data from the location sensor, the GWS transforms the coordinate system of the 3D organ model to the surgical field coordinate system and renders the 3D organ model onto the endoscopic captured video image. The 3D organ models are previously reconstructed from the patient MRI or CT dataset.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>An appearance of the endoscopic robot system (<b>a</b>). The magnetic location sensor is attached to the endoscope’s tip (<b>b</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>After registration, the surgeon inserts the endoscopic robot through the mouth cavity via the esophagus into the stomach. The 3D organ model that surrounds the stomach indicates the location and direction of the robot and is superimposed onto the endoscopic image (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig14">14</a>). In this subjective image, the spine, ribs, liver and hepatic artery models are displayed. In this figure, the top left small window also shows an objective location of the robot’s tip in the coordinate system of the organ model. The viewpoint of this window could be set interactively. The surgeon was able to change the transparency and the color of the organ models, and switch from superimposed video to non-superimposed video, depending on the situation of the surgical field. The frame rate for this experiment was 12–14 fps. In this verified experiment, EMR was performed using the augmented reality technique.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>A scene of navigation at the EMR procedure. The organ models are superimposed onto the endoscope’s image. According to the endoscope movement (in these Figs. a–d, the endoscope moves from left to right while looking down to the spine), the superimposed organ models follow the movement</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In this section, we described the process of loading the augmented reality functions for several different robotic surgeries. There are many advantages of overlaid 3D images for future developments of this kind of system. In addition, there is also a need to develop 3D positioning measurements without using engineering markers that need to be sterilized every time. If we can overcome these issues, we will be able to more easily capture the inner structure of the organ as it changes during the operation.</p><h3 class="c-article__sub-heading" id="Sec7">High-tech operating room for surgical navigation</h3><p>Here, we describe the high-tech operating room for image-guided surgery at our university hospital (Suzuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Suzuki N, Hattori A, Suzuki S, Otake Y, Hayashibe M, Kobayashi S, Nezu T, Sakai H, Umezawa Y (2005) Construction of a high-tech operating room for image-guided surgery using VR. MMVR 13:538–542" href="/article/10.1007/s10055-008-0103-0#ref-CR28" id="ref-link-section-d74431e895">2005</a>).</p><p>We aimed to construct an operating room equipped with devices utilizing high dimensional (3D, 4D) medical imaging techniques that enable the study of new surgical procedures.</p><p>This operating room was designed and constructed to provide new functions such as 3D and 4D images applying real-time imaging and medical-use VR technology, to ascertain the structures of affected regions. In this operating room, a C-arm typed CT (Siemens-Asahi Medical Technologies Ltd.) for acquiring the 3D structures in the operating view and a non-metal operating table with movable type rail (MAQUET GmbH &amp; Co. KG) that does not interrupt CT measurements were installed. As well, to enable the fusion of various data streams, an optical 3D position sensor (Optotrak: Northern Digital Inc.), arm-type monitors for an operator’s view, a computer for processing images, and a large-size transparent monitor were hung from the ceiling of the operating room. Four liquid crystal display (LCD) arm-type monitors to provide an operator’s view, designed and manufactured in consideration of surgical clarity, were installed around the operating table. The monitors were installed using a multi-joint arm, so that the operator would be able to freely choose from various images in the immediate proximity of the operating view. The large-size transparent monitor is installed with the aim of sharing information among staff in the operating room. This system can provide various images during the operation using a 40-in transparent hologram screen and the LCD projector, both hanging from the ceiling. Moreover, to prevent exhaust dust from the LCD projector’s fan polluting the air-conditioning in the operating room, it was built into an exclusively designed transparent-globular form acrylic case, and exhausted to the outside of the operating room with a duct connected to the case. We also decided to install a duct to remove dust generated by fans in the equipment installed in the console, such as the Graphic Workstation, the PC, and the controller for 3D position sensors, so that it would not be discharged into the operating room. A view of this operating room is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig15">15</a>, and the computer console located in the operating room is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig16">16</a>. It is possible to output the images from each computer to optional display units using the matrix switcher on the console. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig17">17</a> shows the ceiling-mounted transparent hologram screen and projector.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Overviews of the high-tech operating room from different points of view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig16_HTML.jpg?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig16_HTML.jpg" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>The computer console in the high-tech operating room</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig17_HTML.jpg?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig17_HTML.jpg" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>The transparent hologram screen and the sealing formula LCD projector</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We have developed two different image display systems for surgical navigation using this equipment. One is a video see-through-type display, and the other is an optical see-through-type display. The systems are composed of the ceiling-hanged LCD monitor and the optical 3D position sensor. Both the monitor and the objective inner structure are registered by an optical sensor. The video see-through-type data fusion display also has a small size video camera on the back of the monitor. The monitor displays in 3D a patient’s inner structure, which is superimposed onto a captured surgical field image. The optical see-through-type data fusion display has a semi-transparent mirror in front of the monitor. The image displayed on the monitor is projected onto the mirror. As well, the surgeon’s head position is measured by a 3D position sensor and the data fusion image is synchronized to the view of the surgeon. The surgeon is able to observe the surgical field and the inner structure of the patient through the mirror from various viewpoints.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig18">18</a> shows the appearance of the video see-through-type display that uses a liver phantom (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig18">18</a>a) and an elbow model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig18">18</a>b). The inner structural models are superimposed onto the captured surgical view. The appearance of the optical see-through-type display is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig19">19</a>. The surgeon’s head position is measured by a 3D position sensor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-008-0103-0#Fig18">18</a>a). The surgeon is able to observe the patient’s inner structure from various viewpoints.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig18_HTML.jpg?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig18_HTML.jpg" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>An appearance of the video see-through-type display using a liver phantom model (<b>a</b>) and an elbow model (<b>b</b>). The reconstructed organ models are superimposed onto the captured surgical view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig19_HTML.jpg?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-008-0103-0/MediaObjects/10055_2008_103_Fig19_HTML.jpg" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>An appearance of the video see-through-type display using a liver phantom model (<b>a</b>) and an elbow model (<b>b</b>). The reconstructed organ models are superimposed onto the captured surgical view</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-008-0103-0/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In completing this operating room, we provided a new site for clinical studies that would not only clinically evaluate the data fusion and robotic systems involved in endoscopy that have been developed to date, but one that would also construct a correspondence with the robotic surgery system or with tele-surgery, both areas that we believe will increase in popularity.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Towards the future</h2><div class="c-article-section__content" id="Sec8-content"><p>We have described the research of our team and the work we have conducted in recent years. From the beginning of the 1980s, for almost 10 years, we had asserted the need for a laboratory. In 1998, the president of our university told us that we could build the lab. For us, the past 10 years has been a very important decade. It was also an important period for researchers in the field as VR began to really be used in the medical field, which we had dreamed of all along. Looking at the MMVR excerpt, it is clear that the advancement of surrounding technology such as workstations also accelerated the improvement of our development. But now, in 2008, when we look around the hospital and in medical wards, there are still only a few medical virtual reality devices that are applied in everyday use. We believe that time will solve this, and that in 10 years time, virtual reality will be applied in everyday medical use. We continue to carry out our research, dreaming of a near future where patients undergo diagnosis and treatment with medical virtual reality devices as routinely as with CT and MRI devices.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Birkfellner, M. Figl, K. Huber, F. Watzinger, F. Wanschitz, J. Hummel, R. Hanel, W. Greimel, P. Homolka, R. Ewers, H. Bergmann, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Birkfellner W, Figl M, Huber K, Watzinger F, Wanschitz F, Hummel J, Hanel R, Greimel W, Homolka P, Ewers R, Be" /><p class="c-article-references__text" id="ref-CR1">Birkfellner W, Figl M, Huber K, Watzinger F, Wanschitz F, Hummel J, Hanel R, Greimel W, Homolka P, Ewers R, Bergmann H (2002) A head-mounted operating binocular for augmented reality visualization in medicine—design and initial evaluation. IEEE Trans Med Imaging 21(8):991–997</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMI.2002.803099" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20head-mounted%20operating%20binocular%20for%20augmented%20reality%20visualization%20in%20medicine%E2%80%94design%20and%20initial%20evaluation&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;volume=21&amp;issue=8&amp;pages=991-997&amp;publication_year=2002&amp;author=Birkfellner%2CW&amp;author=Figl%2CM&amp;author=Huber%2CK&amp;author=Watzinger%2CF&amp;author=Wanschitz%2CF&amp;author=Hummel%2CJ&amp;author=Hanel%2CR&amp;author=Greimel%2CW&amp;author=Homolka%2CP&amp;author=Ewers%2CR&amp;author=Bergmann%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Cotin, H. Delingette, A. Ayache, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Cotin S, Delingette H, Ayache A (1999) Real-time elastic deformations of soft tissues for surgery simulation. " /><p class="c-article-references__text" id="ref-CR2">Cotin S, Delingette H, Ayache A (1999) Real-time elastic deformations of soft tissues for surgery simulation. IEEE Trans Vis Comput Graph 5(1):62–73</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F2945.764872" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20elastic%20deformations%20of%20soft%20tissues%20for%20surgery%20simulation&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=5&amp;issue=1&amp;pages=62-73&amp;publication_year=1999&amp;author=Cotin%2CS&amp;author=Delingette%2CH&amp;author=Ayache%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Devernay F, Mourgues F, Coste-Maniere E (2001) Towards endoscopic augmented reality for robotically assisted m" /><p class="c-article-references__text" id="ref-CR3">Devernay F, Mourgues F, Coste-Maniere E (2001) Towards endoscopic augmented reality for robotically assisted minimally invasive cardiac surgery. Proc. of the international workshop on medical imaging and augmented reality (MIAR2001); pp 16–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Gehrmann, KH. Karl Höhne, W. Linhart, B. Pflesser, A. Pommert, M. Riemer, Ulf. Tiede, J. Windolf, U. Schumacher, JM. Rueger, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Gehrmann S, Karl Höhne KH, Linhart W, Pflesser B, Pommert A, Riemer M, Tiede Ulf, Windolf J, Schumacher U, Rue" /><p class="c-article-references__text" id="ref-CR4">Gehrmann S, Karl Höhne KH, Linhart W, Pflesser B, Pommert A, Riemer M, Tiede Ulf, Windolf J, Schumacher U, Rueger JM (2006) A novel interactive anatomic atlas of the hand. Clin Anat 19(3):258–266</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fca.20266" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20novel%20interactive%20anatomic%20atlas%20of%20the%20hand&amp;journal=Clin%20Anat&amp;volume=19&amp;issue=3&amp;pages=258-266&amp;publication_year=2006&amp;author=Gehrmann%2CS&amp;author=Karl%20H%C3%B6hne%2CKH&amp;author=Linhart%2CW&amp;author=Pflesser%2CB&amp;author=Pommert%2CA&amp;author=Riemer%2CM&amp;author=Tiede%2CUlf&amp;author=Windolf%2CJ&amp;author=Schumacher%2CU&amp;author=Rueger%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JG. Giraldez, M. Caversaccio, I. Pappas, J. Kowal, U. Rohrer, G. Marti, C. Baur, LP. Nolte, MAG. Ballester, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Giraldez JG, Caversaccio M, Pappas I, Kowal J, Rohrer U, Marti G, Baur C, Nolte LP, Ballester MAG (2007) Desig" /><p class="c-article-references__text" id="ref-CR5">Giraldez JG, Caversaccio M, Pappas I, Kowal J, Rohrer U, Marti G, Baur C, Nolte LP, Ballester MAG (2007) Design and clinical evaluation of an image-guided surgical microscope with an integrated tracking system. Int J CARS 1:253–264</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11548-006-0066-0" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20and%20clinical%20evaluation%20of%20an%20image-guided%20surgical%20microscope%20with%20an%20integrated%20tracking%20system&amp;journal=Int%20J%20CARS&amp;volume=1&amp;pages=253-264&amp;publication_year=2007&amp;author=Giraldez%2CJG&amp;author=Caversaccio%2CM&amp;author=Pappas%2CI&amp;author=Kowal%2CJ&amp;author=Rohrer%2CU&amp;author=Marti%2CG&amp;author=Baur%2CC&amp;author=Nolte%2CLP&amp;author=Ballester%2CMAG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Guthart GS, Salisbury JK (2000). The intuitive telesurgery system: overview and application. Proc. of the IEEE" /><p class="c-article-references__text" id="ref-CR6">Guthart GS, Salisbury JK (2000). The intuitive telesurgery system: overview and application. Proc. of the IEEE international conference on robotics and automation (ICRA 2000). San Francisco, CA, April 2000</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hattori, N. Suzuki, M. Hashizume, T. Akahoshi, K. Konishi, S. Yamaguchi, M. Shimada, M. Hayashibe, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Hattori A, Suzuki N, Hashizume M, Akahoshi T, Konishi K, Yamaguchi S, Shimada M, Hayashibe M (2003) A robotic " /><p class="c-article-references__text" id="ref-CR7">Hattori A, Suzuki N, Hashizume M, Akahoshi T, Konishi K, Yamaguchi S, Shimada M, Hayashibe M (2003) A robotic surgery system (da Vinci) with image-guided function. Medicine Meets Virtual Reality 11:110–116</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20robotic%20surgery%20system%20%28da%20Vinci%29%20with%20image-guided%20function&amp;journal=Medicine%20Meets%20Virtual%20Reality&amp;volume=11&amp;pages=110-116&amp;publication_year=2003&amp;author=Hattori%2CA&amp;author=Suzuki%2CN&amp;author=Hashizume%2CM&amp;author=Akahoshi%2CT&amp;author=Konishi%2CK&amp;author=Yamaguchi%2CS&amp;author=Shimada%2CM&amp;author=Hayashibe%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Hattori, N. Suzuki, M. Hayashibe, S. Suzuki, Y. Otake, H. Tajiri, S. Kobayashi, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Hattori A, Suzuki N, Hayashibe M, Suzuki S, Otake Y, Tajiri H, Kobayashi S (2005) Development of a navigation " /><p class="c-article-references__text" id="ref-CR8">Hattori A, Suzuki N, Hayashibe M, Suzuki S, Otake Y, Tajiri H, Kobayashi S (2005) Development of a navigation function for an endoscopic robot surgery system. MMVR 13:167–171</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20navigation%20function%20for%20an%20endoscopic%20robot%20surgery%20system&amp;journal=MMVR&amp;volume=13&amp;pages=167-171&amp;publication_year=2005&amp;author=Hattori%2CA&amp;author=Suzuki%2CN&amp;author=Hayashibe%2CM&amp;author=Suzuki%2CS&amp;author=Otake%2CY&amp;author=Tajiri%2CH&amp;author=Kobayashi%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KH. Höhne, B. Pflesser, A. Pommert, M. Riemer, T. Schiemann, R. Schubert, Ulf. Tiede, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Höhne KH, Pflesser B, Pommert A, Riemer M, Schiemann T, Schubert R, Tiede Ulf (1995) A new representation of k" /><p class="c-article-references__text" id="ref-CR9">Höhne KH, Pflesser B, Pommert A, Riemer M, Schiemann T, Schubert R, Tiede Ulf (1995) A new representation of knowledge concerning human anatomy and function. Nat Med 1(6):506–511</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2Fnm0695-506" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20new%20representation%20of%20knowledge%20concerning%20human%20anatomy%20and%20function&amp;journal=Nat%20Med&amp;volume=1&amp;issue=6&amp;pages=506-511&amp;publication_year=1995&amp;author=H%C3%B6hne%2CKH&amp;author=Pflesser%2CB&amp;author=Pommert%2CA&amp;author=Riemer%2CM&amp;author=Schiemann%2CT&amp;author=Schubert%2CR&amp;author=Tiede%2CUlf">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mendoza C, Laugier C (2003), Tissue cutting using finite elements and force feedback, lecture notes in compute" /><p class="c-article-references__text" id="ref-CR10">Mendoza C, Laugier C (2003), Tissue cutting using finite elements and force feedback, lecture notes in computer science 2673, Surgery Simulation and Soft Tissue Modeling, pp 175–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nicolau SA, Pennec X, Soler L, Ayache N (2005) A Complete augmented reality guidance system for liver puncture" /><p class="c-article-references__text" id="ref-CR11">Nicolau SA, Pennec X, Soler L, Ayache N (2005) A Complete augmented reality guidance system for liver punctures: first clinical evaluation. MICCAI, LNCS 3749:539–547</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Petersik A, Pflesser B, Tiede U, Höhne KH, Leuwer R (2003), Realistic haptic interaction in volume sculpting f" /><p class="c-article-references__text" id="ref-CR12">Petersik A, Pflesser B, Tiede U, Höhne KH, Leuwer R (2003), Realistic haptic interaction in volume sculpting for surgery simulation. In: Ayache N, Delingette H (eds) Surgery simulation and soft tissue modeling, Proc. IS4TM 2003, Lect Notes Comput Sci 2673, Springer-Verlag, Berlin, pp 194–202</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Pflesser, R. Leuwer, A. Petersik, Ulf. Tiede, KH. Höhne, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Pflesser B, Leuwer R, Petersik A, Tiede Ulf, Höhne KH (2002) A computer-based simulation for petrous bone surg" /><p class="c-article-references__text" id="ref-CR13">Pflesser B, Leuwer R, Petersik A, Tiede Ulf, Höhne KH (2002) A computer-based simulation for petrous bone surgery with haptic feedback. Comput Aided Surg 7(2):117</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3109%2F10929080209146018" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20computer-based%20simulation%20for%20petrous%20bone%20surgery%20with%20haptic%20feedback&amp;journal=Comput%20Aided%20Surg&amp;volume=7&amp;issue=2&amp;publication_year=2002&amp;author=Pflesser%2CB&amp;author=Leuwer%2CR&amp;author=Petersik%2CA&amp;author=Tiede%2CUlf&amp;author=H%C3%B6hne%2CKH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Rajagopalan, L. Lu, RA. Robb, MJ. Yaszemski, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Rajagopalan S, Lu L, Robb RA, Yaszemski MJ (2005) Optimal segmentation of microcomputed tomographic images of " /><p class="c-article-references__text" id="ref-CR14">Rajagopalan S, Lu L, Robb RA, Yaszemski MJ (2005) Optimal segmentation of microcomputed tomographic images of porous tissue-engineering scaffolds. J Biomed Mater Res 75A(4):877–887</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fjbm.a.30498" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20segmentation%20of%20microcomputed%20tomographic%20images%20of%20porous%20tissue-engineering%20scaffolds&amp;journal=J%20Biomed%20Mater%20Res&amp;volume=75A&amp;issue=4&amp;pages=877-887&amp;publication_year=2005&amp;author=Rajagopalan%2CS&amp;author=Lu%2CL&amp;author=Robb%2CRA&amp;author=Yaszemski%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Robb RA (1971), Computer-aided contour determination and dynamic display of individual cardiac chambers from d" /><p class="c-article-references__text" id="ref-CR15">Robb RA (1971), Computer-aided contour determination and dynamic display of individual cardiac chambers from digitized serial angiocardiographic film. In: Heintzen PH (ed) Roentgen-, cine-, and videodensitometry. Fundamentals and application for blood flow and heart volume determination. Georg Thieme Verlag, Stuttgart, Germany, pp 170–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Robb RA (1997), Virtual endoscopy: evaluation using the visible human datasets and comparison with real endosc" /><p class="c-article-references__text" id="ref-CR16">Robb RA (1997), Virtual endoscopy: evaluation using the visible human datasets and comparison with real endoscopy in patients. Proceedings of medicine meets virtual reality. In: Morgan KS, Hoffman HM, Stredney D, Weghorst SJ (eds) IOS Press, Netherlands, 39, 195–206</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Robb, EL. Ritman, JF. Greenleaf, RE. Sturm, HK. Liu, PA. Chevalier, EH. Wood, " /><meta itemprop="datePublished" content="1976" /><meta itemprop="headline" content="Robb RA, Ritman EL, Greenleaf JF, Sturm RE, Liu HK, Chevalier PA, Wood EH (1976) Quantitative imaging of dynam" /><p class="c-article-references__text" id="ref-CR17">Robb RA, Ritman EL, Greenleaf JF, Sturm RE, Liu HK, Chevalier PA, Wood EH (1976) Quantitative imaging of dynamic structure and function of the heart, lungs and circulation by computerized reconstruction and subtraction techniques. Comput Graphics 10(2):246–256</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F965143.563318" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Quantitative%20imaging%20of%20dynamic%20structure%20and%20function%20of%20the%20heart%2C%20lungs%20and%20circulation%20by%20computerized%20reconstruction%20and%20subtraction%20techniques&amp;journal=Comput%20Graphics&amp;volume=10&amp;issue=2&amp;pages=246-256&amp;publication_year=1976&amp;author=Robb%2CRA&amp;author=Ritman%2CEL&amp;author=Greenleaf%2CJF&amp;author=Sturm%2CRE&amp;author=Liu%2CHK&amp;author=Chevalier%2CPA&amp;author=Wood%2CEH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Robb, DP. Hanson, RA. Karwoski, AG. Larson, EL. Workman, MC. Stacy, " /><meta itemprop="datePublished" content="1989" /><meta itemprop="headline" content="Robb RA, Hanson DP, Karwoski RA, Larson AG, Workman EL, Stacy MC (1989) ANALYZE: a comprehensive, operator-int" /><p class="c-article-references__text" id="ref-CR18">Robb RA, Hanson DP, Karwoski RA, Larson AG, Workman EL, Stacy MC (1989) ANALYZE: a comprehensive, operator-interactive software package for multidimensional medical image display and analysis. Comput Med Imaging Graph 13(6):433–454</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0895-6111%2889%2990285-1" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ANALYZE%3A%20a%20comprehensive%2C%20operator-interactive%20software%20package%20for%20multidimensional%20medical%20image%20display%20and%20analysis&amp;journal=Comput%20Med%20Imaging%20Graph&amp;volume=13&amp;issue=6&amp;pages=433-454&amp;publication_year=1989&amp;author=Robb%2CRA&amp;author=Hanson%2CDP&amp;author=Karwoski%2CRA&amp;author=Larson%2CAG&amp;author=Workman%2CEL&amp;author=Stacy%2CMC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosen J, Soltanian H, Redett R et al (1996), Evolution of virtual reality-from planning to performing surgery," /><p class="c-article-references__text" id="ref-CR19">Rosen J, Soltanian H, Redett R et al (1996), Evolution of virtual reality-from planning to performing surgery, IEEE Engineering in Medicine and Biology, pp 16–22</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Salisbury JK (1998), The heart of microsurgery. Mechanical Engineering Magazine, ASME Int’l.; 120(12):47–51" /><p class="c-article-references__text" id="ref-CR20">Salisbury JK (1998), The heart of microsurgery. Mechanical Engineering Magazine, ASME Int’l.; 120(12):47–51</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Shahidi, MR. Bax, CR. Maurer, JA. Johnson, EP. Wilkinson, B. Wang, JB. West, MJ. Citardi, KH. Manwaring, R. Khadem, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Shahidi R, Bax MR, Maurer CR Jr, Johnson JA, Wilkinson EP, Wang B, West JB, Citardi MJ, Manwaring KH, Khadem R" /><p class="c-article-references__text" id="ref-CR21">Shahidi R, Bax MR, Maurer CR Jr, Johnson JA, Wilkinson EP, Wang B, West JB, Citardi MJ, Manwaring KH, Khadem R (2002) Implementation, calibration and accuracy testing of an image-enhanced endoscopy system. IEEE Trans Med Imaging 21(12):1524–1535</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMI.2002.806597" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Implementation%2C%20calibration%20and%20accuracy%20testing%20of%20an%20image-enhanced%20endoscopy%20system&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;volume=21&amp;issue=12&amp;pages=1524-1535&amp;publication_year=2002&amp;author=Shahidi%2CR&amp;author=Bax%2CMR&amp;author=Maurer%2CCR&amp;author=Johnson%2CJA&amp;author=Wilkinson%2CEP&amp;author=Wang%2CB&amp;author=West%2CJB&amp;author=Citardi%2CMJ&amp;author=Manwaring%2CKH&amp;author=Khadem%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Suzuki N, Hattori A, Kai S et al (1997) Surgical planning system for soft tissues using virtual reality. Proc." /><p class="c-article-references__text" id="ref-CR22">Suzuki N, Hattori A, Kai S et al (1997) Surgical planning system for soft tissues using virtual reality. Proc. of medicine meets virtual reality 5, 159–63</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Suzuki, A. Hattori, T. Ezumi, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Suzuki N, Hattori A, Ezumi T et al (1998) Simulator for virtual surgery using deformable organ model and force" /><p class="c-article-references__text" id="ref-CR23">Suzuki N, Hattori A, Ezumi T et al (1998) Simulator for virtual surgery using deformable organ model and force feedback system. MMVR 6:227–233</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulator%20for%20virtual%20surgery%20using%20deformable%20organ%20model%20and%20force%20feedback%20system&amp;journal=MMVR&amp;volume=6&amp;pages=227-233&amp;publication_year=1998&amp;author=Suzuki%2CN&amp;author=Hattori%2CA&amp;author=Ezumi%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Suzuki, N. Suzuki, A. Hattori, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Suzuki S, Suzuki N, Hattori A et al (2003a) Dynamic deformation of elastic organ model and the VR cockpit for " /><p class="c-article-references__text" id="ref-CR24">Suzuki S, Suzuki N, Hattori A et al (2003a) Dynamic deformation of elastic organ model and the VR cockpit for virtual surgery and tele-surgery. MMVR 11:354–356</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20deformation%20of%20elastic%20organ%20model%20and%20the%20VR%20cockpit%20for%20virtual%20surgery%20and%20tele-surgery&amp;journal=MMVR&amp;volume=11&amp;pages=354-356&amp;publication_year=2003&amp;author=Suzuki%2CS&amp;author=Suzuki%2CN&amp;author=Hattori%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Suzuki, A. Hattori, M. Hayashibe, S. Suzuki, Y. Otake, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Suzuki N, Hattori A, Hayashibe M, Suzuki S, Otake Y (2003b) Development of dynamic spatial video camera (DSVC)" /><p class="c-article-references__text" id="ref-CR25">Suzuki N, Hattori A, Hayashibe M, Suzuki S, Otake Y (2003b) Development of dynamic spatial video camera (DSVC) for 4D observation, analysis and modeling of human body locomotion. MMVR 11:346–348</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20dynamic%20spatial%20video%20camera%20%28DSVC%29%20for%204D%20observation%2C%20analysis%20and%20modeling%20of%20human%20body%20locomotion&amp;journal=MMVR&amp;volume=11&amp;pages=346-348&amp;publication_year=2003&amp;author=Suzuki%2CN&amp;author=Hattori%2CA&amp;author=Hayashibe%2CM&amp;author=Suzuki%2CS&amp;author=Otake%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Suzuki, K. Sumiyama, A. Hattori, K. Ikeda, EAY. Murakami, S. Suzuki, M. Hayashibe, Y. Otake, H. Tajiri, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Suzuki N, Sumiyama K, Hattori A, Ikeda K, Murakami EAY, Suzuki S, Hayashibe M, Otake Y, Tajiri H (2003c) Devel" /><p class="c-article-references__text" id="ref-CR26">Suzuki N, Sumiyama K, Hattori A, Ikeda K, Murakami EAY, Suzuki S, Hayashibe M, Otake Y, Tajiri H (2003c) Development of an endoscopic robotic system with two hands for various gastric tube surgeries. MMVR 11:349–353</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20an%20endoscopic%20robotic%20system%20with%20two%20hands%20for%20various%20gastric%20tube%20surgeries&amp;journal=MMVR&amp;volume=11&amp;pages=349-353&amp;publication_year=2003&amp;author=Suzuki%2CN&amp;author=Sumiyama%2CK&amp;author=Hattori%2CA&amp;author=Ikeda%2CK&amp;author=Murakami%2CEAY&amp;author=Suzuki%2CS&amp;author=Hayashibe%2CM&amp;author=Otake%2CY&amp;author=Tajiri%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Suzuki, N. Suzuki, A. Hattori, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Suzuki S, Suzuki N, Hattori A et al (2004) Sphere-filled organ model for virtual surgery system. IEEE Trans Me" /><p class="c-article-references__text" id="ref-CR27">Suzuki S, Suzuki N, Hattori A et al (2004) Sphere-filled organ model for virtual surgery system. IEEE Trans Med Imaging 23(6):714–722</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMI.2004.826947" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sphere-filled%20organ%20model%20for%20virtual%20surgery%20system&amp;journal=IEEE%20Trans%20Med%20Imaging&amp;volume=23&amp;issue=6&amp;pages=714-722&amp;publication_year=2004&amp;author=Suzuki%2CS&amp;author=Suzuki%2CN&amp;author=Hattori%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Suzuki, A. Hattori, S. Suzuki, Y. Otake, M. Hayashibe, S. Kobayashi, T. Nezu, H. Sakai, Y. Umezawa, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Suzuki N, Hattori A, Suzuki S, Otake Y, Hayashibe M, Kobayashi S, Nezu T, Sakai H, Umezawa Y (2005) Constructi" /><p class="c-article-references__text" id="ref-CR28">Suzuki N, Hattori A, Suzuki S, Otake Y, Hayashibe M, Kobayashi S, Nezu T, Sakai H, Umezawa Y (2005) Construction of a high-tech operating room for image-guided surgery using VR. MMVR 13:538–542</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Construction%20of%20a%20high-tech%20operating%20room%20for%20image-guided%20surgery%20using%20VR&amp;journal=MMVR&amp;volume=13&amp;pages=538-542&amp;publication_year=2005&amp;author=Suzuki%2CN&amp;author=Hattori%2CA&amp;author=Suzuki%2CS&amp;author=Otake%2CY&amp;author=Hayashibe%2CM&amp;author=Kobayashi%2CS&amp;author=Nezu%2CT&amp;author=Sakai%2CH&amp;author=Umezawa%2CY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tiede U, Schiemann T, Höhne KH (1998), High quality rendering of attributed volume data. In: David Ebert et al" /><p class="c-article-references__text" id="ref-CR29">Tiede U, Schiemann T, Höhne KH (1998), High quality rendering of attributed volume data. In: David Ebert et al (eds) Proc. IEEE visualization 1998. Research Triangle Park, NC, pp 255–262</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-008-0103-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Institute for High Dimensional Medical Imaging, The Jikei University School of Medicine, 4-11-1 Izumihoncho, Komae, Tokyo, 201-8601, Japan</p><p class="c-article-author-affiliation__authors-list">Naoki Suzuki &amp; Asaki Hattori</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Naoki-Suzuki"><span class="c-article-authors-search__title u-h3 js-search-name">Naoki Suzuki</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Naoki+Suzuki&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Naoki+Suzuki" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Naoki+Suzuki%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Asaki-Hattori"><span class="c-article-authors-search__title u-h3 js-search-name">Asaki Hattori</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Asaki+Hattori&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Asaki+Hattori" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Asaki+Hattori%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-008-0103-0/email/correspondent/c1/new">Naoki Suzuki</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=The%20road%20to%20surgical%20simulation%20and%20surgical%20navigation&amp;author=Naoki%20Suzuki%20et%20al&amp;contentID=10.1007%2Fs10055-008-0103-0&amp;publication=1359-4338&amp;publicationDate=2008-12-04&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Suzuki, N., Hattori, A. The road to surgical simulation and surgical navigation.
                    <i>Virtual Reality</i> <b>12, </b>281 (2008). https://doi.org/10.1007/s10055-008-0103-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-008-0103-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-09-25">25 September 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-10-06">06 October 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-12-04">04 December 2008</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-008-0103-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-008-0103-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Medical imaging</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Surgical simulation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Navigation surgery</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-008-0103-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=103;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

