<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Dynamic learning, retrieval, and tracking to augment hundreds of photo"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects,..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Dynamic learning, retrieval, and tracking to augment hundreds of photographs"/>

    <meta name="dc.source" content="Virtual Reality 2013 18:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-09-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and cannot track retrieved objects. In this paper, we present a method that combines the speed and accuracy of tracking with the scalability of image retrieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. Dynamic learning of new viewpoints as the camera moves naturally yields the kind of robustness and reliability expected from an augmented reality engine. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries. We released the source code of our system in a package called Polyora."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-09-13"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="89"/>

    <meta name="prism.endingPage" content="100"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0228-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0228-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0228-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0228-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Dynamic learning, retrieval, and tracking to augment hundreds of photographs"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2014/06"/>

    <meta name="citation_online_date" content="2013/09/13"/>

    <meta name="citation_firstpage" content="89"/>

    <meta name="citation_lastpage" content="100"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0228-7"/>

    <meta name="DOI" content="10.1007/s10055-013-0228-7"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0228-7"/>

    <meta name="description" content="Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. Howev"/>

    <meta name="dc.creator" content="Julien Pilet"/>

    <meta name="dc.creator" content="Hideo Saito"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comp Vis; citation_title=Lucas-kanade 20 years on: a unifying framework; citation_author=S Baker, I Matthews; citation_volume=56; citation_issue=3; citation_publication_date=2004; citation_pages=221-255; citation_id=CR1"/>

    <meta name="citation_reference" content="Bay H, Tuytelaars T, Gool LV (2006) SURF: speeded up robust features. In: European conference on computer vision"/>

    <meta name="citation_reference" content="Fiala M (2005) ARTag, a fiducial marker system using digital techniques. In: Conference on computer vision and pattern recognition, pp 590&#8211;596"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography; citation_author=M Fischler, R Bolles; citation_volume=24; citation_issue=6; citation_publication_date=1981; citation_pages=381-395; citation_doi=10.1145/358669.358692; citation_id=CR4"/>

    <meta name="citation_reference" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Fourth alvey vision conference, Manchester"/>

    <meta name="citation_reference" content="J&#233;gou H, Douze M, Schmid C (2008) Hamming embedding and weak geometric consistency for large scale image search. In: European conference on computer vision, LNCS, vol 1, pp 304&#8211;317"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: International symposium on augmented reality, pp 111&#8211;119"/>

    <meta name="citation_reference" content="citation_journal_title=Found Trends Comp Graph Vis; citation_title=Monocular model-based 3d tracking of rigid objects: a survey; citation_author=V Lepetit, P Fua; citation_volume=1; citation_issue=1; citation_publication_date=2005; citation_pages=1-89; citation_doi=10.1561/0600000001; citation_id=CR8"/>

    <meta name="citation_reference" content="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Conference on computer vision and pattern recognition, Washington, DC"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comp Vis; citation_title=Distinctive image features from scale-invariant keypoints; citation_author=D Lowe; citation_volume=20; citation_issue=2; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR10"/>

    <meta name="citation_reference" content="Lucas B, Kanade T (1981) An Iterative Image Registration Technique with an Application to Stereo Vision. In: International joint conference on artificial intelligence, pp 674&#8211;679"/>

    <meta name="citation_reference" content="Matas J, Chum O, Martin U, Pajdla T (2002) Robust wide baseline stereo from maximally stable extremal regions. In: British machine vision conference, London, pp 384&#8211;393"/>

    <meta name="citation_reference" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition"/>

    <meta name="citation_reference" content="Obdr&#382;&#225;lek &#352;, Matas J (2005) Sub-linear indexing for large scale object recognition. In: British machine vision conference"/>

    <meta name="citation_reference" content="Ozuysal M, Lepetit V, Fleuret F, Fua P (2006) Feature harvesting for tracking-by-detection. In: European conference on computer vision, Graz"/>

    <meta name="citation_reference" content="Ozuysal M, Fua P, Lepetit V (2007) Fast keypoint recognition in ten lines of code. In: Conference on computer vision and pattern recognition, Minneapolis, MI"/>

    <meta name="citation_reference" content="Park Y, Lepetit V, Woo W (2008) Multiple 3d object tracking for augmented reality. In: International symposium on mixed and augmented reality, pp 117&#8211;120"/>

    <meta name="citation_reference" content="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2007) Object retrieval with large vocabularies and fast spatial matching. In: Conference on computer vision and pattern recognition"/>

    <meta name="citation_reference" content="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost in quantization: Improving particular object retrieval in large scale image databases. In: Conference on computer vision and pattern recognition"/>

    <meta name="citation_reference" content="Rosten E, Drummond T (2006) Machine learning for high-speed corner detection. In: European conference on computer vision"/>

    <meta name="citation_reference" content="Shi J, Tomasi C (1994) Good features to track. In: Conference on computer vision and pattern recognition, Seattle"/>

    <meta name="citation_reference" content="Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In: Proceedings of the international conference on computer vision, vol 2, pp 1470&#8211;1477"/>

    <meta name="citation_reference" content="Taylor S, Rosten E, Drummond T (2009) Robust feature matching in 2.3&#956;s. In: IEEE CVPR workshop on feature detectors and descriptors: the state of the art and beyond"/>

    <meta name="citation_reference" content="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: International symposium on mixed and augmented reality, pp 95&#8211;98"/>

    <meta name="citation_reference" content="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008) Pose tracking from natural features on mobile phones. In: International symposium on mixed and augmented reality, Cambridge"/>

    <meta name="citation_reference" content="Wagner D, Schmalstieg D, Bischof H (2009) Multiple target detection and tracking with guaranteed framerates on mobile phones. In: International symposium on mixed and augmented reality, Orlando"/>

    <meta name="citation_reference" content="Wu C (2008) A GPU implementation of David Lowe&#8217;s scale invariant feature transform"/>

    <meta name="citation_author" content="Julien Pilet"/>

    <meta name="citation_author_email" content="julien.pilet@gmail.com"/>

    <meta name="citation_author_institution" content="Keio University, Yokohama, Japan"/>

    <meta name="citation_author" content="Hideo Saito"/>

    <meta name="citation_author_email" content="saito@hvrl.ics.keio.ac.jp"/>

    <meta name="citation_author_institution" content="Keio University, Yokohama, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0228-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0228-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Dynamic learning, retrieval, and tracking to augment hundreds of photographs"/>
        <meta property="og:description" content="Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and cannot track retrieved objects. In this paper, we present a method that combines the speed and accuracy of tracking with the scalability of image retrieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. Dynamic learning of new viewpoints as the camera moves naturally yields the kind of robustness and reliability expected from an augmented reality engine. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries. We released the source code of our system in a package called Polyora."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Dynamic learning, retrieval, and tracking to augment hundreds of photographs | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0228-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Multiple object tracking, Image retrieval","kwrd":["Augmented_reality","Multiple_object_tracking","Image_retrieval"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0228-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0228-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=228;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0228-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Dynamic learning, retrieval, and tracking to augment hundreds of photographs
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0228-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0228-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-09-13" itemprop="datePublished">13 September 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Dynamic learning, retrieval, and tracking to augment hundreds of photographs</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Julien-Pilet" data-author-popup="auth-Julien-Pilet" data-corresp-id="c1">Julien Pilet<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University" /><meta itemprop="address" content="grid.26091.3c, 0000000419369959, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, 223-8522, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Hideo-Saito" data-author-popup="auth-Hideo-Saito">Hideo Saito</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Keio University" /><meta itemprop="address" content="grid.26091.3c, 0000000419369959, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, 223-8522, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">89</span>–<span itemprop="pageEnd">100</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">272 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0228-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and cannot track retrieved objects. In this paper, we present a method that combines the speed and accuracy of tracking with the scalability of image retrieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. Dynamic learning of new viewpoints as the camera moves naturally yields the kind of robustness and reliability expected from an augmented reality engine. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries. We released the source code of our system in a package called Polyora.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Object tracking is an important issue for many applications, especially in the domains of virtual, mixed, and augmented reality. The base process to visually integrate virtual elements on real ones is the following: a camera captures a scene. A registration technique provides the relative pose of the camera with respect to the target object. A standard CG method renders virtual contents from the appropriate point of view and integrates it on the camera image. Examples of such applications are numerous and include augmenting the pages of a book, playing cards, and trading cards. In these cases, as often, the camera is the only registration device available.</p><p>Beside adequate display and computing hardware, high quality augmented reality requires reliable and flexible tracking method. Many methods have been proposed for visual registration (Lepetit and Fua <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lepetit V, Fua P (2005) Monocular model-based 3d tracking of rigid objects: a survey. Found Trends Comp Graph Vis 1(1):1–89" href="/article/10.1007/s10055-013-0228-7#ref-CR8" id="ref-link-section-d48026e304">2005</a>). However, when it comes to tracking multiple natural objects, they all have drawbacks. A classical approach to track multiple objects is to detect a part common to all objects, typically a highly contrasted square, and use the registration to ease searching the characteristic area in the database. However, the necessity of marking target objects severely restricts the application domain of such tracking approaches. Recently, wide-baseline point matching has been demonstrated to effectively detect and track naturally textured objects (Ozuysal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ozuysal M, Fua P, Lepetit V (2007) Fast keypoint recognition in ten lines of code. In: Conference on computer vision and pattern recognition, Minneapolis, MI" href="/article/10.1007/s10055-013-0228-7#ref-CR16" id="ref-link-section-d48026e307">2007</a>; Wagner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008) Pose tracking from natural features on mobile phones. In: International symposium on mixed and augmented reality, Cambridge" href="/article/10.1007/s10055-013-0228-7#ref-CR25" id="ref-link-section-d48026e310">2008</a>). However, when multiple objects are to be recognized, these approaches try to match each known object in turn. Such a linear complexity, both in computation and memory requirements, limits the number of known targets to a few objects (Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Park Y, Lepetit V, Woo W (2008) Multiple 3d object tracking for augmented reality. In: International symposium on mixed and augmented reality, pp 117–120" href="/article/10.1007/s10055-013-0228-7#ref-CR17" id="ref-link-section-d48026e313">2008</a>).</p><p>Image retrieval approaches address the scalability issue. These methods can deal with millions of images (Nister and Stewenius <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR13" id="ref-link-section-d48026e319">2006</a>). However, they focus on static images and are not designed for real-time tracking.</p><p>In this paper, we propose a real-time tracking method that integrates image retrieval techniques to achieve both accurate tracking and scalability with respect to the number of target objects. More specifically, we exploit information available in video stream, as well as the variability of feature descriptors, to efficiently establish correspondences with, index, and retrieve target objects. We also integrate dynamic learning of new viewpoints to increase detection robustness and accuracy. As a result, our method can reliably augment more than 300 objects individually, as depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>To produce this result, our system detects, identifies, and tracks the photographs visible on the input video stream. Each picture is augmented with its corresponding virtual element, precisely registered. The stack of pictures visible on this image contains about 300 images. Our system can recognize all of them. Users can also augment new objects by simply showing them to the camera and clicking. Tracking and augmentation start right away</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>To better describe the algorithmic contribution of our paper, let us sketch a typical image retrieval system. The first step is detection of features such as SIFT (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comp Vis 20(2):91–110" href="/article/10.1007/s10055-013-0228-7#ref-CR10" id="ref-link-section-d48026e350">2004</a>) or MSER (Matas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Matas J, Chum O, Martin U, Pajdla T (2002) Robust wide baseline stereo from maximally stable extremal regions. In: British machine vision conference, London, pp 384–393" href="/article/10.1007/s10055-013-0228-7#ref-CR12" id="ref-link-section-d48026e353">2002</a>), to summarize an image with a set of vectors. These vectors are quantized, turning features into <i>visual words</i>. Vector quantization involves clustering a large number of representative features, forming a <i>visual vocabulary</i>. Once an image is expressed as a <i>bag of word</i>, it can be indexed and searched for using information-retrieval techniques (Nister and Stewenius <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR13" id="ref-link-section-d48026e366">2006</a>; Sivic and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In: Proceedings of the international conference on computer vision, vol 2, pp 1470–1477" href="/article/10.1007/s10055-013-0228-7#ref-CR22" id="ref-link-section-d48026e369">2003</a>). However, quantization errors combined with feature instability reduce retrieval performances. A key contribution of our work is a method that exploits quantization effects and the nonlinear variations of features to improve retrieval and to reach a real-time computation speed.</p><p>Our method observes the variability of descriptors over several frames, by tracking the keypoints from frame to frame. Inspired by Ozuysal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ozuysal M, Fua P, Lepetit V (2007) Fast keypoint recognition in ten lines of code. In: Conference on computer vision and pattern recognition, Minneapolis, MI" href="/article/10.1007/s10055-013-0228-7#ref-CR16" id="ref-link-section-d48026e375">2007</a>), we capture the behavior of features during the training phase. The collected data allow our method to create a stabilized visual vocabulary. At runtime, our system matches full point tracks, as opposed to single features obtained from single frames, against training data. It yields a stable entry that can serve as index key. Because the set of detected features tends to vary with the viewpoint, we dynamically learn new views while tracking an object. As a result, detection of an object under a pose related to one that has previously been tracked is likely to succeed.</p><p>At a higher level, our contribution is a multiple natural object tracking system designed to scale well with the number of targets. Several of its properties make it perfectly suited for augmented reality applications: It can process live video stream, it can deal with a large number of target objects, adding new targets to the database is perceptually immediate, initialization is fully automatic, and it is robust to viewpoint changes, illumination effects, partial occlusion, and other typical tracking hazards.</p><p>We demonstrate the effectiveness of our approach with a toy application that can augment more than 300 pictures and that allows users to interactively augment with virtual elements a large number of new objects.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Pixel level registration techniques have today reached maturity (Lucas and Kanade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Lucas B, Kanade T (1981) An Iterative Image Registration Technique with an Application to Stereo Vision. In: International joint conference on artificial intelligence, pp 674–679" href="/article/10.1007/s10055-013-0228-7#ref-CR11" id="ref-link-section-d48026e390">1981</a>; Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-kanade 20 years on: a unifying framework. Int J Comp Vis 56(3):221–255&#xA;" href="/article/10.1007/s10055-013-0228-7#ref-CR1" id="ref-link-section-d48026e393">2004</a>). Their association with point selection methods to concentrate computation efforts on interesting pixels forms the basis of many computer vision tasks such as object tracking (Harris and Stephens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Harris C, Stephens M (1988) A combined corner and edge detector. In: Fourth alvey vision conference, Manchester" href="/article/10.1007/s10055-013-0228-7#ref-CR5" id="ref-link-section-d48026e396">1988</a>; Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C (1994) Good features to track. In: Conference on computer vision and pattern recognition, Seattle" href="/article/10.1007/s10055-013-0228-7#ref-CR21" id="ref-link-section-d48026e399">1994</a>). Tracking requires initialization, which remained an issue a few years ago. Because marker-based approaches such as ARToolkit did not suffer from this drawback, they quickly became popular in the augmented reality community (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: International symposium on augmented reality, pp 111–119" href="/article/10.1007/s10055-013-0228-7#ref-CR7" id="ref-link-section-d48026e402">2000</a>). Among the many improved versions proposed, ARTag, for example, can recognize 2002 different markers (Fiala <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Fiala M (2005) ARTag, a fiducial marker system using digital techniques. In: Conference on computer vision and pattern recognition, pp 590–596" href="/article/10.1007/s10055-013-0228-7#ref-CR3" id="ref-link-section-d48026e406">2005</a>).</p><p>In parallel, feature point methods grew powerful enough to achieve wide-baseline matching. The most representative of them is probably Lowe’s scale invariant feature transform (SIFT) (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comp Vis 20(2):91–110" href="/article/10.1007/s10055-013-0228-7#ref-CR10" id="ref-link-section-d48026e412">2004</a>). Establishing correspondences between views with strong differences in pose and illumination addresses many issues, including tracking initialization. Approaches such as SIFT aim at providing features robust to viewpoint changes, from a single image. Lepetit et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Conference on computer vision and pattern recognition, Washington, DC" href="/article/10.1007/s10055-013-0228-7#ref-CR9" id="ref-link-section-d48026e415">2004</a>) proposed a real-time tracking by detection method that learns the changing appearance of keypoints from synthetically generated views. Ozuysal et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ozuysal M, Lepetit V, Fleuret F, Fua P (2006) Feature harvesting for tracking-by-detection. In: European conference on computer vision, Graz" href="/article/10.1007/s10055-013-0228-7#ref-CR15" id="ref-link-section-d48026e418">2006</a>) rely on off-line incremental tracking on a training sequence to model the complex appearance changes of a partially transparent object. Taylor et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Taylor S, Rosten E, Drummond T (2009) Robust feature matching in 2.3μs. In: IEEE CVPR workshop on feature detectors and descriptors: the state of the art and beyond" href="/article/10.1007/s10055-013-0228-7#ref-CR23" id="ref-link-section-d48026e421">2009</a>) consider a lighter descriptor and compensate its sensitivity to viewpoint changes by modeling a target object with a large number of features, extracted from an artificial set of views. Uchiyama and Saito (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: International symposium on mixed and augmented reality, pp 95–98" href="/article/10.1007/s10055-013-0228-7#ref-CR24" id="ref-link-section-d48026e424">2009</a>) use a descriptor specifically designed for text documents and learn both new keypoints and changing descriptors online. Our method also updates the models as new views are observed.</p><p>The success of wide-baseline feature matching also opened the way to large-scale image retrieval (Sivic and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In: Proceedings of the international conference on computer vision, vol 2, pp 1470–1477" href="/article/10.1007/s10055-013-0228-7#ref-CR22" id="ref-link-section-d48026e430">2003</a>; Obdržálek and Matas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Obdržálek Š, Matas J (2005) Sub-linear indexing for large scale object recognition. In: British machine vision conference" href="/article/10.1007/s10055-013-0228-7#ref-CR14" id="ref-link-section-d48026e433">2005</a>). Using vector quantization of descriptors, recent approaches effectively retrieve images from databases containing more than a million of images (Nister and Stewenius <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR13" id="ref-link-section-d48026e436">2006</a>; Philbin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2007) Object retrieval with large vocabularies and fast spatial matching. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR18" id="ref-link-section-d48026e439">2007</a>; Jégou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Jégou H, Douze M, Schmid C (2008) Hamming embedding and weak geometric consistency for large scale image search. In: European conference on computer vision, LNCS, vol 1, pp 304–317" href="/article/10.1007/s10055-013-0228-7#ref-CR6" id="ref-link-section-d48026e442">2008</a>). Interestingly, Philbin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost in quantization: Improving particular object retrieval in large scale image databases. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR19" id="ref-link-section-d48026e446">2008</a>) note that quantizing several descriptions of the same physical point can be unstable. They tried to capture this instability by synthesizing affine and noisy deformations of image patches, inspired by a classification-based approach to wide-baseline matching (Lepetit et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Conference on computer vision and pattern recognition, Washington, DC" href="/article/10.1007/s10055-013-0228-7#ref-CR9" id="ref-link-section-d48026e449">2004</a>). However, these attempts did not improve recognition performance, maybe because of an inappropriate choice of image perturbation, as the authors explain. Following a similar goal, our method observes descriptors tracked over several frames to mitigate the effect of their variability.</p><p>Feature matching has also been used to detect and track multiple 3D objects (Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Park Y, Lepetit V, Woo W (2008) Multiple 3d object tracking for augmented reality. In: International symposium on mixed and augmented reality, pp 117–120" href="/article/10.1007/s10055-013-0228-7#ref-CR17" id="ref-link-section-d48026e455">2008</a>). Recently, Wagner et al. proposed a method implemented on mobile phones that guarantees a minimum frame rate (Wagner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wagner D, Schmalstieg D, Bischof H (2009) Multiple target detection and tracking with guaranteed framerates on mobile phones. In: International symposium on mixed and augmented reality, Orlando" href="/article/10.1007/s10055-013-0228-7#ref-CR26" id="ref-link-section-d48026e458">2009</a>). However, these methods are restricted to a limited number of objects, typically under 10. We focus on scaling the database size, while keeping a detection delay compatible with tracking tasks.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Method</h2><div class="c-article-section__content" id="Sec3-content"><p>The primitives upon which our method is built are feature detection and tracking. Keypoints
<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> are detected at each frame, and matched between consecutive frames, using normalized cross-correlation (NCC). When such a simple NCC fails to find correspondences, the Lukas-Kanade (KLT) algorithm tracks lost features (Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-kanade 20 years on: a unifying framework. Int J Comp Vis 56(3):221–255&#xA;" href="/article/10.1007/s10055-013-0228-7#ref-CR1" id="ref-link-section-d48026e484">2004</a>). As a result, we obtain stable tracks of features, and a patch is extracted at every frame, as illustrated by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig2">2</a>a.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Computing <i>visual words</i> by tracking features. <b>a</b> Features are tracked across frames, and patches are collected. <b>b</b> Patches pass through a tree and distribute over its leaves, forming a histogram. <b>c</b> The histogram is compared to the ones observed during the training phase. We use them as <i>visual words</i> and visualize them with<i> geometric shapes</i>. In this example, the histogram matches a word represented with a<i> star</i>. <b>d</b> Some of the <i>visual words</i> detected on an input image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>During a first training phase, we collect many features from a video stream. We cluster their descriptors with a recursive <i>K</i>-mean tree, as suggested by Nister and Stewenius (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR13" id="ref-link-section-d48026e543">2006</a>) and as depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig2">2</a>b. This allows us to summarize a descriptor as the leaf it corresponds to or as an integer, since leaves are numbered.</p><p>In a second training phase, we collect tracks of features. The tree quantizes each descriptor, turning the feature tracks into leaf tracks. We then compute for each track a histogram of leaves. Because the training sequence contains effects such as motion blur, perspective distortion, or moving specular reflections, the collected histograms capture the descriptor’s instability, including the quantization effects of the tree, in presence of such hazard. The set of collected histograms forms a dictionary, or a <i>visual vocabulary</i>. To reduce its ambiguity, similar histograms are recursively merged until ambiguity reaches an acceptable level.</p><p>At run-time, the whole history of each tracked feature is summarized by the most relevant histogram found in the dictionary. Each descriptor passes through the tree and ends up in a leaf, forming a histogram of leaves, as depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig2">2</a>b. We then simply search for the most similar trained histogram (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig2">2</a>c). The advantage of this technique is double: It allows the algorithm to exploit both the variability of descriptors, which is usually viewed as a problem, and the large amount of data collected by tracking points over multiple frames. Figure  <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig2">2</a>d shows an image and some of its detected features, with their associated histograms, represented as geometric shapes.</p><p>Indexing and retrieval follow a standard term frequency-inverse document frequency (TF-IDF) weighting scheme on a bag of word model. In simpler words, a table maps dictionary entries to indexed objects. For retrieval, the table is looked up to collect all the objects on which the visual words of the query appear. The candidates are then ranked using an appropriate criterion.</p><p>The few best results are selected as candidates for geometric verification. If an object is successfully detected, matches are propagated to the next frame using motion flow estimation. Currently, tracked objects are automatically appended to the list of candidates for next frame’s geometric verification stage. Because we eliminate outliers during detection and propagate matches, few outliers remain on the next frame. Geometric verification therefore becomes simpler.</p><p>The successfully verified objects then pass the dynamic learning stage in which observed but unmatched features are integrated to the object model. Our algorithm takes care of not integrating features that do not actually belong to the object, and stops adding keypoints when enough are known. As a result, keypoints appearing on new views contribute to tracking and detection, making them robust and reliable.</p><p>The remaining of this section details our system’s components.</p><h3 class="c-article__sub-heading" id="Sec4">Low-level: repeatable sparse motion flow</h3><p>Our method relies on a particular type of motion flow estimation in which the tracked points are detected in a repeatable way. It means that a point that has previously been tracked is supposed to be tracked again when viewed from another angle. Stability and repeatability are the main goals of well-known feature detectors such as SIFT, SURF, or FAST (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comp Vis 20(2):91–110" href="/article/10.1007/s10055-013-0228-7#ref-CR10" id="ref-link-section-d48026e583">2004</a>; Bay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bay H, Tuytelaars T, Gool LV (2006) SURF: speeded up robust features. In: European conference on computer vision" href="/article/10.1007/s10055-013-0228-7#ref-CR2" id="ref-link-section-d48026e586">2006</a>; Rosten and Drummond <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rosten E, Drummond T (2006) Machine learning for high-speed corner detection. In: European conference on computer vision" href="/article/10.1007/s10055-013-0228-7#ref-CR20" id="ref-link-section-d48026e589">2006</a>). In our experiments, we used a GPU implementation of SIFT (Wu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wu C (2008) A GPU implementation of David Lowe’s scale invariant feature transform" href="/article/10.1007/s10055-013-0228-7#ref-CR27" id="ref-link-section-d48026e592">2008</a>).</p><p>To compute the motion flow from frame to frame, we first detect the keypoints in both frames. We then compare their local neighborhoods using NCC. This quickly provides the flow of most features. However, when the feature detector detects a point on the first frame but fails to detect it on the following one, the KLT algorithm searches its new location. The decision to turn to KLT is taken when the best found correlation drops below a threshold.</p><p>The result of this process is a set of stable tracks. The KLT tracking mitigates the feature detector’s failures, while the feature detector ensures repeatability and prevents the KLT tracker from drifting. In our experiments, this approach can track efficiently hundreds of points over hundreds of frames.</p><p>The choice of using NCC rather than SIFT descriptor is motivated because the invariance provided by the SIFT is not necessary for computing sparse motion flow. NCC gives reliable result and is consistent with the KLT tracker. The KLT tracker is well adapted to track SIFT features, because a local maximum in the difference of Gaussian implies low autocorrelation in all directions, which is precisely what KLT relies on.</p><h3 class="c-article__sub-heading" id="Sec5">Describing tracked keypoints</h3><p>At this stage, our goal is to index target objects and to retrieve the ones visible on the current frame, relying on stable feature tracks. To do so, we aim at constructing a dictionary mapping feature tracks to indexed objects. Our approach has two stages of clustering: at descriptor level and at track level.</p><h3 class="c-article__sub-heading" id="Sec6">
                  <i>K</i>-mean tree</h3><p>The descriptors extracted during the training sequence are clustered using a recursive <i>K</i>-mean, as proposed by Nister and Stewenius (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR13" id="ref-link-section-d48026e624">2006</a>). All descriptors are first clustered by a <i>K</i>-mean algorithm, with <i>K</i> = 4, and using the <i>L</i>
                  <sup>2</sup> norm as a distance measure. The four resulting subsets are then recursively clustered in the same way, in turn. Recursion stops either at depth 8 or when fewer than 64 descriptors remain.</p><h3 class="c-article__sub-heading" id="Sec7">Learning and exploiting feature variability</h3><p>Our system tracks points and assigns them to a leaf of the recursive <i>K</i>-mean tree, making it possible to observe groups of leaves showing the same physical point. During the second training stage, such groups are collected from a video sequence and accumulated into histograms. Such histograms can capture the descriptor’s nonlinear variability that might be caused by viewpoint, illumination, or noise effects.</p><p>Straightforward usage of these histograms as index keys is not possible due to the many redundant elements collected during training. We address this ambiguity issue with a second clustering stage. If two histograms are too difficult to discriminate, they are merged, creating a new, larger histogram. In practice, we compute the dot product between two normalized histograms and use agglomerative clustering. We recursively merge the histogram pair with the highest normalized dot product and stop when it is lower than a threshold (typically 0.1, see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec4">4</a>). Merging two histograms <i>a</i> and <i>b</i> into a new histogram <i>x</i> gives: <i>x</i>(<i>l</i>) = <i>a</i>(<i>l</i>) + <i>b</i>(<i>l</i>), where <i>a</i>(<i>l</i>) is the number of times leaf <i>l</i> appears in <i>a</i>. At the end of this process, the remaining histograms form a stable visual vocabulary.</p><p>Although these histograms seem very large, due to the large number of leafs on the tree, most entries are null. Storing them in a sparse manner is quite compact.</p><h3 class="c-article__sub-heading" id="Sec8">Feature tracks as index keys</h3><p>We build an inverted table mapping leaves of the recursive <i>K</i>-mean tree to trained histograms. Given one or several leaves observed when tracking a point, it becomes possible to efficiently fetch the most similar learned histogram. By doing so, our system assigns an index key to a <i>point track</i>, as opposed to a single feature. The following equation defines the score of the histogram <i>h</i> for the track <i>t</i>:
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ s(t,h)=\frac{1}{\sum_{l}t(l)}\frac{1}{\sum_{l}h(l)} \sum_{l}t(l)h(l){\rm idf}(l), $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <i>t</i>(<i>l</i>) (respectively <i>h</i>(<i>l</i>)) is the number of features assigned to leaf <i>l</i> in the track <i>t</i> (respectively in the trained histogram <i>h</i>). The term <span class="mathjax-tex">\({\rm idf}(l)=-\log(\frac{f(l)}{F}),\)</span> where <i>f</i>(<i>l</i>) denotes the number of trained histograms involving leaf <i>l</i>, and <i>F</i> the total number of trained histograms. This score gives more importance to rare and discriminative leaves and decreases the weight of frequent ones. If <i>f</i>(<i>l</i>) = 0, it means that the observed feature does not match trained data. In this case, the track is ignored.</p><p>The complexity of directly computing this score grows linearly with the track size. Therefore, we remember for each track the scores of potential matching histograms. When a new frame is available, the scores are updated regarding only the newly observed leaf. This incremental approach allows our system to efficiently exploit long tracks.</p><p>Soft visual word assignment, as suggested by Philbin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost in quantization: Improving particular object retrieval in large scale image databases. In: Conference on computer vision and pattern recognition" href="/article/10.1007/s10055-013-0228-7#ref-CR19" id="ref-link-section-d48026e984">2008</a>), can easily be achieved by considering not only the histogram with the highest score, but also the ones at least 90 % as good.</p><h3 class="c-article__sub-heading" id="Sec9">Object detection</h3><p>To detect target objects entering the field of view, the database is queried with all point tracks visible on the current frame. As explained, each point track is assigned to a visual word. The histogram of the observed <i>visual words</i> in a frame forms a query <i>q</i>. The score of stored object <i>d</i> for the query <i>q</i> is as follows:
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ s(q,d)=\frac{1}{\sum_{w}q(w)}\frac{1}{\sum_{w}d(w)}\sum_{w}q(w)d(w){\rm idf}(w), $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <i>q</i>(<i>w</i>) (respectively <i>d</i>(<i>w</i>)) is the number of words <i>w</i> in <i>q</i> (respectively in <i>d</i>), and idf(<i>w</i>) the negative log of the proportion of the stored frames that contain the visual word <i>w</i>. The few objects with the best scores are kept as candidates for geometric verification.</p><p>From a computational point of view, we reduced the complexity of repeating this algorithm at each frame using incremental queries. We keep the scores of objects found in the previous frame and update them with features that appeared or disappeared. The complexity of the query therefore depends on the number of feature addition or subtraction rather than the total number of features present on the current view.</p><h3 class="c-article__sub-heading" id="Sec10">Geometric verification</h3><p>Our algorithm builds a list of candidates for geometric verification. It is initialized with the set of objects successfully tracked on the previous frame. Then, the list is extended with at most three other candidates selected by their query score (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0228-7#Equ2">2</a>).</p><p>For each object in the list, our system tries to match object and frame features in two different ways: based on the tracks index values (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec8">3.5</a>), or propagated from previous frame if the candidate has been detected successfully. In the latter case, propagated correspondences contain usually less outliers than the ones resulting from wide-baseline matching. Because the ratio of outliers has an impact on the required number of RANSAC iterations and computation speed, our system uses for geometric verification at most 20 % of new correspondences. In practice, our system first gathers the correspondences from last frame and then finds at most 20 % more matches with the candidate target. When verifying an object that has not been matched on previous frame, only the noisier wide-baseline correspondences are used, requiring more RANSAC iterations.</p><p>Once the set of potential correspondences is created, the geometry consistency is verified. Each object has a geometric model, in our implementation either homography or epipolar constraints. For detection, the RANSAC (Fischler and Bolles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24(6):381–395" href="/article/10.1007/s10055-013-0228-7#ref-CR4" id="ref-link-section-d48026e1206">1981</a>) algorithm handles the potentially high outlier rate. During tracking, the outlier rate is controlled, and the least median of squares (LMedS) algorithm can optionally replace RANSAC.</p><h3 class="c-article__sub-heading" id="Sec11">Dynamic model update</h3><p>Once an object is detected, it can be tracked on views that may strongly differ from the reference one recorded in the database. In that case, some keypoints disappear and others appear. In that case, our system incrementally improves the model by adding appearing features. For this task, the difficulty is to determine whether a feature actually belongs to the object or not.</p><p>Our algorithm first eliminates geometrically inconsistent keypoints. It scans all the features matched over more than a fixed amount of frames (typically 5). It also skips keypoints assigned by the RANSAC processes to an object. To determine to which object a keypoint could be attached, the nearest keypoint already assigned to an object in the current frame is considered. If its distance to the candidate keypoint exceeds a threshold (typically 30 pixels), the point is rejected: It probably belongs to the background. Otherwise, the geometric consistency between the corresponding object and the considered keypoint is verified over all the previous frame in which the point has been tracked. To do so amounts to verifying that the inverse transformation of the keypoint by the homography computed for the object remains consistent over the frames. In our implementation, we check that the point on previous frames, once backprojected, falls within 3 pixels of current frame’s backprojection.</p><p>To prevent the number of keypoints associated to objects from growing too large, we limit it to a constant (typically 3000). Before adding a new keypoint to an object, the system draws a random number between 0 and the limit. If that number is below the current point count, the keypoint is skipped. This approach gives a logarithmic behavior to the system: The rate of feature addition is high if the object is not yet well modeled and slows down as more and more features are integrated. Other strategies to do so could consider the viewpoint to encourage a uniform distribution of learned features over viewpoints or avoiding adding keypoints in regions already well covered. We consider that the optimal strategy depends on the final application. In our case, this simple method is sufficient.</p><p>Dynamically, updating the database breaks the incremental computation of the objects scores described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec9">3.6</a>. Score computation starts from scratch everytime the database is updated. The impact of this effect is minor compared to the benefit of remembering new views for later detection.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Results</h2><div class="c-article-section__content" id="Sec12-content"><p>We present in this section the experiments we conducted to evaluate our system. We focused on retrieval and tracking capabilities. We finally present a toy application demonstrating that our system fits augmented reality requirements.</p><h3 class="c-article__sub-heading" id="Sec13">Retrieval evaluation</h3><p>The goal of this experiment is to evaluate our system’s ability to retrieve objects visible in a video stream. To do so, we ignore the geometric verification stage and concentrate on the best ranked candidate returned by the query.</p><p>We differentiate two scenarios. If the time to enter the objects in the database is not an issue, it is possible to include their views in the two clustering stages. This yields good performance, at the cost of a longer and less flexible procedure to add the objects to the database. Because some applications cannot tolerate such a long process, we tested our method with and without including the target objects in the learning data. We call the tested scenarios trained and untrained.</p><p>Evaluation was conducted on video sequences showing 13 test objects, three of which are depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig3">3</a>. The image resolution used is 640 × 480. The first sequence does not show the test objects. It is used to build the visual vocabulary. The second sequence shows the objects in turn. Thirteen objects are manually added to the database. The third sequence is a test sequence. It also shows the objects, at most one at a time. Ground truth reference is obtained by manually noting the object visible on each frame. To test a scenario, we use every frame of the test sequence as a query to the database. We count the number of frames in which the first result matches the ground truth.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Pairs of query (<i>left</i>) and retrieved (<i>right</i>) images that our approach made possible to retrieve, without any geometric verification. The direct tree indexing approach failed to handle these frames, as opposed to our method. In the untrained scenario, our method improves the number of successfully retrieved frames of about 5 %, as detailed by Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec13">4.1</a> and Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0228-7#Tab1">1</a>. These three pairs are selected among these 5 %. In the case of the first row, motion blur causes SIFT detector instability. In the second case, specular reflections alter the object appearance. In the third case, the perspective distortion and the moving specular reflection perturb retrieval if no tracking information is used</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Changing the threshold that stops histogram merging during the second clustering phase. If the threshold is chosen above 0.06, our method improves retrieval rate by 4–6 %. This graph was obtained in the untrained scenario, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec13">4.1</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In the trained scenario, the visual vocabulary is built using both the first and the second sequences. It implies that the resulting <i>K</i>-mean tree is more complete. The untrained scenario does not use the second sequence for building the visual vocabulary. In this case, the system has to deal with previously unseen features.</p><p>Each scenario is tested twice. Once using directly the leaves of the <i>K</i>-mean tree as index keys, and once using our approach.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0228-7#Tab1">1</a> presents the evaluation results. In both scenarios, our method improves retrieval. The results obtained in the trained scenario show that even when quantization errors are avoided by using the target objects to create the visual vocabulary, our method can still learn some remaining variability and provide a performance gain of about 4.5 %.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 To evaluate performance, we count the number of frames correctly retrieved for a test sequence.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0228-7/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec14">Tracking objects</h3><p>For this experiment, we printed 325 photographs. We entered the pictures one by one into the system, by capturing them on a uniform background with a handheld video camera. It is important to note that adding a new picture to the system is perceptually instantaneous, because the <i>K</i>-mean tree and histogram clusters remain constant. The user points the target to the camera, clicks, and tracking can start.</p><p>Based on the populated database, the system is able to recognize and track randomly chosen pictures. The recognition delay is short, typically 2–3 frames. Once detected, the photographs are tracked, subject to neither drift nor jittering, as depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Tracking photographs on a desk. The frames are selected from a test sequence in which the user moves a camera above a desk on which lies several tens of photographs. The system recognizes and track the pictures when they enter the field of view. The <i>white grid</i> represents the detected homography. The <i>red marks</i> show the newly established correspondences and the <i>green marks</i> the ones propagated from previous frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>During the training stage, we transformed 125 pictures in their digital form with random homographies to generate synthetic views, out of which about 5 million features were extracted. We recursively applied <i>K</i>-mean clustering with <i>K</i> = 4, stopping at a maximum depth of 8 or when less than 32 descriptors remained in a branch. The resulting tree has 85,434 nodes, 63,955 of which are leafs. During the second training phase, 655,970 histograms were extracted from new synthesized views. The agglomerative clustering process produced 39,749 histograms.</p><p>The tree and cluster set produced during training allow our system to efficiently establish correspondences between an input view and the objects stored in the database. It is interesting to observe that the system can deal with objects that have not been used for training. We verified this behavior by populating the database with 200 unseen objects, in addition to the 125 ones used for training. Our system kept its performance and could successfully detect and track almost all of the 325 targets, except a few.</p><p>Out of 325 pictures, the system fails to detect only 10: 6 among the learned 125 image set and 4 among the 200 other ones. A few of these pathological pictures are illustrated by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Cases of failure. The system fails to handle these images, due to the specific nature of the texture (<b>a</b>), low contrast due to atmospheric effects (<b>b</b> and <b>c</b>), and poorly discriminative texture (<b>b</b> and <b>c</b>). In total, only 10 pictures out of the 325 ones cannot be detected effectively. Half of them are not recognized at all, while the three others are detected only on unoccluded, sharp, and frontal views</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>When several pictures look similar, the system might be confused. For example, we took two pictures of the same building, taken from a slightly different points of view. In this case, the system tends to recognize the building rather than the pictures themselves. At detection, it sometimes mixes up both pictures. The tracking is stable in the sense that, once the system selected one picture, it will stick to that choice as long as the target can be tracked. This behavior is acceptable for augmented reality applications.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig7">7</a> depicts the ambiguity of keypoints. The histogram shows that, within our database, most of the keypoints have a quantized appearance that occurs only a few times. Therefore, they provide a strong support to the retrieval process. However, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig7">7</a> also shows that a few descriptors appear very often. These points are less discriminative but can still bring information for registration.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>This <i>histogram</i> shows the ambiguity of the index dictionary. The <i>horizontal axis</i> shows the number of occurrences. The <i>vertical one </i>shows the number of index keys. For example, 11213 features have their own index key. The most ambiguous feature appears at 200 places. This figure clearly shows that the indexing scheme is discriminative because most features are assigned to a unique index key, and there is only a small number of ambiguous features that appear often</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec15">Dynamic update evaluation</h3><p>Dynamically inserting new keypoints in the database as they are discovered under new viewpoints allows the system to match more features while detecting the target in a similar viewpoint. To evaluate our system’s robustness to viewpoint changes, and to quantify the improvement given by the dynamic update, we conducted the following experiment. We first randomly picked a picture that we mounted on the rotating support depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig9">9</a>. We then measured the number of matches validated by the RANSAC process in four different cases: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Tracking, prior to learning;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Detection, prior to learning;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Tracking with dynamic learning enabled;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Detection, after learning.</p>
                      
                    </li>
                  </ol>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>In these frames selected from a longer sequence, our system automatically augments the picture with a virtual hole through which a hand passes. The virtual hand appears stable on the moving photograph, despite illumination changes, partial occlusion, camera defocus, and specular reflection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Some of the images used for the experiment described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec15">4.3</a>. The target is attached to a rotating support. The rotation angles are, from<i> left</i> to<i> right</i>, 0°, 30°, 60°, and 75°. These angles are represented on the horizontal axes of Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig10">10</a>
                        </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In the case of tracking, we slowly rotate the support starting from 0° to 90°. Every 15°, we count the number of correct matches. In the case of detection, we orient the support, completely occlude the object to break tracking, show the object again, and inspect the matches. Tracking with dynamic learning enabled enriches the database with the keypoints visible on new views. Repeating the detection experiment after that learning process yields much better result because the model to match with is more complete.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig10">10</a> shows the result of this experiment conducted on four different target pictures. When no dynamic learning is used, the tracking and detection curves start from the same point. Tracking is then more performant, as expected. The number of matches for 0° varies from one picture to the other due to their different appearance. Picture #3 is clearly the most textured one.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>These <i>charts</i> show the number of matches as a function of rotation angle, as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0228-7#Sec15">4.3</a>. The <i>four charts</i> correspond to <i>four different pictures</i>, the last of which is visible on Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig9">9</a>. These <i>four cases</i> show that the dynamic learning process significantly improves the system’s detection abilities</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>While tracking and learning, many points are added to the model, yielding a high number of matches. However, some of these matches might not be reliable. The keypoint could have been detected outside of the object, or it might have been created by a specular reflection. The detection after learning is more relevant, because only the features that can actually be matched are counted. The number of matches after learning is consistently and significantly higher than before learning. By automatically completing the target representation, dynamic learning allows the system to recognize the target in more difficult situation. In the case of picture 1, straight detection fails as early as 30°. After updating the model, detection is still reliable at 60°.</p><h3 class="c-article__sub-heading" id="Sec16">Application to augmented reality</h3><p>To demonstrate our system’s suitability to AR applications, we used it to augment pictures with virtual drawings. The database of target objects contains the 325 photographs mentioned in the previous section. When the system recognizes a known picture, it overlays it with its virtual counterpart warped with the appropriate homography.</p><p>As depicted by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig8">8</a>, our method’s stable tracking yields convincing augmented reality, despite hazards such as viewpoint changes, illumination changes, partial occlusion, camera defocus, and specular reflection. The frames of Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig8">8</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig11">11</a> were produced directly and in real time by our system fed with a live video stream (except cropping).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0228-7/MediaObjects/10055_2013_228_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Augmenting multiple objects simultaneously. Our system retrieves, tracks, and augments the pictures lying on the desk. On the<i> first row</i>, all the objects are augmented with the same virtual content. On the<i> second row</i>, the virtual elements vary from object to object. In these examples, 3–6 pictures are tracked and augmented simultaneously</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0228-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>As illustrated by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0228-7#Fig11">11</a>, when several photographs appear in the field of view, our system augments them as long as they appear large enough on the input image. Since an homography is computed for each target picture, the system can augment them even if they move independently.</p><p>The frame rate of our system is typically 6–8 frames per second. The computationally heaviest component in our implementation is the SIFT feature detector, despite its implementation on the GPU.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusion</h2><div class="c-article-section__content" id="Sec17-content"><p>In this paper, we presented an image retrieval approach to multiple object tracking. We demonstrated its effectiveness and scalability by running experiments on more than 300 target objects. Our system is user friendly because it is responsive, fully automated, and reliable. For example, augmenting a new object simply amounts to pointing the camera at it and clicking. The augmentation starts immediately, and the internal object representation is automatically improved as new views become available. Detection for tracking initialization is 100 % automatic and has a low delay. Our system can process live video streams and is robust to partial occlusion, viewpoint changes, illumination effects, and other hazards.</p><p>These properties make our approach ideal for augmented reality applications that overlay virtual elements on real objects. Possible applications include:
</p><ul class="u-list-style-bullet">
                  <li>
                    <p>animating pages of books,</p>
                  </li>
                  <li>
                    <p>tagging virtual messages on real walls,</p>
                  </li>
                  <li>
                    <p>virtually annotating objects for maintenance tasks,</p>
                  </li>
                  <li>
                    <p>augmenting card games with virtual scenes,</p>
                  </li>
                  <li>
                    <p>augmenting panorama views with landmark names.</p>
                  </li>
                </ul>
              <p>To encourage further development of such AR applications, we provide the source code of our system in a packaged named Polyora.<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup>
              </p><p>We demonstrated the validity of our approach with planar objects. We believe it could be quite adapted to handle more complex shapes, because of its multi-stage learning process, which could handle the varying appearance of features lying on nonplanar objects. In future work, we plan to replace the homography by a full 3-D transformation to extend the proposed system to any type of textured object. We also aim to reduce our system’s dependency on texture. Currently, only very textured objects can be detected easily. Taking into account the geometric relationship of keypoints could extend indexing and retrieval to printed text or uniform objects with a specific 3-D shape, such as a chair or a tripod.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>In this text, we define a <i>keypoint</i> as a location of interest on an image, a <i>descriptor</i> as a vector describing a keypoint neighborhood, and a <i>feature</i> as both a keypoint and its descriptor.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>
                    <a href="https://github.com/jpilet/polyora">https://github.com/jpilet/polyora</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Baker, I. Matthews, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Baker S, Matthews I (2004) Lucas-kanade 20 years on: a unifying framework. Int J Comp Vis 56(3):221–255&#xA;" /><p class="c-article-references__text" id="ref-CR1">Baker S, Matthews I (2004) Lucas-kanade 20 years on: a unifying framework. Int J Comp Vis 56(3):221–255
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lucas-kanade%2020%20years%20on%3A%20a%20unifying%20framework&amp;journal=Int%20J%20Comp%20Vis&amp;volume=56&amp;issue=3&amp;pages=221-255&amp;publication_year=2004&amp;author=Baker%2CS&amp;author=Matthews%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bay H, Tuytelaars T, Gool LV (2006) SURF: speeded up robust features. In: European conference on computer visi" /><p class="c-article-references__text" id="ref-CR2">Bay H, Tuytelaars T, Gool LV (2006) SURF: speeded up robust features. In: European conference on computer vision</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fiala M (2005) ARTag, a fiducial marker system using digital techniques. In: Conference on computer vision and" /><p class="c-article-references__text" id="ref-CR3">Fiala M (2005) ARTag, a fiducial marker system using digital techniques. In: Conference on computer vision and pattern recognition, pp 590–596</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Fischler, R. Bolles, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image a" /><p class="c-article-references__text" id="ref-CR4">Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24(6):381–395</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F358669.358692" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=618158" aria-label="View reference 4 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20sample%20consensus%3A%20a%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography&amp;journal=Commun%20ACM&amp;volume=24&amp;issue=6&amp;pages=381-395&amp;publication_year=1981&amp;author=Fischler%2CM&amp;author=Bolles%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harris C, Stephens M (1988) A combined corner and edge detector. In: Fourth alvey vision conference, Mancheste" /><p class="c-article-references__text" id="ref-CR5">Harris C, Stephens M (1988) A combined corner and edge detector. In: Fourth alvey vision conference, Manchester</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jégou H, Douze M, Schmid C (2008) Hamming embedding and weak geometric consistency for large scale image searc" /><p class="c-article-references__text" id="ref-CR6">Jégou H, Douze M, Schmid C (2008) Hamming embedding and weak geometric consistency for large scale image search. In: European conference on computer vision, LNCS, vol 1, pp 304–317</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top A" /><p class="c-article-references__text" id="ref-CR7">Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: International symposium on augmented reality, pp 111–119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Lepetit, P. Fua, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Lepetit V, Fua P (2005) Monocular model-based 3d tracking of rigid objects: a survey. Found Trends Comp Graph " /><p class="c-article-references__text" id="ref-CR8">Lepetit V, Fua P (2005) Monocular model-based 3d tracking of rigid objects: a survey. Found Trends Comp Graph Vis 1(1):1–89</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1561%2F0600000001" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Monocular%20model-based%203d%20tracking%20of%20rigid%20objects%3A%20a%20survey&amp;journal=Found%20Trends%20Comp%20Graph%20Vis&amp;volume=1&amp;issue=1&amp;pages=1-89&amp;publication_year=2005&amp;author=Lepetit%2CV&amp;author=Fua%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose es" /><p class="c-article-references__text" id="ref-CR9">Lepetit V, Pilet J, Fua P (2004) Point matching as a classification problem for fast and robust object pose estimation. In: Conference on computer vision and pattern recognition, Washington, DC</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comp Vis 20(2):91–110" /><p class="c-article-references__text" id="ref-CR10">Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comp Vis 20(2):91–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints&amp;journal=Int%20J%20Comp%20Vis&amp;volume=20&amp;issue=2&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lucas B, Kanade T (1981) An Iterative Image Registration Technique with an Application to Stereo Vision. In: I" /><p class="c-article-references__text" id="ref-CR11">Lucas B, Kanade T (1981) An Iterative Image Registration Technique with an Application to Stereo Vision. In: International joint conference on artificial intelligence, pp 674–679</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Matas J, Chum O, Martin U, Pajdla T (2002) Robust wide baseline stereo from maximally stable extremal regions." /><p class="c-article-references__text" id="ref-CR12">Matas J, Chum O, Martin U, Pajdla T (2002) Robust wide baseline stereo from maximally stable extremal regions. In: British machine vision conference, London, pp 384–393</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision an" /><p class="c-article-references__text" id="ref-CR13">Nister D, Stewenius H (2006) Scalable recognition with a vocabulary tree. In: Conference on computer vision and pattern recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Obdržálek Š, Matas J (2005) Sub-linear indexing for large scale object recognition. In: British machine vision" /><p class="c-article-references__text" id="ref-CR14">Obdržálek Š, Matas J (2005) Sub-linear indexing for large scale object recognition. In: British machine vision conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ozuysal M, Lepetit V, Fleuret F, Fua P (2006) Feature harvesting for tracking-by-detection. In: European confe" /><p class="c-article-references__text" id="ref-CR15">Ozuysal M, Lepetit V, Fleuret F, Fua P (2006) Feature harvesting for tracking-by-detection. In: European conference on computer vision, Graz</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ozuysal M, Fua P, Lepetit V (2007) Fast keypoint recognition in ten lines of code. In: Conference on computer " /><p class="c-article-references__text" id="ref-CR16">Ozuysal M, Fua P, Lepetit V (2007) Fast keypoint recognition in ten lines of code. In: Conference on computer vision and pattern recognition, Minneapolis, MI</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park Y, Lepetit V, Woo W (2008) Multiple 3d object tracking for augmented reality. In: International symposium" /><p class="c-article-references__text" id="ref-CR17">Park Y, Lepetit V, Woo W (2008) Multiple 3d object tracking for augmented reality. In: International symposium on mixed and augmented reality, pp 117–120</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2007) Object retrieval with large vocabularies and fast spat" /><p class="c-article-references__text" id="ref-CR18">Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2007) Object retrieval with large vocabularies and fast spatial matching. In: Conference on computer vision and pattern recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost in quantization: Improving particular object retr" /><p class="c-article-references__text" id="ref-CR19">Philbin J, Chum O, Isard M, Sivic J, Zisserman A (2008) Lost in quantization: Improving particular object retrieval in large scale image databases. In: Conference on computer vision and pattern recognition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosten E, Drummond T (2006) Machine learning for high-speed corner detection. In: European conference on compu" /><p class="c-article-references__text" id="ref-CR20">Rosten E, Drummond T (2006) Machine learning for high-speed corner detection. In: European conference on computer vision</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C (1994) Good features to track. In: Conference on computer vision and pattern recognition, Seat" /><p class="c-article-references__text" id="ref-CR21">Shi J, Tomasi C (1994) Good features to track. In: Conference on computer vision and pattern recognition, Seattle</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In: Proceedi" /><p class="c-article-references__text" id="ref-CR22">Sivic J, Zisserman A (2003) Video google: a text retrieval approach to object matching in videos. In: Proceedings of the international conference on computer vision, vol 2, pp 1470–1477</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taylor S, Rosten E, Drummond T (2009) Robust feature matching in 2.3μs. In: IEEE CVPR workshop on feature dete" /><p class="c-article-references__text" id="ref-CR23">Taylor S, Rosten E, Drummond T (2009) Robust feature matching in 2.3μs. In: IEEE CVPR workshop on feature detectors and descriptors: the state of the art and beyond</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In:" /><p class="c-article-references__text" id="ref-CR24">Uchiyama H, Saito H (2009) Augmenting text document by on-line learning of local arrangement of keypoints. In: International symposium on mixed and augmented reality, pp 95–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008) Pose tracking from natural features on mobil" /><p class="c-article-references__text" id="ref-CR25">Wagner D, Reitmayr G, Mulloni A, Drummond T, Schmalstieg D (2008) Pose tracking from natural features on mobile phones. In: International symposium on mixed and augmented reality, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Schmalstieg D, Bischof H (2009) Multiple target detection and tracking with guaranteed framerates on" /><p class="c-article-references__text" id="ref-CR26">Wagner D, Schmalstieg D, Bischof H (2009) Multiple target detection and tracking with guaranteed framerates on mobile phones. In: International symposium on mixed and augmented reality, Orlando</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu C (2008) A GPU implementation of David Lowe’s scale invariant feature transform" /><p class="c-article-references__text" id="ref-CR27">Wu C (2008) A GPU implementation of David Lowe’s scale invariant feature transform</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0228-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa, 223-8522, Japan</p><p class="c-article-author-affiliation__authors-list">Julien Pilet &amp; Hideo Saito</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Julien-Pilet"><span class="c-article-authors-search__title u-h3 js-search-name">Julien Pilet</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Julien+Pilet&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Julien+Pilet" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Julien+Pilet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Hideo-Saito"><span class="c-article-authors-search__title u-h3 js-search-name">Hideo Saito</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Hideo+Saito&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Hideo+Saito" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Hideo+Saito%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0228-7/email/correspondent/c1/new">Julien Pilet</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Dynamic%20learning%2C%20retrieval%2C%20and%20tracking%20to%20augment%20hundreds%20of%20photographs&amp;author=Julien%20Pilet%20et%20al&amp;contentID=10.1007%2Fs10055-013-0228-7&amp;publication=1359-4338&amp;publicationDate=2013-09-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Pilet, J., Saito, H. Dynamic learning, retrieval, and tracking to augment hundreds of photographs.
                    <i>Virtual Reality</i> <b>18, </b>89–100 (2014). https://doi.org/10.1007/s10055-013-0228-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0228-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-19">19 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-15">15 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-09-13">13 September 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06">June 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0228-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0228-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multiple object tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Image retrieval</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0228-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=228;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

