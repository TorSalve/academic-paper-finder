<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An experimental study of spatial sound usefulness in searching and nav"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were asked to find three objects hidden within..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An experimental study of spatial sound usefulness in searching and navigating through AR environments"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-10-20"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 The Author(s)"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were asked to find three objects hidden within no-sound and spatial sound AR environments. The experiment showed that the participants of the spatialized sound group performed faster and more efficiently than working in no-sound configuration. What is more, 3D sound was a valuable cue for navigation in AR environment. The collected data suggest that the use of spatial sound in AR environments can be a significant factor in searching and navigating for hidden objects within indoor AR scenes. To conduct the experiment, the CARE approach was applied, while its CARL language was extended with new elements responsible for controlling audio in 3D space."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-10-20"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="223"/>

    <meta name="prism.endingPage" content="233"/>

    <meta name="prism.copyright" content="2015 The Author(s)"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0274-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0274-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0274-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0274-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An experimental study of spatial sound usefulness in searching and navigating through AR environments"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2015/11"/>

    <meta name="citation_online_date" content="2015/10/20"/>

    <meta name="citation_firstpage" content="223"/>

    <meta name="citation_lastpage" content="233"/>

    <meta name="citation_article_type" content="S.I: Spatial Sound"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0274-4"/>

    <meta name="DOI" content="10.1007/s10055-015-0274-4"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0274-4"/>

    <meta name="description" content="This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were"/>

    <meta name="dc.creator" content="Dariusz Rumi&#324;ski"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Android (2015) Android Platform Home Page. 
                    http://www.android.com/
                    
                  
                        "/>

    <meta name="citation_reference" content="Apache (2015) Apache CXF Home Page. 
                    http://cxf.apache.org/
                    
                  
                        "/>

    <meta name="citation_reference" content="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtual Reality Annual International Symposium, Proceedings, IEEE 1998, pp 20&#8211;27. doi:
                    10.1109/VRAIS.1998.658418
                    
                  
                        "/>

    <meta name="citation_reference" content="Cohen M, Villegas J (2011) From whereware to whence- and whitherware: augmented audio reality for position-aware services. In: VR Innovation (ISVRI), 2011 IEEE International Symposium on, pp 273&#8211;280. doi:
                    10.1109/ISVRI.2011.5759650
                    
                  
                        "/>

    <meta name="citation_reference" content="Cohen M, Wenzel EM (1995) Virtual environments and advanced interface design. In: The Design of Multidimensional Sound Interfaces. Oxford University Press, Inc., New York, NY, USA, pp 291&#8211;346. 
                    http://dl.acm.org/citation.cfm?id=216164.216180
                    
                  
                        "/>

    <meta name="citation_reference" content="Floty&#324;ski J, Walczak K (2014) Conceptual knowledge-based modeling of interactive 3d content. Vis Comput. doi:
                    10.1007/s00371-014-1011-9
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Web Semant; citation_title=Rules and ontologies in support of real-time ubiquitous application; citation_author=M Hatala, R Wakkary, L Kalantari; citation_volume=3; citation_issue=1; citation_publication_date=2005; citation_pages=5-22; citation_doi=10.1016/j.websem.2005.05.004; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Ubiquitous Comput; citation_title=Ontrack: dynamically adapting music playback to support navigation; citation_author=M Jones, S Jones, G Bradley, N Warren, D Bainbridge, G Holmes; citation_volume=12; citation_issue=7; citation_publication_date=2008; citation_pages=513-525; citation_doi=10.1007/s00779-007-0155-2; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_title=Human factors research in audio augmented reality; citation_publication_date=2013; citation_id=CR9; citation_author=N Mariette; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Spatial sound for computer games and virtual reality; citation_publication_date=2011; citation_id=CR10; citation_author=D Murphy; citation_author=F Neff; citation_publisher=IGI Global"/>

    <meta name="citation_reference" content="Poeschl S, Wall K, Doering N (2013) Integration of spatial sound in immersive virtual environments an experimental study on effects of spatial sound on presence. In: Virtual Reality (VR), 2013 IEEE, pp 129&#8211;130. doi:
                    10.1109/VR.2013.6549396
                    
                  
                        "/>

    <meta name="citation_reference" content="Qualcomm (2015) Vuforia. 
                    https://www.qualcomm.com/products/vuforia
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Using augmented virtuality for remote collaboration; citation_author=H Regenbrecht, T Lum, P Kohler, C Ott, M Wagner, W Wilke, E Mueller; citation_volume=13; citation_issue=3; citation_publication_date=2004; citation_pages=338-354; citation_doi=10.1162/1054746041422334; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_title=Carl: a language for modelling contextual augmented reality environments; citation_inbook_title=Technological innovation for collective awareness systems; citation_publication_date=2014; citation_pages=183-190; citation_id=CR14; citation_author=D Rumi&#324;ski; citation_author=K Walczak; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Rumi&#324;ski D, Walczak K (2014a) Dynamic composition of interactive ar scenes with the carl language. In: The 5th International Conference on Information, Intelligence, Systems and Applications, IEEE, pp 329&#8211;334. doi:
                    10.1109/IISA.2014.6878808
                    
                  
                        "/>

    <meta name="citation_reference" content="Rumi&#324;ski D, Walczak K (2014b) Semantic contextual augmented reality environments. In: The 13th IEEE International Symp. on Mixed and Augmented Reality (ISMAR 2014), IEEE, pp 401&#8211;404. doi:
                    10.1109/ISMAR.2014.6948506
                    
                  
                        "/>

    <meta name="citation_reference" content="Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented reality environment. In: Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, ACM, New York, NY, USA, OZCHI &#8217;06, pp 111&#8211;118. doi:
                    10.1145/1228175.1228197
                    
                  
                        "/>

    <meta name="citation_reference" content="Spring (2015) Spring home page. 
                    http://www.spring.io/
                    
                  
                        "/>

    <meta name="citation_reference" content="Stampfl P (2003) Augmented reality disk jockey: Ar/dj. In: ACM SIGGRAPH 2003 Sketches and Amp; Applications, ACM, New York, NY, USA, SIGGRAPH &#8217;03, pp 1. doi:
                    10.1145/965400.965556
                    
                  
                        "/>

    <meta name="citation_reference" content="Villegas J, Cohen M (2010) Hrir&#160;: modulating range in headphone-reproduced spatial audio. In: Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, ACM, New York, NY, USA, VRCAI &#8217;10, pp 89&#8211;94. doi:
                    10.1145/1900179.1900198
                    
                  
                        "/>

    <meta name="citation_reference" content="Walczak K, Rumi&#324;ski D, Floty&#324;ski J (2014) Building contextual augmented reality environments with semantics. In: Virtual Systems Multimedia (VSMM), pp 353&#8211;361. doi:
                    10.1109/VSMM.2014.7136656
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=An experimental study on the role of 3d sound in augmented reality environment; citation_author=Z Zhou, AD Cheok, X Yang, Y Qiu; citation_volume=16; citation_issue=6; citation_publication_date=2004; citation_pages=1043-1068; citation_doi=10.1016/j.intcom.2004.06.016; citation_id=CR22"/>

    <meta name="citation_author" content="Dariusz Rumi&#324;ski"/>

    <meta name="citation_author_email" content="ruminski@kti.ue.poznan.pl"/>

    <meta name="citation_author_institution" content="Department of Information Technology, Pozna&#324; University of Economics, Pozna&#324;, Poland"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0274-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0274-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An experimental study of spatial sound usefulness in searching and navigating through AR environments"/>
        <meta property="og:description" content="This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were asked to find three objects hidden within no-sound and spatial sound AR environments. The experiment showed that the participants of the spatialized sound group performed faster and more efficiently than working in no-sound configuration. What is more, 3D sound was a valuable cue for navigation in AR environment. The collected data suggest that the use of spatial sound in AR environments can be a significant factor in searching and navigating for hidden objects within indoor AR scenes. To conduct the experiment, the CARE approach was applied, while its CARL language was extended with new elements responsible for controlling audio in 3D space."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An experimental study of spatial sound usefulness in searching and navigating through AR environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0274-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Spatial sound, Augmented reality, 3D sound in AR, Audio augmented reality, AR, HCI, CARE, CARL","kwrd":["Spatial_sound","Augmented_reality","3D_sound_in_AR","Audio_augmented_reality","AR","HCI","CARE","CARL"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"vgzm.415900-10.1007-s10055-015-0274-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0274-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=274;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0274-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An experimental study of spatial sound usefulness in searching and navigating through AR environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0274-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0274-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I: Spatial Sound</li>
    
    
        <li class="c-article-identifiers__item">
            <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
        </li>
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-10-20" itemprop="datePublished">20 October 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An experimental study of spatial sound usefulness in searching and navigating through AR environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Dariusz-Rumi_ski" data-author-popup="auth-Dariusz-Rumi_ski" data-corresp-id="c1">Dariusz Rumiński<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0001-8179-9894"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-8179-9894</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Poznań University of Economics" /><meta itemprop="address" content="grid.423871.b, 0000000109406494, Department of Information Technology, Poznań University of Economics, Al. Niepodległości 10, 61-875, Poznań, Poland" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">223</span>–<span itemprop="pageEnd">233</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2823 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">9 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0274-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were asked to find three objects hidden within no-sound and spatial sound AR environments. The experiment showed that the participants of the spatialized sound group performed faster and more efficiently than working in no-sound configuration. What is more, 3D sound was a valuable cue for navigation in AR environment. The collected data suggest that the use of spatial sound in AR environments can be a significant factor in searching and navigating for hidden objects within indoor AR scenes. To conduct the experiment, the CARE approach was applied, while its CARL language was extended with new elements responsible for controlling audio in 3D space.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Augmented reality (AR) technology enables superimposing computer-generated content, such as interactive 2D and 3D multimedia objects, in real time, on a view of real objects. Widespread use of AR technology has been enabled in the recent years by the remarkable progress in consumer-level hardware performance—in particular—in the computational and graphical performance of mobile devices and the quickly growing bandwidth of mobile networks. Education, entertainment, e-commerce, and tourism are examples of application domains in which AR-based systems are increasingly being used. Since, in AR systems, synthetic content can be superimposed directly on a view of real objects, AR offers a powerful tool for presentation of different kinds of contextual information.</p><p>While scientists are focused on engineering techniques of recognizing and tracking patterns as well as overlaying 2D and 3D models onto real environments, methods of modeling spatial sound have not been sufficiently addressed in the context of creating AR applications, especially in the mobile domain. What is more, spatial sound can play a significant role in human–computer interaction (HCI) systems including augmented reality applications. Researches reported that presence of spatial sound in AR applications improves efficiency in performing tasks (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtual Reality Annual International Symposium, Proceedings, IEEE 1998, pp 20–27. doi:&#xA;                    10.1109/VRAIS.1998.658418&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR3" id="ref-link-section-d78084e367">1998</a>; Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e370">2004</a>), enhances depth perception of AR scenes, contributes to the AR experience (Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e373">2004</a>), and helps to identify various 3D objects placed in AR scenes (Sodnik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented reality environment. In: Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, ACM, New York, NY, USA, OZCHI ’06, pp 111–118. doi:&#xA;                    10.1145/1228175.1228197&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR17" id="ref-link-section-d78084e376">2006</a>; Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e379">2004</a>).</p><p>
Cohen and Wenzel (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Cohen M, Wenzel EM (1995) Virtual environments and advanced interface design. In: The Design of Multidimensional Sound Interfaces. Oxford University Press, Inc., New York, NY, USA, pp 291–346. &#xA;                    http://dl.acm.org/citation.cfm?id=216164.216180&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR5" id="ref-link-section-d78084e385">1995</a>) extensively explain multidimensional sound techniques including VR and to some extent AR domains. On the other hand, issues related to design, development, evaluation, and application of audio AR systems including perceptual studies are well summarized by Mariette (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Mariette N (2013) Human factors research in audio augmented reality. Springer, New York. doi:&#xA;                    10.1007/978-1-4614-4205-9_2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR9" id="ref-link-section-d78084e388">2013</a>). Audio technologies that can be used to build immersive virtual environments and gaming are discussed by Murphy and Neff (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Murphy D, Neff F (2011) Spatial sound for computer games and virtual reality. IGI Global, Hershey" href="/article/10.1007/s10055-015-0274-4#ref-CR10" id="ref-link-section-d78084e391">2011</a>). The presented review takes into consideration the widely varying levels of audio sophistication, which is being implemented in games. Authors present various audio capabilities that are important while implementing 3D sound in virtual environments. Playing multiple sounds at the same time, looping, volume modulation, or audio balancing are examples of 3D sound techniques applied in gaming.</p><p>Not only 3D sound is important in gaming, but also in a music industry. Stampfl (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stampfl P (2003) Augmented reality disk jockey: Ar/dj. In: ACM SIGGRAPH 2003 Sketches and Amp; Applications, ACM, New York, NY, USA, SIGGRAPH ’03, pp 1. doi:&#xA;                    10.1145/965400.965556&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR19" id="ref-link-section-d78084e397">2003</a>) combined AR with 3D sound to create immersive 3D musical presentations. The augmented reality disk jockey (AR/DJ) system allows two music DJs to play various sound effects and place them anywhere in 3D space in a music club. Sound sources are visualized in a 3D model of a dance floor and can be controlled using a pen with visual tracking markers placed on it. While this solution looks interesting and promising to use, no scientific results have been published yet.</p><p>Many researches reported that an application of 3D sound in AR applications enhances user experience. For instance, Hatala et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hatala M, Wakkary R, Kalantari L (2005) Rules and ontologies in support of real-time ubiquitous application. Web Semant 3(1):5–22. doi:&#xA;                    10.1016/j.websem.2005.05.004&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR7" id="ref-link-section-d78084e404">2005</a>) presented <i>ec(h)o</i>—an AR interface utilizing spatialized soundscapes and employing a semantic web approach to retrieve artefacts’ data and spatialized audio. The study was designed to create an interactive audio museum guide. The AR interface tracks the visitor’s location and dynamically plays audio information related to soundscapes the visitor is seeing. Authors conducted usability tests of the <i>ec(h)o</i> system. Participants were asked to use this system in real conditions and complete a questionnaire. Most of the participants gave navigation and engagement of the audio information a high rank (a mean was 4.0 in a 5-point scale).</p><p>Findings, reported by Regenbrecht et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W, Mueller E (2004) Using augmented virtuality for remote collaboration. Presence 13(3):338–354. doi:&#xA;                    10.1162/1054746041422334&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR13" id="ref-link-section-d78084e416">2004</a>), showed also that 3D sound was an important element of a video conferencing system (<i>cAR/PE!</i>). Authors presented a prototype which comprises live video streams of the participants arranged around a virtual table with spatial sound support. Spatial sound driven by headphones audio hardware was used to indicate different user positions. The task was to decide on the most esthetic out of five car models placed on one side of the virtual meeting room. Following, the participants were asked to complete a questionnaire. The <i>cAR/PE!</i> system was rated as easy to use and overall user satisfaction was good. Moreover, a method of exchanging information verbally was also rated as satisfactory. In turn, Poeschl et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Poeschl S, Wall K, Doering N (2013) Integration of spatial sound in immersive virtual environments an experimental study on effects of spatial sound on presence. In: Virtual Reality (VR), 2013 IEEE, pp 129–130. doi:&#xA;                    10.1109/VR.2013.6549396&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR11" id="ref-link-section-d78084e425">2013</a>) reported that integrating 3D sound in a synthetic scene leads to higher levels of presence experienced compared to no-sound display. Participants of the experiment were asked to complete a questionnaire including six questions related to audio perception. The process was repeated subsequently with a scene where the audio display swapped from no-sound to spatial sound or the opposite.</p><p>While 3D sound can enhance an immersive user experience, performed studies show that an application of spatial sound can improve efficiency in performing tasks. For instance, Billinghurst et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtual Reality Annual International Symposium, Proceedings, IEEE 1998, pp 20–27. doi:&#xA;                    10.1109/VRAIS.1998.658418&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR3" id="ref-link-section-d78084e431">1998</a>) presented a wearable augmented reality interface that provides spatialized 3D graphics and audio cues to aid performance in finding objects. In the presented experiment, authors examined how spatial cues affected performance using various configurations combining spatialized visual and audio information. Authors measured average task completion time of searching information from eight pages of data displayed in various conditions. Authors found that adding spatial audio cues to the body-stabilized display configuration dramatically improved performance in finding target icons.</p><p>An improved efficiency in performing tasks was also reported by Zhou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e437">2004</a>). Authors investigated the use of 3D sound in an experimental AR game. Participants of the experiment searched for a virtual princess using visual interface and the same interface enhanced with 3D sound. Participants judged also the relative depth of augmented virtual objects. Experimental data suggest that the use of 3D sound significantly improves task performance and accuracy of depth judgement. The results of the study also indicate that 3D sound contributes to the feeling of human presence and collaboration and helps to identify spatial objects. It is also worth emphasizing that Sodnik et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented reality environment. In: Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, ACM, New York, NY, USA, OZCHI ’06, pp 111–118. doi:&#xA;                    10.1145/1228175.1228197&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR17" id="ref-link-section-d78084e440">2006</a>) affirmed findings presented by Zhou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e443">2004</a>). Sodnik et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented reality environment. In: Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, ACM, New York, NY, USA, OZCHI ’06, pp 111–118. doi:&#xA;                    10.1145/1228175.1228197&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR17" id="ref-link-section-d78084e446">2006</a>) presented an evaluation of the perception and localization of 3D sound in a tabletop AR environment. The goal of the research was to explore sound–visual search in a number of conditions, whereby different spatial cues were compared and evaluated. Findings of the experiment indicate that humans localize the azimuth of a sound source much better than elevation or distance. Presented findings show that the distance perception of near sources is poor and in this case an elevation can play an important role to distinguish sound sources.</p><p>Localization study on applying spatial sound to virtual reality environments was presented by Jones et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Jones M, Jones S, Bradley G, Warren N, Bainbridge D, Holmes G (2008) Ontrack: dynamically adapting music playback to support navigation. Pers Ubiquitous Comput 12(7):513–525. doi:&#xA;                    10.1007/s00779-007-0155-2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR8" id="ref-link-section-d78084e452">2008</a>). Authors reported that users could find routes while using spatial audio cues. This study investigated navigation of three routes in VR environments. Participants of the study performed three evaluation tasks. The goal of each of the tasks was to walk from a start location to a target location along a route. The only directional information provided was the continuously adapted music which was played according to the user’s location. The study measured completion rate and mean time to complete each route. Results of the study show that an approach of audio panning to indicate heading direction and volume adjustment for distance to target were sufficient factors to finish tasks.</p><p>An overview concerning challenges of augmented audio reality for position-aware services can be found in a technical paper presented by Cohen and Villegas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Cohen M, Villegas J (2011) From whereware to whence- and whitherware: augmented audio reality for position-aware services. In: VR Innovation (ISVRI), 2011 IEEE International Symposium on, pp 273–280. doi:&#xA;                    10.1109/ISVRI.2011.5759650&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR4" id="ref-link-section-d78084e458">2011</a>). As the authors state, methods of distance reproduction and tracking the observer’s orientation in such AR environments are required to enhance value of AR applications. For instance, Villegas and Cohen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Villegas J, Cohen M (2010) Hrir : modulating range in headphone-reproduced spatial audio. In: Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, ACM, New York, NY, USA, VRCAI ’10, pp 89–94. doi:&#xA;                    10.1145/1900179.1900198&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR20" id="ref-link-section-d78084e461">2010</a>) presented a software audio filter—head-related impulse response (HRIR)—which can be used to create distance effects within AR environments. This filter allows dynamic modification of a sound source’s apparent location by modulating its virtual azimuth, elevation, and range in realtime.</p><p>This paper presents a study that was designed to test the hypothesis that an application of spatial sound significantly improves performance in searching and navigating through indoor augmented reality environments. To conduct the experiment, a novel approach to model spatial sound in AR environments was used. The approach allows dynamic composition of visual objects with aural objects within AR environments. As a result, a user can detect and navigate to individual sound sources which lead to hidden virtual objects.</p><p>The remainder of this paper is structured as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec2">2</a> introduces the concept of contextual augmented reality environments (CARE) and presents an extension of CARL language with audio elements and its implementation. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec5">3</a> describes how spatial sound is integrated with CARE environments. The experimental design of the study is described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec6">4</a>. This is followed by an analysis of the gathered data and an interpretation (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec11">5</a>). Next, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec15">6</a> contains a discussion of the experiment. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec16">7</a> concludes the paper and indicates the possible directions of future research.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Dynamic contextual AR environments</h2><div class="c-article-section__content" id="Sec2-content"><p>This section provides an overview of the semantic CARE concept that was used to design an experimental study concerning usefulness of spatial sound in indoor AR environments (Rumiński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Rumiński D, Walczak K (2014b) Semantic contextual augmented reality environments. In: The 13th IEEE International Symp. on Mixed and Augmented Reality (ISMAR 2014), IEEE, pp 401–404. doi:&#xA;                    10.1109/ISMAR.2014.6948506&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR16" id="ref-link-section-d78084e498">2014b</a>). The CARE approach avoids fragmentation of AR functionality between multiple independent applications and to simplify integration of various information services, providing visual and aural objects, into seamless, contextual and personalized AR interfaces. CARE handles with updating of AR data without requiring actualization of the AR browser application based on the user context. In the CARE environment, AR presentations are described by three semantically described elements:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>
                                 <i>Trackables</i>—a set of visual markers, indicating the elements (views) of reality that can be augmented (e.g., paintings, posters);</p>
                  </li>
                  <li>
                    <p>
                                 <i>Content objects</i>—a set of virtual content objects including visual and auditory data provided by the available information/content sources (e.g., 3D models representing objects drawn on paintings, audio, and video);</p>
                  </li>
                  <li>
                    <p>
                                 <i>Interface</i>—a description of the user interface, indicating the forms of interaction available to user (e.g., gestures affecting AR scenes).</p>
                  </li>
                </ul><p>These elements are encoded in contextual augmented reality language (CARL) and are sent to a generic <i>CARL Browser</i> to compose an interactive AR scene (Rumiński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Rumiński D, Walczak K (2014) Carl: a language for modelling contextual augmented reality environments. In: Camarinha-Matos LM, Barrento NS, Mendonça R (eds) Technological innovation for collective awareness systems, vol 432., IFIP advances in information and communication technologySpringer, Berlin, pp 183–190. doi:&#xA;                    10.1007/978-3-642-54734-8&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR14" id="ref-link-section-d78084e534">2014</a>). Examples of CARL descriptions are provided in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec5">3</a>. In general, the particular elements of a CARE environment are partially independent and may be offered by different service providers in a distributed architecture. Discovery and matching between these elements are possible based on their semantic descriptions with the use of shared ontologies and vocabularies. To enable creation of contextualized AR presentations, the specific elements of AR scenes sent to the client browser can be selected based on the current <i>context</i>—user location, preferences, privileges, device capabilities, etc.
</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig1">1</a> depicts a process of dynamic configuration of AR elements to be presented to a user. In each iteration cycle, the AR browser updates context information. Based on the user location and preferences, the visual markers (trackables) and content objects (visual and audio) are selected. Privileges and device capabilities influence the selection of content objects and their representations to be presented to a user. The mapping between context properties and AR elements can differ in different applications: In particular, complex semantic rules are used for selection and modification of these elements (Flotyński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Flotyński J, Walczak K (2014) Conceptual knowledge-based modeling of interactive 3d content. Vis Comput. doi:&#xA;                    10.1007/s00371-014-1011-9&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR6" id="ref-link-section-d78084e550">2014</a>; Rumiński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014b" title="Rumiński D, Walczak K (2014b) Semantic contextual augmented reality environments. In: The 13th IEEE International Symp. on Mixed and Augmented Reality (ISMAR 2014), IEEE, pp 401–404. doi:&#xA;                    10.1109/ISMAR.2014.6948506&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR16" id="ref-link-section-d78084e553">2014b</a>; Walczak et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Walczak K, Rumiński D, Flotyński J (2014) Building contextual augmented reality environments with semantics. In: Virtual Systems Multimedia (VSMM), pp 353–361. doi:&#xA;                    10.1109/VSMM.2014.7136656&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR21" id="ref-link-section-d78084e556">2014</a>). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Dynamic composition of contextual AR scenes in a CARE environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec3">The CARL language</h3><p>The CARL language has been designed to compose dynamic contextual augmented reality environments consisting of visual content. To provide spatialized audio content, the CARL language has been extended with new audio elements responsible for modeling audio within CARE environment.</p><p>To enable conditional steering of various auditory as well as visual content depending on the relative position of the camera and a real-world object, the <i>Sector</i> element was used (Rumiński and Walczak <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014a" title="Rumiński D, Walczak K (2014a) Dynamic composition of interactive ar scenes with the carl language. In: The 5th International Conference on Information, Intelligence, Systems and Applications, IEEE, pp 329–334. doi:&#xA;                    10.1109/IISA.2014.6878808&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR15" id="ref-link-section-d78084e590">2014a</a>). The <i>Sector</i> element declares an active area in 3D space, which can trigger actions when the camera enters or leaves the sector boundaries. It contains information about what actions should be triggered when the camera enters or leaves the sector, e.g., pan-specific sound. The actions that should be triggered when the camera enters the sector are specified within the <i>In</i> element. In turn, the ’leaving’ actions are specified within the <i>Out</i> element.</p><p>The sector boundaries are described by four ranges. The first range <span class="mathjax-tex">\((\min \_\theta , \max \_\theta )\)</span> indicates pitch angle between the camera’s position and the reference plane of a trackable object. The second range <span class="mathjax-tex">\((\min \_\phi , \max \_\phi )\)</span> indicates yaw angle between the camera position and tracked real object. The third range (<i>minDist, maxDist</i>) specifies the minimal and the maximal distance from the camera to the center of the trackable object. This distance is calculated on the basis of the length of the translation vector. The last range (<i>minHeight, maxHeight</i>) describes the minimal and the maximal height from the camera to the trackable object.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figa_HTML.gif?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figa_HTML.gif" alt="figurea" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div><h3 class="c-article__sub-heading" id="Sec4">Implementation</h3><p>The <i>CARL browser</i> application has been implemented using Qualcomm’s Vuforia computer vision library to recognize and track planar images and 3D objects in the real time (Qualcomm <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Qualcomm (2015) Vuforia. &#xA;                    https://www.qualcomm.com/products/vuforia&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR12" id="ref-link-section-d78084e720">2015</a>). The application is implemented in Java and runs on the <i>Android</i> platform (Android <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Android (2015) Android Platform Home Page. &#xA;                    http://www.android.com/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR1" id="ref-link-section-d78084e726">2015</a>). <i>CARL Browser</i> processes CARL descriptions provided by the distributed REST-based AR services. Each AR service can supply various information including models of 3D objects, audio sources, or specification of possible interaction between a user and synthetic content.</p><p>The CARE implementation is based on the REST architectural paradigm where <i>CARL Browser</i> can communicate with multiple distributed servers providing CARL specifications of particular elements of the CARE environment (trackables, content objects, and interfaces). The architecture of CARE’s server is based on Spring and Apache CXF frameworks (Spring <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Spring (2015) Spring home page. &#xA;                    http://www.spring.io/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR18" id="ref-link-section-d78084e738">2015</a>; Apache <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Apache (2015) Apache CXF Home Page. &#xA;                    http://cxf.apache.org/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR2" id="ref-link-section-d78084e741">2015</a>).</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figb_HTML.gif?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figb_HTML.gif" alt="figureb" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Modeling spatial sound in CARE</h2><div class="c-article-section__content" id="Sec5-content"><p>Each auditory object is described as a <i>ContentObject</i> element consisting of <i>Resources</i> as well as <i>Actions</i> elements. A <i>Resources</i> element points to locations of the same audio element with different levels of quality. The <i>Actions</i> element describes actions that can be called on a content object. In this case, actions are responsible for controlling audio (e.g., starting/stopping audio, setting right/left channel’s volume, and setting a sound’s looping). These actions are triggered by the <i>CARL Browser</i>, depending on where the camera is situated in 3D space.</p><p>Listing 1 presents the <i>ContentObject</i> with an <i>id=lionFX</i> representing a lion’s roaring. The sound’s resource is identified by <i>uri=http://.../res/lionFX</i>. The six actions are declared within the <i>lionFX</i> content object that can be called by CARL Browser. The first action—<i>startRoaring</i>—is responsible for setting up and starting the sound effect (lines 10–14). At first, it sets the sound’s looping to true to repeat it without any gap between its end and start (line 11). Next, it sets the volume level to five units on <i>left</i> and <i>right</i> channels (line 12). In the end, it starts to play the sound effect with the <i>play</i> command (line 13).</p><p>The next action—<i>stopRoaring</i>—is responsible for stopping audio content representing lion’s roaring sound (lines 16–18). The next two actions, <i>setVolume10</i> and <i>setVolume20</i>, presented in lines 20–26, set the volume to ten and twenty units, respectively (where one hundred is the maximum volume). Within line 27, there could be written more <i>setVolumeN</i> actions where <i>N</i> means a sound volume, but the description of listing 1 was specially shortened to only show the idea of constructing volume actions. Next, the <i>moreOnRight</i> action increases volume level to 60 on the right channel and 30 on the left channel, giving a sense that the sound’s source comes from the right side of a user’s camera (lines 28–30). The last action—<i>roar</i>—sets the volume for both channels to the maximum value indicating that the user just found the source of sound (lines 32–34). These actions will be used when a user’s camera approaches the source of sound to give the effect of depth perception.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figc_HTML.gif?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Figc_HTML.gif" alt="figurec" loading="lazy" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div><p>Listing 2 presents a shortened description of the trackable object with declared sectors responsible for starting and controlling sounds. Lines 3–7 present audio as well as visual objects that are associated with the <i>landscape</i> object.</p><p>The first sector <i>startToPlay</i> is responsible for starting sound effects of animals (lines 10–25). When a user’s camera enters the sector boundaries, then the application triggers <i>startRoaring</i>, <i>startSquealing</i>, and <i>startNeighing</i> actions using <i>ObjectAction</i> elements declared within the <i>IN</i> element (lines 15–19). In turn, when the camera leaves the <i>startToPlay</i> sector, then the application calls actions declared within the <i>Out</i> element—in this case sounds of animals will be stopped (lines 21–24). The next two sectors—<i>lionVolume10</i> and <i>lionVolume20</i>—are used when a user approaches the lion’s sound source (lines 27–45). In this case, decreasing the distance causes the volume of lion’s sound to be turned up.</p><p>Listing 3 describes sectors that trigger actions responsible for combining spatial sound with visual content. The <i>lionOnRight</i> sector declares actions that will be called when a camera points to the center of a trackable object from the right side and the distance between the camera and the trackable object is small (lines 1–12). A visualization of being in the <i>lionOnRight</i> sector is depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig6">6</a>b. The first three actions are responsible for giving an effect that the neighing (line 7) is the most audible sound. What is more, the roaring sound is also audible, but in this case only on the right channel (line 8). The squealing sound is little audible on a left channel (line 9), but it does not play an important role in this sector. This construction gives a clue that the lion object can be located on the right side of the horse. The last action (<i>showHorse</i>—line 10), called on content object with the <i>id=horse3D</i>, shows a 3D model representing the horse object.</p><p>The last sector—<i>oppositeToLion</i>—contains actions that will be called when the camera points to the right side of the trackable object (lines 16–27). The first action—<i>roar</i> (line 22)—is responsible for turning the roaring sound up. This action is called on content object with the <i>id=lionFX</i>. The next action—<i>showLion</i> (line 23)—triggered on content object with <i>id=lion3D</i>, is responsible for showing a 3D model of the lion object.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">The experiment</h2><div class="c-article-section__content" id="Sec6-content"><p>In this section, a study investigating whether 3D sound can be valuable for localizing hidden objects and navigating between them through indoor augmented reality environments, even if visual objects are not visible in an AR interface, is presented. Following sections describe the design of the study, the characteristics of participants that took part in the study, as well as the experiment’s procedure.</p><h3 class="c-article__sub-heading" id="Sec7">Design of the study</h3><p>To conduct the experiment, two CARE environments—<i>Env1</i> and <i>Env2</i>—were designed. In both environments, three content objects representing 3D models of animals augment a real-world object.</p><p>The first environment—<i>Env1</i>—does not use sound-based contents. The <i>Env2</i> environment uses a spatial auditory information to give a user audio cues, which can help to search and navigate to hidden content objects. The context of a user was static—it did not change in a runtime and the user was not able to change preferences and privileges while performing a task. The reader can refer to the video<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> presenting how audio was controlled while conducting the experiment.</p><p>3D models of animals are hidden in 3D space and only appear when the user’s camera is located in one of the following sectors: <i>oppositeToPig</i>, <i>horseOnRight</i>, <i>pigOnLeft</i>, <i>oppositeToHorse</i>, <i>horseOnLeft</i>, and <i>oppositeToLion</i>.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig2">2</a> visualizes active sectors of the <i>Env2</i> environment, in which various audio contents are turned on with varying sound volume switched between different channels. The first space, the green hemisphere, presents boundaries of the <i>startToPlay</i> sector (described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-015-0274-4#Sec5">3</a>, list 2). When the user’s camera enters this sector, <i>CARL Browser</i> triggers actions declared within the <i>In</i> element of the <i>startToPlay</i> sector. In this case, <i>CARL Browser</i> starts to play audio giving a sense that various animals are making noise, but ‘being’ in this sector it is hardly possible to locate positions of hidden objects. When the camera leaves, this sector sound effects will be stopped: <i>CARL Browser</i> will trigger actions declared within the <i>Out</i> element.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Visualization of modeled audio sectors</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The colored triangular pyramids represent spaces in which a user can hear animals’ sounds, such as a pig’s squealing (the pink color), a horse’s neighing (the gray color), and a lion’s roaring (the yellow color). Every single gradient visualizes the sound intensity level which dynamically increases when the user’s camera moves closer and decreases, when camera moves away. For instance, when the camera enters the <i>pigVolume10</i> sector, the volume of the squealing sound is set to ten. Moving forward, <i>CARL browser</i> sets volume to twenty, thirty, and forty until the camera enters the loudest sector—<i>oppositeToPig</i>—where squealing sound is the most audible.</p><p>The next interesting sector is <i>horseOnRight</i>, which is responsible for playing the squealing of pig as well as the neighing of horse. The sound volume of neighing is set quieter relative to the sound volume of squealing sound. Additionally, the neighing sound is more audible on the right channel giving a sense that on the right side of the pig object is situated the horse object, even if the camera does not point to the center of the scene, in which the horse object is hidden. Conversely to the <i>horseOnRight</i> sector, entering the <i>pigOnLeft</i> sector causes that <i>CARL Browser</i> automatically pans the neighing sound to be more audible than squealing sound. However, in this case, the squealing effect is more audible on the left channel. The same thing happens when the camera enters the <i>lionOnRight</i> or <i>horseOnLeft</i> sectors, but in these cases, appropriate sounds are dynamically panned between the left/right channels depending on where the camera is situated in the 3D space.</p><h3 class="c-article__sub-heading" id="Sec8">Task</h3><p>Participants were randomly divided into two separate groups—the control group and the spatialized sound group. The users from each group were asked to find hidden objects within AR environment. The participants of the control group used the <i>CARL Browser</i> application with configured no-sound environment. The users of the spatialized sound group were also using <i>CARL Browser</i>, but in this case, with configured audio environment. The participants were also equipped with Sennheiser HD 202 headphones.</p><h3 class="c-article__sub-heading" id="Sec9">Participants</h3><p>A total of 16 test subjects (5 female and 11 male) participated in the study. The subjects ranged in age from 20 to 27 years (<i>M</i> = 22.19 years, SD = 2.04 years). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0274-4#Tab1">1</a> presents participants’ characteristics. All the participants reported normal or corrected-to-normal eyesight and hearing.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig3">3</a> presents the user equipped with headphones performing the task. The user’s camera is situated in the <i>oppositeToPig</i> sector, in which the user interacts with visual and auditory contents. In the presented example, perceiving various sound effects, dynamically panned between channels with different sound intensity dependent on the camera’s position, helps to search for hidden objects and navigate through the AR environment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Performing the task using spatial sound environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Participants’ characteristics</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0274-4/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec10">Procedure</h3><p>After a brief introduction to the experiment, the participants were asked to provide information including their age, gender, hearing, and eyesight disabilities. Next, the participants were given instructions regarding what is a sector in context of CARE environment, how an exemplary sector works, and what exemplary conditions must be met to recognize that user’s camera has just entered a sector’s boundaries by <i>CARL Browser</i>. After that, a mobile device with running <i>CARL Browser</i> and configured test environment was given to the users to get used to the task and the application. Lastly, the participants from the control group ran the AR application with configured no-sound environment. The participants from the spatialized sound group started the AR application with configured audio environment.</p><p>During the experiment, the task completion time was measured. The measurements began when a participant started performing the task and were stopped when all hidden objects were discovered. After that, the users were asked to complete a short questionnaire.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Results and analysis</h2><div class="c-article-section__content" id="Sec11-content"><h3 class="c-article__sub-heading" id="Sec12">Task completion time</h3><p>While conducting the experiment, the time taken to complete the task was measured for all the participants. The mean time taken to complete the task, as presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig4">4</a>, was analyzed using an independent <i>t</i> test. The two-sample unequal variance (heteroscedastic) test was performed. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Mean time taken to complete the task for the control and spatialized sound groups</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The test confirmed that the significant differences in the task completion times existed between control and spatialized sound groups: <span class="mathjax-tex">\(\textit{T} = 2.89, \textit{p} &lt; 0.05\)</span>. The critical value is 2.145 for a 95 % confidence interval and 14 degrees of freedom (<i>df</i> = 14).</p><p>The task completion time was significantly shorter for the participants of the spatialized sound group (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig4">4</a>). The perception of auditory events such as hearing various sounds of animals with dynamically changing sound volume and panning audio, depending on where the camera is situated, might have caused the participants to be more focused on localizing hidden objects, which as a result shortened task completion times. For instance, the participants carefully decreased distance to the trackable object, having the right angle, so that at least one sound object was increasing its volume. As a result, the participants did not want to lose auditory cues by using random camera’s movements. The participants could assume that in some specific location, a hidden object could be placed, even if it was not visible in the application.</p><p>Conversely, the task completion time was significantly greater for participants of the control group. Participants used no-sound environment. No aural cues might have caused the participants to test various randomly chosen distances, angles, and heights to find hidden object, which as a result might have given greater task completion times for the participants of the control group.</p><h3 class="c-article__sub-heading" id="Sec13">Evaluation of the spatial sound usefulness</h3><p>After completing the task, each participant completed a questionnaire with a 1–10 rating scale that was used as a measurement of the spatial sound usefulness in searching and navigating through the augmented reality environment. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0274-4#Tab2">2</a> was used in rating the use of spatial sound within augmented reality environment.</p><p>The results of an independent <i>t</i> test with the 95 % confidence interval confirmed the existence of a significant difference between the ratings of usefulness spatial sound in two groups: <span class="mathjax-tex">\(\textit{T} = 3.28,\,\, \textit{p} &lt; 0.05\)</span>. Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig5">5</a> presents ratings of spatial sound usefulness given by the participants of both groups. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The ratings of the spatial sound usefulness</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The participants of the control group were asked to rate whether the use of spatial sound would help in searching for hidden objects and navigating through the AR environment, if they could use it. Most of the participants answered that the use of spatial sound would be very useful in locating and navigating hidden objects in such AR environment. Only one person rated the use of spatial sound on five. In total, seven participants (<i>M</i> = 7.0) indicated that the use of spatial sound could improve the task performance for the participants in the control group.</p><p>The participants of the spatialized sound group were asked to rate whether the use of spatial sound helped them in seeking out hidden objects and navigating between them. The use of spatial sound was very useful for five of the participants. What is more, for four of them, spatial sound was a critical factor to find hidden objects. In total, nine participants (<i>M</i> = 8.44) indicated that the use of spatial sound was a significant factor to solve the task for the participants of the spatialized sound group.</p><h3 class="c-article__sub-heading" id="Sec14">Evaluation of searching and navigating methods</h3><p>This evaluation was only applied to the participants of the spatialized sound group. Before starting the experiment, the participants were asked to identify what is being heard. Next, the participants started searching for hidden objects. When the camera entered the <i>startToPlay</i> sector, the participants answered that they heard animals.</p><p>The participants limited the area of search by slightly shortening the distance to the part of trackable object telling which object was being sought. Depending on where the camera was situated, all the participants gave the correct answer, e.g., when the camera was in one of the horse’s sectors, the participant answered that the horse was being sought. Shortening distance to the particular part of trackable object caused the volume of the aural content object to increase. These audio hints, as a result, were limiting boundaries of the searching space.</p><p>The questionnaire included a question asking about the usefulness of audio panning technique while recognizing the direction of possible location of hidden objects. Six of participants (67 %) answered that the panning technique gave a cue to navigate to possible location of hidden object. For the rest (33 %), this technique was distracting while searching for hidden objects.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 A scale of 1–10 used in rating the usefulness of spatial sound</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0274-4/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0274-4/MediaObjects/10055_2015_274_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Depending on various camera positions, a user hears various sounds. For both pictures, the most audible sound is neighing. Additionally, in <b>a</b> a user can hear squealing on the left channel, while in <b>b</b>, roaring sound is audible on the right channel</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0274-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Discussion</h2><div class="c-article-section__content" id="Sec15-content"><p>The collected data suggest that an application of spatial sound within indoor AR environment can significantly improve search as well as navigation to find hidden virtual objects. These results affirm that an application of 3D sound in AR applications improves efficiency in searching tasks (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtual Reality Annual International Symposium, Proceedings, IEEE 1998, pp 20–27. doi:&#xA;                    10.1109/VRAIS.1998.658418&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR3" id="ref-link-section-d78084e1613">1998</a>; Zhou et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:&#xA;                    10.1016/j.intcom.2004.06.016&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR22" id="ref-link-section-d78084e1616">2004</a>). What is more, the use of spatial auditory information helped the participants to identify hidden objects and navigate between them.</p><p>The experiment showed that the participants of the spatialized sound group performed faster and more efficiently than the participants of the control group. A clear strategy was used by the participants relying on perceiving sounds in 3D space. When the participants of the spatialized sound group were shortening the distance, at the right angle, to the particular part of the trackable object, they heard that the volume of the audio object was changing. These audio cues resulted in limiting boundaries of the searching space. The participants could assume that the hidden object could be located in particular parts of 3D space.</p><p>Moreover, when a single object was found and the camera was within the <i>pigOnLeft</i> sector, the participants could assume that on the left side of the horse the pig object could be located, though it was not visible in the AR interface (as presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig6">6</a>a when only the horse object is visible). As a result, most participants navigated to the left side of the trackable object to seek the pig. The same reaction was when, e.g., the camera entered boundaries of the <i>lionOnRight</i>’s sectors (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0274-4#Fig6">6</a>b). In this case, the participants navigated to the right side of the trackable object to see the lion object. These results confirm that an approach of audio panning to indicate heading direction and volume adjustment for distance to target can be a valuable navigation cue—not only applied to VR environments (Jones et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Jones M, Jones S, Bradley G, Warren N, Bainbridge D, Holmes G (2008) Ontrack: dynamically adapting music playback to support navigation. Pers Ubiquitous Comput 12(7):513–525. doi:&#xA;                    10.1007/s00779-007-0155-2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0274-4#ref-CR8" id="ref-link-section-d78084e1637">2008</a>), but also to indoor AR environments.</p><p>To carry out the presented experiment, CARE approach was used to model auditory AR environment. <i>CARE Browser</i> allowed playing various sounds together, coming from independent sources, and panning audio in an automatic way. What is more, these sounds were automatically panned, depending on where the camera was situated in 3D space. The AR application permitted also to easily manage sound volume. However, <i>CARL Browser</i> and <i>CARL</i> do not fully support sophisticated spatial sound techniques such as interaural differences of a sound source intensity, time, or direct to reverberant energy ratio for distance estimation. Currently, it is possible to place a sound in a 3D space giving an impression that the sound can be heard in one ear or the other accompanied by overall level adjustments for distance effects taking into account pitch and yaw angles. Although current research findings are promising, additional research is needed. In the presented experiment, three models were equally distributed in the horizontal direction. The study should be repeated with other AR environment configurations, including randomly distributed models in the horizontal as well as the vertical directions. Another limitation was the number of participants. This study should be repeated with more participants to confirm presented results.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Conclusions and future work</h2><div class="c-article-section__content" id="Sec16-content"><p>In this paper, an experimental study of spatial sound usefulness in augmented reality environments was presented. The study explores the possibilities of localizing and navigating in augmented reality environments.</p><p>Spatial sound is a great tool which can help in locating and navigating for virtual objects within augmented reality environments. Moreover, the results suggest that the spatial sound may increase a user’s perception in three dimensions while searching for hidden objects. Shortening the distance to a particular part of a trackable object, having the right angle, caused the volume of the aural content object to increase, which helped to limit boundaries of the search space. Moreover, 3D sound was a valuable factor to perform the task for the most participants. The participants of the control group, who were using no-sound environment, were at least aware that the usage of spatial sound could be an important factor to perform the task.</p><p>To perform the experiment, a novel approach to model spatial auditory information in the CARE environment was used. This approach allows playing various sounds together coming from independent sources, dynamically panning audio dependent on the camera’s position. Apart from that, CARE enables combining sound contents with visual 3D models coming from distributed AR services. The presented approach goes beyond the current state of the art in the field of modeling spatial sound in AR environments. The CARE environment and the CARL language provide a basis for building a generic augmented reality audio browser that takes into account data representing real-world objects, multimedia content (visual as well as audio), various interfaces, and user context coming from independent and distributed sources. The proposed approach avoids fragmentation of AR functionality between multiple independent applications and simplifying integration of various information services into unified, ubiquitous, contextual AR interfaces. The presented design of the study is one example of AR spatialized audio application. The approach to model 3D sound in the CARE environment can be also applied to building AR applications including domains such as gaming, education, entertainment, cultural heritage, and advertising.</p><p>Possible directions of future research incorporate several facets. First, a study including randomly distributed models in the horizontal as well as vertical direction should be conducted to confirm the results of the presented research. Second, an evaluation of the spatial sound in a learning process will be examined. Finally, an extension of the CARL language and <i>CARL Browser</i> to more sophisticated spatial sound techniques within augmented reality environments should be implemented including interaural differences in intensity, time, and shadowing.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>
                                 <a href="https://youtu.be/XAzRyHin9io">https://youtu.be/XAzRyHin9io</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Android (2015) Android Platform Home Page. http://www.android.com/&#xA;                        " /><p class="c-article-references__text" id="ref-CR1">Android (2015) Android Platform Home Page. <a href="http://www.android.com/">http://www.android.com/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Apache (2015) Apache CXF Home Page. http://cxf.apache.org/&#xA;                        " /><p class="c-article-references__text" id="ref-CR2">Apache (2015) Apache CXF Home Page. <a href="http://cxf.apache.org/">http://cxf.apache.org/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtua" /><p class="c-article-references__text" id="ref-CR3">Billinghurst M, Bowskill J, Dyer N, Morphett J (1998) An evaluation of wearable information spaces. In: Virtual Reality Annual International Symposium, Proceedings, IEEE 1998, pp 20–27. doi:<a href="https://doi.org/10.1109/VRAIS.1998.658418">10.1109/VRAIS.1998.658418</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen M, Villegas J (2011) From whereware to whence- and whitherware: augmented audio reality for position-awa" /><p class="c-article-references__text" id="ref-CR4">Cohen M, Villegas J (2011) From whereware to whence- and whitherware: augmented audio reality for position-aware services. In: VR Innovation (ISVRI), 2011 IEEE International Symposium on, pp 273–280. doi:<a href="https://doi.org/10.1109/ISVRI.2011.5759650">10.1109/ISVRI.2011.5759650</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen M, Wenzel EM (1995) Virtual environments and advanced interface design. In: The Design of Multidimension" /><p class="c-article-references__text" id="ref-CR5">Cohen M, Wenzel EM (1995) Virtual environments and advanced interface design. In: The Design of Multidimensional Sound Interfaces. Oxford University Press, Inc., New York, NY, USA, pp 291–346. <a href="http://dl.acm.org/citation.cfm?id=216164.216180">http://dl.acm.org/citation.cfm?id=216164.216180</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Flotyński J, Walczak K (2014) Conceptual knowledge-based modeling of interactive 3d content. Vis Comput. doi:1" /><p class="c-article-references__text" id="ref-CR6">Flotyński J, Walczak K (2014) Conceptual knowledge-based modeling of interactive 3d content. Vis Comput. doi:<a href="https://doi.org/10.1007/s00371-014-1011-9">10.1007/s00371-014-1011-9</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Hatala, R. Wakkary, L. Kalantari, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Hatala M, Wakkary R, Kalantari L (2005) Rules and ontologies in support of real-time ubiquitous application. W" /><p class="c-article-references__text" id="ref-CR7">Hatala M, Wakkary R, Kalantari L (2005) Rules and ontologies in support of real-time ubiquitous application. Web Semant 3(1):5–22. doi:<a href="https://doi.org/10.1016/j.websem.2005.05.004">10.1016/j.websem.2005.05.004</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.websem.2005.05.004" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Rules%20and%20ontologies%20in%20support%20of%20real-time%20ubiquitous%20application&amp;journal=Web%20Semant&amp;doi=10.1016%2Fj.websem.2005.05.004&amp;volume=3&amp;issue=1&amp;pages=5-22&amp;publication_year=2005&amp;author=Hatala%2CM&amp;author=Wakkary%2CR&amp;author=Kalantari%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Jones, S. Jones, G. Bradley, N. Warren, D. Bainbridge, G. Holmes, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Jones M, Jones S, Bradley G, Warren N, Bainbridge D, Holmes G (2008) Ontrack: dynamically adapting music playb" /><p class="c-article-references__text" id="ref-CR8">Jones M, Jones S, Bradley G, Warren N, Bainbridge D, Holmes G (2008) Ontrack: dynamically adapting music playback to support navigation. Pers Ubiquitous Comput 12(7):513–525. doi:<a href="https://doi.org/10.1007/s00779-007-0155-2">10.1007/s00779-007-0155-2</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00779-007-0155-2" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ontrack%3A%20dynamically%20adapting%20music%20playback%20to%20support%20navigation&amp;journal=Pers%20Ubiquitous%20Comput&amp;doi=10.1007%2Fs00779-007-0155-2&amp;volume=12&amp;issue=7&amp;pages=513-525&amp;publication_year=2008&amp;author=Jones%2CM&amp;author=Jones%2CS&amp;author=Bradley%2CG&amp;author=Warren%2CN&amp;author=Bainbridge%2CD&amp;author=Holmes%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="N. Mariette, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Mariette N (2013) Human factors research in audio augmented reality. Springer, New York. doi:10.1007/978-1-461" /><p class="c-article-references__text" id="ref-CR9">Mariette N (2013) Human factors research in audio augmented reality. Springer, New York. doi:<a href="https://doi.org/10.1007/978-1-4614-4205-9_2">10.1007/978-1-4614-4205-9_2</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factors%20research%20in%20audio%20augmented%20reality&amp;publication_year=2013&amp;author=Mariette%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Murphy, F. Neff, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Murphy D, Neff F (2011) Spatial sound for computer games and virtual reality. IGI Global, Hershey" /><p class="c-article-references__text" id="ref-CR10">Murphy D, Neff F (2011) Spatial sound for computer games and virtual reality. IGI Global, Hershey</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20sound%20for%20computer%20games%20and%20virtual%20reality&amp;publication_year=2011&amp;author=Murphy%2CD&amp;author=Neff%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Poeschl S, Wall K, Doering N (2013) Integration of spatial sound in immersive virtual environments an experime" /><p class="c-article-references__text" id="ref-CR11">Poeschl S, Wall K, Doering N (2013) Integration of spatial sound in immersive virtual environments an experimental study on effects of spatial sound on presence. In: Virtual Reality (VR), 2013 IEEE, pp 129–130. doi:<a href="https://doi.org/10.1109/VR.2013.6549396">10.1109/VR.2013.6549396</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Qualcomm (2015) Vuforia. https://www.qualcomm.com/products/vuforia&#xA;                        " /><p class="c-article-references__text" id="ref-CR12">Qualcomm (2015) Vuforia. <a href="https://www.qualcomm.com/products/vuforia">https://www.qualcomm.com/products/vuforia</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Regenbrecht, T. Lum, P. Kohler, C. Ott, M. Wagner, W. Wilke, E. Mueller, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W, Mueller E (2004) Using augmented virtuality for remo" /><p class="c-article-references__text" id="ref-CR13">Regenbrecht H, Lum T, Kohler P, Ott C, Wagner M, Wilke W, Mueller E (2004) Using augmented virtuality for remote collaboration. Presence 13(3):338–354. doi:<a href="https://doi.org/10.1162/1054746041422334">10.1162/1054746041422334</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F1054746041422334" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20augmented%20virtuality%20for%20remote%20collaboration&amp;journal=Presence&amp;doi=10.1162%2F1054746041422334&amp;volume=13&amp;issue=3&amp;pages=338-354&amp;publication_year=2004&amp;author=Regenbrecht%2CH&amp;author=Lum%2CT&amp;author=Kohler%2CP&amp;author=Ott%2CC&amp;author=Wagner%2CM&amp;author=Wilke%2CW&amp;author=Mueller%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Rumiński, K. Walczak, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Rumiński D, Walczak K (2014) Carl: a language for modelling contextual augmented reality environments. In: Cam" /><p class="c-article-references__text" id="ref-CR14">Rumiński D, Walczak K (2014) Carl: a language for modelling contextual augmented reality environments. In: Camarinha-Matos LM, Barrento NS, Mendonça R (eds) Technological innovation for collective awareness systems, vol 432., IFIP advances in information and communication technologySpringer, Berlin, pp 183–190. doi:<a href="https://doi.org/10.1007/978-3-642-54734-8">10.1007/978-3-642-54734-8</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Technological%20innovation%20for%20collective%20awareness%20systems&amp;pages=183-190&amp;publication_year=2014&amp;author=Rumi%C5%84ski%2CD&amp;author=Walczak%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rumiński D, Walczak K (2014a) Dynamic composition of interactive ar scenes with the carl language. In: The 5th" /><p class="c-article-references__text" id="ref-CR15">Rumiński D, Walczak K (2014a) Dynamic composition of interactive ar scenes with the carl language. In: The 5th International Conference on Information, Intelligence, Systems and Applications, IEEE, pp 329–334. doi:<a href="https://doi.org/10.1109/IISA.2014.6878808">10.1109/IISA.2014.6878808</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rumiński D, Walczak K (2014b) Semantic contextual augmented reality environments. In: The 13th IEEE Internatio" /><p class="c-article-references__text" id="ref-CR16">Rumiński D, Walczak K (2014b) Semantic contextual augmented reality environments. In: The 13th IEEE International Symp. on Mixed and Augmented Reality (ISMAR 2014), IEEE, pp 401–404. doi:<a href="https://doi.org/10.1109/ISMAR.2014.6948506">10.1109/ISMAR.2014.6948506</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented re" /><p class="c-article-references__text" id="ref-CR17">Sodnik J, Tomazic S, Grasset R, Duenser A, Billinghurst M (2006) Spatial sound localization in an augmented reality environment. In: Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments, ACM, New York, NY, USA, OZCHI ’06, pp 111–118. doi:<a href="https://doi.org/10.1145/1228175.1228197">10.1145/1228175.1228197</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spring (2015) Spring home page. http://www.spring.io/&#xA;                        " /><p class="c-article-references__text" id="ref-CR18">Spring (2015) Spring home page. <a href="http://www.spring.io/">http://www.spring.io/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stampfl P (2003) Augmented reality disk jockey: Ar/dj. In: ACM SIGGRAPH 2003 Sketches and Amp; Applications, A" /><p class="c-article-references__text" id="ref-CR19">Stampfl P (2003) Augmented reality disk jockey: Ar/dj. In: ACM SIGGRAPH 2003 Sketches and Amp; Applications, ACM, New York, NY, USA, SIGGRAPH ’03, pp 1. doi:<a href="https://doi.org/10.1145/965400.965556">10.1145/965400.965556</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Villegas J, Cohen M (2010) Hrir : modulating range in headphone-reproduced spatial audio. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR20">Villegas J, Cohen M (2010) Hrir : modulating range in headphone-reproduced spatial audio. In: Proceedings of the 9th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry, ACM, New York, NY, USA, VRCAI ’10, pp 89–94. doi:<a href="https://doi.org/10.1145/1900179.1900198">10.1145/1900179.1900198</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Walczak K, Rumiński D, Flotyński J (2014) Building contextual augmented reality environments with semantics. I" /><p class="c-article-references__text" id="ref-CR21">Walczak K, Rumiński D, Flotyński J (2014) Building contextual augmented reality environments with semantics. In: Virtual Systems Multimedia (VSMM), pp 353–361. doi:<a href="https://doi.org/10.1109/VSMM.2014.7136656">10.1109/VSMM.2014.7136656</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhou, AD. Cheok, X. Yang, Y. Qiu, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality envi" /><p class="c-article-references__text" id="ref-CR22">Zhou Z, Cheok AD, Yang X, Qiu Y (2004) An experimental study on the role of 3d sound in augmented reality environment. Interact Comput 16(6):1043–1068. doi:<a href="https://doi.org/10.1016/j.intcom.2004.06.016">10.1016/j.intcom.2004.06.016</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.intcom.2004.06.016" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20experimental%20study%20on%20the%20role%20of%203d%20sound%20in%20augmented%20reality%20environment&amp;journal=Interact%20Comput&amp;doi=10.1016%2Fj.intcom.2004.06.016&amp;volume=16&amp;issue=6&amp;pages=1043-1068&amp;publication_year=2004&amp;author=Zhou%2CZ&amp;author=Cheok%2CAD&amp;author=Yang%2CX&amp;author=Qiu%2CY">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0274-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research work has been supported by the Polish National Science Centre (NCN) Grants No. DEC-2012/07/B/ST6/01523.</p>
                <h3 class="c-article__sub-heading">Conflict of interest</h3>
                <p>The author declares that they have no conflict of interest.</p>
              </div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Information Technology, Poznań University of Economics, Al. Niepodległości 10, 61-875, Poznań, Poland</p><p class="c-article-author-affiliation__authors-list">Dariusz Rumiński</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Dariusz-Rumi_ski"><span class="c-article-authors-search__title u-h3 js-search-name">Dariusz Rumiński</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Dariusz+Rumi%C5%84ski&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dariusz+Rumi%C5%84ski" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Dariusz+Rumi%C5%84ski%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0274-4/email/correspondent/c1/new">Dariusz Rumiński</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p>
                           <b>Open Access</b> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<a href="http://creativecommons.org/licenses/by/4.0/" rel="license" itemprop="license">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20experimental%20study%20of%20spatial%20sound%20usefulness%20in%20searching%20and%20navigating%20through%20AR%20environments&amp;author=Dariusz%20Rumi%C5%84ski&amp;contentID=10.1007%2Fs10055-015-0274-4&amp;publication=1359-4338&amp;publicationDate=2015-10-20&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0274-4" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0274-4" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Rumiński, D. An experimental study of spatial sound usefulness in searching and navigating through AR environments.
                    <i>Virtual Reality</i> <b>19, </b>223–233 (2015). https://doi.org/10.1007/s10055-015-0274-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0274-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-03-05">05 March 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-08-27">27 August 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-10-20">20 October 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11">November 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0274-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0274-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Spatial sound</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D sound in AR</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Audio augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">AR</span></li><li class="c-article-subject-list__subject"><span itemprop="about">HCI</span></li><li class="c-article-subject-list__subject"><span itemprop="about">CARE</span></li><li class="c-article-subject-list__subject"><span itemprop="about">CARL</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0274-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=274;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

