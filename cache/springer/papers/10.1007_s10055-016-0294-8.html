<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Remote collaboration in virtual reality: asymmetrical effects of task "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/20/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload"/>

    <meta name="dc.source" content="Virtual Reality 2016 20:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-09-06"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in spatial dialogs. Pairs of distant participants with specific roles (a guide and a manipulator) had to collaboratively move a virtual object in a plane factory mock-up. The displays allowed the participants to be immersed together in the virtual environment. We analyzed the dialogs that took place according to the frames of reference and the mental transformations required to produce the spatial statements. We also measured the associated mental workload. Results showed that when participants took a perspective, the manipulator&#8217;s point of view was preferred. Perspective-taking only yielded a moderate increase in mental rotations, which may explain a specifically high mental demand score for the guides&#8217; NASA-TLX. Overall, this is in accordance with the least collaborative effort principle. This study reinforces the idea that, in collaboration, operators do not need the same aids as each other. Thus, it is not necessary to develop symmetrical tools, i.e., the same tools for all co-workers; instead, the needs of each operator should be taken into account, according to the task he has to perform. In our case, the guides would be helped with perspective-taking aids, while the manipulators would be helped with action-oriented tools."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2016-09-06"/>

    <meta name="prism.volume" content="20"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="213"/>

    <meta name="prism.endingPage" content="220"/>

    <meta name="prism.copyright" content="2016 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-016-0294-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-016-0294-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-016-0294-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-016-0294-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload"/>

    <meta name="citation_volume" content="20"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2016/11"/>

    <meta name="citation_online_date" content="2016/09/06"/>

    <meta name="citation_firstpage" content="213"/>

    <meta name="citation_lastpage" content="220"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-016-0294-8"/>

    <meta name="DOI" content="10.1007/s10055-016-0294-8"/>

    <meta name="citation_doi" content="10.1007/s10055-016-0294-8"/>

    <meta name="description" content="In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial"/>

    <meta name="dc.creator" content="Lauriane Pouliquen-Lardy"/>

    <meta name="dc.creator" content="Isabelle Milleville-Pennel"/>

    <meta name="dc.creator" content="Fran&#231;ois Guillaume"/>

    <meta name="dc.creator" content="Franck Mars"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Psychol (Amst); citation_title=Mental rotation in perspective problems; citation_author=LC Boer; citation_volume=76; citation_publication_date=1991; citation_pages=1-9; citation_doi=10.1016/0001-6918(91)90050-A; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_title=The effects of group collaboration on presence in a collaborative virtual environment; citation_inbook_title=Virtual environments 2000; citation_publication_date=2000; citation_pages=85-94; citation_id=CR2; citation_author=J Casanueva; citation_author=E Blake; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Chellali AM, Dumas C, Milleville-Pennel I, Nouri E (2007) Common Frame of reference in collaborative virtual environments and their impact on presence. In: Proceedings of the 10th annual international workshop on presence, Starlab Barcelona, Barcelone, Spain, pp 371&#8211;372"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments; citation_author=A Chellali, I Milleville-Pennel, C Dumas; citation_volume=17; citation_publication_date=2012; citation_pages=1-15; citation_doi=10.1007/s10055-012-0214-5; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Collaborative virtual environments; citation_publication_date=2001; citation_id=CR5; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Perspect Soc Shar Cogn; citation_title=Grounding in communication; citation_author=HH Clark, SE Brennan; citation_volume=13; citation_publication_date=1991; citation_pages=127-149; citation_doi=10.1037/10096-006; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=Listeners invest in an assumed other&#8217;s perspective despite cognitive cost; citation_author=ND Duran, R Dale, RJ Kreuz; citation_volume=121; citation_publication_date=2011; citation_pages=22-40; citation_doi=10.1016/j.cognition.2011.06.009; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Toward a theory of situation awareness in dynamic systems; citation_author=MR Endsley; citation_volume=37; citation_publication_date=1995; citation_pages=32-64; citation_doi=10.1518/001872095779049543; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Res; citation_title=Updating representations of learned scenes; citation_author=CA Finlay, MA Motes, M Kozhevnikov; citation_volume=71; citation_publication_date=2007; citation_pages=265-276; citation_doi=10.1007/s00426-006-0082-2; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Res; citation_title=Humans do not switch between path knowledge and landmarks when learning a new environment; citation_author=P Foo, A Duchon, WH Warren, MJ Tarr; citation_volume=71; citation_publication_date=2007; citation_pages=240-251; citation_doi=10.1007/s00426-006-0080-4; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Top Cogn Sci; citation_title=Joint action, interactive alignment, and dialog; citation_author=S Garrod, MJ Pickering; citation_volume=1; citation_publication_date=2009; citation_pages=292-304; citation_doi=10.1111/j.1756-8765.2009.01020.x; citation_id=CR11"/>

    <meta name="citation_reference" content="Hart SG (2006) NASA-task load index (NASA-TLX); 20&#160;years later. In: Proceedings of the human factors and ergonomics society annual meeting. Sage Publications, London, pp 904&#8211;908"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Psychol; citation_title=Orientation in cognitive maps; citation_author=DL Hintzman, CS O&#8217;Dell, DR Arndt; citation_volume=13; citation_publication_date=1981; citation_pages=149-206; citation_doi=10.1016/0010-0285(81)90007-4; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Towards a cognitive approach to human&#8211;machine cooperation in dynamic situations; citation_author=J-M Hoc; citation_volume=54; citation_publication_date=2001; citation_pages=509-540; citation_doi=10.1006/ijhc.2000.0454; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_title=Cerveau d&#8217;homme, cerveau de femme?; citation_publication_date=2001; citation_id=CR15; citation_author=D Kimura; citation_publisher=Odile Jacob"/>

    <meta name="citation_reference" content="citation_title=Cerveau &amp; comportement; citation_publication_date=2002; citation_id=CR16; citation_author=B Kolb; citation_author=I Whishaw; citation_publisher=De Boeck"/>

    <meta name="citation_reference" content="citation_journal_title=Sex Roles; citation_title=Gender and regional differences in spatial referents used in direction giving; citation_author=CA Lawton; citation_volume=44; citation_publication_date=2001; citation_pages=321-337; citation_doi=10.1023/A:1010981616842; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Two kinds of visual perspective taking; citation_author=P Michelon, JM Zacks; citation_volume=68; citation_publication_date=2006; citation_pages=327-337; citation_doi=10.3758/BF03193680; citation_id=CR18"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Psychometric properties of subjective workload measurement techniques: implications for their use in the assessment of perceived mental workload; citation_author=TE Nygren; citation_volume=33; citation_publication_date=1991; citation_pages=17-33; citation_id=CR19"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Process; citation_title=Virtual collaboration: effect of spatial configuration on spatial statements production; citation_author=L Pouliquen-Lardy, F Mars, F Guillaume, I Milleville-Pennel; citation_volume=16; citation_publication_date=2015; citation_pages=337-342; citation_doi=10.1007/s10339-015-0672-2; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Res; citation_title=Spatial updating in virtual reality: the sufficiency of visual information; citation_author=BE Riecke, DW Cunningham, H B&#252;lthoff; citation_volume=71; citation_publication_date=2007; citation_pages=298-313; citation_doi=10.1007/s00426-006-0085-z; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Child Dev; citation_title=Developmental differences in giving directions: spatial frames of reference and mental rotation; citation_author=RJ Roberts, CJ Aman; citation_volume=64; citation_publication_date=1993; citation_pages=1258-1270; citation_doi=10.2307/1131338; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Cogn Psychol; citation_title=Landmark frames of reference in interactive route description tasks; citation_author=M Roger, D Knutsen, N Bonnardel, L Bigot; citation_volume=27; citation_publication_date=2013; citation_pages=497-504; citation_doi=10.1002/acp.2927; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Learn Mem Cogn; citation_title=Mental representations of large and small spatial layouts are orientation dependent; citation_author=B Roskos-Ewoldsen, TP McNamara, AL Shelton, W Carr; citation_volume=24; citation_publication_date=1998; citation_pages=215; citation_doi=10.1037/0278-7393.24.1.215; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Discourse Process; citation_title=Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations?; citation_author=MF Schober; citation_volume=20; citation_publication_date=1995; citation_pages=219-247; citation_doi=10.1080/01638539509544939; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=Commun Res; citation_title=Virtual team work: group decision making in 3D virtual environments; citation_author=AP Schouten, B Hooff, F Feldberg; citation_publication_date=2013; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Philos Trans R Soc B Biol Sci; citation_title=Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments; citation_author=M Slater; citation_volume=364; citation_publication_date=2009; citation_pages=3549-3557; citation_doi=10.1098/rstb.2009.0138; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_title=Collaborative design in virtual environments; citation_publication_date=2011; citation_id=CR28; citation_publisher=Springer"/>

    <meta name="citation_author" content="Lauriane Pouliquen-Lardy"/>

    <meta name="citation_author_institution" content="IRT Jules Verne, Bouguenais, France"/>

    <meta name="citation_author_institution" content="UMR CNRS 6597, Institut de Recherche en Communications et Cybern&#233;tique de Nantes (IRCCyN), Nantes Cedex 03, France"/>

    <meta name="citation_author" content="Isabelle Milleville-Pennel"/>

    <meta name="citation_author_institution" content="UMR CNRS 6597, Institut de Recherche en Communications et Cybern&#233;tique de Nantes (IRCCyN), Nantes Cedex 03, France"/>

    <meta name="citation_author" content="Fran&#231;ois Guillaume"/>

    <meta name="citation_author_institution" content="Airbus Group, Suresnes, France"/>

    <meta name="citation_author" content="Franck Mars"/>

    <meta name="citation_author_email" content="franck.mars@irccyn.ec-nantes.fr"/>

    <meta name="citation_author_institution" content="UMR CNRS 6597, Institut de Recherche en Communications et Cybern&#233;tique de Nantes (IRCCyN), Nantes Cedex 03, France"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-016-0294-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2016/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-016-0294-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload"/>
        <meta property="og:description" content="In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in spatial dialogs. Pairs of distant participants with specific roles (a guide and a manipulator) had to collaboratively move a virtual object in a plane factory mock-up. The displays allowed the participants to be immersed together in the virtual environment. We analyzed the dialogs that took place according to the frames of reference and the mental transformations required to produce the spatial statements. We also measured the associated mental workload. Results showed that when participants took a perspective, the manipulator’s point of view was preferred. Perspective-taking only yielded a moderate increase in mental rotations, which may explain a specifically high mental demand score for the guides’ NASA-TLX. Overall, this is in accordance with the least collaborative effort principle. This study reinforces the idea that, in collaboration, operators do not need the same aids as each other. Thus, it is not necessary to develop symmetrical tools, i.e., the same tools for all co-workers; instead, the needs of each operator should be taken into account, according to the task he has to perform. In our case, the guides would be helped with perspective-taking aids, while the manipulators would be helped with action-oriented tools."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-016-0294-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Remote collaboration, Spatial common frame of reference, Spatial cognition, Virtual reality, Mental workload","kwrd":["Remote_collaboration","Spatial_common_frame_of_reference","Spatial_cognition","Virtual_reality","Mental_workload"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-016-0294-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-016-0294-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=294;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-016-0294-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0294-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0294-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-09-06" itemprop="datePublished">06 September 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Lauriane-Pouliquen_Lardy" data-author-popup="auth-Lauriane-Pouliquen_Lardy">Lauriane Pouliquen-Lardy</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="IRT Jules Verne" /><meta itemprop="address" content="grid.481982.e, 0000 0004 6000 1283, IRT Jules Verne, Bouguenais, France" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN)" /><meta itemprop="address" content="grid.503212.7, UMR CNRS 6597, Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN), Campus de l’École Centrale de Nantes, 1, rue de la Noë, B.P. 92101, 44321, Nantes Cedex 03, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Isabelle-Milleville_Pennel" data-author-popup="auth-Isabelle-Milleville_Pennel">Isabelle Milleville-Pennel</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN)" /><meta itemprop="address" content="grid.503212.7, UMR CNRS 6597, Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN), Campus de l’École Centrale de Nantes, 1, rue de la Noë, B.P. 92101, 44321, Nantes Cedex 03, France" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Fran_ois-Guillaume" data-author-popup="auth-Fran_ois-Guillaume">François Guillaume</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Airbus Group" /><meta itemprop="address" content="grid.424413.4, 0000000405003075, Airbus Group, Suresnes, France" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Franck-Mars" data-author-popup="auth-Franck-Mars" data-corresp-id="c1">Franck Mars<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-4140-0049"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-4140-0049</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN)" /><meta itemprop="address" content="grid.503212.7, UMR CNRS 6597, Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN), Campus de l’École Centrale de Nantes, 1, rue de la Noë, B.P. 92101, 44321, Nantes Cedex 03, France" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">213</span>–<span itemprop="pageEnd">220</span>(<span data-test="article-publication-year">2016</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">962 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">12 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-016-0294-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in spatial dialogs. Pairs of distant participants with specific roles (a guide and a manipulator) had to collaboratively move a virtual object in a plane factory mock-up. The displays allowed the participants to be immersed together in the virtual environment. We analyzed the dialogs that took place according to the frames of reference and the mental transformations required to produce the spatial statements. We also measured the associated mental workload. Results showed that when participants took a perspective, the manipulator’s point of view was preferred. Perspective-taking only yielded a moderate increase in mental rotations, which may explain a specifically high mental demand score for the guides’ NASA-TLX. Overall, this is in accordance with the least collaborative effort principle. This study reinforces the idea that, in collaboration, operators do not need the same aids as each other. Thus, it is not necessary to develop symmetrical tools, i.e., the same tools for all co-workers; instead, the needs of each operator should be taken into account, according to the task he has to perform. In our case, the guides would be helped with perspective-taking aids, while the manipulators would be helped with action-oriented tools.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Geographically distributed companies use immersive collaborative virtual environments (ICVEs) to integrate expertize located in different sites. An ICVE is a 3D virtual environment shared by at least two remote sites, which uses immersive displays connected through a network (Wang and Tsai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang X, Tsai JJ-H (eds) (2011) Collaborative design in virtual environments. Springer, New York" href="/article/10.1007/s10055-016-0294-8#ref-CR28" id="ref-link-section-d75805e367">2011</a>). For instance, distributed teams may meet remotely in a virtual plane mock-up to plan scheduling changes in the industrial process of plane construction. Engineers, represented by avatars, are immersed in the plane and connected via phones. Later, in the real factory, they can decide collectively the best way to operate. Such new technical devices raise new questions about how people collaborate. While some studies have already looked at remote collaboration (Churchill et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Churchill EF, Snowdon DN, Munro AJ (eds) (2001) Collaborative virtual environments. Springer, London" href="/article/10.1007/s10055-016-0294-8#ref-CR5" id="ref-link-section-d75805e370">2001</a>; Wang and Tsai <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wang X, Tsai JJ-H (eds) (2011) Collaborative design in virtual environments. Springer, New York" href="/article/10.1007/s10055-016-0294-8#ref-CR28" id="ref-link-section-d75805e373">2011</a>), few have focused on the spatiality of these virtual environments (Chellali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Chellali A, Milleville-Pennel I, Dumas C (2012) Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments. Virtual Real 17:1–15. doi:&#xA;                    10.1007/s10055-012-0214-5&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR4" id="ref-link-section-d75805e376">2012</a>). In this study, we are particularly interested in the construction of the shared representation of the environment and the link to the collective management of the mental workload.</p><p>Every operator has a singular mental representation of the situation. Endsley (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Endsley MR (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37:32–64" href="/article/10.1007/s10055-016-0294-8#ref-CR8" id="ref-link-section-d75805e382">1995</a>) called this representation “situation awareness”. During spatial tasks, such as navigation, the spatial representation of the environment is an important part of situation awareness (Finlay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Finlay CA, Motes MA, Kozhevnikov M (2007) Updating representations of learned scenes. Psychol Res 71:265–276" href="/article/10.1007/s10055-016-0294-8#ref-CR9" id="ref-link-section-d75805e385">2007</a>; Foo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Foo P, Duchon A, Warren WH Jr, Tarr MJ (2007) Humans do not switch between path knowledge and landmarks when learning a new environment. Psychol Res 71:240–251. doi:&#xA;                    10.1007/s00426-006-0080-4&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR10" id="ref-link-section-d75805e388">2007</a>). In a collaborative activity, to ensure mutual understanding and good coordination of tasks, the collaborators need to build a shared mental representation of the situation: the common frame of reference (Hoc <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hoc J-M (2001) Towards a cognitive approach to human–machine cooperation in dynamic situations. Int J Hum Comput Stud 54:509–540. doi:&#xA;                    10.1006/ijhc.2000.0454&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR14" id="ref-link-section-d75805e391">2001</a>; Chellali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Chellali A, Milleville-Pennel I, Dumas C (2012) Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments. Virtual Real 17:1–15. doi:&#xA;                    10.1007/s10055-012-0214-5&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR4" id="ref-link-section-d75805e394">2012</a>; Schouten et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Schouten AP, van den Hooff B, Feldberg F (2013) Virtual team work: group decision making in 3D virtual environments. Commun Res. doi:&#xA;                    10.1177/0093650213509667&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR26" id="ref-link-section-d75805e398">2013</a>). This is also called the common ground—“that is, mutual knowledge, mutual beliefs, and mutual assumptions” (Clark and Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149" href="/article/10.1007/s10055-016-0294-8#ref-CR6" id="ref-link-section-d75805e401">1991</a>). To build this representation, both collaborators need to attribute some knowledge to their co-worker: They have to take into account the singular mental representation of the other. In spatial tasks, an important question is how operators represent and share space. To exchange spatial information, collaborators use verbal communication. Clark and Brennan (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149" href="/article/10.1007/s10055-016-0294-8#ref-CR6" id="ref-link-section-d75805e404">1991</a>) pointed out that communication is a collective activity in which people have to coordinate on content. They argued in favor of a principle of least collaborative effort: “In conversation, the participants try to minimize their collaborative effort—the work that both do from the initiation of each contribution to its mutual acceptance.” This principle replaces the least (individual) effort principle. Many studies about spatial dialogs have argued in favor of least collaborative effort (Schober <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:&#xA;                    10.1080/01638539509544939&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR25" id="ref-link-section-d75805e407">1995</a>; Pouliquen-Lardy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Pouliquen-Lardy L, Mars F, Guillaume F, Milleville-Pennel I (2015) Virtual collaboration: effect of spatial configuration on spatial statements production. Cogn Process 16:337–342. doi:&#xA;                    10.1007/s10339-015-0672-2&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR20" id="ref-link-section-d75805e410">2015</a>). Roger et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Roger M, Knutsen D, Bonnardel N, Le Bigot L (2013) Landmark frames of reference in interactive route description tasks. Appl Cogn Psychol 27:497–504. doi:&#xA;                    10.1002/acp.2927&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR23" id="ref-link-section-d75805e413">2013</a>) showed that during a remote navigation task, roles (i.e., guides or those being guided) influenced spatial communication. Guides introduced more landmarks than those being guided and were also more likely to use perspective-taking. In their study, asymmetry was induced by task distribution (between the guides and the guided) and also by the technical device used: Only the guided person could physically explore the (real) environment. The difference with the context of ICVE is that both collaborators are immersed in the same virtual environment: they can see each other, but both of them have mediated experience of the environment.</p><p>Clark and Brennan (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149" href="/article/10.1007/s10055-016-0294-8#ref-CR6" id="ref-link-section-d75805e419">1991</a>) listed the different sources of costs in dialog. For the situations we are interested in, we will retain the production and understanding costs. Many studies have pointed out that the time needed to take another perspective varies with the degree of rotation (Roberts and Aman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Roberts RJ, Aman CJ (1993) Developmental differences in giving directions: spatial frames of reference and mental rotation. Child Dev 64:1258–1270" href="/article/10.1007/s10055-016-0294-8#ref-CR22" id="ref-link-section-d75805e422">1993</a>; Schober <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:&#xA;                    10.1080/01638539509544939&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR25" id="ref-link-section-d75805e425">1995</a>; Michelon and Zacks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Michelon P, Zacks JM (2006) Two kinds of visual perspective taking. Percept Psychophys 68:327–337" href="/article/10.1007/s10055-016-0294-8#ref-CR18" id="ref-link-section-d75805e428">2006</a>; Duran et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Duran ND, Dale R, Kreuz RJ (2011) Listeners invest in an assumed other’s perspective despite cognitive cost. Cognition 121:22–40" href="/article/10.1007/s10055-016-0294-8#ref-CR7" id="ref-link-section-d75805e431">2011</a>). For example, if Peter and some friends are in the same car, and Peter says, “look right, there is a beautiful rainbow”, neither he nor his friends have to operate any mental transformation to understand where the rainbow is located. Conversely, if Peter is facing Maria and says, “can you give me the glass on your right, please?” he has to operate a 180° mental rotation to take Maria’s perspective. At the same time, Maria does not have to perform a mental transformation to understand which glass Peter is talking about. Thus, the relative orientation of the speakers is required in order to determine what kind of mental transformation is needed. In addition, it is not always necessary to operate mental rotation, particularly when no perspective is taken. For instance, if Thomas is waiting for Karl at the door of a building, watching the lobby, and says to Karl, “be careful when you come down, because there is a puddle in the lobby”, Thomas has no mental transformation to operate because the scene is available for him. Karl, on the other hand, has to mentally imagine the lobby (“imaginal updating”, see Hintzman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Hintzman DL, O’Dell CS, Arndt DR (1981) Orientation in cognitive maps. Cogn Psychol 13:149–206" href="/article/10.1007/s10055-016-0294-8#ref-CR13" id="ref-link-section-d75805e435">1981</a>; Riecke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Riecke BE, Cunningham DW, Bülthoff H (2007) Spatial updating in virtual reality: the sufficiency of visual information. Psychol Res 71:298–313. doi:&#xA;                    10.1007/s00426-006-0085-z&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR21" id="ref-link-section-d75805e438">2007</a>). As Roskos-Ewoldsen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Roskos-Ewoldsen B, McNamara TP, Shelton AL, Carr W (1998) Mental representations of large and small spatial layouts are orientation dependent. J Exp Psychol Learn Mem Cogn 24:215" href="/article/10.1007/s10055-016-0294-8#ref-CR24" id="ref-link-section-d75805e441">1998</a>) argued, even if location is encoded in an orientation-dependent manner, Karl can choose his preferred perspective because Thomas’s utterance does not impose any particular point of view (i.e., it is perspective-free). The current study will look at mental transformation requirements for each utterance in order to complete a frames of reference analysis and get a better understanding of the costs of producing and understanding spatial statements. However, although mental transformation requirements can identify any cognitive costs, they cannot measure them. Online measurement of cognitive workload is difficult. However, it would be interesting to see if both guides and manipulators experience the same cognitive effort during the task. Indeed, the least collaborative principle suggests that, in some situations, people may individually assume a higher cost in order to reduce collaborative cost (Clark and Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149" href="/article/10.1007/s10055-016-0294-8#ref-CR6" id="ref-link-section-d75805e444">1991</a>). While spatial dialog informs about the verbal strategies, it does not inform directly about the cognitive workload. Moreover, to our knowledge, no study yet exists which looks at spatial dialog and mental workload measurements at the same time.</p><p>The current study is focused on spatial communication during remote collaboration in ICVEs. A manipulator had to move a virtual object according to the instructions given by a guide. Collaborators were immersed in the same virtual environment. They were represented by avatars and interacted over the phone. Dialogs were recorded, transcribed and analyzed. We looked at the frames of references used and the mental transformations required to produce and understand each spatial utterance. Finally, participants evaluated the cognitive workload needed to perform the whole task. According to the least collaborative principle, the aim of the current study was to test three closely related hypotheses.</p><ul class="u-list-style-bullet">
                  <li>
                    <p>First, the choice of the reference frames used may be determined by role asymmetry and the requirements of the task. Since the guide gives information to the manipulator, he will be more likely to take the other’s perspective (Roger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Roger M, Knutsen D, Bonnardel N, Le Bigot L (2013) Landmark frames of reference in interactive route description tasks. Appl Cogn Psychol 27:497–504. doi:&#xA;                    10.1002/acp.2927&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR23" id="ref-link-section-d75805e456">2013</a>).</p>
                  </li>
                  <li>
                    <p>Second, changing the perspective imposes spatial transformations, including mental rotations, which will have a cognitive cost.</p>
                  </li>
                  <li>
                    <p>Third, both speakers will produce spatial statements that serve to minimize their collaborative effort.</p>
                  </li>
                </ul>
                     </div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Method</h2><div class="c-article-section__content" id="Sec2-content"><p>To study spatial communication in remote collaboration, we proposed a naturalistic collaborative spatial task to be carried out between two participants. The task consisted of moving an object from one point to two others in a constraint-based ICVE. One participant moved the object (manipulator), while the other guided him (guide).</p><h3 class="c-article__sub-heading" id="Sec3">Participants</h3><p>Twenty-eight native French speakers participated in this study (6 women, 22 men; mean age 24 years, age range 20–54). Participants comprised either undergraduate students from Ecole Centrale de Nantes (23 participants) or interns working for Airbus Group (five participants). Only one of them had previous experience of virtual reality systems. They worked in pairs (six male–female pairs, eight male pairs). Some studies have shown lower levels of performance in spatial tasks for women (Lawton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:321–337. doi:&#xA;                    10.1023/A:1010981616842&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR17" id="ref-link-section-d75805e489">2001</a>; Kimura <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Kimura D (2001) Cerveau d’homme, cerveau de femme?. Odile Jacob, Paris" href="/article/10.1007/s10055-016-0294-8#ref-CR15" id="ref-link-section-d75805e492">2001</a>; Kolb and Whishaw <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kolb B, Whishaw I (2002) Cerveau &amp; comportement. De Boeck, Paris" href="/article/10.1007/s10055-016-0294-8#ref-CR16" id="ref-link-section-d75805e495">2002</a>). Thus, no female pairs were formed in our study and the role attribution was balanced, with three women assigned the guide role, and three the manipulator role.</p><h3 class="c-article__sub-heading" id="Sec4">Apparatus</h3><p>For each session, two participants collaborated in an ICVE representing an aircraft construction site. We used two immersive walls (3.5 × 2.2 m and 2 × 3 m) with rear double projections for stereoscopy. A tracking system was used both on the glasses (for parallax) and on the joystick (for 3D location and orientation of the pointing laser). The joystick was used for the participants’ displacements (in all directions) in the virtual environment and to select menus for object manipulation. Each participant was represented by an avatar composed of a head and a laser (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig1">1</a>). </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Picture of a participant in front of the immersive wall. The <i>pink head</i> with the <i>black glasses</i> on the <i>left of the picture</i> is the avatar of the remote collaborator. The <i>transparent blue cuboid</i> on the <i>right</i> of the screen is the object the manipulator participant had to move in the virtual environment (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec5">Procedure</h3><p>Each participant was given a role. He or she was to act as either the guide or the manipulator. The role was described by written explanations followed by a specific training phase. For the guide, the training phase took place in the same virtual environment used for the collaborative session. It represented a generic factory building where a plane was in the process of being assembled and equipped (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig2">2</a>a). During his training, he was instructed to visit the three floors of the factory and remember the entrances and obstacles. To ensure he had effectively explored the entire space, he had to draw a map of each floor after training.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Map of the collaborative virtual environment. <b>a</b> Participants had to move from the starting point (level 0) to point A (level 1) and then to point B (level 2), taking into account four difficulties. <b>b</b> Empty map of the environment received by manipulators</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>At the same time, the manipulator received a map with a non-detailed schema of the place (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig2">2</a>b). His training took place in an empty virtual building where he had to get used to object manipulation. The target object was a transparent cuboid (.7 × 1 × .7 m, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig1">1</a>).</p><p>After training, each team had 45 min to move the target object from the starting point (level 0) to two other points (point A in level 1 and point B in level 2, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig2">2</a>). To find the only possible solution, collaborators had to deal with four spatial constraints, two for each point. These difficulties were either obstructions (i.e., paths too narrow) or spatial rearrangements (i.e., gate opening).</p><p>Once the remote session started, they were free to move in the virtual place and speak through a headset phone. All the sessions were videotaped. The conversations were digitally recorded. After the session ended and a short break, participants answered a questionnaire. It consisted of the National Aeronautics and Space Administration task load index (NASA-TLX; Hart <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergonomics society annual meeting. Sage Publications, London, pp 904–908" href="/article/10.1007/s10055-016-0294-8#ref-CR12" id="ref-link-section-d75805e597">2006</a>) and a few additional questions. The NASA-TLX is a six-component scale that measures: mental demand, physical demand, temporal demand, performance, and effort and frustration level. Mental demand was measured by asking the following questions: “How much mental and perceptual activity was required (e.g., thinking, deciding, calculating, remembering, looking, and searching)? Was the task easy or demanding, simple or complex, exacting or forgiving?” Nygren (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Nygren TE (1991) Psychometric properties of subjective workload measurement techniques: implications for their use in the assessment of perceived mental workload. Hum Factors 33:17–33. doi:&#xA;                    10.1177/001872089103300102&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR19" id="ref-link-section-d75805e600">1991</a>) has already offered a detailed examination of the NASA-TLX scale. He concluded that the test was efficient enough to solve “workload problems in many applied settings, such as systems development, by accurately predicting operator workload levels both across operators and across a wide range of relevant operator tasks”.</p><p>Several studies have shown that the properties of a collaborative task can influence the presence in virtual environments (Casanueva and Blake <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Casanueva J, Blake E (2000) The effects of group collaboration on presence in a collaborative virtual environment. In: Mulder J, van Liere R (eds) Virtual environments 2000. Springer, Vienna, pp 85–94" href="/article/10.1007/s10055-016-0294-8#ref-CR2" id="ref-link-section-d75805e607">2000</a>; Chellali et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Chellali AM, Dumas C, Milleville-Pennel I, Nouri E (2007) Common Frame of reference in collaborative virtual environments and their impact on presence. In: Proceedings of the 10th annual international workshop on presence, Starlab Barcelona, Barcelone, Spain, pp 371–372" href="/article/10.1007/s10055-016-0294-8#ref-CR3" id="ref-link-section-d75805e610">2007</a>). In our study, in order to ascertain whether or not the presence and co-presence feelings influenced collaboration in return, a simplified evaluation procedure was used. The goal was not to measure the presence, for which validated scales already exist (Slater <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Philos Trans R Soc B Biol Sci 364:3549–3557" href="/article/10.1007/s10055-016-0294-8#ref-CR27" id="ref-link-section-d75805e613">2009</a>), but rather to check that role attribution did not yield a major difference in that respect. Participants answered two additional questions about the presence feeling (“how did you feel ‘there’ in the environment?”) and co-presence feeling (“Did it feel that your co-worker was present with you in the virtual place?”). This was assessed by means of a 10-cm scale bounded by “absolutely not” and “absolutely”.</p><h3 class="c-article__sub-heading" id="Sec6">Analysis of spatial statements</h3><p>Videos and conversations were edited with Adobe Premier Elements® and analyzed with Actogram Kronos® (timestamp and coding observations software). In total, 1808 spatial utterances were coded by the first author. The categories were designed to be univocal to ensure an objective coding. Any ambiguous utterances were discarded. This happened on 206 occasions, which resulted in 1602 classified utterances. Each spatial utterance was time stamped and classified in one of the five following categories:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Neutral: “those that do not depend on any one view” (Schober <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:&#xA;                    10.1080/01638539509544939&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR25" id="ref-link-section-d75805e631">1995</a>). This category mainly includes utterances that contain place names (e.g., “let’s go to level 1”).</p>
                    </li>
                    <li>
                      <p>Ego-centered: when the utterance takes the speaker’s perspective (e.g., “it is just in front of me”).</p>
                    </li>
                    <li>
                      <p>Addressee-centered: when the speaker uses the addressee’s perspective (e.g., “turn on your right”).</p>
                    </li>
                    <li>
                      <p>Object-centered: when the reference is an object, usually the plane (e.g., “at the front of the plane”).</p>
                    </li>
                    <li>
                      <p>Other-centered: when another perspective is needed (e.g., “when you get out of the elevator, it will be just in front of the table”).</p>
                    </li>
                  </ul>
                        <p>Each spatial utterance was also classified according to the mental transformation needs. This was achieved through the examination of the videos. For each utterance, two assessments were made: (1) Does the utterance need mental transformation to be carried out by the speaker? This was achieved by assessing the relative orientation of the speaker in the environment and the point of view he was taking when speaking. (2) Does the utterance need mental transformation to be understood by the addressee? This was achieved by assessing the relative orientation of the listener in the environment, and the perspective taken by the speaker when speaking. As explained in the introduction, two mental transformations were identified: mental rotations that require either a change of perspective or other mental transformations based on mental imagery, where no particular perspective is taken (perspective-free). For example, if the guide spoke about a table just near him, but the manipulator was far away and was not able to see the table: “this is near the table” is classified as: (1) no mental transformation for the speaker (guide); and (2) perspective-free transformation for the listener (manipulator), because he needs to imagine the table without taking a particular perspective.</p><h3 class="c-article__sub-heading" id="Sec7">Data analysis</h3><p>Four pairs of participants were excluded from the spatial utterances analysis because of the poor quality of the audio recording; however, they were included in the mental workload analysis. Frequency analyses (i.e., counting the number of reference frames used and mental transformations made) were performed using Chi-square tests. Means were compared using bilateral <i>t</i> tests. For all tests, the level of significance used was <i>p</i> &lt; .05.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Results</h2><div class="c-article-section__content" id="Sec8-content"><p>Descriptively, the guides produced more spatial utterances than the manipulators (respectively, M = 93.2, SD = 50.5 and M = 67.0, SD = 25.0). This difference is not significant due to a large individual variability [<i>t</i>(18) = 1.46, <i>p</i> = .16].</p><h3 class="c-article__sub-heading" id="Sec9">Reference frames</h3><p>The comparison between guides and manipulators showed a significant global difference in the reference frames used [<i>χ</i>
                           <sup>2</sup>(5, 1602) = 116.08, <i>p</i> &lt; .001], see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig3">3</a>. The guides used significantly more neutral (referring to the general environment) and addressee-centered utterances than the manipulators [respectively, <i>χ</i>
                           <sup>2</sup>(1, 1602) = 8.09, <i>p</i> &lt; .01 and <i>χ</i>
                           <sup>2</sup>(1, 1602) = 76.20, <i>p</i> &lt; .001]. Conversely, the manipulators used significantly more ego-centered utterances than the guides [<i>χ</i>
                           <sup>2</sup>(1, 1602) = 53.59, <i>p</i> &lt; .001].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Total number of spatial utterances produced by the guides and the manipulators. Spatial utterances are classified according to five frames of reference, **<i>p</i> &lt; .01</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig4">4</a> shows the evolution of the number of ego-centered and addressee-centered utterances between the first and the fourth quarters of the sessions. The number of addressee-centered utterances increased over time for the guides [<i>χ</i>
                           <sup>2</sup>(1, 151) = 8.9, <i>p</i> &lt; .01]. Conversely, the number of ego-centered utterances increased for the manipulators [χ<sup>2</sup>(1, 88) = 14.31, <i>p</i> &lt; .001].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Difference between ego-centered and addressee-centered utterances produced by the guides and the manipulators for the first (<i>1</i>) and the last (<i>4</i>) quarters of the sessions, **<i>p</i> &lt; .01</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec10">Mental transformations for the speaker</h3><p>A comparison of the guides and the manipulators showed a significant global difference in the mental transformations required by the speaker to produce the utterances [<i>χ</i>
                           <sup>2</sup>(2, 1602) = 116.08, <i>p</i> &lt; .001], see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig5">5</a>. The guides made significantly more utterances that required mental rotations to be made than the manipulators [<i>χ</i>
                           <sup>2</sup>(1, 1602) = 34.35, <i>p</i> &lt; .001]. They also made more utterances that did not require any mental transformation [<i>χ</i>
                           <sup>2</sup>(1, 1602) = 19.44, <i>p</i> &lt; .001].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Number of spatial utterances according to the mental transformations required to be produced, i.e., for the speaker, for the guides and the manipulators. **<i>p</i> &lt; .01</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec11">Mental transformations for the listener</h3><p>A comparison of the guides and the manipulators showed a significant global difference in mental transformations required by the listener to understand spatial utterances [<i>χ</i>
                           <sup>2</sup>(2, 1602) = 45.12, <i>p</i> &lt; .001, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig6">6</a>]. The manipulators made significantly more utterances requiring mental rotations to be understood than the guides [<i>χ</i>
                           <sup>2</sup>(1, 1602) = 40.16, <i>p</i> &lt; .001], i.e., the guides produced fewer utterances requiring mental rotation to be understood, and the manipulators had to make fewer mental rotations to understand what the guides said. The guides produced significantly more utterances that did not require any mental transformation to be understood than the manipulators [<i>χ</i>
                           <sup>2</sup>(1, 1602) = 20.34, <i>p</i> &lt; .001].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Number of spatial utterances according to the mental transformations required to be understood, i.e., for the listener, produced by the guides and the manipulators. **<i>p</i> &lt; .001</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec12">Questionnaires</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig7">7</a> presents the means scores of the different subscales of the NASA-TLX questionnaire. The analysis of the global score revealed that the workload was higher for the manipulators than for the guides [<i>t</i>(12) = 2.52, <i>p</i> &lt; .05]. A components analysis did not show any differences for the manipulators: All subscales yielded similar scores. However, for the guides, the mental demand component was significantly higher than all other components (<i>p</i> &lt; .05 in all cases).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0294-8/MediaObjects/10055_2016_294_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Mean scores for the six components of the NASA-TLX questionnaire for the guides and the manipulators. The <i>asterisk</i> represents the fact that the mental demand score was significantly higher than all other components for the guides only (*<i>p</i> &lt; .05)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0294-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>On the 10-cm scale, on average, people scored their presence feeling at 6.5 cm (SD = 1.9) and co-present feeling at 8 cm (SD = 1.97). There was no difference between the guides and the manipulators [presence: <i>t</i>(26) = .63, <i>p</i> = .53; co-presence: <i>t</i>(26) = .54, <i>p</i> = .59].</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Discussion</h2><div class="c-article-section__content" id="Sec13-content"><p>In the context of remote collaboration using ICVE, the aim of the current study was to investigate how guides and manipulators exchange spatial information. The results show that task distribution influenced spatial information sharing, with the manipulators favoring an ego-centered reference frame, while the guides made more addressee-centered statements. Perspective-taking yielded a moderate increase in mental rotations only, which translated as a specific increase in the mental demand component of mental workload for the guides. Overall, this suggests that the guides tried to diminish the manipulator’s workload by taking their point of view, but without imposing too much workload on themselves, in accordance with the least collaborative effort principle.</p><p>First, the results show that the guides and the manipulators did not share spatial information in the same way. With regard to perspective-taking, it seems that all the dyads preferentially used the manipulator’s perspective. Dialog alignment may reflect a deeper alignment at the level of the representations (Garrod and Pickering <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Garrod S, Pickering MJ (2009) Joint action, interactive alignment, and dialog. Top Cogn Sci 1:292–304" href="/article/10.1007/s10055-016-0294-8#ref-CR11" id="ref-link-section-d75805e996">2009</a>). In this study, the common frame of reference was oriented toward the performance of moving the object in the environment. Since the guide provided information to the manipulator, who had to move the object in the environment, both guides and manipulators privileged the manipulator’s perspective when sharing spatial information. This strategy was observed early on during the collaboration, but increased markedly over time, as illustrated by Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0294-8#Fig4">4</a>. This suggests that the spatial dialog adapted as participants interacted with each other.</p><p>Moreover, looking at the frames of reference, the guides produced more exo-centered statements than ego-centered or neutral statements. The high number of perspective changes could have been expected to lead to a massive mental workload. Indeed, it has been repeatedly reported that reaction times increase with the mental rotation necessary to change perspective (Hintzman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Hintzman DL, O’Dell CS, Arndt DR (1981) Orientation in cognitive maps. Cogn Psychol 13:149–206" href="/article/10.1007/s10055-016-0294-8#ref-CR13" id="ref-link-section-d75805e1005">1981</a>; Boer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Boer LC (1991) Mental rotation in perspective problems. Acta Psychol (Amst) 76:1–9" href="/article/10.1007/s10055-016-0294-8#ref-CR1" id="ref-link-section-d75805e1008">1991</a>; Roberts and Aman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Roberts RJ, Aman CJ (1993) Developmental differences in giving directions: spatial frames of reference and mental rotation. Child Dev 64:1258–1270" href="/article/10.1007/s10055-016-0294-8#ref-CR22" id="ref-link-section-d75805e1011">1993</a>; Schober <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:&#xA;                    10.1080/01638539509544939&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR25" id="ref-link-section-d75805e1014">1995</a>; Michelon and Zacks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Michelon P, Zacks JM (2006) Two kinds of visual perspective taking. Percept Psychophys 68:327–337" href="/article/10.1007/s10055-016-0294-8#ref-CR18" id="ref-link-section-d75805e1017">2006</a>; Duran et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Duran ND, Dale R, Kreuz RJ (2011) Listeners invest in an assumed other’s perspective despite cognitive cost. Cognition 121:22–40" href="/article/10.1007/s10055-016-0294-8#ref-CR7" id="ref-link-section-d75805e1021">2011</a>). However, the mental transformation analysis revealed that a large part of the statements did not require any mental rotation or perspective-free transformations to be produced. Thus, the guides may have used different strategies to take the perspective of the manipulators and at the same time to minimize their own production costs. They may have moved in the environment in order to be most often aligned with the manipulator’s point of view or they may have chosen a perspective (object or other-centered) that was compatible with their current situation in space. This would be consistent with Schober (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:&#xA;                    10.1080/01638539509544939&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-016-0294-8#ref-CR25" id="ref-link-section-d75805e1024">1995</a>), a study in which neutral statements were preferred when the protagonists were not physically aligned. This strategy minimizes both production and understanding costs. Moreover, the guides minimized the manipulators’ understanding costs by producing utterances that did not require any mental transformation. This can be seen as a manifestation of the least collaborative effort: to reduce the cognitive workload of their collaborator, the guides operated perspective-taking, but at the lowest cost to them, too. Of course, this depends on an accurate representation of the whole virtual environment and its spatial configuration. This was only the case for the guides, who had the opportunity to explore the environment before performing the collaborative task. This may be one of the reasons why they chose to take upon themselves the global reduction in mental workload.</p><p>Although a large proportion of the spatial statements did not require mental transformations, the guides still had to make more mental rotations than the manipulators in order to make and understand spatial utterances. This did not translate as a global increase in their mental workload, which remained lower than that of the manipulators. However, a specific increase in the mental demand component of the NASA-TLX was observed for the guides only. Mental demand is about “thinking, deciding, calculating, remembering, looking, and searching”; thus, it makes sense that this component alone is affected by the mental rotations that underlie perspective-taking.</p><p>The mental workload measurement was performed at the end of the task. Of course, it cannot be attributed only to the spatial dialog. It was also influenced by the difficulty of the task. Overall, the mental workload was higher for the manipulators than for the guides, probably because the manipulation of the object was a demanding task. Further investigations should be carried out to assess if this was an essential condition for the guides to assume more mental rotations. In other words, with equal levels of workload or with more workload associated with the guiding task than to the manipulation task, it remains to be determined how the minimization of the collaborative effort would be achieved.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Conclusion</h2><div class="c-article-section__content" id="Sec14-content"><p>To sum up, task distribution affects spatial communication and associated mental workload. Both the guides and the manipulators centered their common frame of reference on action, i.e., on the manipulator; in so doing, they influenced perspective-taking. This phenomenon required the guides to operate more mental rotations to take the manipulator’s point of view and to understand their statements. According to the least collaborative effort principle, since the manipulation task required more mental workload, the guides assumed extra cognitive workload in verbal communication. This suggests that, in order to develop efficient tools in IVCE, it is necessary to take into account asymmetrical task distribution and the associated mental processes. Guides can be helped by the use of perspective-taking aids, such as multiple views, whereas manipulation tools, such as collision visualization, would be more beneficial for manipulators.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LC. Boer, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Boer LC (1991) Mental rotation in perspective problems. Acta Psychol (Amst) 76:1–9" /><p class="c-article-references__text" id="ref-CR1">Boer LC (1991) Mental rotation in perspective problems. Acta Psychol (Amst) 76:1–9</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0001-6918%2891%2990050-A" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mental%20rotation%20in%20perspective%20problems&amp;journal=Acta%20Psychol%20%28Amst%29&amp;volume=76&amp;pages=1-9&amp;publication_year=1991&amp;author=Boer%2CLC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Casanueva, E. Blake, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Casanueva J, Blake E (2000) The effects of group collaboration on presence in a collaborative virtual environm" /><p class="c-article-references__text" id="ref-CR2">Casanueva J, Blake E (2000) The effects of group collaboration on presence in a collaborative virtual environment. In: Mulder J, van Liere R (eds) Virtual environments 2000. Springer, Vienna, pp 85–94</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environments%202000&amp;pages=85-94&amp;publication_year=2000&amp;author=Casanueva%2CJ&amp;author=Blake%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chellali AM, Dumas C, Milleville-Pennel I, Nouri E (2007) Common Frame of reference in collaborative virtual e" /><p class="c-article-references__text" id="ref-CR3">Chellali AM, Dumas C, Milleville-Pennel I, Nouri E (2007) Common Frame of reference in collaborative virtual environments and their impact on presence. In: Proceedings of the 10th annual international workshop on presence, Starlab Barcelona, Barcelone, Spain, pp 371–372</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Chellali, I. Milleville-Pennel, C. Dumas, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Chellali A, Milleville-Pennel I, Dumas C (2012) Influence of contextual objects on spatial interactions and vi" /><p class="c-article-references__text" id="ref-CR4">Chellali A, Milleville-Pennel I, Dumas C (2012) Influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments. Virtual Real 17:1–15. doi:<a href="https://doi.org/10.1007/s10055-012-0214-5">10.1007/s10055-012-0214-5</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-012-0214-5" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Influence%20of%20contextual%20objects%20on%20spatial%20interactions%20and%20viewpoints%20sharing%20in%20virtual%20environments&amp;journal=Virtual%20Real&amp;doi=10.1007%2Fs10055-012-0214-5&amp;volume=17&amp;pages=1-15&amp;publication_year=2012&amp;author=Chellali%2CA&amp;author=Milleville-Pennel%2CI&amp;author=Dumas%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Churchill EF, Snowdon DN, Munro AJ (eds) (2001) Collaborative virtual environments. Springer, London" /><p class="c-article-references__text" id="ref-CR5">Churchill EF, Snowdon DN, Munro AJ (eds) (2001) Collaborative virtual environments. Springer, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20virtual%20environments&amp;publication_year=2001">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HH. Clark, SE. Brennan, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149" /><p class="c-article-references__text" id="ref-CR6">Clark HH, Brennan SE (1991) Grounding in communication. Perspect Soc Shar Cogn 13:127–149</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F10096-006" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grounding%20in%20communication&amp;journal=Perspect%20Soc%20Shar%20Cogn&amp;volume=13&amp;pages=127-149&amp;publication_year=1991&amp;author=Clark%2CHH&amp;author=Brennan%2CSE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ND. Duran, R. Dale, RJ. Kreuz, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Duran ND, Dale R, Kreuz RJ (2011) Listeners invest in an assumed other’s perspective despite cognitive cost. C" /><p class="c-article-references__text" id="ref-CR7">Duran ND, Dale R, Kreuz RJ (2011) Listeners invest in an assumed other’s perspective despite cognitive cost. Cognition 121:22–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cognition.2011.06.009" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Listeners%20invest%20in%20an%20assumed%20other%E2%80%99s%20perspective%20despite%20cognitive%20cost&amp;journal=Cognition&amp;volume=121&amp;pages=22-40&amp;publication_year=2011&amp;author=Duran%2CND&amp;author=Dale%2CR&amp;author=Kreuz%2CRJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MR. Endsley, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Endsley MR (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37:32–64" /><p class="c-article-references__text" id="ref-CR8">Endsley MR (1995) Toward a theory of situation awareness in dynamic systems. Hum Factors 37:32–64</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872095779049543" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20a%20theory%20of%20situation%20awareness%20in%20dynamic%20systems&amp;journal=Hum%20Factors&amp;volume=37&amp;pages=32-64&amp;publication_year=1995&amp;author=Endsley%2CMR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CA. Finlay, MA. Motes, M. Kozhevnikov, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Finlay CA, Motes MA, Kozhevnikov M (2007) Updating representations of learned scenes. Psychol Res 71:265–276" /><p class="c-article-references__text" id="ref-CR9">Finlay CA, Motes MA, Kozhevnikov M (2007) Updating representations of learned scenes. Psychol Res 71:265–276</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00426-006-0082-2" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Updating%20representations%20of%20learned%20scenes&amp;journal=Psychol%20Res&amp;volume=71&amp;pages=265-276&amp;publication_year=2007&amp;author=Finlay%2CCA&amp;author=Motes%2CMA&amp;author=Kozhevnikov%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Foo, A. Duchon, WH. Warren, MJ. Tarr, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Foo P, Duchon A, Warren WH Jr, Tarr MJ (2007) Humans do not switch between path knowledge and landmarks when l" /><p class="c-article-references__text" id="ref-CR10">Foo P, Duchon A, Warren WH Jr, Tarr MJ (2007) Humans do not switch between path knowledge and landmarks when learning a new environment. Psychol Res 71:240–251. doi:<a href="https://doi.org/10.1007/s00426-006-0080-4">10.1007/s00426-006-0080-4</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00426-006-0080-4" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Humans%20do%20not%20switch%20between%20path%20knowledge%20and%20landmarks%20when%20learning%20a%20new%20environment&amp;journal=Psychol%20Res&amp;doi=10.1007%2Fs00426-006-0080-4&amp;volume=71&amp;pages=240-251&amp;publication_year=2007&amp;author=Foo%2CP&amp;author=Duchon%2CA&amp;author=Warren%2CWH&amp;author=Tarr%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Garrod, MJ. Pickering, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Garrod S, Pickering MJ (2009) Joint action, interactive alignment, and dialog. Top Cogn Sci 1:292–304" /><p class="c-article-references__text" id="ref-CR11">Garrod S, Pickering MJ (2009) Joint action, interactive alignment, and dialog. Top Cogn Sci 1:292–304</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1756-8765.2009.01020.x" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Joint%20action%2C%20interactive%20alignment%2C%20and%20dialog&amp;journal=Top%20Cogn%20Sci&amp;volume=1&amp;pages=292-304&amp;publication_year=2009&amp;author=Garrod%2CS&amp;author=Pickering%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergon" /><p class="c-article-references__text" id="ref-CR12">Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergonomics society annual meeting. Sage Publications, London, pp 904–908</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DL. Hintzman, CS. O’Dell, DR. Arndt, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Hintzman DL, O’Dell CS, Arndt DR (1981) Orientation in cognitive maps. Cogn Psychol 13:149–206" /><p class="c-article-references__text" id="ref-CR13">Hintzman DL, O’Dell CS, Arndt DR (1981) Orientation in cognitive maps. Cogn Psychol 13:149–206</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2F0010-0285%2881%2990007-4" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Orientation%20in%20cognitive%20maps&amp;journal=Cogn%20Psychol&amp;volume=13&amp;pages=149-206&amp;publication_year=1981&amp;author=Hintzman%2CDL&amp;author=O%E2%80%99Dell%2CCS&amp;author=Arndt%2CDR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J-M. Hoc, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hoc J-M (2001) Towards a cognitive approach to human–machine cooperation in dynamic situations. Int J Hum Comp" /><p class="c-article-references__text" id="ref-CR14">Hoc J-M (2001) Towards a cognitive approach to human–machine cooperation in dynamic situations. Int J Hum Comput Stud 54:509–540. doi:<a href="https://doi.org/10.1006/ijhc.2000.0454">10.1006/ijhc.2000.0454</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fijhc.2000.0454" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20a%20cognitive%20approach%20to%20human%E2%80%93machine%20cooperation%20in%20dynamic%20situations&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;doi=10.1006%2Fijhc.2000.0454&amp;volume=54&amp;pages=509-540&amp;publication_year=2001&amp;author=Hoc%2CJ-M">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="D. Kimura, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kimura D (2001) Cerveau d’homme, cerveau de femme?. Odile Jacob, Paris" /><p class="c-article-references__text" id="ref-CR15">Kimura D (2001) Cerveau d’homme, cerveau de femme?. Odile Jacob, Paris</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cerveau%20d%E2%80%99homme%2C%20cerveau%20de%20femme%3F&amp;publication_year=2001&amp;author=Kimura%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="B. Kolb, I. Whishaw, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Kolb B, Whishaw I (2002) Cerveau &amp; comportement. De Boeck, Paris" /><p class="c-article-references__text" id="ref-CR16">Kolb B, Whishaw I (2002) Cerveau &amp; comportement. De Boeck, Paris</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cerveau%20%26%20comportement&amp;publication_year=2002&amp;author=Kolb%2CB&amp;author=Whishaw%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CA. Lawton, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:3" /><p class="c-article-references__text" id="ref-CR17">Lawton CA (2001) Gender and regional differences in spatial referents used in direction giving. Sex Roles 44:321–337. doi:<a href="https://doi.org/10.1023/A:1010981616842">10.1023/A:1010981616842</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1010981616842" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20and%20regional%20differences%20in%20spatial%20referents%20used%20in%20direction%20giving&amp;journal=Sex%20Roles&amp;doi=10.1023%2FA%3A1010981616842&amp;volume=44&amp;pages=321-337&amp;publication_year=2001&amp;author=Lawton%2CCA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Michelon, JM. Zacks, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Michelon P, Zacks JM (2006) Two kinds of visual perspective taking. Percept Psychophys 68:327–337" /><p class="c-article-references__text" id="ref-CR18">Michelon P, Zacks JM (2006) Two kinds of visual perspective taking. Percept Psychophys 68:327–337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03193680" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two%20kinds%20of%20visual%20perspective%20taking&amp;journal=Percept%20Psychophys&amp;volume=68&amp;pages=327-337&amp;publication_year=2006&amp;author=Michelon%2CP&amp;author=Zacks%2CJM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TE. Nygren, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Nygren TE (1991) Psychometric properties of subjective workload measurement techniques: implications for their" /><p class="c-article-references__text" id="ref-CR19">Nygren TE (1991) Psychometric properties of subjective workload measurement techniques: implications for their use in the assessment of perceived mental workload. Hum Factors 33:17–33. doi:<a href="https://doi.org/10.1177/001872089103300102">10.1177/001872089103300102</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Psychometric%20properties%20of%20subjective%20workload%20measurement%20techniques%3A%20implications%20for%20their%20use%20in%20the%20assessment%20of%20perceived%20mental%20workload&amp;journal=Hum%20Factors&amp;doi=10.1177%2F001872089103300102&amp;volume=33&amp;pages=17-33&amp;publication_year=1991&amp;author=Nygren%2CTE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Pouliquen-Lardy, F. Mars, F. Guillaume, I. Milleville-Pennel, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Pouliquen-Lardy L, Mars F, Guillaume F, Milleville-Pennel I (2015) Virtual collaboration: effect of spatial co" /><p class="c-article-references__text" id="ref-CR20">Pouliquen-Lardy L, Mars F, Guillaume F, Milleville-Pennel I (2015) Virtual collaboration: effect of spatial configuration on spatial statements production. Cogn Process 16:337–342. doi:<a href="https://doi.org/10.1007/s10339-015-0672-2">10.1007/s10339-015-0672-2</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10339-015-0672-2" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20collaboration%3A%20effect%20of%20spatial%20configuration%20on%20spatial%20statements%20production&amp;journal=Cogn%20Process&amp;doi=10.1007%2Fs10339-015-0672-2&amp;volume=16&amp;pages=337-342&amp;publication_year=2015&amp;author=Pouliquen-Lardy%2CL&amp;author=Mars%2CF&amp;author=Guillaume%2CF&amp;author=Milleville-Pennel%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BE. Riecke, DW. Cunningham, H. Bülthoff, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Riecke BE, Cunningham DW, Bülthoff H (2007) Spatial updating in virtual reality: the sufficiency of visual inf" /><p class="c-article-references__text" id="ref-CR21">Riecke BE, Cunningham DW, Bülthoff H (2007) Spatial updating in virtual reality: the sufficiency of visual information. Psychol Res 71:298–313. doi:<a href="https://doi.org/10.1007/s00426-006-0085-z">10.1007/s00426-006-0085-z</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00426-006-0085-z" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20updating%20in%20virtual%20reality%3A%20the%20sufficiency%20of%20visual%20information&amp;journal=Psychol%20Res&amp;doi=10.1007%2Fs00426-006-0085-z&amp;volume=71&amp;pages=298-313&amp;publication_year=2007&amp;author=Riecke%2CBE&amp;author=Cunningham%2CDW&amp;author=B%C3%BClthoff%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RJ. Roberts, CJ. Aman, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Roberts RJ, Aman CJ (1993) Developmental differences in giving directions: spatial frames of reference and men" /><p class="c-article-references__text" id="ref-CR22">Roberts RJ, Aman CJ (1993) Developmental differences in giving directions: spatial frames of reference and mental rotation. Child Dev 64:1258–1270</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F1131338" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developmental%20differences%20in%20giving%20directions%3A%20spatial%20frames%20of%20reference%20and%20mental%20rotation&amp;journal=Child%20Dev&amp;volume=64&amp;pages=1258-1270&amp;publication_year=1993&amp;author=Roberts%2CRJ&amp;author=Aman%2CCJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Roger, D. Knutsen, N. Bonnardel, L. Bigot, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Roger M, Knutsen D, Bonnardel N, Le Bigot L (2013) Landmark frames of reference in interactive route descripti" /><p class="c-article-references__text" id="ref-CR23">Roger M, Knutsen D, Bonnardel N, Le Bigot L (2013) Landmark frames of reference in interactive route description tasks. Appl Cogn Psychol 27:497–504. doi:<a href="https://doi.org/10.1002/acp.2927">10.1002/acp.2927</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Facp.2927" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Landmark%20frames%20of%20reference%20in%20interactive%20route%20description%20tasks&amp;journal=Appl%20Cogn%20Psychol&amp;doi=10.1002%2Facp.2927&amp;volume=27&amp;pages=497-504&amp;publication_year=2013&amp;author=Roger%2CM&amp;author=Knutsen%2CD&amp;author=Bonnardel%2CN&amp;author=Bigot%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Roskos-Ewoldsen, TP. McNamara, AL. Shelton, W. Carr, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Roskos-Ewoldsen B, McNamara TP, Shelton AL, Carr W (1998) Mental representations of large and small spatial la" /><p class="c-article-references__text" id="ref-CR24">Roskos-Ewoldsen B, McNamara TP, Shelton AL, Carr W (1998) Mental representations of large and small spatial layouts are orientation dependent. J Exp Psychol Learn Mem Cogn 24:215</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0278-7393.24.1.215" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mental%20representations%20of%20large%20and%20small%20spatial%20layouts%20are%20orientation%20dependent&amp;journal=J%20Exp%20Psychol%20Learn%20Mem%20Cogn&amp;volume=24&amp;publication_year=1998&amp;author=Roskos-Ewoldsen%2CB&amp;author=McNamara%2CTP&amp;author=Shelton%2CAL&amp;author=Carr%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MF. Schober, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations ab" /><p class="c-article-references__text" id="ref-CR25">Schober MF (1995) Speakers, addressees, and frames of reference: whose effort is minimized in conversations about locations? Discourse Process 20:219–247. doi:<a href="https://doi.org/10.1080/01638539509544939">10.1080/01638539509544939</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F01638539509544939" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Speakers%2C%20addressees%2C%20and%20frames%20of%20reference%3A%20whose%20effort%20is%20minimized%20in%20conversations%20about%20locations%3F&amp;journal=Discourse%20Process&amp;doi=10.1080%2F01638539509544939&amp;volume=20&amp;pages=219-247&amp;publication_year=1995&amp;author=Schober%2CMF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AP. Schouten, B. Hooff, F. Feldberg, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Schouten AP, van den Hooff B, Feldberg F (2013) Virtual team work: group decision making in 3D virtual environ" /><p class="c-article-references__text" id="ref-CR26">Schouten AP, van den Hooff B, Feldberg F (2013) Virtual team work: group decision making in 3D virtual environments. Commun Res. doi:<a href="https://doi.org/10.1177/0093650213509667">10.1177/0093650213509667</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20team%20work%3A%20group%20decision%20making%20in%203D%20virtual%20environments&amp;journal=Commun%20Res&amp;doi=10.1177%2F0093650213509667&amp;publication_year=2013&amp;author=Schouten%2CAP&amp;author=Hooff%2CB&amp;author=Feldberg%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Slater, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environme" /><p class="c-article-references__text" id="ref-CR27">Slater M (2009) Place illusion and plausibility can lead to realistic behaviour in immersive virtual environments. Philos Trans R Soc B Biol Sci 364:3549–3557</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1098%2Frstb.2009.0138" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Place%20illusion%20and%20plausibility%20can%20lead%20to%20realistic%20behaviour%20in%20immersive%20virtual%20environments&amp;journal=Philos%20Trans%20R%20Soc%20B%20Biol%20Sci&amp;volume=364&amp;pages=3549-3557&amp;publication_year=2009&amp;author=Slater%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Wang X, Tsai JJ-H (eds) (2011) Collaborative design in virtual environments. Springer, New York" /><p class="c-article-references__text" id="ref-CR28">Wang X, Tsai JJ-H (eds) (2011) Collaborative design in virtual environments. Springer, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Collaborative%20design%20in%20virtual%20environments&amp;publication_year=2011">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-016-0294-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We would like to thank Jérémy Le Thiec, Sidi Set, and Jean-Pierre Collet of the Airbus NemoLab who provided the virtual reality setup, as well as substantial technical assistance. This study is part of the PIVIPP project managed by IRT Jules Verne (French Institute in Research and Technology in Advanced Manufacturing Technologies for Composite, Metallic and Hybrid Structures). The authors wish to acknowledge the support of the industrial and academic partners of this project, respectively, Airbus Group, Airbus and IRCCyN.</p>
                <h3 class="c-article__sub-heading">Authors’ contributions</h3>
                <p>All authors participated in the design of the study. LPL conducted the experiment. LPL, IM, and FM analyzed the results and wrote the manuscript. All authors read and approved the final manuscript.</p>
              </div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">IRT Jules Verne, Bouguenais, France</p><p class="c-article-author-affiliation__authors-list">Lauriane Pouliquen-Lardy</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">UMR CNRS 6597, Institut de Recherche en Communications et Cybernétique de Nantes (IRCCyN), Campus de l’École Centrale de Nantes, 1, rue de la Noë, B.P. 92101, 44321, Nantes Cedex 03, France</p><p class="c-article-author-affiliation__authors-list">Lauriane Pouliquen-Lardy, Isabelle Milleville-Pennel &amp; Franck Mars</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Airbus Group, Suresnes, France</p><p class="c-article-author-affiliation__authors-list">François Guillaume</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Lauriane-Pouliquen_Lardy"><span class="c-article-authors-search__title u-h3 js-search-name">Lauriane Pouliquen-Lardy</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Lauriane+Pouliquen-Lardy&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lauriane+Pouliquen-Lardy" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lauriane+Pouliquen-Lardy%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Isabelle-Milleville_Pennel"><span class="c-article-authors-search__title u-h3 js-search-name">Isabelle Milleville-Pennel</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Isabelle+Milleville-Pennel&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Isabelle+Milleville-Pennel" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Isabelle+Milleville-Pennel%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Fran_ois-Guillaume"><span class="c-article-authors-search__title u-h3 js-search-name">François Guillaume</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Fran%C3%A7ois+Guillaume&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Fran%C3%A7ois+Guillaume" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Fran%C3%A7ois+Guillaume%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Franck-Mars"><span class="c-article-authors-search__title u-h3 js-search-name">Franck Mars</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Franck+Mars&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Franck+Mars" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Franck+Mars%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-016-0294-8/email/correspondent/c1/new">Franck Mars</a>.</p></div></div></section><section aria-labelledby="ethics"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">
              
              
                <h3 class="c-article__sub-heading">Conflict of interest</h3>
                <p>The authors declare that they have no competing interests.</p>
              
            </div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Remote%20collaboration%20in%20virtual%20reality%3A%20asymmetrical%20effects%20of%20task%20distribution%20on%20spatial%20processing%20and%20mental%20workload&amp;author=Lauriane%20Pouliquen-Lardy%20et%20al&amp;contentID=10.1007%2Fs10055-016-0294-8&amp;publication=1359-4338&amp;publicationDate=2016-09-06&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-016-0294-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-016-0294-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Pouliquen-Lardy, L., Milleville-Pennel, I., Guillaume, F. <i>et al.</i> Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload.
                    <i>Virtual Reality</i> <b>20, </b>213–220 (2016). https://doi.org/10.1007/s10055-016-0294-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-016-0294-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-01-29">29 January 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-08-31">31 August 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-09-06">06 September 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-11">November 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-016-0294-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-016-0294-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Remote collaboration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Spatial common frame of reference</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Spatial cognition</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mental workload</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0294-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=294;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

