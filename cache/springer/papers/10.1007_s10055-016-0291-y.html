<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Optimizing human model reconstruction from RGB-D images based on skin "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper reconstructs human model from multi-view RGB-D images of an Xbox One Kinect. We preprocess the depth images by implicit surface de-noising and then part-wisely register them into a point..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/20/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Optimizing human model reconstruction from RGB-D images based on skin detection"/>

    <meta name="dc.source" content="Virtual Reality 2016 20:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-08-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper reconstructs human model from multi-view RGB-D images of an Xbox One Kinect. We preprocess the depth images by implicit surface de-noising and then part-wisely register them into a point cloud. A template model is selected from the human model database to fit the registered point cloud of a human body by Laplacian deformation. Skin detection of RGB-D images helps to tightly constrain the skin parts of human body in template fitting step in order to get more precise and lifelike human model. We propose a robust skin detection method that is not affected by clothing pattern and background. Experiments demonstrate the effectiveness of our method."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2016-08-12"/>

    <meta name="prism.volume" content="20"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="159"/>

    <meta name="prism.endingPage" content="172"/>

    <meta name="prism.copyright" content="2016 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-016-0291-y"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-016-0291-y"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-016-0291-y.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-016-0291-y"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Optimizing human model reconstruction from RGB-D images based on skin detection"/>

    <meta name="citation_volume" content="20"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2016/09"/>

    <meta name="citation_online_date" content="2016/08/12"/>

    <meta name="citation_firstpage" content="159"/>

    <meta name="citation_lastpage" content="172"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-016-0291-y"/>

    <meta name="DOI" content="10.1007/s10055-016-0291-y"/>

    <meta name="citation_doi" content="10.1007/s10055-016-0291-y"/>

    <meta name="description" content="This paper reconstructs human model from multi-view RGB-D images of an Xbox One Kinect. We preprocess the depth images by implicit surface de-noising and t"/>

    <meta name="dc.creator" content="Guang Chen"/>

    <meta name="dc.creator" content="Jituo Li"/>

    <meta name="dc.creator" content="Jiping Zeng"/>

    <meta name="dc.creator" content="Bei Wang"/>

    <meta name="dc.creator" content="Guodong Lu"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=SCAPE: shape completion and animation of people; citation_author=D Anguelov, P Srinivasan, D Koller, S Thrun, J Rodgers, J Davis; citation_volume=24; citation_publication_date=2005; citation_pages=408-416; citation_doi=10.1145/1073204.1073207; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_title=The naked truth: estimating body shape under clothing; citation_inbook_title=Computer Vision&#8211;ECCV 2008; citation_publication_date=2008; citation_pages=15-29; citation_id=CR4; citation_author=AO B&#259;lan; citation_author=MJ Black; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Ann Math Stat; citation_title=A note on the generation of random normal deviates; citation_author=GEP Box, ME Muller; citation_volume=29; citation_publication_date=1958; citation_pages=610-611; citation_doi=10.1214/aoms/1177706645; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Global registration of dynamic range scans for articulated model reconstruction; citation_author=W Chang, M Zwicker; citation_volume=30; citation_publication_date=2011; citation_pages=171-179; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=Deformable model for estimating clothed and naked human shapes from a single image; citation_author=X Chen, Y Guo, B Zhou, Q Zhao; citation_volume=29; citation_publication_date=2013; citation_pages=1187-1196; citation_doi=10.1007/s00371-013-0775-7; citation_id=CR7"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Anim Virtual Worlds; citation_title=Reconstructing 3D human models with a Kinect; citation_author=G Chen, J Li, B Wang, J Zeng, G Lu, D Zhang; citation_volume=27; citation_publication_date=2016; citation_pages=72-85; citation_doi=10.1002/cav.1632; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=A statistical model of human pose and body shape; citation_author=N Hasler, C Stoll, M Sunkel, B Rosenhahn, H-P Seidel; citation_volume=28; citation_publication_date=2009; citation_pages=337-346; citation_doi=10.1111/j.1467-8659.2009.01373.x; citation_id=CR9"/>

    <meta name="citation_reference" content="Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation of dressed subjects from image sets. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR), pp 1823&#8211;1830"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph Forum; citation_title=Non-rigid registration under isometric deformations; citation_author=QX Huang, B Adams, M Wicke, LJ Guibas; citation_volume=27; citation_publication_date=2008; citation_pages=1449-1457; citation_doi=10.1111/j.1467-8659.2008.01285.x; citation_id=CR11"/>

    <meta name="citation_reference" content="Izadi S et al (2011) KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. In: Proceedings of the 24th annual ACM symposium on user interface software and technology, pp 559&#8211;568"/>

    <meta name="citation_reference" content="Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 international conference on imaging science, systems, and technology"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Skeleton driven animation based on implicit skinning; citation_author=J Li, G Lu; citation_volume=35; citation_publication_date=2011; citation_pages=945-954; citation_doi=10.1016/j.cag.2011.07.005; citation_id=CR13"/>

    <meta name="citation_reference" content="Li J, Wang Y (2007) Automatically constructing skeletons and parametric structures for polygonal human bodies. In: Proceedings of the 25th computer graphics international conference"/>

    <meta name="citation_reference" content="Mitra NJ, Fl&#246;ry S, Ovsjanikov M, Gelfand N, Guibas L, Pottmann H (2007) Dynamic geometry registration. In: Symposium on geometry processing, pp 173&#8211;182"/>

    <meta name="citation_reference" content="Newcombe RA et al (2011) KinectFusion: real-time dense surface mapping and tracking. In: 2011 10th IEEE international symposium on mixed and augmented reality (ISMAR), pp 127&#8211;136"/>

    <meta name="citation_reference" content="citation_journal_title=Automatica; citation_title=A threshold selection method from gray-level histograms; citation_author=N Otsu; citation_volume=11; citation_publication_date=1975; citation_pages=23-27; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=Grabcut: interactive foreground extraction using iterated graph cuts; citation_author=C Rother, V Kolmogorov, A Blake; citation_volume=23; citation_publication_date=2004; citation_pages=309-314; citation_doi=10.1145/1015706.1015720; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Scanning 3D full human bodies using Kinects; citation_author=J Tong, J Zhou, L Liu, Z Pan, H Yan; citation_volume=18; citation_publication_date=2012; citation_pages=643-650; citation_doi=10.1109/TVCG.2012.56; citation_id=CR18"/>

    <meta name="citation_reference" content="Weiss A, Hirshberg D, Black MJ (2011) Home 3D body scans from noisy image and range data. In: Proceedings of the 2011 international conference on computer vision, pp 1951&#8211;1958"/>

    <meta name="citation_reference" content="Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In: 2013 IEEE conference on computer vision and pattern recognition (CVPR), pp 145&#8211;152"/>

    <meta name="citation_reference" content="citation_journal_title=Neurocomputing; citation_title=Estimation of human body shape and cloth field in front of a Kinect; citation_author=M Zeng, L Cao, H Dong, K Lin, M Wang, J Tong; citation_volume=151; citation_publication_date=2015; citation_pages=626-631; citation_doi=10.1016/j.neucom.2014.06.087; citation_id=CR21"/>

    <meta name="citation_author" content="Guang Chen"/>

    <meta name="citation_author_institution" content="Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Hangzhou, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Jituo Li"/>

    <meta name="citation_author_email" content="jituo_li@zju.edu.cn"/>

    <meta name="citation_author_institution" content="Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Hangzhou, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Jiping Zeng"/>

    <meta name="citation_author_institution" content="Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Hangzhou, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Bei Wang"/>

    <meta name="citation_author_institution" content="Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Hangzhou, People&#8217;s Republic of China"/>

    <meta name="citation_author" content="Guodong Lu"/>

    <meta name="citation_author_institution" content="Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Hangzhou, People&#8217;s Republic of China"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-016-0291-y&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2016/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-016-0291-y"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Optimizing human model reconstruction from RGB-D images based on skin detection"/>
        <meta property="og:description" content="This paper reconstructs human model from multi-view RGB-D images of an Xbox One Kinect. We preprocess the depth images by implicit surface de-noising and then part-wisely register them into a point cloud. A template model is selected from the human model database to fit the registered point cloud of a human body by Laplacian deformation. Skin detection of RGB-D images helps to tightly constrain the skin parts of human body in template fitting step in order to get more precise and lifelike human model. We propose a robust skin detection method that is not affected by clothing pattern and background. Experiments demonstrate the effectiveness of our method."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Optimizing human model reconstruction from RGB-D images based on skin detection | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-016-0291-y","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Human model reconstruction, Kinect, RGB-D image, Skin detection","kwrd":["Human_model_reconstruction","Kinect","RGB-D_image","Skin_detection"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-016-0291-y","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-016-0291-y","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=291;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-016-0291-y">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Optimizing human model reconstruction from RGB-D images based on skin detection
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0291-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0291-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-08-12" itemprop="datePublished">12 August 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Optimizing human model reconstruction from RGB-D images based on skin detection</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guang-Chen" data-author-popup="auth-Guang-Chen">Guang Chen</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="grid.13402.34, 000000041759700X, Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jituo-Li" data-author-popup="auth-Jituo-Li" data-corresp-id="c1">Jituo Li<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="grid.13402.34, 000000041759700X, Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jiping-Zeng" data-author-popup="auth-Jiping-Zeng">Jiping Zeng</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="grid.13402.34, 000000041759700X, Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bei-Wang" data-author-popup="auth-Bei-Wang">Bei Wang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="grid.13402.34, 000000041759700X, Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Guodong-Lu" data-author-popup="auth-Guodong-Lu">Guodong Lu</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Zhejiang University" /><meta itemprop="address" content="grid.13402.34, 000000041759700X, Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">159</span>–<span itemprop="pageEnd">172</span>(<span data-test="article-publication-year">2016</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">470 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-016-0291-y/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper reconstructs human model from multi-view RGB-D images of an Xbox One Kinect. We preprocess the depth images by implicit surface de-noising and then part-wisely register them into a point cloud. A template model is selected from the human model database to fit the registered point cloud of a human body by Laplacian deformation. Skin detection of RGB-D images helps to tightly constrain the skin parts of human body in template fitting step in order to get more precise and lifelike human model. We propose a robust skin detection method that is not affected by clothing pattern and background. Experiments demonstrate the effectiveness of our method.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>With the booming of 3D movies and virtual fitting room, it is of vital importance to reconstruct accurate 3D human models with cost-effective equipment. Such equipment as Kinect can provide RGB-D images simultaneously, but the raw depth images are of low precision and with quantities of noise. Thus, researchers have put much effort to overcome this challenge in order to get precise and complete human models using cost-effective RGB-D equipment.</p><p>Human model reconstruction by Kinect can be divided into two forms, i.e., reconstruction of dressed human and estimation of human bodies. Tong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Tong J, Zhou J, Liu L, Pan Z, Yan H (2012) Scanning 3D full human bodies using Kinects. IEEE Trans Vis Comput Graph 18:643–650" href="/article/10.1007/s10055-016-0291-y#ref-CR18" id="ref-link-section-d61399e350">2012</a>) and Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In: 2013 IEEE conference on computer vision and pattern recognition (CVPR), pp 145–152" href="/article/10.1007/s10055-016-0291-y#ref-CR20" id="ref-link-section-d61399e353">2013</a>) reconstructed the dressed human with the help of multi-view images from Kinect. The former one used three Kinects to capture RGB-D images of a human body standing on a turntable and applied a rough template to non-rigidly register multi-view depth images. Global alignment algorithm has been performed to solve the loop closure problem efficiently. However, the employed device is not convenient for ordinary users. The latter one represented the shape motion by a deformation graph and proposed a model-to-part method to gradually integrate sampled points of depth images into the deformation graph. Two-stage registration and topology-adaptive integration were exerted to handle the drift and topological error for the deformation graph. Template is avoided in this method, but the modeling precision is relatively low, which is unable to meet many application requirements.</p><p>The above achievements are salient, but the dressed human models are not sufficient for virtual try-on or fitness. Weiss et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Weiss A, Hirshberg D, Black MJ (2011) Home 3D body scans from noisy image and range data. In: Proceedings of the 2011 international conference on computer vision, pp 1951–1958" href="/article/10.1007/s10055-016-0291-y#ref-CR19" id="ref-link-section-d61399e359">2011</a>) and Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e362">2015</a>) put emphasis on estimating human body shapes using a single Kinect, which are both based on SCAPE (Anguelov et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Anguelov D, Srinivasan P, Koller D, Thrun S, Rodgers J, Davis J (2005) SCAPE: shape completion and animation of people. ACM Trans Graph 24:408–416" href="/article/10.1007/s10055-016-0291-y#ref-CR3" id="ref-link-section-d61399e365">2005</a>) model. Weiss et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Weiss A, Hirshberg D, Black MJ (2011) Home 3D body scans from noisy image and range data. In: Proceedings of the 2011 international conference on computer vision, pp 1951–1958" href="/article/10.1007/s10055-016-0291-y#ref-CR19" id="ref-link-section-d61399e368">2011</a>) used the shape constancy of the body across frames and combined silhouettes and depth data with a novel silhouette dissimilarity term in the optimization while matching the SCAPE model to the silhouettes without correspondence. However, the modeling speed is relatively slow, taking approximately 65 min per body. And the users should wear tight clothes or bare their bodies in front of the Kinect, which is embarrassing sometimes. Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e371">2015</a>) first reconstructed the dressed human body shape with a Kinect and then estimated the body shape from the skin regions of the dressed shape by leveraging a statistical model of human body. The skin regions are recognized by a skin classifier in the RGB images and then taken as tight constraints for the body estimation. The dressed human body shape is composed of two raw depth frames without de-noising, which is quite rough to model accurate naked human body shapes. Furthermore, the users are asked to lay out the same pose as the SCAPE model.</p><p>In this paper, we optimize the human model reconstruction method of naked body using a single Kinect (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e377">2016</a>) based on a proposed skin classifier of RGB images. The users rotate freely in front of a Kinect sensor of Xbox One to acquire their dynamic RGB-D images and skeletons. Due to the low resolution and high noise of raw depth data, we first de-noise the depth data by building an implicit surface and then part-wisely register the depth images of dressed human body with the help of skeleton. Having the registered point cloud, we choose a template model from the human model database to fit it by Laplacian deformation according to part-wise correspondences. The weights of correspondences on skin and non-skin region are set equally by Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e380">2016</a>), which is unable to consider the skin impact on fitting step. In this paper, depth-based skin detection resulting from RGB images is applied to put tight constraints on the skin parts of human body during the fitting step. Different from the existing methods, our skin detection method will not be affected by clothing pattern given the depth information of human body. Once the template has been deformed, we project it back into the space of human body in order to make the template stay in the human body domain.</p><p>We propose a pattern-free skin classifier to detect the skin region in RGB image and give tight constraints on skin region for human body reconstruction. Method proposed by Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e387">2015</a>) is the most correlative to our work. Different from Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e390">2015</a>), who used Bayesian skin classifier, our pattern-free skin detection method can recognize the region on clothes with similar color to skin, which cannot be handled by Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e393">2015</a>). The skin and non-skin regions of human body are segmented coarsely using a single skin classifier [elliptical boundary model (Lee and Yoo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 international conference on imaging science, systems, and technology" href="/article/10.1007/s10055-016-0291-y#ref-CR12" id="ref-link-section-d61399e396">2002</a>)], and then the bounding box of non-skin region’s contour is extracted as the input of GrabCut (Rother et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23:309–314" href="/article/10.1007/s10055-016-0291-y#ref-CR17" id="ref-link-section-d61399e399">2004</a>) algorithm in order to detect the accurate clothes. We also register and de-noise a number of depth frames in order to get denser and more precise depth data. Compared with only using two raw depth frames without de-nosing (Zeng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e403">2015</a>), this procedure provides more accurate and authentic morphing target. Moreover, we do not require the users to pose similarly as the template, which is more flexible and feasible.</p><p>The main contributions of this method are as follows. A new method that estimates the whole human body shape under various clothes using an Xbox One Kinect is designed, which combines the de-noised depth frames to give more reliable fitting target and applies skin information to constraint in the fitting step. Accurate expanded skin detection is proposed to add tight skin constraints in Laplacian deformation step, which is based on RGB-D information and not fully relied on the clothing pattern. This method does not require the users to pose the same as the template.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Non-rigid registration</h3><p>Since it is difficult to keep the human body static in front of the scanning devices, non-rigid registration should be considered to resolve the movement, breathing and even rotation of human body. Huang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Huang QX, Adams B, Wicke M, Guibas LJ (2008) Non-rigid registration under isometric deformations. Comput Graph Forum 27:1449–1457" href="/article/10.1007/s10055-016-0291-y#ref-CR11" id="ref-link-section-d61399e421">2008</a>) registered two high-quality point clouds with isometric deformation by computing correspondences based on the preservation of geodesic distances. This method is template-less, but changes in topology will cause abrupt changes in geodesic distances on the surface and lead to very few or even wrong correspondences. Mitra et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra NJ, Flöry S, Ovsjanikov M, Gelfand N, Guibas L, Pottmann H (2007) Dynamic geometry registration. In: Symposium on geometry processing, pp 173–182" href="/article/10.1007/s10055-016-0291-y#ref-CR15" id="ref-link-section-d61399e424">2007</a>) proposed an algorithm for registering continuous scans using fundamental kinematic properties of the space–time surface formed by the input points without template. The method does not require establishing correspondence across different scans, but is computational expensive. Nonetheless, Huang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Huang QX, Adams B, Wicke M, Guibas LJ (2008) Non-rigid registration under isometric deformations. Comput Graph Forum 27:1449–1457" href="/article/10.1007/s10055-016-0291-y#ref-CR11" id="ref-link-section-d61399e427">2008</a>) and Mitra et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Mitra NJ, Flöry S, Ovsjanikov M, Gelfand N, Guibas L, Pottmann H (2007) Dynamic geometry registration. In: Symposium on geometry processing, pp 173–182" href="/article/10.1007/s10055-016-0291-y#ref-CR15" id="ref-link-section-d61399e430">2007</a>) both require high-quality point clouds, which is not suitable for low precision equipment.</p><p>Chang and Zwicker (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Chang W, Zwicker M (2011) Global registration of dynamic range scans for articulated model reconstruction. ACM Trans Graph 30:171–179" href="/article/10.1007/s10055-016-0291-y#ref-CR6" id="ref-link-section-d61399e436">2011</a>) presented a global registration algorithm to reconstruct articulated 3D models from dynamic range scan sequences. A limitation of this method is that enough overlap between adjacent frames is required to obtain a good alignment. Severe occlusion will result in the failure of tracking arms in human body rotation sequence, even though overlap is enough.</p><p>Tong et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Tong J, Zhou J, Liu L, Pan Z, Yan H (2012) Scanning 3D full human bodies using Kinects. IEEE Trans Vis Comput Graph 18:643–650" href="/article/10.1007/s10055-016-0291-y#ref-CR18" id="ref-link-section-d61399e442">2012</a>) used a rough template constructed by the first depth frame. Based on the feature correspondences tracked in the corresponding RGB images, the template is deformed and thus drives the points accordingly. The optical flow method for tracking correspondences is not stable for such low resolution as 640 × 480 pixels. Furthermore, the reconstruction employs three Kinects to capture the RGB-D data.</p><p>Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In: 2013 IEEE conference on computer vision and pattern recognition (CVPR), pp 145–152" href="/article/10.1007/s10055-016-0291-y#ref-CR20" id="ref-link-section-d61399e448">2013</a>) exerted model-to-part registration constituted by global rigid transformation and local non-rigid deformation without template, which takes 10–15 min to obtain a complete dressed human model.</p><h3 class="c-article__sub-heading" id="Sec4">Human body shape estimation</h3><p>Hasler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation of dressed subjects from image sets. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR), pp 1823–1830" href="/article/10.1007/s10055-016-0291-y#ref-CR10" id="ref-link-section-d61399e459">2010</a>) and Bălan and Black (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bălan AO, Black MJ (2008) The naked truth: estimating body shape under clothing. Computer Vision–ECCV 2008. Springer, Marseille, pp 15–29" href="/article/10.1007/s10055-016-0291-y#ref-CR4" id="ref-link-section-d61399e462">2008</a>) estimated body shapes under clothes based on RGB images and a statistical model of human bodies. The RGB images provide the silhouette of human body shapes, and the statistical model provides estimation sufficient shape priors. For example, Hasler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation of dressed subjects from image sets. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR), pp 1823–1830" href="/article/10.1007/s10055-016-0291-y#ref-CR10" id="ref-link-section-d61399e465">2010</a>) used multi-linear and analytical model of human pose and body shape, which is learned from a 3D model database of undressed subjects, in order to constrain the estimation of silhouettes in images. Bălan and Black (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bălan AO, Black MJ (2008) The naked truth: estimating body shape under clothing. Computer Vision–ECCV 2008. Springer, Marseille, pp 15–29" href="/article/10.1007/s10055-016-0291-y#ref-CR4" id="ref-link-section-d61399e468">2008</a>) exploited visual hull to obtain the loose bound on the body shape of a dressed body and gained tight bounds by combining constraints from multiple poses of a single 3D shape based on pose independence. Furthermore, a skin detector has been integrated to provide tight constraints on 3D shape when parts of the body are seen unclothed. Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chen X, Guo Y, Zhou B, Zhao Q (2013) Deformable model for estimating clothed and naked human shapes from a single image. Vis Comput 29:1187–1196" href="/article/10.1007/s10055-016-0291-y#ref-CR7" id="ref-link-section-d61399e471">2013</a>) expanded the method of Hasler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation of dressed subjects from image sets. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR), pp 1823–1830" href="/article/10.1007/s10055-016-0291-y#ref-CR10" id="ref-link-section-d61399e475">2010</a>), whose deformation model can be factorized into pose-, shape- and cloth-dependent components, and simultaneously estimated 3D clothed and naked shapes from a single image for similar clothes in their dataset. All these methods are lack of “thickness” of human bodies, which is unable to provide strong geometry information. Thus, the estimation results are only visually realistic, but are far from the ground truth.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Methodology</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">System setup</h3><p>In order to make it easy to operate, our system only includes an Xbox One Kinect and a computer. To get the RGB-D images and skeleton of the whole human body, the user is required to rotate freely at about 2–2.5 m away from the Kinect sensor, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig1">1</a>. That means the user does not need to maintain one posture or pose like the template during the shooting process, which is different from most aforementioned methods (Tong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Tong J, Zhou J, Liu L, Pan Z, Yan H (2012) Scanning 3D full human bodies using Kinects. IEEE Trans Vis Comput Graph 18:643–650" href="/article/10.1007/s10055-016-0291-y#ref-CR18" id="ref-link-section-d61399e494">2012</a>; Zeng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In: 2013 IEEE conference on computer vision and pattern recognition (CVPR), pp 145–152" href="/article/10.1007/s10055-016-0291-y#ref-CR20" id="ref-link-section-d61399e497">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e500">2015</a>). Furthermore, the skeleton tracking will shake during the human body movement; thus, we smooth the skeleton by Kinect SDK.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>System setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To initialize human skeleton, the user is asked to keep facing the Kinect for a few seconds and move slightly to get smoothed skeleton. The skeletons of these depth frames are averaged. The depth frame that has the most proximal skeleton to the average skeleton facing Kinect is selected as the reference frame. Then, we build the corresponding coordinate relationship between RGB image and depth image of the same frame, since the RGB image is 1920 × 1080 pixels and depth image is 524 × 414 pixels.</p><h3 class="c-article__sub-heading" id="Sec7">Overview</h3><p>The pipeline of our system is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig2">2</a>. Based on the work of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e535">2016</a>), we first segment the depth images of human body using their skeletons and then part-wisely register depth images of different views by articulated iterative closest point (ICP) algorithm. Then, we choose a human model in the database as template. The database is from Anguelov et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Anguelov D, Srinivasan P, Koller D, Thrun S, Rodgers J, Davis J (2005) SCAPE: shape completion and animation of people. ACM Trans Graph 24:408–416" href="/article/10.1007/s10055-016-0291-y#ref-CR3" id="ref-link-section-d61399e538">2005</a>) and Hasler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Hasler N, Stoll C, Sunkel M, Rosenhahn B, Seidel H-P (2009) A statistical model of human pose and body shape. Comput Graph Forum 28:337–346" href="/article/10.1007/s10055-016-0291-y#ref-CR9" id="ref-link-section-d61399e541">2009</a>) and composed of a set of naked human models. Since the template may have different posture with the user, it should be deformed by skeleton-driven Laplacian deformation in order to find correspondences between registered point cloud and it. Then, we extend the work of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e544">2016</a>) in this paper. Relevant parts on the reconstruction result are constrained to have close shape to the human skin regions in depth images; thus, the reconstruction result would be more reliable. Skin regions are detected from RGB images with our expanded skin detection model, which is robust to the clothing patterns. The reconstruction result is further humanized by projecting it back into the human shape space. This process is iterated to get an accurate estimation of human body under the clothes. Detailed steps can be described as follows.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>System overview</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">De-noising, segmentation and registration</h4><p>First, we de-noise the human body depth images from Kinect using an implicit surface-based method, which can not only remove outliers and noise in depth images, but also give a reasonable and smooth result. The human body depth images are separated from the background using bounding box. Second, the de-noised reference depth image of human body is segmented into 11 parts according to the joints position of its skeleton and other frames are segmented accordingly as detailed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec12">3.4</a>. Last, we register all segmented human body depth images to the reference frame by using articulated ICP.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Expanded skin detection</h4><p>In order to get precise skin region in depth images of human bodies, we detect the skin region in the reference RGB image by elliptical boundary model (Lee and Yoo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 international conference on imaging science, systems, and technology" href="/article/10.1007/s10055-016-0291-y#ref-CR12" id="ref-link-section-d61399e584">2002</a>) first. The limitation of the elliptical boundary model is the region on the clothes with similar color of skin tends to be recognized as skin. Thus, we use expanded skin detection to find more reliable skin region in RGB image. Specifically, the non-skin region is regarded as clothes and we extract the coarse boundary of it based on the method of Otsu (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Otsu N (1975) A threshold selection method from gray-level histograms. Automatica 11:23–27" href="/article/10.1007/s10055-016-0291-y#ref-CR16" id="ref-link-section-d61399e587">1975</a>). Then, we perform GrabCut (Rother et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23:309–314" href="/article/10.1007/s10055-016-0291-y#ref-CR17" id="ref-link-section-d61399e590">2004</a>) to get the precise region of clothes, the input of which is the bounding box of the clothes boundary as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec13">3.5</a>. The final skin region in depth image can be found by the correspondence between depth and RGB images.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Constrained morphing</h4><p>We apply skeleton-based method (Li and Lu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Lu G (2011) Skeleton driven animation based on implicit skinning. Comput Graph 35:945–954" href="/article/10.1007/s10055-016-0291-y#ref-CR13" id="ref-link-section-d61399e604">2011</a>) to deform the template, making the template have the same pose with registered point cloud. After that, we find part-to-part correspondences between template and registered point cloud by establishing cylinder coordinate for each body part. Given these correspondences, we put large weight for the correspondences that belong to skin region and perform Laplacian deformation to fit the template to the registered point cloud. Finally, we project the morphed template to the human body space, which is expanded in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec14">3.6</a>. This step iterates to get a reasonable and precise reconstruction result.</p><h3 class="c-article__sub-heading" id="Sec11">Statistical human model representation</h3><p>All models in the database are scanned in a basic pose, and some of them are scanned in several poses chosen randomly from a set of 34 poses, which have unified mesh structure. The skeletons of models in the database are created automatically (Li and Wang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Li J, Wang Y (2007) Automatically constructing skeletons and parametric structures for polygonal human bodies. In: Proceedings of the 25th computer graphics international conference" href="/article/10.1007/s10055-016-0291-y#ref-CR14" id="ref-link-section-d61399e619">2007</a>). For each model, the joint’s position is parameterized by the positions of an appropriate set of surface vertices around it (Li and Lu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Lu G (2011) Skeleton driven animation based on implicit skinning. Comput Graph 35:945–954" href="/article/10.1007/s10055-016-0291-y#ref-CR13" id="ref-link-section-d61399e622">2011</a>). That is <span class="mathjax-tex">\(\varvec{J}_{q} = \sum\nolimits_{l} {\lambda_{l}^{q} \varvec{p}_{l} }\)</span>, in which <i>q</i> is the joint index, <span class="mathjax-tex">\(\varvec{p}_{l}\)</span> is the coordinate of a vertex, and <span class="mathjax-tex">\({\lambda }_{l}^{q}\)</span> is a nonnegative coefficient. It is rational to set the local transformation of a joint as the spherical linear average of the transformations of its adjacent bones. The transformation of each bone can be derived from the skeleton of the sample model in the database. Considering pose impact, the human body space can be described as:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\varvec{H} = \left[ {\begin{array}{*{20}c} \varvec{A} \\ {\omega \varvec{R}} \\ \end{array} } \right] = {\mathbf{Y}}_{k}^{\text{T}} \cdot \varvec{b} + \left[ {\begin{array}{*{20}c} {\bar{\varvec{a}}} \\ {\omega \bar{\varvec{r}}} \\ \end{array} } \right] ,$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>in which <span class="mathjax-tex">\(\varvec{A} = [{a}_{1} \;{a}_{2} \cdots {a}_{{m}} ]_{{3{n} \times {m}}}\)</span>, and <i>a</i>
                           <sub>
                    <i>i</i>
                  </sub> is a vector with <i>n</i> vertex coordinates of the <i>i</i>th mesh in the database and <i>m</i> is the number of models in the database; <span class="mathjax-tex">\(\varvec{R}\)</span> is the rotation of human models’ joints in quaternion form; <span class="mathjax-tex">\(\bar{\varvec{a}}\)</span> is the vector of average vertex coordinates of sample human models; <span class="mathjax-tex">\(\bar{\varvec{r}}\)</span> is the vector of average rotation of each joint at different models; <span class="mathjax-tex">\(\omega\)</span> is the weight coefficient that controls pose impact; <span class="mathjax-tex">\({\mathbf{Y}}_{k}^{\text{T}}\)</span> is a matrix with its columns being the <span class="mathjax-tex">\({k}\)</span> eigenvectors of the sample human models extracted by principal component analysis (PCA); <span class="mathjax-tex">\(\varvec{b}\)</span> is a vector of coefficients, each of which equals to the dot product between the vector of <span class="mathjax-tex">\(\varvec{h}_{{i}}\)</span>, a column vector of <span class="mathjax-tex">\(\varvec{H}\)</span>, subtracts <span class="mathjax-tex">\(\bar{\varvec{h}}\)</span> and a column vector in <span class="mathjax-tex">\({\mathbf{Y}}_{{k}}^{\text{T}}\)</span>.</p><h3 class="c-article__sub-heading" id="Sec12">Part-wise registration</h3><p>The depth images are de-noised first with an implicit surface filter as described by Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e1331">2016</a>). The geometry of a depth image is the surface with minimal distance to it. We refer to the idea of distance field (Izadi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Izadi S et al (2011) KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. In: Proceedings of the 24th annual ACM symposium on user interface software and technology, pp 559–568" href="/article/10.1007/s10055-016-0291-y#ref-CR1" id="ref-link-section-d61399e1334">2011</a>; Newcombe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Newcombe RA et al (2011) KinectFusion: real-time dense surface mapping and tracking. In: 2011 10th IEEE international symposium on mixed and augmented reality (ISMAR), pp 127–136" href="/article/10.1007/s10055-016-0291-y#ref-CR2" id="ref-link-section-d61399e1337">2011</a>) to compute the implicit surface. For a given point, its distance function can be computed by implicit moving least squares, that is <span class="mathjax-tex">\(f(x) = {\sum\nolimits_{i} {n_i}(x - {p_i}){\varphi_i} (x)} {\bigg/ {\vphantom {\sum\nolimits_{i}{\varphi _i}(x)}}}\sum\nolimits_i \varphi_{i} (x),\)</span> where <span class="mathjax-tex">\(\varphi_{i} (x) = \exp ( - {{\left\| {x - p_{i} } \right\|^{2} } \mathord{\left/ {\vphantom {{\left\| {x - p_{i} } \right\|^{2} } {2\sigma^{2} }}} \right. \kern-0pt} {2\sigma^{2} }})\)</span>, <span class="mathjax-tex">\({p}_{i}\)</span> is the coordinate of the <i>i</i>th point in depth image, <span class="mathjax-tex">\(n_{i}\)</span> is the normal vector of <span class="mathjax-tex">\(p_{i}\)</span> and <span class="mathjax-tex">\(\sigma\)</span> is the variance, <span class="mathjax-tex">\(n_{i}\)</span> is computed approximately by the surface composed of <span class="mathjax-tex">\(p_{i}\)</span> and its proximal points. We acquire the surface of dressed human body when <span class="mathjax-tex">\(f(x) = 0\)</span>. The points with large distance from the surface of human body are discarded. This step iterates until the distance of most points to the implicit surface is less than a threshold.</p><p>Given the human depth images and the skeleton of reference frame, we segment the reference depth image by its skeleton based on geometry information as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig3">3</a>, which makes full use of bisecting planes of bone angles and the method of scanning lines to find armpits and crotch point. We part-wisely register all segmented depth frames to the reference depth image using part-to-part articulated ICP algorithm in order to obtain the registered point cloud and finally de-noise it (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Segmentation result of depth image by skeleton. <b>a</b> Skeleton of reference frame. <b>b</b> Segmentation result of the reference frame by skeleton</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Part-wise registration result. <b>a</b> Several raw depth images of the front body. <b>b</b> Part-wise registration result after de-noising</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec13">Skin detection expansion</h3><p>Inspired by Zeng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631" href="/article/10.1007/s10055-016-0291-y#ref-CR21" id="ref-link-section-d61399e1821">2015</a>), we separate the skin and clothes region in RGB image in order to put tight constraints in the fitting step of our method. RGB images are converted to YCrCb color space, and then, the skin region is detected by a parametric skin distribution classifier, elliptical boundary model (Lee and Yoo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 international conference on imaging science, systems, and technology" href="/article/10.1007/s10055-016-0291-y#ref-CR12" id="ref-link-section-d61399e1824">2002</a>). The elliptical boundary model is defined as:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\varPhi (c) = (c - \phi )^{T} \varLambda^{ - 1} (c - \phi )$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                        <p>First, up to 5 % of the training color samples with low frequency are eliminated to remove noise and negligible data. Next, model parameters (<span class="mathjax-tex">\(\phi\)</span> and <span class="mathjax-tex">\(\varLambda\)</span>) are estimated by <span class="mathjax-tex">\(\phi = \frac{1}{n}\sum\nolimits_{i = 1}^{n} {c_{i} }\)</span>, <span class="mathjax-tex">\(\varLambda = \frac{1}{N}\sum\nolimits_{i = 1}^{N} {f_{i} \cdot \left( {c_{i} - \mu } \right)} \left( {c_{i} - \mu } \right)^{T}\)</span>, in which <span class="mathjax-tex">\(\mu = \frac{1}{N}\sum\nolimits_{i = 1}^{n} {f_{i} c_{i} }\)</span>, <span class="mathjax-tex">\(N = \sum\nolimits_{i = 1}^{n} {f_{i} }\)</span>. Here, <i>n</i> is the total number of distinctive training color vectors <span class="mathjax-tex">\(c_{i}\)</span> of the training skin pixel set, and <span class="mathjax-tex">\(f_{i}\)</span> is the number of skin samples of color vector <span class="mathjax-tex">\(c_{i}\)</span>. Pixel with color <i>c</i> is classified as skin when <span class="mathjax-tex">\(\varPhi (c) &lt; \theta\)</span>.</p><p>Here, we use an experiential elliptical model, which is a binary image including an ellipse. The pixel belongs to skin region if the value is not zero in CrCb space. Given the pixels that belong to skin region in RGB images, we map them to the depth images. It is worth noting that the background region might be recognized as skin region with similar color of the skin. Thus, we detect the true skin region according to the body region in depth images.</p><p>Furthermore, the pattern of the clothes will affect the skin detection result, so we propose a pattern-free skin detection method by using GrabCut (Rother et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23:309–314" href="/article/10.1007/s10055-016-0291-y#ref-CR17" id="ref-link-section-d61399e2351">2004</a>) algorithm based on depth information. The GrabCut method is efficient and with simple user interaction, which only need the user to drag a rectangle loosely around an object. This provides us the opportunity to take the bounding box of the clothes as input to GrabCut. However, directly inputting the bounding box of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>c will mistakenly identify the clothes due to the noise on the naked human body parts of RGB image. The concrete steps of our method are as follows, and the detection results are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Skin detection result. <b>a</b> Result of elliptical skin model detection of the whole RGB image, the <i>white region</i> is regarded as skin. <b>b</b> Depth image of human body after discarding background and mapping skin detection result onto it, in which the <i>pink region</i> is the skin. <b>c</b> Clothes region with noise in RGB image. <b>d</b> Contour and bounding <i>box</i> of (<b>c</b>). <b>e</b> Clothes region in RGB image recognized by GrabCut algorithm. <b>f</b> Skin (<i>pink</i>) and clothes (<i>blue</i>) regions in depth image after performing our expanded skin detection method (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>First, we detect skin region of RGB image using elliptical skin model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>a) and find out skin region on human body by the mapping between RGB image and its relevant depth image, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>b.</p><p>Second, the non-skin region of human body in RGB image is treated as clothes with a lot of noise (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>c) and then its contour (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>d) is extracted based on the method of Otsu (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Otsu N (1975) A threshold selection method from gray-level histograms. Automatica 11:23–27" href="/article/10.1007/s10055-016-0291-y#ref-CR16" id="ref-link-section-d61399e2436">1975</a>) after several iterations between erosion and dilation. Since the non-skin region is not connected, the contour boundary is not smooth and intersects with the real clothes. Afterward, the bounding box of non-skin region is found (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>d) as target area for GrabCut and the clothes region (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>e) is detected by GrabCut in the original RGB image. Setting the background pixels for GrabCut could get better result. Before finding the background pixels, we inflate the clothes contour until the whole clothes are encircled in it. Those pixels between the inflated contour and bounding box can be set as the background pixels for GrabCut.</p><p>At last, the area that is not clothes on human body is regarded as skin region (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig5">5</a>f) by mapping the GrabCut result in RGB image to the depth image.</p><h3 class="c-article__sub-heading" id="Sec14">Body shape reconstruction</h3><p>We select a template in the human model database, which has the most similar skeleton length with that from Kinect. Given the registered point cloud and its skin region, the template model <span class="mathjax-tex">\(\varvec{M}\)</span> is deformed to fit it by using a two-step Laplacian deformation.</p><p>First, we apply the implicit skinning-based skeleton-driven method (Li and Lu <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Li J, Lu G (2011) Skeleton driven animation based on implicit skinning. Comput Graph 35:945–954" href="/article/10.1007/s10055-016-0291-y#ref-CR13" id="ref-link-section-d61399e2485">2011</a>) to deform the template, so that the template poses the same as the human body in reference depth image, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig6">6</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Initial pose alignment. <b>a</b> Template model. <b>b</b> Skeleton of reference depth frame. <b>c</b> Result after skeleton-driven pose alignment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Second, the template is morphed to fit the registered point cloud by adding corresponding points as constraints in the Laplacian deformation. The corresponding points are found with the help of shared cylinder domain between each body part of <span class="mathjax-tex">\(\varvec{M}\)</span> and the registered point cloud. Specifically, projecting the point on registered point cloud to its bone, the ray of projection intersects <span class="mathjax-tex">\(\varvec{M}\)</span> at least one point. We choose the intersection with closer normal to the ray and drop the correspondences with big distance (&gt;10 cm) or normal difference (&gt;30°). The energy function of the second step is <span class="mathjax-tex">\(E = \mu 1E_{L} + \mu 2E_{C}\)</span>, where <span class="mathjax-tex">\(E_{L} = \sum\nolimits_{i = 0} {\omega_{i} \left\| {L(v_{i}^{{\prime }} ) - {\varvec{\updelta}}_{i}^{{\prime }} } \right\|^{2} }\)</span> and <span class="mathjax-tex">\(E_{C} = \sum\nolimits_{i = 0} {\omega_{i} ||\sum\nolimits_{j = 0}^{2} {\alpha_{j} {\mathbf{v}}'_{i,j} - c_{i} } ||^{2} }\)</span>, <span class="mathjax-tex">\(\omega_{i} = \left\{ {\begin{array}{*{20}c} {10} &amp; {i \in {\text{Skin}}} \\ 1 &amp; {i \in {\text{Clothes}}} \\ \end{array} } \right.\)</span>and <span class="mathjax-tex">\(c_{i}\)</span> is the corresponding point coordinate on registered point cloud, and <span class="mathjax-tex">\((\alpha_{0} ,\alpha_{1} ,\alpha_{2} )\)</span> is the barycentric coordinate of the corresponding point of <span class="mathjax-tex">\(c_{i}\)</span> on template.</p><p>The template is denoted as <span class="mathjax-tex">\(\varvec{M}^{{\prime }}\)</span> after the above deformation steps and is projected to <span class="mathjax-tex">\({\mathbf{Y}}_{k}^{\text{T}}\)</span> to get <span class="mathjax-tex">\(\varvec{b'}\)</span> in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-016-0291-y#Equ1">1</a>). Then, the personalized <span class="mathjax-tex">\(\varvec{H}^{{\prime }}\)</span> can be acquired by solving a linear equation. By setting <span class="mathjax-tex">\(\varvec{A}^{{\prime }}\)</span> in <span class="mathjax-tex">\(\varvec{H}^{{\prime }}\)</span> as <span class="mathjax-tex">\(\varvec{M}\)</span>, we iterate the above steps in order to better estimate the naked human body under clothes.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Experiments</h2><div class="c-article-section__content" id="Sec15-content"><p>In order to evaluate the feasibility of our proposed method, a woman wearing a dress rotates in front of an Xbox One Kinect sensor (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig1">1</a>). The RGB-D images and skeleton information are captured as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig7">7</a>. The result of the reconstruction method explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec5">3</a> is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig8">8</a> and seen from different views. To evaluate the effectiveness of our method, we compare the body sizes between the reconstructed model in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig8">8</a> and the ground truth as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0291-y#Tab1">1</a>. The body sizes of reconstructed model are computed by the length of cross sections on reconstructed model and the skeleton extracted from it, while those of the ground truth are measured by hand.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>RGB-D images and skeleton of the reference frame. <b>a</b> RGB image, <b>b</b> depth image and <b>c</b> skeleton of the reference frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Different views of reconstructed naked human model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Body sizes of estimation and ground truth</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0291-y/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec16">Non-expanded/expanded skin detection</h3><p>We compare the skin detection result on human body using elliptical boundary model with our expanded method as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig9">9</a>. From Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig9">9</a>a, we can see that the pixels that have similar color with skin on clothes in RGB image are recognized as skin as well when we directly perform skin detection algorithm by elliptical boundary model. In order to find more precise skin region, we first detect the clothes region on human body and use Otsu-based method to find the contour of clothes as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec13">3.5</a>. Given the contour of clothes, even though the contour is not smooth as the clothes boundary, we take the bounding box of contour as the input region to apply GrabCut algorithm on it. Meanwhile, we set the pixels between inflated contour and bounding box as the background pixels of GrabCut as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0291-y#Sec13">3.5</a>. The skin region after cutting the clothes region of human body by GrabCut is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig9">9</a>b, which does not require the pattern of the clothes to have large difference with the skin.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Comparison between non-expanded and expanded skin detection results. <b>a</b> Skin detection result without our expanded method. <b>b</b> Result of our expanded skin detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec17">Comparison with existing method</h3><p>We compare the naked human model reconstructed with skin constraints with that reconstructed without skin constraints based on Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3583">2016</a>), as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig10">10</a>. The fitting results with and without skin constraints are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig10">10</a>a, which has large differences on the clothed region of human body. The reconstructed results after several iterations can be found in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig10">10</a>b, in which the model with skin constraints is more reasonable, especially on the arms. We further show the reference depth image with these two results (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig10">10</a>d), from which we can observe that the poses of the results are both similar to the reference frame and the result with skin constraints is more visually precise.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>The fitting and reconstructed results of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig7">7</a> with/without skin constraints. The <i>yellow</i> one is with skin constraints, and the <i>violet</i> one is without skin constraints. <b>a</b> Fitting results with/without skin constraints. <b>b</b> Reconstructed human models with/without skin constraints. <b>c</b> Reconstructed human models in (<b>b</b>) shown together. <b>d</b> Reconstructed human models with reference depth image (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec18">Experiments on virtual depth images</h3><p>In order to demonstrate the feasibility of our method under different circumstances, we put virtual clothes (dresses) on human models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>a) and simulate the depth images by getting the front and outside vertices of the clothed human models. Then, we add random Gaussian noise to it (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>b) and get the artificial depth images (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>c). The Gaussian noise is generated based on Box–Muller Transformation (Box and Muller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1958" title="Box GEP, Muller ME (1958) A note on the generation of random normal deviates. Ann Math Stat 29:610–611" href="/article/10.1007/s10055-016-0291-y#ref-CR5" id="ref-link-section-d61399e3660">1958</a>). A model is selected from the human model database as template to fit the artificial depth image as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>d. The reconstructed results with and without skin constraints of two different views for two women are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>e, from which we can see that the body parts under clothes without skin constraints bulge due to their tight constraints to the dress. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>f–h shows the distance error between reconstructed human bodies without/with skin constraints and the original virtual human body (right one in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>a). The distance error is the distance between the vertex on reconstructed human body and the intersection with the original human body along its normal vector. The sign of the distance error is positive if the vertex is outside of the intersection. The distance distribution is illustrated by a histogram in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig11">11</a>i, from which we can see that the distance error with skin constraints is distributed in the middle, making it more robust and precise than without skin constraints.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Results of virtual depth images. <b>a</b> Clothed human model. <b>b</b> Adding Gaussian noise. <b>c</b> Point clouds regarded as the artificial depth images. <b>d</b> Template and artificial depth images. <b>e</b> Reconstructed human models with/without skin constraints. The <i>yellow</i> model and <i>blue</i> model are with and without skin constraints separately. <b>f</b> Original virtual human body of the right model in (<b>a</b>). <b>g</b>, <b>h</b> are the models without and with skin constraints separately, whose color is shown according to distance error. <b>i</b> Distance error statistical histogram of (<b>g</b>) and (<b>h</b>) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We perform more experiments to demonstrate our proposed method, including three control groups altogether. The control factors are clothes, body shape and pose. Each time we change one factor. Different clothes on the same woman with almost the same pose are tested as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a>. Specifically, the performer wears different types of clothes, such as white dress with geometric pattern, red skirt with white T-shirt, jeans with long sleeves and gray shorts with striped shirt. The experiments of the same clothes on the same woman but with different poses in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a> are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig13">13</a>. Meanwhile, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig14">14</a> exhibits the results of the same clothes on two women with almost the same pose. In addition, more reconstructed results are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig15">15</a>. From diverse contrast experiments, we can see that the reconstructed naked human models of our method are visually convincing. However, there are still some factors influencing the reconstruction results. For example, diverse shapes of invisible body parts under clothes and various clothes on a same person might cause imprecise estimation of naked human body under clothes.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Reconstruction results of woman A in different clothes with almost the same pose. The colored human models are reconstructed by our method with skin detection; the <i>gray</i> human models are reconstructed using the method of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3774">2016</a>) without skin detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Reconstruction results of woman A posing differently with Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a> in the same clothes of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a>a–c. The colored human models are reconstructed by our method with skin detection; the <i>gray</i> human models are reconstructed using the method of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3805">2016</a>) without skin detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Reconstruction results of the same clothes on women B and woman C, respectively. The colored human models are reconstructed by our method with skin detection; the <i>gray</i> human models are reconstructed using the method of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3830">2016</a>) without skin detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0291-y/MediaObjects/10055_2016_291_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>More reconstructed results. The <i>first row</i> is woman D, the <i>second row</i> is woman B and C, respectively, and the last row is woman A. The colored human models are reconstructed by our method with skin detection; the <i>gray</i> human models are reconstructed using the method of Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3861">2016</a>) without skin detection</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0291-y/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Furthermore, the reconstructed results without skin detection (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85" href="/article/10.1007/s10055-016-0291-y#ref-CR8" id="ref-link-section-d61399e3876">2016</a>) are presented in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig13">13</a>, <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig14">14</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig15">15</a> for comparison, which are displayed in gray. The measured body sizes of woman A in different reconstruction experiments and the average error of them for each condition and for each body size are listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0291-y#Tab2">2</a>. From the comparison, we can see that the results without skin detection are less robust than those with skin detection, especially on the region covered with clothes. One exception is Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0291-y#Fig12">12</a>d, because woman A is in a suit of very tight shirt and skinny jeans, making the result with skin detection not good as expected. However, the overall performance of the method with skin detection is more precise and stable. In addition, the big clearances between clothes and the naked human body will lead to imprecise reconstruction results.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Body sizes of estimation in different poses under different clothes of woman A</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0291-y/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusions</h2><div class="c-article-section__content" id="Sec19-content"><p>We present a system to estimate the naked human models under clothes using the incomplete data from a single Kinect based on human model database. The system captures the RGB-D images and skeleton and then performs de-noising, segmentation, registration and fitting steps in order to acquire accurate reconstruction results. Expanded skin detection method is applied to add tight morphing constraints on skin region of human body during the fitting step. The morphing result is further projected back into the space of human bodies to keep human body shape in a reasonable domain. Experiments show the feasibility for different types of clothes and body shapes.</p><p>However, there are still a lot of limitations in the proposed method, which should be solved in future research. For example, the uncertainty of the skeleton of human body from Kinect when the sides and the back facing Kinect or under loose clothes hinders the reconstruction precision. A more effective skeleton-free method needs to be further analyzed.</p><p>Besides, recovering the performance of the human body rotating in front of the Kinect can be studied in future work, which will provide more realistic virtual try-on effect.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers, J. Davis, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Anguelov D, Srinivasan P, Koller D, Thrun S, Rodgers J, Davis J (2005) SCAPE: shape completion and animation o" /><p class="c-article-references__text" id="ref-CR3">Anguelov D, Srinivasan P, Koller D, Thrun S, Rodgers J, Davis J (2005) SCAPE: shape completion and animation of people. ACM Trans Graph 24:408–416</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1073204.1073207" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=SCAPE%3A%20shape%20completion%20and%20animation%20of%20people&amp;journal=ACM%20Trans%20Graph&amp;volume=24&amp;pages=408-416&amp;publication_year=2005&amp;author=Anguelov%2CD&amp;author=Srinivasan%2CP&amp;author=Koller%2CD&amp;author=Thrun%2CS&amp;author=Rodgers%2CJ&amp;author=Davis%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AO. Bălan, MJ. Black, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bălan AO, Black MJ (2008) The naked truth: estimating body shape under clothing. Computer Vision–ECCV 2008. Sp" /><p class="c-article-references__text" id="ref-CR4">Bălan AO, Black MJ (2008) The naked truth: estimating body shape under clothing. Computer Vision–ECCV 2008. Springer, Marseille, pp 15–29</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20Vision%E2%80%93ECCV%202008&amp;pages=15-29&amp;publication_year=2008&amp;author=B%C4%83lan%2CAO&amp;author=Black%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GEP. Box, ME. Muller, " /><meta itemprop="datePublished" content="1958" /><meta itemprop="headline" content="Box GEP, Muller ME (1958) A note on the generation of random normal deviates. Ann Math Stat 29:610–611" /><p class="c-article-references__text" id="ref-CR5">Box GEP, Muller ME (1958) A note on the generation of random normal deviates. Ann Math Stat 29:610–611</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1214%2Faoms%2F1177706645" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0085.13720" aria-label="View reference 3 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20note%20on%20the%20generation%20of%20random%20normal%20deviates&amp;journal=Ann%20Math%20Stat&amp;volume=29&amp;pages=610-611&amp;publication_year=1958&amp;author=Box%2CGEP&amp;author=Muller%2CME">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Chang, M. Zwicker, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Chang W, Zwicker M (2011) Global registration of dynamic range scans for articulated model reconstruction. ACM" /><p class="c-article-references__text" id="ref-CR6">Chang W, Zwicker M (2011) Global registration of dynamic range scans for articulated model reconstruction. ACM Trans Graph 30:171–179</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Global%20registration%20of%20dynamic%20range%20scans%20for%20articulated%20model%20reconstruction&amp;journal=ACM%20Trans%20Graph&amp;volume=30&amp;pages=171-179&amp;publication_year=2011&amp;author=Chang%2CW&amp;author=Zwicker%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Chen, Y. Guo, B. Zhou, Q. Zhao, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chen X, Guo Y, Zhou B, Zhao Q (2013) Deformable model for estimating clothed and naked human shapes from a sin" /><p class="c-article-references__text" id="ref-CR7">Chen X, Guo Y, Zhou B, Zhao Q (2013) Deformable model for estimating clothed and naked human shapes from a single image. Vis Comput 29:1187–1196</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-013-0775-7" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Deformable%20model%20for%20estimating%20clothed%20and%20naked%20human%20shapes%20from%20a%20single%20image&amp;journal=Vis%20Comput&amp;volume=29&amp;pages=1187-1196&amp;publication_year=2013&amp;author=Chen%2CX&amp;author=Guo%2CY&amp;author=Zhou%2CB&amp;author=Zhao%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Chen, J. Li, B. Wang, J. Zeng, G. Lu, D. Zhang, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim V" /><p class="c-article-references__text" id="ref-CR8">Chen G, Li J, Wang B, Zeng J, Lu G, Zhang D (2016) Reconstructing 3D human models with a Kinect. Comput Anim Virtual Worlds 27:72–85</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fcav.1632" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Reconstructing%203D%20human%20models%20with%20a%20Kinect&amp;journal=Comput%20Anim%20Virtual%20Worlds&amp;volume=27&amp;pages=72-85&amp;publication_year=2016&amp;author=Chen%2CG&amp;author=Li%2CJ&amp;author=Wang%2CB&amp;author=Zeng%2CJ&amp;author=Lu%2CG&amp;author=Zhang%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Hasler, C. Stoll, M. Sunkel, B. Rosenhahn, H-P. Seidel, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Hasler N, Stoll C, Sunkel M, Rosenhahn B, Seidel H-P (2009) A statistical model of human pose and body shape. " /><p class="c-article-references__text" id="ref-CR9">Hasler N, Stoll C, Sunkel M, Rosenhahn B, Seidel H-P (2009) A statistical model of human pose and body shape. Comput Graph Forum 28:337–346</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2009.01373.x" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20statistical%20model%20of%20human%20pose%20and%20body%20shape&amp;journal=Comput%20Graph%20Forum&amp;volume=28&amp;pages=337-346&amp;publication_year=2009&amp;author=Hasler%2CN&amp;author=Stoll%2CC&amp;author=Sunkel%2CM&amp;author=Rosenhahn%2CB&amp;author=Seidel%2CH-P">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation" /><p class="c-article-references__text" id="ref-CR10">Hasler N, Ackermann H, Rosenhahn B, Thormahlen T, Seidel H-P (2010) Multilinear pose and body shape estimation of dressed subjects from image sets. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR), pp 1823–1830</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="QX. Huang, B. Adams, M. Wicke, LJ. Guibas, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Huang QX, Adams B, Wicke M, Guibas LJ (2008) Non-rigid registration under isometric deformations. Comput Graph" /><p class="c-article-references__text" id="ref-CR11">Huang QX, Adams B, Wicke M, Guibas LJ (2008) Non-rigid registration under isometric deformations. Comput Graph Forum 27:1449–1457</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2008.01285.x" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-rigid%20registration%20under%20isometric%20deformations&amp;journal=Comput%20Graph%20Forum&amp;volume=27&amp;pages=1449-1457&amp;publication_year=2008&amp;author=Huang%2CQX&amp;author=Adams%2CB&amp;author=Wicke%2CM&amp;author=Guibas%2CLJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Izadi S et al (2011) KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. In" /><p class="c-article-references__text" id="ref-CR1">Izadi S et al (2011) KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera. In: Proceedings of the 24th annual ACM symposium on user interface software and technology, pp 559–568</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 inter" /><p class="c-article-references__text" id="ref-CR12">Lee JY, Yoo SI (2002) An elliptical boundary model for skin color detection. In: Proceedings of the 2002 international conference on imaging science, systems, and technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Li, G. Lu, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Li J, Lu G (2011) Skeleton driven animation based on implicit skinning. Comput Graph 35:945–954" /><p class="c-article-references__text" id="ref-CR13">Li J, Lu G (2011) Skeleton driven animation based on implicit skinning. Comput Graph 35:945–954</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2011.07.005" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Skeleton%20driven%20animation%20based%20on%20implicit%20skinning&amp;journal=Comput%20Graph&amp;volume=35&amp;pages=945-954&amp;publication_year=2011&amp;author=Li%2CJ&amp;author=Lu%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li J, Wang Y (2007) Automatically constructing skeletons and parametric structures for polygonal human bodies." /><p class="c-article-references__text" id="ref-CR14">Li J, Wang Y (2007) Automatically constructing skeletons and parametric structures for polygonal human bodies. In: Proceedings of the 25th computer graphics international conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mitra NJ, Flöry S, Ovsjanikov M, Gelfand N, Guibas L, Pottmann H (2007) Dynamic geometry registration. In: Sym" /><p class="c-article-references__text" id="ref-CR15">Mitra NJ, Flöry S, Ovsjanikov M, Gelfand N, Guibas L, Pottmann H (2007) Dynamic geometry registration. In: Symposium on geometry processing, pp 173–182</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Newcombe RA et al (2011) KinectFusion: real-time dense surface mapping and tracking. In: 2011 10th IEEE intern" /><p class="c-article-references__text" id="ref-CR2">Newcombe RA et al (2011) KinectFusion: real-time dense surface mapping and tracking. In: 2011 10th IEEE international symposium on mixed and augmented reality (ISMAR), pp 127–136</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Otsu, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Otsu N (1975) A threshold selection method from gray-level histograms. Automatica 11:23–27" /><p class="c-article-references__text" id="ref-CR16">Otsu N (1975) A threshold selection method from gray-level histograms. Automatica 11:23–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20threshold%20selection%20method%20from%20gray-level%20histograms&amp;journal=Automatica&amp;volume=11&amp;pages=23-27&amp;publication_year=1975&amp;author=Otsu%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Rother, V. Kolmogorov, A. Blake, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. A" /><p class="c-article-references__text" id="ref-CR17">Rother C, Kolmogorov V, Blake A (2004) Grabcut: interactive foreground extraction using iterated graph cuts. ACM Trans Graph 23:309–314</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1015706.1015720" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Grabcut%3A%20interactive%20foreground%20extraction%20using%20iterated%20graph%20cuts&amp;journal=ACM%20Trans%20Graph&amp;volume=23&amp;pages=309-314&amp;publication_year=2004&amp;author=Rother%2CC&amp;author=Kolmogorov%2CV&amp;author=Blake%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Tong, J. Zhou, L. Liu, Z. Pan, H. Yan, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Tong J, Zhou J, Liu L, Pan Z, Yan H (2012) Scanning 3D full human bodies using Kinects. IEEE Trans Vis Comput " /><p class="c-article-references__text" id="ref-CR18">Tong J, Zhou J, Liu L, Pan Z, Yan H (2012) Scanning 3D full human bodies using Kinects. IEEE Trans Vis Comput Graph 18:643–650</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2012.56" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scanning%203D%20full%20human%20bodies%20using%20Kinects&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=18&amp;pages=643-650&amp;publication_year=2012&amp;author=Tong%2CJ&amp;author=Zhou%2CJ&amp;author=Liu%2CL&amp;author=Pan%2CZ&amp;author=Yan%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Weiss A, Hirshberg D, Black MJ (2011) Home 3D body scans from noisy image and range data. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR19">Weiss A, Hirshberg D, Black MJ (2011) Home 3D body scans from noisy image and range data. In: Proceedings of the 2011 international conference on computer vision, pp 1951–1958</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In:" /><p class="c-article-references__text" id="ref-CR20">Zeng M, Zheng J, Cheng X, Liu X (2013) Templateless quasi-rigid shape modeling with implicit loop-closure. In: 2013 IEEE conference on computer vision and pattern recognition (CVPR), pp 145–152</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Zeng, L. Cao, H. Dong, K. Lin, M. Wang, J. Tong, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of" /><p class="c-article-references__text" id="ref-CR21">Zeng M, Cao L, Dong H, Lin K, Wang M, Tong J (2015) Estimation of human body shape and cloth field in front of a Kinect. Neurocomputing 151:626–631</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.neucom.2014.06.087" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20of%20human%20body%20shape%20and%20cloth%20field%20in%20front%20of%20a%20Kinect&amp;journal=Neurocomputing&amp;volume=151&amp;pages=626-631&amp;publication_year=2015&amp;author=Zeng%2CM&amp;author=Cao%2CL&amp;author=Dong%2CH&amp;author=Lin%2CK&amp;author=Wang%2CM&amp;author=Tong%2CJ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-016-0291-y-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was partially supported by National Natural Science Foundation of China (51575481, 61379096) and Project of Public Technology Research in Industry of Zhejiang Province (2014C31048).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Research Center of Design and Production Innovation, College of Mechanical Engineering, Zhejiang University, Room 416, No. 1 Teaching Building, Zhejiang University Yuquan Campus, No. 38 Zheda Road, Hangzhou, 310027, People’s Republic of China</p><p class="c-article-author-affiliation__authors-list">Guang Chen, Jituo Li, Jiping Zeng, Bei Wang &amp; Guodong Lu</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Guang-Chen"><span class="c-article-authors-search__title u-h3 js-search-name">Guang Chen</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guang+Chen&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guang+Chen" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guang+Chen%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jituo-Li"><span class="c-article-authors-search__title u-h3 js-search-name">Jituo Li</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jituo+Li&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jituo+Li" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jituo+Li%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jiping-Zeng"><span class="c-article-authors-search__title u-h3 js-search-name">Jiping Zeng</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jiping+Zeng&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jiping+Zeng" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jiping+Zeng%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Bei-Wang"><span class="c-article-authors-search__title u-h3 js-search-name">Bei Wang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Bei+Wang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Bei+Wang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bei+Wang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Guodong-Lu"><span class="c-article-authors-search__title u-h3 js-search-name">Guodong Lu</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Guodong+Lu&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Guodong+Lu" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Guodong+Lu%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-016-0291-y/email/correspondent/c1/new">Jituo Li</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Optimizing%20human%20model%20reconstruction%20from%20RGB-D%20images%20based%20on%20skin%20detection&amp;author=Guang%20Chen%20et%20al&amp;contentID=10.1007%2Fs10055-016-0291-y&amp;publication=1359-4338&amp;publicationDate=2016-08-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-016-0291-y" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-016-0291-y" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chen, G., Li, J., Zeng, J. <i>et al.</i> Optimizing human model reconstruction from RGB-D images based on skin detection.
                    <i>Virtual Reality</i> <b>20, </b>159–172 (2016). https://doi.org/10.1007/s10055-016-0291-y</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-016-0291-y.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-07-19">19 July 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-08-01">01 August 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-08-12">12 August 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-09">September 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-016-0291-y" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-016-0291-y</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Human model reconstruction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Kinect</span></li><li class="c-article-subject-list__subject"><span itemprop="about">RGB-D image</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Skin detection</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0291-y.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=291;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

