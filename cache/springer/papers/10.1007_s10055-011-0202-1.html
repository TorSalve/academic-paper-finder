<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Interactive 3-D indoor modeler for virtualizing service fields"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler supports the creation of 3-D models from..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Interactive 3-D indoor modeler for virtualizing service fields"/>

    <meta name="dc.source" content="Virtual Reality 2011 17:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2011-12-15"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler supports the creation of 3-D models from photographs by implementing interaction techniques that use geometric constraints estimated from photographs and visualization techniques that help to easily understand shapes of 3-D models. We evaluated the availability and usability by applying the modeler to model service fields where actual workers provide services and an experience-based exhibit. Our results confirmed that the modeler enables the creation of large-scale indoor environments such as hot-spring inns and event sites at a relatively modest cost. We also confirmed that school children could learn modeling operations and create 3-D models from a photograph for approximately 20&#160;min because of the easy operations. In addition, we describe additional functions that increase the effectiveness of 3-D modeling based on knowledge from service-field modeling. We present applications for behavior analysis of service workers and for 3-D indoor navigation using augmented virtuality (AV)-based visualization realized by photo-realistic 3-D models."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2011-12-15"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="89"/>

    <meta name="prism.endingPage" content="109"/>

    <meta name="prism.copyright" content="2011 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-011-0202-1"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-011-0202-1"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-011-0202-1.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-011-0202-1"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Interactive 3-D indoor modeler for virtualizing service fields"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2013/06"/>

    <meta name="citation_online_date" content="2011/12/15"/>

    <meta name="citation_firstpage" content="89"/>

    <meta name="citation_lastpage" content="109"/>

    <meta name="citation_article_type" content="SI: Mixed and Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-011-0202-1"/>

    <meta name="DOI" content="10.1007/s10055-011-0202-1"/>

    <meta name="citation_doi" content="10.1007/s10055-011-0202-1"/>

    <meta name="description" content="This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler s"/>

    <meta name="dc.creator" content="Tomoya Ishikawa"/>

    <meta name="dc.creator" content="Kalaivani Thangamani"/>

    <meta name="dc.creator" content="Masakatsu Kourogi"/>

    <meta name="dc.creator" content="Andrew P. Gee"/>

    <meta name="dc.creator" content="Walterio Mayol-Cuevas"/>

    <meta name="dc.creator" content="Jungwoo Hyun"/>

    <meta name="dc.creator" content="Takeshi Kurata"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Speeded-up robust features (SURF); citation_author=H Bay, A Ess, T Tuytelaars, L Gool; citation_volume=110; citation_issue=3; citation_publication_date=2008; citation_pages=346-359; citation_doi=10.1016/j.cviu.2007.09.014; citation_id=CR1"/>

    <meta name="citation_reference" content="Bunnum P, Mayol-Cuevas W (2008) Outlin AR: an assisted interactive model building system with reduced computational effort. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 61&#8211;64"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Single view metrology; citation_author=A Criminisi, I Reid, A Zisserman; citation_volume=40; citation_issue=2; citation_publication_date=2000; citation_pages=123-148; citation_doi=10.1023/A:1026598000963; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Image Process; citation_title=Region filling and object removal by exemplar-based image inpainting; citation_author=A Criminisi, P Perez, K Toyama; citation_volume=13; citation_publication_date=2004; citation_pages=1200-1212; citation_doi=10.1109/TIP.2004.833105; citation_id=CR4"/>

    <meta name="citation_reference" content="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of SIGGRAPH, pp 11&#8211;20"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography; citation_author=M Fischler, R Bolles; citation_volume=24; citation_issue=6; citation_publication_date=1981; citation_pages=381-395; citation_doi=10.1145/358669.358692; citation_id=CR6"/>

    <meta name="citation_reference" content="Furukawa Y, Curless B, Seitz M, Szeliski R (2009) Reconstructing building interiors from images. In: Proceedings of international conference on computer vision, pp 80&#8211;87"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot; citation_title=Discovering higher level structure in Visual SLAM; citation_author=A Gee, D Chekhlov, A Calway, W Mayol-Cuevas; citation_volume=26; citation_issue=5; citation_publication_date=2008; citation_pages=980-990; citation_doi=10.1109/TRO.2008.2004641; citation_id=CR8"/>

    <meta name="citation_reference" content="Goesele M, Snavely N, Curless B, Hppe H, Seitz M (2007) Multi-view stereo for community photo collections. In: Proceedings of international conference on computer vision, pp 14&#8211;20"/>

    <meta name="citation_reference" content="Google (2011) Google SketchUp, 
                    http://sketchup.google.com/
                    
                  
                "/>

    <meta name="citation_reference" content="Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, ISBN: 0521623049"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Organ Collect Intell; citation_title=Economic and synergistic pedestrian tracking system with service cooperation for indoor environments; citation_author=T Ishikawa, M Kourogi, T Kurata; citation_volume=2; citation_issue=1; citation_publication_date=2011; citation_pages=1-20; citation_id=CR12"/>

    <meta name="citation_reference" content="Kitajima M, Nakajima M, Toyota M (2010) Cognitive chrono-ethnography: a method for studying behavioral selections in daily activities. In: Proceedings of annual meeting of human factors and ergonomics society"/>

    <meta name="citation_reference" content="Kourogi M, Sakata N, Okuma T, Kurata T (2006) Indoor/outdoor pedestrian navigation with an embedded GPS/RFID/self-contained sensor system. In: Proceedings of the 16th international conference on artificial reality and telexistence, pp 1310&#8211;1321"/>

    <meta name="citation_reference" content="Kurata T, Kourogi M, Ishikawa T, Hyun J, Park A, (2010) Service cooperation and co-creative intelligence cycles based on mixed-reality technology. In: Proceedings of international conference on industrial informatics, pp. 967&#8211;972"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Vis; citation_title=Distinctive image features from scale-invariant keypoints, Int; citation_author=D Lowe; citation_volume=60; citation_issue=2; citation_publication_date=2004; citation_pages=91-110; citation_doi=10.1023/B:VISI.0000029664.99615.94; citation_id=CR16"/>

    <meta name="citation_reference" content="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 79&#8211;89"/>

    <meta name="citation_reference" content="Oh B, Chen M, Dorsey J, Durand F (2001) Image-based modeling and photo editing. In: Proceedings of SIGGRAPH, pp 433&#8211;442"/>

    <meta name="citation_reference" content="Oh J, Stuerzlinger W, Danahy J (2005) Comparing SESAME and sketching for conceptual 3D design. In: Proceedings eurographics workshop on sketch based interface and modeling, pp 81&#8211;88"/>

    <meta name="citation_reference" content="Simon G (2010) In situ 3D sketching using a video camera as an interaction and tracking device. In: Proceedings of eurographics"/>

    <meta name="citation_reference" content="Sinha SN, Steedly D, Szeliski R, Agrawala M, Pollefeys M (2008) Interactive 3D architectural modeling from unordered photo collections. ACM Trans Graph&#160;27(5):159:1&#8211;159:10. doi:
                    10.1145/1409060.1409112
                    
                  
                "/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Modeling the world from internet photo collections; citation_author=N Snavely, M Seitz, R Szeliski; citation_volume=80; citation_publication_date=2008; citation_pages=189-210; citation_doi=10.1007/s11263-007-0107-3; citation_id=CR22"/>

    <meta name="citation_reference" content="van den Hengel A, Dick A, Thorm&#228;hlen T, Ward B, Torr PHS (2007) VideoTrace: rapid interactive scene modeling from video. ACM Trans Graph&#160;26(3). doi:
                    10.1145/1276377.1276485
                    
                  
                "/>

    <meta name="citation_reference" content="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 107&#8211;110"/>

    <meta name="citation_reference" content="Williams L (1978) Casting curved shadows on curved surfaces. In: Proceedings of SIGGRAPH, pp 270&#8211;274"/>

    <meta name="citation_author" content="Tomoya Ishikawa"/>

    <meta name="citation_author_email" content="tomoya-ishikawa@aist.go.jp"/>

    <meta name="citation_author_institution" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan"/>

    <meta name="citation_author" content="Kalaivani Thangamani"/>

    <meta name="citation_author_email" content="thangamani.kalaivani@aist.go.jp"/>

    <meta name="citation_author_institution" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan"/>

    <meta name="citation_author" content="Masakatsu Kourogi"/>

    <meta name="citation_author_email" content="m.kourogi@aist.go.jp"/>

    <meta name="citation_author_institution" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan"/>

    <meta name="citation_author" content="Andrew P. Gee"/>

    <meta name="citation_author_email" content="gee@cs.bris.ac.uk"/>

    <meta name="citation_author_institution" content="University of Bristol, Bristol, UK"/>

    <meta name="citation_author" content="Walterio Mayol-Cuevas"/>

    <meta name="citation_author_email" content="wmayol@cs.bris.ac.uk"/>

    <meta name="citation_author_institution" content="University of Bristol, Bristol, UK"/>

    <meta name="citation_author" content="Jungwoo Hyun"/>

    <meta name="citation_author_email" content="jw.hyun@aist.go.jp"/>

    <meta name="citation_author_institution" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan"/>

    <meta name="citation_author" content="Takeshi Kurata"/>

    <meta name="citation_author_email" content="t.kurata@aist.go.jp"/>

    <meta name="citation_author_institution" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-011-0202-1&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-011-0202-1"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Interactive 3-D indoor modeler for virtualizing service fields"/>
        <meta property="og:description" content="This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler supports the creation of 3-D models from photographs by implementing interaction techniques that use geometric constraints estimated from photographs and visualization techniques that help to easily understand shapes of 3-D models. We evaluated the availability and usability by applying the modeler to model service fields where actual workers provide services and an experience-based exhibit. Our results confirmed that the modeler enables the creation of large-scale indoor environments such as hot-spring inns and event sites at a relatively modest cost. We also confirmed that school children could learn modeling operations and create 3-D models from a photograph for approximately 20&amp;nbsp;min because of the easy operations. In addition, we describe additional functions that increase the effectiveness of 3-D modeling based on knowledge from service-field modeling. We present applications for behavior analysis of service workers and for 3-D indoor navigation using augmented virtuality (AV)-based visualization realized by photo-realistic 3-D models."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Interactive 3-D indoor modeler for virtualizing service fields | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-011-0202-1","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Interaction, 3-D indoor model, Service field, Augmented virtuality","kwrd":["Interaction","3-D_indoor_model","Service_field","Augmented_virtuality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-011-0202-1","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-011-0202-1","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=202;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-011-0202-1">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Interactive 3-D indoor modeler for virtualizing service fields
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0202-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0202-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Mixed and Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2011-12-15" itemprop="datePublished">15 December 2011</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Interactive 3-D indoor modeler for virtualizing service fields</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tomoya-Ishikawa" data-author-popup="auth-Tomoya-Ishikawa" data-corresp-id="c1">Tomoya Ishikawa<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology" /><meta itemprop="address" content="grid.208504.b, 0000000122307538, Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kalaivani-Thangamani" data-author-popup="auth-Kalaivani-Thangamani">Kalaivani Thangamani</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology" /><meta itemprop="address" content="grid.208504.b, 0000000122307538, Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Masakatsu-Kourogi" data-author-popup="auth-Masakatsu-Kourogi">Masakatsu Kourogi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology" /><meta itemprop="address" content="grid.208504.b, 0000000122307538, Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Andrew_P_-Gee" data-author-popup="auth-Andrew_P_-Gee">Andrew P. Gee</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bristol" /><meta itemprop="address" content="grid.5337.2, 0000000419367603, University of Bristol, Woodland Road, Bristol, BS8 1UB, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Walterio-Mayol_Cuevas" data-author-popup="auth-Walterio-Mayol_Cuevas">Walterio Mayol-Cuevas</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Bristol" /><meta itemprop="address" content="grid.5337.2, 0000000419367603, University of Bristol, Woodland Road, Bristol, BS8 1UB, UK" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jungwoo-Hyun" data-author-popup="auth-Jungwoo-Hyun">Jungwoo Hyun</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology" /><meta itemprop="address" content="grid.208504.b, 0000000122307538, Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Takeshi-Kurata" data-author-popup="auth-Takeshi-Kurata">Takeshi Kurata</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Center for Service Research, National Institute of Advanced Industrial Science and Technology" /><meta itemprop="address" content="grid.208504.b, 0000000122307538, Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">89</span>–<span itemprop="pageEnd">109</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">382 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-011-0202-1/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper describes an interactive 3-D indoor modeler that effectively creates photo-realistic 3-D indoor models from multiple photographs. This modeler supports the creation of 3-D models from photographs by implementing interaction techniques that use geometric constraints estimated from photographs and visualization techniques that help to easily understand shapes of 3-D models. We evaluated the availability and usability by applying the modeler to model service fields where actual workers provide services and an experience-based exhibit. Our results confirmed that the modeler enables the creation of large-scale indoor environments such as hot-spring inns and event sites at a relatively modest cost. We also confirmed that school children could learn modeling operations and create 3-D models from a photograph for approximately 20 min because of the easy operations. In addition, we describe additional functions that increase the effectiveness of 3-D modeling based on knowledge from service-field modeling. We present applications for behavior analysis of service workers and for 3-D indoor navigation using augmented virtuality (AV)-based visualization realized by photo-realistic 3-D models.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The service industry accounts for around 70% of GDP (gross domestic product) in Japan, and it is often reported that productivity in the service industry is lower than that in the manufacturing industry. One of the reasons for lower productivity is that the design and operation of many service fields rely on tacit knowledge, such as personal intuition and experience. We consider that the service productivity can be improved by clarifying such implicit knowledge based on objective and quantitative data. For the improvement, it is important to perform a service optimization loop consisting of measuring service fields, analyzing and redesigning services based on the measured data, and applying the redesigned services to the service fields.</p><p>We developed a pedestrian tracking system (Kourogi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kourogi M, Sakata N, Okuma T, Kurata T (2006) Indoor/outdoor pedestrian navigation with an embedded GPS/RFID/self-contained sensor system. In: Proceedings of the 16th international conference on artificial reality and telexistence, pp 1310–1321" href="/article/10.1007/s10055-011-0202-1#ref-CR14" id="ref-link-section-d18667e413">2006</a>; Ishikawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ishikawa T, Kourogi M, Kurata T (2011) Economic and synergistic pedestrian tracking system with service cooperation for indoor environments. Int J Organ Collect Intell 2(1):1–20" href="/article/10.1007/s10055-011-0202-1#ref-CR12" id="ref-link-section-d18667e416">2011</a>) in which wearable self-contained sensors are used to obtain the position and orientation of a pedestrian in not only outdoor spaces but also indoor spaces where services are frequently provided. In addition, we measured the behaviors of service-providers and consumers in service fields. The measured data are used to analyze services. To be effective, the analysis requires intuitive visualization of the measured data corresponding to service fields for utilizing it by service workers who do not understand technologies in the fields. Generally, the measured behaviors and trajectories of workers are visualized on a 2-D plan or map of the service field, and the results are used for discussions during service analysis. However, it is difficult to use the 2-D visualization to intuitively understand relationships between the context of measured actions and places where the actions occurred. If we virtualize real service fields that have rich visual information as photo-realistic 3-D models, visualization combining the models and measured data allows us to develop a more effective and intuitive analysis.</p><p>However, when applying virtualization technologies to actual service fields, the 3-D models have to be practical and stable. In addition, the costs for preparation and creation of 3-D models in the service fields should be reduced as much as possible. To achieve such 3-D modeling, we developed an interactive 3-D indoor modeler targeting indoor spaces where services are provided and mainly consumed. Our modeler enables users to stably create practical photo-realistic 3-D indoor models by using interaction and visualization techniques that takes advantage of photographs and geometric constraints estimated from the photographs. The modeling process consists of local modeling to create a 3-D model from a single photograph and global modeling to integrate multiple local models. The process provides scalability needed to create large-scale indoor environments and does not restrict the capturing of photographs. Hence, a user can quickly complete the capturing of photographs in the service fields. Furthermore, the modeler is based on the mixed reality (MR)-based information-sharing framework (Kurata et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kurata T, Kourogi M, Ishikawa T, Hyun J, Park A, (2010) Service cooperation and co-creative intelligence cycles based on mixed-reality technology. In: Proceedings of international conference on industrial informatics, pp. 967–972" href="/article/10.1007/s10055-011-0202-1#ref-CR15" id="ref-link-section-d18667e422">2010</a>), which improves the efficiency and quality of measuring, analyzing, and designing services. The framework makes the tracking and 3-D modeling technologies easy and possible for applying to service fields by cooperating with behavior measurements and service analysis.</p><p>In this paper, we describe the modeler and evaluate the availability and usability by applying the modeler to actual service fields and an experience-based exhibit. In addition, we describe functions that increase effectiveness of 3-D modeling based on knowledge from service-field modeling. We present applications for behavior analysis of service workers and for 3-D indoor navigation using augmented virtuality (AV)-based visualization, which is realized by photo-realistic 3-D models.</p><p>This paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec2">2</a> describes related studies, and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec3">3</a> explains our interactive 3-D indoor modeler. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec10">4</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec18">5</a> describe evaluations of the modeler used in service fields and experience-based exhibit and improvements of the modeler based on the results of our experiments. AV-based applications using photo-realistic 3-D indoor models are presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec23">6</a>. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec26">7</a> summarizes this paper.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Services are generally provided and consumed in indoor environments rather than outdoor ones; therefore, we focused on 3-D modeling for indoor environments. To effectively create 3-D indoor models, approaches appropriate for indoor spaces are needed. Various 3-D modeling methods from photographs can be roughly classified into two types. One type includes automatic modeling methods that can automatically reconstruct 3-D models without interaction techniques. The other includes interactive/semiautomatic modeling methods with interaction techniques.</p><p>State-of-the-art methods for automatic 3-D modeling have been proposed by Goesele et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Goesele M, Snavely N, Curless B, Hppe H, Seitz M (2007) Multi-view stereo for community photo collections. In: Proceedings of international conference on computer vision, pp 14–20" href="/article/10.1007/s10055-011-0202-1#ref-CR9" id="ref-link-section-d18667e460">2007</a>) and Furukawa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Furukawa Y, Curless B, Seitz M, Szeliski R (2009) Reconstructing building interiors from images. In: Proceedings of international conference on computer vision, pp 80–87" href="/article/10.1007/s10055-011-0202-1#ref-CR7" id="ref-link-section-d18667e463">2009</a>). Their methods reconstruct 3-D models by using stereo matching to obtain dense 3-D shapes from photographs, camera parameters, and a 3-D point cloud estimated by structure-from-motion (SfM) (Snavely et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Snavely N, Seitz M, Szeliski R (2008) Modeling the world from internet photo collections. Int J Comput Vis 80:189–210" href="/article/10.1007/s10055-011-0202-1#ref-CR22" id="ref-link-section-d18667e466">2008</a>). In stereo methods, the scene objects have to be captured at a number of different viewpoints by observing overlapped regions to create accurate 3-D models. Therefore, it is time-consuming to capture enough photographs or video sequences of indoor environments which require inside-out video acquisition. In addition, the computational cost increases when the video sequences are long, and the accuracy often does not meet practical needs.</p><p>Interactive/semiautomatic modeling methods can produce high-quality models by taking advantage of the users’ knowledge. Oh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Oh J, Stuerzlinger W, Danahy J (2005) Comparing SESAME and sketching for conceptual 3D design. In: Proceedings eurographics workshop on sketch based interface and modeling, pp 81–88" href="/article/10.1007/s10055-011-0202-1#ref-CR19" id="ref-link-section-d18667e472">2005</a>) have proposed SESAME, a sketch-based modeling tool, for supporting architectural 3-D designs. The SESAME mainly focuses on designing creative architectures rather than 3-D reconstruction of existing architectures, thus functions for effectively mapping textures of photographs for creating photo-realistic models are not provided in the tool. Google SketchUp (Google <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Google (2011) Google SketchUp, &#xA;                    http://sketchup.google.com/&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0202-1#ref-CR10" id="ref-link-section-d18667e475">2011</a>) provides sketch interfaces on photographs for creating 3-D models, but the displayed photographs are used only for texture mapping and checking correspondences between photographs and 3-D models. The system proposed by Oh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Oh B, Chen M, Dorsey J, Durand F (2001) Image-based modeling and photo editing. In: Proceedings of SIGGRAPH, pp 433–442" href="/article/10.1007/s10055-011-0202-1#ref-CR18" id="ref-link-section-d18667e478">2001</a>) utilizes geometric information from an input photograph for constraining 3-D models on lines of sight (LoS) while the user models and edits them for efficient modeling. However, this system requires a large amount of time for dividing the photograph into regions before modeling. The SketchUp and the system proposed by Oh et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Oh J, Stuerzlinger W, Danahy J (2005) Comparing SESAME and sketching for conceptual 3D design. In: Proceedings eurographics workshop on sketch based interface and modeling, pp 81–88" href="/article/10.1007/s10055-011-0202-1#ref-CR19" id="ref-link-section-d18667e481">2005</a>) are based on single-view geometry estimated from a photograph. The detailed methodology to estimate the geometry and how the geometry is used for applications has been well organized by Criminisi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Criminisi A, Reid I, Zisserman A (2000) Single view metrology. Int J Comput Vis 40(2):123–148" href="/article/10.1007/s10055-011-0202-1#ref-CR3" id="ref-link-section-d18667e484">2000</a>).</p><p>All modeling processes in automatic methods using SfM fail to properly estimate correspondences among photographs. To compensate for the weakness, Debevec et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of SIGGRAPH, pp 11–20" href="/article/10.1007/s10055-011-0202-1#ref-CR5" id="ref-link-section-d18667e490">1996</a>) proposed a semiautomatic method that can carry out stable SfM and create models consisting of basic primitives by manually adding correspondences between edges on 3-D primitives and on images. In this method, however, target objects have to be approximated by the predetermined basic primitives.</p><p>Sinha et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Sinha SN, Steedly D, Szeliski R, Agrawala M, Pollefeys M (2008) Interactive 3D architectural modeling from unordered photo collections. ACM Trans Graph 27(5):159:1–159:10. doi:&#xA;                    10.1145/1409060.1409112&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0202-1#ref-CR21" id="ref-link-section-d18667e497">2008</a>) and van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="van den Hengel A, Dick A, Thormählen T, Ward B, Torr PHS (2007) VideoTrace: rapid interactive scene modeling from video. ACM Trans Graph 26(3). doi:&#xA;                    10.1145/1276377.1276485&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-011-0202-1#ref-CR23" id="ref-link-section-d18667e500">2007</a>) have proposed interactive 3-D modelers that use a sparse map and camera parameters estimated by SfM. Assuming that SfM must be able to estimate all parameters successfully, these systems heavily utilize data from SfM in order to reduce manpower needed for modeling. Accordingly, when SfM cannot work, all modeling processes no longer function. Furthermore, in case where the created models have crucial untextured regions, the user has to revisit the site to recapture texture images of the regions. The ability to capture photographs in the service field is a rare opportunity that requires permission to occupy the service field and incurs travel cost and other expense. If photographs must be recaptured and if the opportunity is still available, revisiting the service field can significantly increase costs.</p><p>A method of preventing such a shortage of texture images is to employ in situ modeling. In terms of this strategy, Neubert et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 79–89" href="/article/10.1007/s10055-011-0202-1#ref-CR17" id="ref-link-section-d18667e506">2007</a>), Bunnum and Mayol-Cuevas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bunnum P, Mayol-Cuevas W (2008) Outlin AR: an assisted interactive model building system with reduced computational effort. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 61–64" href="/article/10.1007/s10055-011-0202-1#ref-CR2" id="ref-link-section-d18667e509">2008</a>), van den Hengel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 107–110" href="/article/10.1007/s10055-011-0202-1#ref-CR24" id="ref-link-section-d18667e512">2009</a>), and Simon (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simon G (2010) In situ 3D sketching using a video camera as an interaction and tracking device. In: Proceedings of eurographics" href="/article/10.1007/s10055-011-0202-1#ref-CR20" id="ref-link-section-d18667e515">2010</a>) have proposed 3-D modelers that can effectively and quickly create 3-D models near target objects. Because of the augmented reality (AR) techniques that overlay 3-D models being created on videos capturing real scenes by real-time tracking and modeling techniques, these modelers allow the users to confirm completeness of models and textures on the sites. They are suitable for creating simple models for tracking, brief prototyping, and updating small parts of 3-D models; however, they are not suitable for creating large indoor models to be used for visualization of indoor environments, since the 3-D modeling with AR for large environments is a hard task that requires much time. Additionally, occupying service fields for a long time is costly and impractical.</p><p>As previously mentioned, SfM is often used to obtain automation and efficiency improvements, but it causes errors of camera parameters and 3-D feature points in large-scale environments. Therefore, 3-D models generated by using SfM are difficult to use for measuring and visualizing workers’ behaviors. Moreover, because of narrow corridors and rooms in indoor environments such as hot-spring inns described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec10">4</a>, SfM and stereo methods cannot work well. Therefore, a modeler should have other ways to create 3-D models for stable 3-D modeling when SfM methods fail. In terms of interactive modeling, it is difficult to apply the modeler to service fields unless it allows the user to create 3-D models easily and effectively because the time of modeling directly affects costs.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Interactive 3-D indoor modeler</h2><div class="c-article-section__content" id="Sec3-content"><h3 class="c-article__sub-heading" id="Sec4">Overview</h3><p>In our interactive 3-D indoor modeler, a user first captures photographs in a service field. Then, the user creates local models for each photograph and integrates them in a global coordinate system (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig1">1</a>). Using both local and global modeling, the modeler creates a global model as the virtualized service field.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Flowchart of 3-D indoor modeling</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>During local modeling, the user creates a local 3-D model interactively from a single photograph. To effectively create local models from a single photograph with less information, the modeler estimates geometric information between the scene and the camera by camera calibration utilizing features of indoor environments. During modeling, the modeler helps the user to create 3-D polygon models by interaction techniques using the geometric constraints and to comprehend correspondences between a photograph and the shapes of 3-D models by real-time projective texture mapping (PTM), depth mapping, and automatically controlled smart secondary viewing. The created 3-D models are photo-realistic because all the textures on the models are projected from input photographs. Furthermore, a 3-D model can be created from a single photograph; thus, the 3-D modeling technique does not heavily constrain photo capture. Therefore, we can reduce the time needed to occupy the service field. Additionally, the user can create 3-D models stably because the process does not require correspondences between images, unlike in SfM.</p><p>The local models created from photographs are integrated in a global coordinate system by global modeling. During the global modeling, the modeler can reduce the degrees of freedom (DOF) of integration in order to easily integrate local models by utilizing geometric information estimated during local modeling. In case a plan data of the field is available, the user can create real-scale 3-D models that can be used for tracking workers and visualizing the tracking results by verifying the position, orientation, and scale of the models and the plan. Furthermore, rooms, floors, and furniture that have similar shapes are effectively modeled by copying them. In the modeler, a local model does not affect the entire process of 3-D modeling and local modeling can be carried out in parallel. Hence, the modeler has required scalability to create large-scale indoor environments.</p><p>By using the modeler, the user can quickly obtain photographs in service fields and can create photo-realistic 3-D indoor models stably and effectively at low costs. In addition, the created models can be used not only for visualizations needed to analyze services but also for improvements in the accuracy of tracking methods. Thus, the costs for modeling can be reduced in the service optimization loop. Local modeling from a single photograph and global modeling are described in the following sections.</p><h3 class="c-article__sub-heading" id="Sec5">Local modeling from a single photograph</h3><p>This section describes local modeling processes used to create 3-D models for each photograph. Our modeler supports the creation of models easily and effectively by taking advantages of features in indoor environments. Furthermore, the difficulty in understanding correspondences between a photograph and the shapes of the 3-D model caused by single-view modeling is addressed by real-time PTM, depth mapping, and automatically controlled smart secondary viewing.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Camera calibration</h4><p>In indoor spaces, floors, walls, and furniture are typically parallel or perpendicular with respect to each other. These arrangements facilitate 3-D modeling by allowing the modeler to apply an orthogonal coordinate system to floors and walls occupying large areas of a photograph. Our modeler utilizes these features and estimates the camera parameters that represent geometric information between the local and the camera coordinate systems by using computer-vision (CV)-supported simple user interactions.</p><p>A local coordinate system can be estimated by selecting two pairs of lines that are parallel in the actual 3-D room. The modeler first executes the Hough transform to detect straight lines in the photograph, and then displays the lines to the user (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig2">2</a>a, b). By clicking on the displayed lines, the user can identify pairs of parallel lines as inputs to the modeler. The 2-D intersection points of the selected lines are vanishing points within the photograph. From the two vanishing points <b>e</b>
                    <sub>1</sub>, <b>e</b>
                    <sub>2</sub>, the focal length <i>f</i> of the camera and the rotation matrix <b>R</b> between the local and camera coordinate systems are calculated by the following equations (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, ISBN: 0521623049" href="/article/10.1007/s10055-011-0202-1#ref-CR11" id="ref-link-section-d18667e599">2000</a>).</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f = \sqrt {\left| {{\mathbf{e}}_{1} \cdot {\mathbf{e}}_{2} } \right|} $$</span></div></div>
                    <div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\mathbf{R}} = \left( {\begin{array}{*{20}c} {{\mathbf{v^{\prime}}}_{1} } &amp; {{\mathbf{v^{\prime}}}_{2} } &amp; {{\mathbf{v^{\prime}}}_{1} \times {\mathbf{v^{\prime}}}_{2} } \\ \end{array} } \right) $$</span></div></div><p>where <span class="mathjax-tex">\( {\mathbf{e}}_{i} = (x_{i} ,y_{i} )^{T} (i = 1,2),\quad {\mathbf{v}}_{i} = (x_{i} ,y_{i} ,f)^{T} ,\quad {\mathbf{v^{\prime}}}_{i} = {{{\mathbf{v}}_{i} } \mathord{\left/ {\vphantom {{{\mathbf{v}}_{i} } {\left\| {{\mathbf{v}}_{i} } \right\|}}} \right. \kern-\nulldelimiterspace} {\left\| {{\mathbf{v}}_{i} } \right\|}}. \)</span>
                    </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Camera calibration from a single photograph. <b>a</b> Input photograph, <b>b</b> detected <i>lines</i> by Hough transform, <b>c</b> setting grid and upward vertical direction by user interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>After estimating <b>R</b>, the user sets a displayed grid and a blue bar that corresponds to the <i>x</i>–<i>y</i> plane and upward direction of <i>z</i>-axis, respectively, in the local coordinate system. The displayed grid and the blue bar refer to the ground plane and the upward vertical direction in the photograph, respectively (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig2">2</a>c). If necessary, the user can rotate the grid for more accurate adjustments. The estimated geometric information can be used to effectively place 3-D polygons (this process is described in next section). During global modeling, the ground plane and upward vertical direction can be used to place the local model from a photograph in the global coordinate system on the assumption that both ground planes in each coordinate system lie on the same plane. Additionally, real-scale 3-D modeling can be achieved by providing the height from the ground of the camera used for capturing the photographs. If the ground regions are not captured on a photograph, the user should set a grid parallel to the ground plane.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Interactive modeling tools</h4><p>We can assume that floors, walls, and furniture are composed of a set of planes. In particular, textures on walls are important for understanding locations in environments. Therefore, composing floors, walls, and furniture using a set of textured planes has important impact on the understandability and amount of data. With this impact in mind, assuming that each object in an indoor photograph can be modeled with a set of quadrangular and freeform planes, our modeler provides two types of tools to create planes for the user:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <i>Quadrangular tool</i>: Creates a 3-D quadrangular plane by providing the 3-D coordinates of opposite corners identified by mouse clicks. This tool is suitable for simple objects such as floors, walls, tables, and shelves.</p>
                      </li>
                      <li>
                        <p>
                          <i>Freeform tool</i>: Creates a 3-D freeform plane by providing a set of 3-D points lying on a contour identified by several mouse clicks. This tool is used for complex objects.</p>
                      </li>
                    </ul><p>The user selects the tool depending on the complexity of the targeted object and uses the tools to create 3-D planes, starting at the optical center of a photograph as an initial viewpoint. By modeling using the displayed photograph at the initial viewpoint, the user can intuitively create 3-D models corresponding to the textures on the photograph. In particular, such a viewpoint is effective in creating freeform planes, because interaction points that identify the contour points of the object being modeled are same as the clicked 2-D points on the photograph. For both tools, the depth of the first-clicked point can be obtained by calculating the intersection between the LoS passing through the clicked point on the photograph and the plane nearest to the optical center of the photograph. If there is no intersection with the planes, intersections with <i>x</i>–<i>y</i>, <i>y</i>–<i>z</i>, and <i>z</i>–<i>x</i> planes of a coordinate system are used for this purpose. Because of real-time PTM, the user can easily understand the correspondences between the photograph and 3-D planes even from other viewpoints.</p><p>Most of the 3-D planes used for walls and floors are along the <i>x</i>–<i>y</i>, <i>y</i>–<i>z</i>, and <i>z</i>–<i>x</i> planes. Therefore, the user can quickly create these targets by changing the normal vector of the plane being created to one of the planes of the coordinate system. The user performs this change by utilizing camera parameters estimated by camera calibration. The created 3-D planes can be translated, deformed, and deleted. In terms of translation and deformation, by using the view-volume constraint, the user can control the depth and normal vector of the plane being created without changing the 2-D shapes projected onto the photograph (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig3">3</a>). This constraint allows the 2-D region on the photograph to be fixed at an initial viewpoint even if the 3-D planes are manipulated in the 3-D space. Hence, the user can manipulate the 3-D plane without changing the 2-D regions at different viewpoints. Additionally, vertices on the created 3-D planes can be added, moved, and deleted, and edges can be moved for dilation and diminution.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Manipulation of normal vector (<i>top row</i>) and depth (<i>bottom row</i>) with geometric constraint (the plane with a shelf texture and the lines connecting vertices on the plane represent the plane being manipulated and the view volume)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Visualization for checking 3-D model</h4><p>In our modeler, the user performs 3-D modeling primarily at an initial viewpoint. From the initial viewpoint, 3-D models corresponding to 2-D regions on the photograph can be directly created, but it is difficult to comprehend the shapes of the models only from the viewpoint. To comprehensively present the shapes of the models being created, our modeler provides real-time PTM, depth mapping, and smart secondary view functions to the user.</p><p>
                    <i>Texture and depth representation</i>. The modeler provides three types of texture and depth presentation modes to the user as follows (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig4">4</a>):</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                          <i>PTM</i>: Reprojects the texture in the photograph onto 3-D models and shows the correspondence between the shapes of the models and the textures.</p>
                      </li>
                      <li>
                        <p>
                          <i>Depth mapping</i>: Displays the depth from the viewpoint to the models as a gray-scale view image and shows the shapes of the models clearly.</p>
                      </li>
                      <li>
                        <p>
                          <i>Mixed mapping</i>: Displays the models by combining PTM and depth mapping and shows a more shape-enhanced view image compared with PTM.</p>
                      </li>
                    </ul><p>These modes of presentation can be rendered by a GPU in real time not only while viewing the models but also while creating and editing them. Therefore, these modes are effective for confirming the shape of models being created. Users often face difficulties in confirming shapes of the models from the initial viewpoint using only PTM. In such cases, depth mapping or mixed mapping method provides good clues to confirm the shapes, to find missing planes, and to adjust the depth. Normally, during local and global modeling, the created 3-D models are rendered by PTM.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Examples of PTM (<i>left</i>), mixed mapping (<i>middle</i>), and depth mapping (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>
                    <i>Smart Secondary View</i>. In order to easily understand the shapes of models while they are being constructed, our modeler displays not only a primary view but also a secondary view (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig5">5</a>). The secondary view is automatically controlled to capture better views that can help comprehend the shape of the 3-D plane being created according to the primary view parameters and the 3-D plane.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Close-up of secondary view (<i>left</i>) and primary view (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>We define the criteria for determining the secondary view parameters as follows:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Updating frequency: Viewing parameters should not be changed frequently.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Point visibility: The next point that will be created (corresponding to the mouse cursor) must not be occluded by other planes.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Front-side visibility: The view must not show the rear side of the target plane.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Parallelism: The view should be parallel to the target plane.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Field of view (FoV) difference: The view should have a wide FoV when the primary view has a narrow FoV, and vice versa.</p>
                        
                      </li>
                    </ol><p>The modeler searches for the position, posture, and focal length of the secondary view based on the above criteria. For a real-time search of viewing parameters, the parameters are coarsely sampled.</p><h3 class="c-article__sub-heading" id="Sec9">Global modeling from local models</h3><p>In global modeling, the user integrates the created local models by placing them in a global coordinate system and creates the global model as the entire 3-D model of the field. In the modeler, the integration tasks are carried out by manually, but the user can easily perform them using the geometric constraints estimated by camera calibration.</p><p>For 3-D modeling of actual service fields, we need to consider both situations: (a) a plan of the field is obtained and (b) a plan of the field is not obtained. When the user can obtain a plan of the field, the real- and large-scale global model is accurately created by displaying the plan data in the global coordinate system and by placing local models on the plan as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig6">6</a>. This task normally requires manipulations of the seven DOF, three DOF for 3-D position, three DOF for 3-D orientation, and a scale parameter for transforming an indeterminate size of a local model to the size in real world (Criminisi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Criminisi A, Reid I, Zisserman A (2000) Single view metrology. Int J Comput Vis 40(2):123–148" href="/article/10.1007/s10055-011-0202-1#ref-CR3" id="ref-link-section-d18667e961">2000</a>). The modeler can reduce 7DOF to 4DOF that represents 2-D locations on the <i>x</i>–<i>y</i> plane, rotation of the <i>z</i>-axis, and scale, because the ground plane and upward vertical direction in both local and global coordinate systems are assented from geometric constraints estimated by camera calibration. In addition, when the height of the camera from the ground is known during photo capture, 4DOF for the manipulations becomes nearly 3DOF because the real-scale model can be estimated from the height.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Manual integration of a local model in a global coordinate system with a plan</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Plans of service fields often cannot be obtained or are inaccurate, particularly in classic Japanese buildings. In such cases, the user has to adjust the scale parameter by setting the grid at 1-m intervals and matching the dimensions of a part in the 3-D model to that of the corresponding part in the real environment, such as the width of a corridor. After placing the real-scale local model in the global coordinate system, the scale of the adjacent local model can be adjusted by matching textures of both models.</p><p>In indoor environments, facilities and furniture such as desks and chairs are often placed in a building. Thus, the user can reduce the time needed for 3-D modeling by copying and placing the local model that was previously created. The modeler supports the ability to copy a local model and a group of local models and places the copies in different position and orientations and at a different scale from the original models.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Experiments</h2><div class="c-article-section__content" id="Sec10-content"><p>This section describes experiments for 3-D modeling by applying our modeler to actual service fields and an experience-based exhibit and shows the availability and usability of the modeler from the results.</p><h3 class="c-article__sub-heading" id="Sec11">Service-field modeling</h3><p>We conducted experiments for 3-D indoor modeling of 11 indoor environments primarily consisting of actual service fields, Japanese restaurants, nursing-care facilities, hot-spring inns, and other environments. With these experiments, we show that our modeler allows users to create large-scale and photo-realistic 3-D indoor models.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Conditions of 3-D indoor modeling</h4><p>In the experiments, we conducted 3-D modeling using plan images digitized from plan hardcopies and building guides in most of the environments because CAD data containing 3-D structural information were unavailable in all the environments. Additionally, in the hot-spring inn 3 (to be presented in the next section), we carried out 3-D modeling without any plan data, because the simplified plan in the inn’s guide booklet had not been updated after extension or reconstruction of the building. Moreover, we could not obtain any plan data for the conference site; hence, we also tried to create 3-D models without the plan data for that environment.</p><p>In each environment, the user of the modeler first captured photographs. The user used a standard digital still camera (Panasonic, DMC-LX3) for capturing photographs in all the environments. The intrinsic camera parameter is calibrated in advance for accurate 3-D modeling. The photographs used for modeling are sized at 512 × 384 pixels and the horizontal FoV is approximately 70°. The user strove to create 3-D models similar to real environments in terms of visualization and composition, and the user did not copy models when the models contained specific textures important for recognizing the locations. The created 3-D models were to be used for analysis of workers’ behaviors; therefore, the areas that the service workers did not use were not modeled or only walls were created.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Creating 3-D indoor models</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig7">7</a> shows all the global models and examples of first-person views of the created 3-D indoor models for all the 11 environments. All the first-person views in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig7">7</a> contain multiple local models. For each environment, Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0202-1#Tab1">1</a> gives the specifications of 3-D modeling, floor spaces, number of polygons, number of photographs captured, number of photographs used, duration of the photo capture, and time needed for model making. For modeling hot-spring inns 1–3, many users created local models in parallel and one user carried out global modeling to integrate them. In this case, the time needed for model making was estimated as the sum of time duration of 3-D modeling carried out by all users.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7a-d_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7a-d_HTML.jpg" alt="figure7" loading="lazy" /></picture><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7e-h_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7e-h_HTML.jpg" alt="figure7" loading="lazy" /></picture><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7i-k_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig7i-k_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>3-D indoor models (<i>left</i> entire global model, <i>right</i> first-person views). <b>a</b> Office building, <b>b</b> conference site, <b>c</b> Japanese restaurant 1, <b>d</b> Japanese restaurant 2, <b>e</b> nursing-care facility 1, <b>f</b> nursing-care facility 2, <b>g</b> hot-spring inn 1, <b>h</b> hot-spring inn 2, <b>i</b> hot-spring inn 3, <b>j</b> event site 1, <b>k</b> event site 2</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Specifications of 3-D indoor models</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-011-0202-1/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig7">7</a>, the created models are photo-realistic and the first-person views rendered from the 3-D models look similar to photographs taken in the real environments. We can confirm that our modeler allowed users to create a large-scale indoor model over 4,000 m<sup>2</sup> and to render the model as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-011-0202-1#Tab1">1</a> and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig7">7</a>g. This result was achieved by the advantages of scalability realized by local and global modeling and models with fewer polygons.</p><p>For the office building, conference site, nursing-care facilities, and event sites, the number of polygons, the number of photographs used for 3-D modeling, and time needed for model making per unit of floor space are lower than those for other environments, because the interactions using parallelism and orthogonality of indoor features to create 3-D planes are effectively used in environments consisting largely of simple planes. In addition, nursing-care facilities are effectively created by copying other models because the facilities have many similar structures. Furthermore, for Japanese restaurants and hot-spring inns consisting of nonlinear and complex structures, time for 3-D modeling, the number of polygons, and number of photographs were greater than those required by the other environments. Moreover, the users needed to capture multiple photographs in order to photograph each part of the environment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig8">8</a>) because of narrow rooms and corridors in the environments. This led to increase in the time needed for photo capture and 3-D modeling. Because of the complexity, large floor space, and number of photographs needed for each environment, the users often confused the correspondences between photographs and the capture positions. We believe that ambiguity increases the time needed for modeling by introducing mistakes during integration of local models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Inefficient photographs captured because of narrow environment (<i>left</i> and <i>right</i> photographs are <i>bottom</i> and <i>top</i> parts of a corridor, respectively)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>We could not obtain accurate plans for the conference site and hot-spring inn 2. Hence, the users created real-scale 3-D models by relying on the scale parameter estimated from the height of the camera from ground, widths of rooms and corridors measured in the fields, and a grid at 1-m intervals in the global coordinate system. Furthermore, by comparing photographs taken in the real environments (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig9">9</a> left) and the rendered views in the global models at the viewpoint close to the real one (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig9">9</a> right), the users could check the consistency of both real and virtual environments and create 3-D models in case of unavailability of plan data.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Similarity check for global modeling without plan data (<i>left column</i> photographs capturing a real environment, <i>right column</i> rendered views in a 3-D indoor model at a viewpoint close to that of the <i>left</i> photograph)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec14">Experience-based exhibit</h3><p>We applied our modeler to an experience-based exhibit at an open house event of our research institute and evaluated the usability of the modeler by asking participants who were not specialists in 3-D modeling to complete questionnaires and 3-D models created by them.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Conditions</h4><p>Every year, our research institute holds an open house for school children and their families in order to publicize our research activities. During the open house, we conducted an experience-based exhibit that featured our modeler and an omnidirectional hands-free walk-through simulator. As the part of the exhibit, participants (eight men and nine women) first used our modeler to create a local model from a given photograph. Then, using the walk-through simulator, they virtually walked around the global model which included the local model that they created. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig10">10</a> shows the overview of the exhibit excerpted from the guide booklet used in the open house.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Experience-based exhibit using interactive 3-D indoor modeler and walk-through simulator</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The participants did not have any experience in 3-D modeling. Their average age was 12.6 years old (the youngest was 6 years old, the oldest 37). Each participant received a simple tutorial from the staff to learn the process of 3-D modeling and the use of the modeler. The tutorial primarily explained how to select parallel lines, how to set the upward vertical direction, and how to create 3-D planes as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig11">11</a>. After the tutorial, each participant independently created the local model from the given photograph. The modeler can be controlled with mouse operations, and the photograph given to the participant contained simple objects that can be easily created. The total time spent in the tutorial and 3-D modeling was approximately 20 min. The local models created by the participants were integrated by the staff, and the participants used the walk-through simulator to virtually walk around the global model. After the walk through, each participant answered a questionnaire regarding the 3-D indoor modeler.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Participants and samples of tutorial slides explaining the 3-D indoor modeler (<i>top row</i> participants creating 3-D models, <i>bottom row</i> samples of tutorial slides)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Results of experiments</h4><p>Using a questionnaire, we asked participants to answer the following five questions regarding the 3-D indoor modeler:</p><ul class="u-list-style-none">
                      <li>
                        <p>Q1. Did you enjoy creating 3-D modeling from a photograph? (0: did not enjoy, 4: did enjoy)</p>
                      </li>
                      <li>
                        <p>Q2. How often do you use a personal computer (PC)? (0: infrequently, 4: frequently)</p>
                      </li>
                      <li>
                        <p>Q3. Were you able to select parallel lines easily? (0: difficult, 4: easy)</p>
                      </li>
                      <li>
                        <p>Q4. Were you able to set the grid on the floor easily? (0: difficult, 4: easy)</p>
                      </li>
                      <li>
                        <p>Q5. Were you able to create 3-D planes as you expected? (0: difficult, 4: easy)</p>
                      </li>
                    </ul><p>The participants evaluated each questionnaire item using a five-point Likert scale. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig12">12</a> shows the results. For each question, three averages are displayed: (1) average score of all participants, (2) average score for those who use the PC frequently (score of 3 or 4 in Q2), and (3) the average score for those who do not use the PC very often (score of 0, 1, or 2 in Q2).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Average scores of questionnaires about interactive 3-D modeler (<i>0</i> negative score, <i>4</i> positive score)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The answers to Q1 suggest that most of the participants thoroughly enjoyed the exhibit of creating the 3-D models. The average scores for Q3–5, which asked about the usability of the modeler, were more than 2; hence, we can confirm that most of the participants were able to create 3-D models easily. However, scores of Q3 and 4, which asked about the operation of camera calibration, differed based on how frequently a participant used a PC. Those who frequently used a PC scored more than 2, but the other participants scored 2 or less. In particular, for Q3, we confirmed that the difference in average scores of the groups of participants categorized by frequency of PC usage was statistically significant (<i>p</i> &lt; 0.05) based on the<i> t</i> test assuming a unified variance. For Q5, which asked about operations for creating 3-D planes after camera calibration, we could not confirm a significant difference between the groups. In terms of Q3 and 4, participants who did not use the PC frequently thought that the operations were difficult to perform. We think that the reason for this perception is that they had to use accurate mouse clicks to select parallel lines from detected lines on a photograph and button clicks to rotate grids. However, after setting correct camera parameters, they could create 3-D planes easily with interactions that utilized parallelism and orthogonality in indoor environments.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig13">13</a> right shows the global model that integrated the local models created by participants, and Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig13">13</a> left and center show the local models created by the participants who scored less than 3 for at least one of Q1–4 and who scored more than 2 for all questions, respectively. The quality levels of both local models are sufficient to be used for the global model shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig13">13</a> right, particularly for the detailed model presenting concavity and convexity that was created by a conational participant. For this model, we confirmed that even school children could create a 3-D indoor model within a short time.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Samples of local models created by participants (<i>left</i> and <i>center</i>) and global model (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec17">Discussion</h3><p>As a result of these experiments, we learned about the availability and usability of the modeler. We conducted experiments for 3-D modeling of the 11 indoor environments and confirmed that the users of the modeler were able to create large-scale and photo-realistic 3-D indoor models. Additionally, the models were created from the photographs taken during photo capture in the field, which demonstrated that revisiting and occupying the field to capture additional photographs is unlikely to be necessary. The modeler offers this capability because it allows the users to create a model from a single photograph, and it is less susceptible to the restriction of establishing correspondences between images as that in SfM. Furthermore, the use of global modeling by integrating local models enables scalability and also allows users to perform 3-D modeling in parallel. Hence, the large-scale models were created within a short time using parallel modeling by some users. However, in complex environments, such as Japanese restaurants and hot-spring inns, the time needed for photo capture and model making was increased because of the need to capture additional photographs in narrow environments, limitations of the FoV of the camera, and ambiguity in correspondences between photographs and locations. For capturing narrow environments, we consider that the modeler should allow users to input various photographs, such as photographs captured by wide FoV lens, ones captured longitudinally, and panoramic photographs. For the ambiguity between photographs and locations, we present a method to address these problems in the next section. When modeling old buildings, the plan data are often not obtainable, but the users of the modeler were able to create the 3-D models consistent with real environments by measuring widths of rooms and corridors at several points and by checking similarities between the photographs and the rendered views in the global model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig9">9</a>).</p><p>The costs of 3-D modeling for each environment can be approximately estimated by multiplying the sum of the times needed for model making and photo capture and labor costs for modeling staff. If we specify the labor cost at $20 per hour, most of the 3-D models can be created at less than $1,000 except for hot-spring inn 1, which is particularly large floor space. As a reference of modeling costs, when we order the 3-D modeling of these environments from companies, the costs are estimated at more than $10,000. Hence, the modeler allows users to create 3-D models relatively inexpensive.</p><p>In terms of the usability of the modeler, we confirmed that even school children who are about 10 years old were able to create 3-D models well. However, for participants who did not use PC frequently, the operations for camera calibration were considered difficult. We will improve the modeler by enhancing CV support for difficult operations and reducing the number of steps required for the operations. By improving usability, 3-D models of service fields for measuring and analyzing behavior can be created by workers in the fields, and the quality of services can be improved at a low cost.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Advanced work for effective modeling</h2><div class="c-article-section__content" id="Sec18-content"><p>We improved the modeler by adding some functions for more effective 3-D indoor modeling, based on knowledge obtained during experiments. Then, we demonstrated the new functions. In this section, we describe that the improvements were implemented to reduce the time needed for model making, to estimate the locations of photo capture, and to generate visually natural 3-D models without recapturing photographs.</p><h3 class="c-article__sub-heading" id="Sec19">Photo capture and global modeling with a camera and self-contained sensors</h3><p>Capturing photographs in large-scale indoor environments often requires a long time, resulting in ambiguity in correspondences between photographs and locations. This ambiguity causes mistakes when placing local models in a global coordinate system, and it is one of the reasons for increase in the time needed for model making. Additionally, quality of the models created from photographs of each environment taken once as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec10">4</a> is sufficient to allow the models to be used for the applications described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec23">6</a>. However, to create more complete models for the other purposes, the photographs need to be recaptured. In addition, the recapture should be planned to incur the lowest possible cost because recapturing photographs is costly. We address these two concerns by implementing automatic globalization and recommendations to support photography using a behavior measurement technology.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Automatic globalization using visual SLAM and PDR</h4><p>By estimating capture locations, global modeling can be automated and made more effective by placing local models in environments. For this purpose, our modeler utilizes visual SLAM and pedestrian dead-reckoning (PDR) by acquiring video sequences for capturing large amount of visual data and by measuring behavioral data obtained from self-contained sensors worn by users. In addition, the modeler automatically and accurately integrates local models in a global coordinate system by using 3-D information of the local models.</p><p>For automatic globalization, first, the modeler user creates sparse maps of the environment by using visual SLAM and PDR. Video sequences are suitable for capturing wide areas in a short time compared with photographs. Our modeler generates sparse maps of indoor environments consisting of a set of point clouds by using visual SLAM (Gee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Gee A, Chekhlov D, Calway A, Mayol-Cuevas W (2008) Discovering higher level structure in Visual SLAM. IEEE Trans Robot 26(5):980–990" href="/article/10.1007/s10055-011-0202-1#ref-CR8" id="ref-link-section-d18667e1437">2008</a>) with video sequences and PDR with self-contained sensors (Kourogi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kourogi M, Sakata N, Okuma T, Kurata T (2006) Indoor/outdoor pedestrian navigation with an embedded GPS/RFID/self-contained sensor system. In: Proceedings of the 16th international conference on artificial reality and telexistence, pp 1310–1321" href="/article/10.1007/s10055-011-0202-1#ref-CR14" id="ref-link-section-d18667e1440">2006</a>). SfM usually requires high computational costs and long calculation times to accurately estimate camera motion parameters and a map. In contrast, we can check the success or failure of the estimation on the site by using visual SLAM. Our modeler employs visual SLAM, which can estimate camera motion parameters and a map simultaneously and quickly. Furthermore, measurements of a user’s position and orientation by PDR can be used to set the position and direction of photographs and video sequences in the global coordinate system as well as establish the scale of the sparse maps by simultaneously carrying out PDR with visual SLAM. When the modeler handles multiple maps, they are placed in a global coordinate system based on locations estimated by PDR. Additionally, the global coordinate system is configured as the <i>z</i>-axis corresponds to the upward vertical direction and the ground plane corresponds to the <i>x</i>–<i>y</i> plane. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig14">14</a> shows two sparse maps placed in a global coordinate system and the camera paths. The sparse maps are used for automatic globalization to integrate local and global models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Example of sparse maps created by visual SLAM and PDR in a global coordinate system (<i>white points</i> and <i>colored pyramids</i> show <i>sparse maps</i> and <i>camera paths</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>In automatic globalization, the modeler uses the relocalization engine of visual SLAM by using the photograph that was used for local modeling (Gee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Gee A, Chekhlov D, Calway A, Mayol-Cuevas W (2008) Discovering higher level structure in Visual SLAM. IEEE Trans Robot 26(5):980–990" href="/article/10.1007/s10055-011-0202-1#ref-CR8" id="ref-link-section-d18667e1490">2008</a>). By the relocalization engine, the modeler obtains camera motion parameters and its uncertainties in the global coordinate system, and the modeler uses them as transform parameters between the local and global coordinate systems. When relocalization is successful for multiple maps, the modeler selects the most reliable camera motion parameters according to uncertainties and the position and orientation from PDR. If relocalization fails, the modeler uses the position and orientation only from PDR. However, the estimated camera motion parameters supplied by visual SLAM and PDR are not sufficiently accurate for registration of the local models.</p><p>For more accurate registration, the modeler carries out image feature matching between two photographs: one is used for creating a local model, and the other is used for creating local model nearest to the targeted local model. In recent years, robust feature detectors and descriptors such as SIFT (Lowe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lowe D (2004) Distinctive image features from scale-invariant keypoints, Int. J Comput Vis 60(2):91–110" href="/article/10.1007/s10055-011-0202-1#ref-CR16" id="ref-link-section-d18667e1496">2004</a>) and SURF (Bay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bay H, Ess A, Tuytelaars T, Gool L (2008) Speeded-up robust features (SURF). Comput Vis Image Underst 110(3):346–359" href="/article/10.1007/s10055-011-0202-1#ref-CR1" id="ref-link-section-d18667e1499">2008</a>) have been proposed, and they are useful for such image feature matching. The 2-D point correspondences are converted into 3-D point correspondences by reprojecting 2-D points onto 3-D polygons of local models. Then, the transform parameters between the local and global coordinate systems are accurately estimated using RANSAC (Fischler and Bolles <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24(6):381–395" href="/article/10.1007/s10055-011-0202-1#ref-CR6" id="ref-link-section-d18667e1502">1981</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig15">15</a> shows an example of automatic globalization. We confirmed that automatic globalization can be used for global modeling. We will evaluate the effectiveness of the function in the near future.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Example of automatic globalization (<i>left</i> 2-D corresponding points, <i>top right</i> global model before globalization, <i>bottom right</i> global model after globalization)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Untextured region detection and view recommendation for photo capture</h4><p>Recapturing photographs incurs costs for traveling to the sites, using manpower, and occupying the sites. However, costs can be reduced by asking workers on the site to take photographs. Our modeler automatically detects untextured regions where photographs need to be captured, and it shows the regions and recommended capture locations and orientations to the users or remote workers (substitute for the actual user) for notifying them of how to take photographs efficiently.</p><p>Untextured regions are detected from the global model and the depth maps at the capture locations of photographs used for local modeling in the global coordinate system. The automatic detector searches for planar regions occluded from all the capture locations of photographs by approximating 3-D planes to point clouds. Then, the detector projects them onto depth maps and checks the visibility for each point. This visibility check is a function similar to a shadow mapping technique (Williams <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1978" title="Williams L (1978) Casting curved shadows on curved surfaces. In: Proceedings of SIGGRAPH, pp 270–274" href="/article/10.1007/s10055-011-0202-1#ref-CR25" id="ref-link-section-d18667e1546">1978</a>) when we assume a position of a light source is same as a viewpoint of the depth map. The modeler finds a dominant region that has the largest space of the occluded regions by using a 3-D window search.</p><p>The modeler searches for an appropriate viewpoint to capture the region by estimating a cost function from virtual viewpoints as candidates for the viewpoint. The set of virtual viewpoints <i>S</i> are determined as intersections on a 3-D grid that covers the region (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig16">16</a>). At each virtual viewpoint, the modeler evaluates a cost that enters into considerations of ease of photo capture and quality of texture images. The cost function is defined from the following criteria:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Observability <i>C</i>
                            <sub>
                              <i>o</i>
                            </sub>: The viewpoint should capture the entire untextured region. This cost is simply estimated the absolute number of the difference between the total number of points on the untextured region and visible points from a virtual viewpoint.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Ease <i>C</i>
                            <sub>
                              <i>e</i>
                            </sub>: The viewpoint should be below the eye level of the users. This cost is estimated by the difference between the eye level of the user and a height of a virtual viewpoint.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Parallelism <i>C</i>
                            <sub>
                              <i>p</i>
                            </sub>: The image plane of the view should be parallel to the untextured region. This cost is estimated by the difference of the angle between the normal vector of the untextured region and a normal vector of an image plane of a virtual viewpoint.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Distance <i>C</i>
                            <sub>
                              <i>d</i>
                            </sub>: The viewpoint should be close to the untextured region. This cost is estimated by the distance between the gravity center of the untextured region and virtual viewpoint.</p>
                        
                      </li>
                    </ol><p>The modeler selects an appropriate viewpoint <i>V</i>
                    <sub>
                      <i>best</i>
                    </sub> from the virtual viewpoints <i>S</i> as follows.</p><div id="Equc" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ V_{best} = \mathop {\arg \min }\limits_{s \in S} \left( {w_{o} C_{o}^{s} + w_{e} C_{e}^{s} + w_{p} C_{p}^{s} + w_{d} C_{d}^{s} } \right)\;{\text{where}}\;w_{o} ,w_{e} ,w_{p} ,w_{d} \;{\text{are}}\;{\text{weights}}. $$</span></div></div><p>The weights are used for normalizing each unit of the criteria such as distance, angle, and number of points. Besides, for reflecting the user’s demands, each weight should be adjusted, for example, the weight <i>w</i>
                    <sub>
                      <i>e</i>
                    </sub> should be greater than the other weights when the user wants to easily capture photographs without losing a balance. These weights are experimentally adjusted by a process that tries to search for a recommended viewpoint and checks whether the viewpoint is desirable or not in advance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Illustration of searching for an appropriate viewpoint to capture an untextured region (<i>green points</i> approximated untextured regions, <i>blue mesh</i> a part of a 3-D grid to determine virtual viewpoints) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>When the recommended viewpoint is placed in unreachable positions, a user can interactively choose another viewpoint ranked by the above cost function. The position and orientation of remote workers whose behavior is being measured are also shown on the monitor with the global model, untextured region, and the intuitively recommended viewpoint (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig17">17</a>); therefore, the remote workers can easily understand how to photograph the region.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Example of untextured region detection and view recommendation (<i>left</i>
                            <i>green point</i> represents untextured regions and <i>blue</i> and <i>red pyramids</i> show recommended capture viewpoint and user’s current viewpoint, respectively, <i>right</i> the real environment and the modeler user) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec22">Texture inpainting for untextured regions</h3><p>Our modeler renders incorrect textures on occluded regions from photographs when it uses PTM. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig18">18</a> left bottom shows the example of incorrect texture mapping. The texture of a table on the center of the photograph is projected onto a plane of a shelf, and the other tables are also projected onto a plane of a floor incorrectly. The method described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec19">5.1</a> is one way to remove these incorrectly rendered textures, but it needs at least data concerning modeling costs and labor costs. If the region’s wrongly mapped textures have enough structural information so that additional models do not need to be created from new photographs, then the wrong textures can be replaced with visually natural textures by applying the texture inpainting function of our modeler.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Example of texture inpainting (<i>green-colored</i> region shows the untextured region) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Using the inpainting function, the modeler first detects untextured regions to be inpainted by the same way as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec21">5.1.2</a>. When the regions are detected, the texture image projected onto the planes, including the regions, is rectified (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig18">18</a> center). By the rectification, inpainting is not affected by projective distortion of the photograph as the texture image. Then, exemplar-based inpainting (Criminisi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Criminisi A, Perez P, Toyama K (2004) Region filling and object removal by exemplar-based image inpainting. IEEE Trans Image Process 13:1200–1212" href="/article/10.1007/s10055-011-0202-1#ref-CR4" id="ref-link-section-d18667e1785">2004</a>) is applied to the untextured regions of the rectified texture image. The method of exemplar-based inpainting finds image patches from regions, surrounding the untextured regions with correct textures, for filling the untextured regions by the image patches. Finally, the modeler presents the visually natural models by reprojecting the inpainted textures onto the 3-D plane (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig18">18</a> right).</p></div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Applications</h2><div class="c-article-section__content" id="Sec23-content"><p>This section presents the applications, visualization of service workers’ activities for behavior analysis and 3-D indoor pedestrian navigation, using the 3-D indoor models created in the experiments described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec11">4.1</a>.</p><h3 class="c-article__sub-heading" id="Sec24">Visualization of service workers’ activities</h3><p>We conducted behavior measurements and analysis of workers in the service field, hot-spring inns 1–3, by using the 3-D indoor models created in the experiment. This experiment was planned to clarify how workers move in the fields and what they do as part of their responsibilities. Therefore, we first measured the workers’ position, orientation, and actions with our service worker tracking system (Ishikawa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ishikawa T, Kourogi M, Kurata T (2011) Economic and synergistic pedestrian tracking system with service cooperation for indoor environments. Int J Organ Collect Intell 2(1):1–20" href="/article/10.1007/s10055-011-0202-1#ref-CR12" id="ref-link-section-d18667e1811">2011</a>) that provided accurate estimations using 3-D indoor models and wearable self-contained sensors. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig19">19</a> shows the results of the visualization of worker’s trajectory for each hot-spring inn. From the photo-realistic models, we can easily understand their activities. Then, the behavior analysis specialists interviewed the workers while presenting their first-person views virtually generated from the measured data and the 3-D models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig20">20</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig19_HTML.gif?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig19_HTML.gif" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Visualization of worker’s trajectory for each hot-spring inn (<i>Left</i> hot-spring inn 1, <i>center</i> hot-spring inn 2, <i>right</i> hot-spring inn 3)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig20_HTML.jpg?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig20_HTML.jpg" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>Retrospect interview with a worker of a hot-spring inn using generated first-person views by an interviewer (<i>left</i>) and an interviewee (<i>right</i>) in <i>left photos</i>. <i>Right photo</i> shows first-person views</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In cognitive chrono-ethnography (Kitajima et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kitajima M, Nakajima M, Toyota M (2010) Cognitive chrono-ethnography: a method for studying behavioral selections in daily activities. In: Proceedings of annual meeting of human factors and ergonomics society" href="/article/10.1007/s10055-011-0202-1#ref-CR13" id="ref-link-section-d18667e1882">2010</a>), a cognitive behavior analysis using retrospective interviews based on objective data, videos of worker’s first-person views usually captured by wearable cameras. However, the method has many problems, for example, detracting from the appearance of the workers, capturing irrelevant guests, and invading worker’s privacy. By virtually generating the first-person views from 3-D models and behavioral data, we can avoid such problems. Furthermore, the behavior analysis specialist determined that the interviews using the generated views were comparable to interviews using wearable cameras. In addition, the workers commented that the positions of the presented views were well comprehended and the first-person views were very interesting. In future, we will conduct to determine how workers understand the position of the views and which part is important in 3-D models. By clarifying this aspect, we will be able to create 3-D models for behavior analysis more effectively.</p><h3 class="c-article__sub-heading" id="Sec25">3-D indoor pedestrian navigation</h3><p>As mentioned in the previous section, the photo-realistic 3-D indoor model created by our modeler is useful for understanding correspondences between real and virtual environments, and it can display 3-D indoor pedestrian navigation in combining with the real-time pedestrian tracking system. We demonstrated the 3-D navigation system at the conference site (ISMAR2009) and at the events of a technology exhibition. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-011-0202-1#Fig21">21</a> shows the user, 3-D navigation views, and top views of the 3-D models.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig21_HTML.jpg?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-011-0202-1/MediaObjects/10055_2011_202_Fig21_HTML.jpg" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Demonstration of 3-D indoor pedestrian navigation (<i>top row</i> ISMAR2009 conference site, <i>bottom row</i> exhibition event site)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-011-0202-1/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Our navigation system places content with annotations in the virtualized real environments and presents the user’s views in the virtual environments according to the user’s position and orientation. Hence, registration errors between content and real environments are not affected by the tracking errors. Even if the position and orientation of the user involve measurable errors, the context of correspondences between the contents and the environments can be preserved. Furthermore, the system using 3-D models can present any views at arbitrary viewpoints, which are useful for understanding the comprehensive positions.</p><p>Because the 3-D indoor models created by our modeler have high consistency with real environments and rich texture information, we believe that position and orientation can be estimated for the tracking system by capturing photographs and by matching the photographs and 3-D models. If we implement this type of function, the accuracy of the tracking system can be improved and used to only capture photographs to updated textures on the 3-D models.</p></div></div></section><section aria-labelledby="Sec26"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Summary</h2><div class="c-article-section__content" id="Sec26-content"><p>This paper described an interactive 3-D indoor modeler that allows users to effectively create photo-realistic 3-D indoor models from multiple photographs. We confirmed that large-scale service fields, event sites, and the other indoor environments can be stably virtualized by our modeler. We also confirmed that the created models could be applied to behavior measurements and analysis and 3-D indoor pedestrian navigation. Furthermore, we conducted the experiments for 3-D modeling and confirmed that the 3-D models were created at a relatively low cost. In addition, we confirmed that the operations of the modeler could be quickly learned by users and that they could create 3-D models by applying our modeler to the experience-based exhibit. Based on the knowledge from the experiments, we described improvements to the modeler to enable more effective modeling.</p><p>In future, we will investigate a method to effectively create models of sufficient quality for specific purposes. As described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec23">6</a>, if we know which features are important to the interviewees for understanding their positions in the models, then we can focus on improving these features and place a low priority on improving features that are irrelevant to the user. The level of quality of the 3-D models is higher than is required to generate adequate first-person views. Therefore, for the purpose of improving the accuracy of the tracking system, the modeler can be modified to generate simpler models, which will be sufficient for first-person views. Therefore, we believe that clarifying the quality of the models required for each purpose is important for applying our modeler to the service fields extensively and for cost-effective 3-D modeling. We also have to investigate a method to effectively update created models because layout, textures on walls, and positions of furniture in indoor environments are often changed. We consider that the integration of the automatic globalization method described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec20">5.1.1</a> and in situ 3-D modeling techniques by utilizing the created 3-D models such as the system proposed by Simon (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Simon G (2010) In situ 3D sketching using a video camera as an interaction and tracking device. In: Proceedings of eurographics" href="/article/10.1007/s10055-011-0202-1#ref-CR20" id="ref-link-section-d18667e1942">2010</a>) will be a better solution to achieve the update of models. We will evaluate the availability and effectiveness of the functions for improving the modeler presented in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-011-0202-1#Sec18">5</a> and the methods mentioned above by applying them to real service fields.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Bay, A. Ess, T. Tuytelaars, L. Gool, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bay H, Ess A, Tuytelaars T, Gool L (2008) Speeded-up robust features (SURF). Comput Vis Image Underst 110(3):3" /><p class="c-article-references__text" id="ref-CR1">Bay H, Ess A, Tuytelaars T, Gool L (2008) Speeded-up robust features (SURF). Comput Vis Image Underst 110(3):346–359</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2007.09.014" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Speeded-up%20robust%20features%20%28SURF%29&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=110&amp;issue=3&amp;pages=346-359&amp;publication_year=2008&amp;author=Bay%2CH&amp;author=Ess%2CA&amp;author=Tuytelaars%2CT&amp;author=Gool%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bunnum P, Mayol-Cuevas W (2008) Outlin AR: an assisted interactive model building system with reduced computat" /><p class="c-article-references__text" id="ref-CR2">Bunnum P, Mayol-Cuevas W (2008) Outlin AR: an assisted interactive model building system with reduced computational effort. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 61–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Criminisi, I. Reid, A. Zisserman, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Criminisi A, Reid I, Zisserman A (2000) Single view metrology. Int J Comput Vis 40(2):123–148" /><p class="c-article-references__text" id="ref-CR3">Criminisi A, Reid I, Zisserman A (2000) Single view metrology. Int J Comput Vis 40(2):123–148</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1012.68704" aria-label="View reference 3 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1026598000963" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Single%20view%20metrology&amp;journal=Int%20J%20Comput%20Vis&amp;volume=40&amp;issue=2&amp;pages=123-148&amp;publication_year=2000&amp;author=Criminisi%2CA&amp;author=Reid%2CI&amp;author=Zisserman%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Criminisi, P. Perez, K. Toyama, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Criminisi A, Perez P, Toyama K (2004) Region filling and object removal by exemplar-based image inpainting. IE" /><p class="c-article-references__text" id="ref-CR4">Criminisi A, Perez P, Toyama K (2004) Region filling and object removal by exemplar-based image inpainting. IEEE Trans Image Process 13:1200–1212</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIP.2004.833105" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Region%20filling%20and%20object%20removal%20by%20exemplar-based%20image%20inpainting&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=13&amp;pages=1200-1212&amp;publication_year=2004&amp;author=Criminisi%2CA&amp;author=Perez%2CP&amp;author=Toyama%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- a" /><p class="c-article-references__text" id="ref-CR5">Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of SIGGRAPH, pp 11–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Fischler, R. Bolles, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image a" /><p class="c-article-references__text" id="ref-CR6">Fischler M, Bolles R (1981) Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun ACM 24(6):381–395</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=618158" aria-label="View reference 6 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F358669.358692" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Random%20sample%20consensus%3A%20a%20paradigm%20for%20model%20fitting%20with%20applications%20to%20image%20analysis%20and%20automated%20cartography&amp;journal=Commun%20ACM&amp;volume=24&amp;issue=6&amp;pages=381-395&amp;publication_year=1981&amp;author=Fischler%2CM&amp;author=Bolles%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Furukawa Y, Curless B, Seitz M, Szeliski R (2009) Reconstructing building interiors from images. In: Proceedin" /><p class="c-article-references__text" id="ref-CR7">Furukawa Y, Curless B, Seitz M, Szeliski R (2009) Reconstructing building interiors from images. In: Proceedings of international conference on computer vision, pp 80–87</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Gee, D. Chekhlov, A. Calway, W. Mayol-Cuevas, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Gee A, Chekhlov D, Calway A, Mayol-Cuevas W (2008) Discovering higher level structure in Visual SLAM. IEEE Tra" /><p class="c-article-references__text" id="ref-CR8">Gee A, Chekhlov D, Calway A, Mayol-Cuevas W (2008) Discovering higher level structure in Visual SLAM. IEEE Trans Robot 26(5):980–990</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTRO.2008.2004641" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Discovering%20higher%20level%20structure%20in%20Visual%20SLAM&amp;journal=IEEE%20Trans%20Robot&amp;volume=26&amp;issue=5&amp;pages=980-990&amp;publication_year=2008&amp;author=Gee%2CA&amp;author=Chekhlov%2CD&amp;author=Calway%2CA&amp;author=Mayol-Cuevas%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Goesele M, Snavely N, Curless B, Hppe H, Seitz M (2007) Multi-view stereo for community photo collections. In:" /><p class="c-article-references__text" id="ref-CR9">Goesele M, Snavely N, Curless B, Hppe H, Seitz M (2007) Multi-view stereo for community photo collections. In: Proceedings of international conference on computer vision, pp 14–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Google (2011) Google SketchUp, http://sketchup.google.com/&#xA;                " /><p class="c-article-references__text" id="ref-CR10">Google (2011) Google SketchUp, <a href="http://sketchup.google.com/">http://sketchup.google.com/</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, ISBN: 052" /><p class="c-article-references__text" id="ref-CR11">Hartley R, Zisserman A (2000) Multiple view geometry in computer vision. Cambridge University Press, ISBN: 0521623049</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Ishikawa, M. Kourogi, T. Kurata, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ishikawa T, Kourogi M, Kurata T (2011) Economic and synergistic pedestrian tracking system with service cooper" /><p class="c-article-references__text" id="ref-CR12">Ishikawa T, Kourogi M, Kurata T (2011) Economic and synergistic pedestrian tracking system with service cooperation for indoor environments. Int J Organ Collect Intell 2(1):1–20</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Economic%20and%20synergistic%20pedestrian%20tracking%20system%20with%20service%20cooperation%20for%20indoor%20environments&amp;journal=Int%20J%20Organ%20Collect%20Intell&amp;volume=2&amp;issue=1&amp;pages=1-20&amp;publication_year=2011&amp;author=Ishikawa%2CT&amp;author=Kourogi%2CM&amp;author=Kurata%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kitajima M, Nakajima M, Toyota M (2010) Cognitive chrono-ethnography: a method for studying behavioral selecti" /><p class="c-article-references__text" id="ref-CR13">Kitajima M, Nakajima M, Toyota M (2010) Cognitive chrono-ethnography: a method for studying behavioral selections in daily activities. In: Proceedings of annual meeting of human factors and ergonomics society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kourogi M, Sakata N, Okuma T, Kurata T (2006) Indoor/outdoor pedestrian navigation with an embedded GPS/RFID/s" /><p class="c-article-references__text" id="ref-CR14">Kourogi M, Sakata N, Okuma T, Kurata T (2006) Indoor/outdoor pedestrian navigation with an embedded GPS/RFID/self-contained sensor system. In: Proceedings of the 16th international conference on artificial reality and telexistence, pp 1310–1321</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurata T, Kourogi M, Ishikawa T, Hyun J, Park A, (2010) Service cooperation and co-creative intelligence cycle" /><p class="c-article-references__text" id="ref-CR15">Kurata T, Kourogi M, Ishikawa T, Hyun J, Park A, (2010) Service cooperation and co-creative intelligence cycles based on mixed-reality technology. In: Proceedings of international conference on industrial informatics, pp. 967–972</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Lowe, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lowe D (2004) Distinctive image features from scale-invariant keypoints, Int. J Comput Vis 60(2):91–110" /><p class="c-article-references__text" id="ref-CR16">Lowe D (2004) Distinctive image features from scale-invariant keypoints, Int. J Comput Vis 60(2):91–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000029664.99615.94" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distinctive%20image%20features%20from%20scale-invariant%20keypoints%2C%20Int&amp;journal=J%20Comput%20Vis&amp;volume=60&amp;issue=2&amp;pages=91-110&amp;publication_year=2004&amp;author=Lowe%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image" /><p class="c-article-references__text" id="ref-CR17">Neubert J, Pretlove J, Drummond T (2007) Semi-autonomous generation of appearance-based edge models from image sequences. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 79–89</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oh B, Chen M, Dorsey J, Durand F (2001) Image-based modeling and photo editing. In: Proceedings of SIGGRAPH, p" /><p class="c-article-references__text" id="ref-CR18">Oh B, Chen M, Dorsey J, Durand F (2001) Image-based modeling and photo editing. In: Proceedings of SIGGRAPH, pp 433–442</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oh J, Stuerzlinger W, Danahy J (2005) Comparing SESAME and sketching for conceptual 3D design. In: Proceedings" /><p class="c-article-references__text" id="ref-CR19">Oh J, Stuerzlinger W, Danahy J (2005) Comparing SESAME and sketching for conceptual 3D design. In: Proceedings eurographics workshop on sketch based interface and modeling, pp 81–88</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Simon G (2010) In situ 3D sketching using a video camera as an interaction and tracking device. In: Proceeding" /><p class="c-article-references__text" id="ref-CR20">Simon G (2010) In situ 3D sketching using a video camera as an interaction and tracking device. In: Proceedings of eurographics</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sinha SN, Steedly D, Szeliski R, Agrawala M, Pollefeys M (2008) Interactive 3D architectural modeling from uno" /><p class="c-article-references__text" id="ref-CR21">Sinha SN, Steedly D, Szeliski R, Agrawala M, Pollefeys M (2008) Interactive 3D architectural modeling from unordered photo collections. ACM Trans Graph 27(5):159:1–159:10. doi:<a href="https://doi.org/10.1145/1409060.1409112">10.1145/1409060.1409112</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Snavely, M. Seitz, R. Szeliski, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Snavely N, Seitz M, Szeliski R (2008) Modeling the world from internet photo collections. Int J Comput Vis 80:" /><p class="c-article-references__text" id="ref-CR22">Snavely N, Seitz M, Szeliski R (2008) Modeling the world from internet photo collections. Int J Comput Vis 80:189–210</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11263-007-0107-3" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20the%20world%20from%20internet%20photo%20collections&amp;journal=Int%20J%20Comput%20Vis&amp;volume=80&amp;pages=189-210&amp;publication_year=2008&amp;author=Snavely%2CN&amp;author=Seitz%2CM&amp;author=Szeliski%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Hengel A, Dick A, Thormählen T, Ward B, Torr PHS (2007) VideoTrace: rapid interactive scene modeling f" /><p class="c-article-references__text" id="ref-CR23">van den Hengel A, Dick A, Thormählen T, Ward B, Torr PHS (2007) VideoTrace: rapid interactive scene modeling from video. ACM Trans Graph 26(3). doi:<a href="https://doi.org/10.1145/1276377.1276485">10.1145/1276377.1276485</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of IEEE/ACM inte" /><p class="c-article-references__text" id="ref-CR24">van den Hengel A, Hill R, Ward B, Dick A (2009) In situ image-based modeling. In: Proceedings of IEEE/ACM international symposium on mixed and augmented reality, pp 107–110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Williams L (1978) Casting curved shadows on curved surfaces. In: Proceedings of SIGGRAPH, pp 270–274" /><p class="c-article-references__text" id="ref-CR25">Williams L (1978) Casting curved shadows on curved surfaces. In: Proceedings of SIGGRAPH, pp 270–274</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-011-0202-1-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was entrusted by the Ministry of Economy, Trade and Industry (METI) in Japan.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Center for Service Research, National Institute of Advanced Industrial Science and Technology, AIST Tsukuba Central 2, 1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568, Japan</p><p class="c-article-author-affiliation__authors-list">Tomoya Ishikawa, Kalaivani Thangamani, Masakatsu Kourogi, Jungwoo Hyun &amp; Takeshi Kurata</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">University of Bristol, Woodland Road, Bristol, BS8 1UB, UK</p><p class="c-article-author-affiliation__authors-list">Andrew P. Gee &amp; Walterio Mayol-Cuevas</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Tomoya-Ishikawa"><span class="c-article-authors-search__title u-h3 js-search-name">Tomoya Ishikawa</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tomoya+Ishikawa&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tomoya+Ishikawa" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tomoya+Ishikawa%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kalaivani-Thangamani"><span class="c-article-authors-search__title u-h3 js-search-name">Kalaivani Thangamani</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kalaivani+Thangamani&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kalaivani+Thangamani" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kalaivani+Thangamani%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Masakatsu-Kourogi"><span class="c-article-authors-search__title u-h3 js-search-name">Masakatsu Kourogi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Masakatsu+Kourogi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Masakatsu+Kourogi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Masakatsu+Kourogi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Andrew_P_-Gee"><span class="c-article-authors-search__title u-h3 js-search-name">Andrew P. Gee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Andrew P.+Gee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Andrew P.+Gee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Andrew P.+Gee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Walterio-Mayol_Cuevas"><span class="c-article-authors-search__title u-h3 js-search-name">Walterio Mayol-Cuevas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Walterio+Mayol-Cuevas&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Walterio+Mayol-Cuevas" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Walterio+Mayol-Cuevas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jungwoo-Hyun"><span class="c-article-authors-search__title u-h3 js-search-name">Jungwoo Hyun</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jungwoo+Hyun&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jungwoo+Hyun" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jungwoo+Hyun%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Takeshi-Kurata"><span class="c-article-authors-search__title u-h3 js-search-name">Takeshi Kurata</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Takeshi+Kurata&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Takeshi+Kurata" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Takeshi+Kurata%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-011-0202-1/email/correspondent/c1/new">Tomoya Ishikawa</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Interactive%203-D%20indoor%20modeler%20for%20virtualizing%20service%20fields&amp;author=Tomoya%20Ishikawa%20et%20al&amp;contentID=10.1007%2Fs10055-011-0202-1&amp;publication=1359-4338&amp;publicationDate=2011-12-15&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ishikawa, T., Thangamani, K., Kourogi, M. <i>et al.</i> Interactive 3-D indoor modeler for virtualizing service fields.
                    <i>Virtual Reality</i> <b>17, </b>89–109 (2013). https://doi.org/10.1007/s10055-011-0202-1</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-011-0202-1.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-04-03">03 April 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11-29">29 November 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-12-15">15 December 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06">June 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-011-0202-1" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-011-0202-1</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3-D indoor model</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Service field</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented virtuality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-011-0202-1.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=202;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

