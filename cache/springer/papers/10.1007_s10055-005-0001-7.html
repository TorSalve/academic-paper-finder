<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A hybrid reality environment and its application to the study of earth"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/9/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A hybrid reality environment and its application to the study of earthquake engineering"/>

    <meta name="dc.source" content="Virtual Reality 2005 9:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-10-08"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. In this paper, an interactive 2D and 3D (hybrid) environment is described, which facilitates collaborative learning and research and utilizes techniques in visualization and VR, therefore enhancing the interpretation of physical problems within these fields. The environment described, termed VizClass, incorporates a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. The environment is particularly appealing for studying critical, complex engineering problems, for example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed evaluation survey was also conducted to determine the merits of the presented environment and the techniques implemented. Results substantiate the plausibility of using these techniques for more general, everyday users. Over 70% of the survey participants believed that the techniques implemented were valuable for engineers."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-10-08"/>

    <meta name="prism.volume" content="9"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="17"/>

    <meta name="prism.endingPage" content="33"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0001-7"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0001-7"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0001-7.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0001-7"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A hybrid reality environment and its application to the study of earthquake engineering"/>

    <meta name="citation_volume" content="9"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2005/12"/>

    <meta name="citation_online_date" content="2005/10/08"/>

    <meta name="citation_firstpage" content="17"/>

    <meta name="citation_lastpage" content="33"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0001-7"/>

    <meta name="DOI" content="10.1007/s10055-005-0001-7"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0001-7"/>

    <meta name="description" content="Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) ca"/>

    <meta name="dc.creator" content="Tara C. Hutchinson"/>

    <meta name="dc.creator" content="Falko Kuester"/>

    <meta name="dc.creator" content="Tung-Ju Hsieh"/>

    <meta name="dc.creator" content="Rebecca Chadwick"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="3dMax (2003) 3D Studio Max Software Platform. 
                    http://www.3dmax.com/
                    
                  ,2003."/>

    <meta name="citation_reference" content="citation_journal_title=IBM Syst J; citation_title=Classroom 2000: an experiment with the instrumentation of a living educational environment; citation_author=GD Abowd; citation_volume=38; citation_issue=4; citation_publication_date=1999; citation_pages=508-530; citation_doi=10.1147/sj.384.0508; citation_id=CR2"/>

    <meta name="citation_reference" content="Abowd GD, Brotherton JA, Bhalodia J (2000) Classroom 2000: a system for capturing and accessing multimedia classroom experiences. In: Conference on Human Factors in Computing Systems CHI 98, May 2000"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Civil Eng; citation_title=Computer imagery and visualization in civil engineering education; citation_author=N Bouchlaghem, W Sher, N Beacham; citation_volume=14; citation_issue=2; citation_publication_date=2000; citation_pages=134-140; citation_doi=10.1061/(ASCE)0887-3801(2000)14:2(134); citation_id=CR4"/>

    <meta name="citation_reference" content="California Geological Survey (2003) California Strong Motion Instrumentation Program (CSMIP). 
                    http://www.consrv.ca.gov/cgs/smip/about.htm
                    
                  
                        "/>

    <meta name="citation_reference" content="Cross B, Smith S, Kennedy C, Arbogast S (2002) Virtual reality retrofit demonstrations. In: National Conference on Earthquake Engineering, Boston, MA"/>

    <meta name="citation_reference" content="CUREE (1998) Conference and workshop on research on the Northridge, California Earthquake of January 17, 1994. Consortium of Universities for research in earthquake engineering. Sponsored by the National Earthquake Hazards Reduction Program (NEHRP)"/>

    <meta name="citation_reference" content="Future Computing Environments Group Georgia Institute of Technology eClass (2003) (Formerly Classroom2000). 
                    http://www.cc.gatech.edu/fce/eclass/
                    
                  
                        "/>

    <meta name="citation_reference" content="Haque ME (2001) Web-based visualization techniques for structural design education American Society for Engineering Education (ASEE) Annual Conference and Exposition"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Pervasive Comput; citation_title=The interactive workspaces project: experiences with ubiquitous computing rooms; citation_author=B Johanson, A Fox, T Winograd; citation_volume=1; citation_issue=2; citation_publication_date=2002; citation_pages=71-78; citation_doi=10.1109/MPRV.2002.1012339; citation_id=CR10"/>

    <meta name="citation_reference" content="Lin C-R, Loftin RB (1998) Application of virtual reality in the interpretation of geoscience data. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 187&#8211;194"/>

    <meta name="citation_reference" content="Ponnekanti SR, Johanson B, Kiciman E, Fox A (2003) Portability extensibility and robustness in iROS. In: IEEE International Conference on Pervasive Computing and Communications (Percom 2003), Dallas-Fort Worth TX, March 2003, pp 11&#8211;19"/>

    <meta name="citation_reference" content="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: Computer Graphics Proceedings SIGGRAPH, Orlando Florida July 1998. ACM"/>

    <meta name="citation_reference" content="SCEDC (2003) Southern California Earthquake Data Center (SCEDC). 
                    http://www.scecdc.scec.org/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Navigation in desktop virtual environments: an evaluation and recommendations for supporting usability; citation_author=A Sebok, E Nystad, S Helgar; citation_volume=8; citation_issue=1; citation_publication_date=2004; citation_pages=26-40; citation_doi=10.1007/s10055-004-0133-1; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Reality; citation_title=Evaluating design guidelines for reducing user disorientation in a desktop virtual environment; citation_author=SP Smith, T Marsh; citation_volume=8; citation_issue=1; citation_publication_date=2004; citation_pages=55-62; citation_doi=10.1007/s10055-004-0137-x; citation_id=CR16"/>

    <meta name="citation_reference" content="Stanford University Department of Computer Science (2003) iRoom&#8212;Stanford Interactive Workspaces Project. 
                    http://graphics.stanford.edu/projects/iwork/room.html
                    
                  
                        "/>

    <meta name="citation_reference" content="Trifunac MD, Ivanovic SS, Todorovska MI (1999) Instrumented 7-storey reinforced concrete building in Van Nuys California: description of the damage from the 1994 Northridge earthquake and strong motion data. Technical Report, 1999"/>

    <meta name="citation_reference" content="UNC (2003) University of North Carolina at Chapel Hill, Department of Computer Science. Office of the Future Project. 
                    http://www.cs.unc.edu/raskar/Office/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Measuring presence in virtual environments: a presence questionnaire; citation_author=BG Witmer, MJ Singer; citation_volume=7; citation_issue=3; citation_publication_date=1998; citation_pages=225-240; citation_doi=10.1162/105474698565686; citation_id=CR20"/>

    <meta name="citation_author" content="Tara C. Hutchinson"/>

    <meta name="citation_author_email" content="thutchin@uci.edu"/>

    <meta name="citation_author_institution" content="Department of Civil and Environmental Engineering, University of California, Irvine, USA"/>

    <meta name="citation_author" content="Falko Kuester"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Science, University of California, Irvine, USA"/>

    <meta name="citation_author" content="Tung-Ju Hsieh"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Science, University of California, Irvine, USA"/>

    <meta name="citation_author" content="Rebecca Chadwick"/>

    <meta name="citation_author_institution" content="Department of Civil and Environmental Engineering, University of California, Irvine, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0001-7&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/12/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0001-7"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A hybrid reality environment and its application to the study of earthquake engineering"/>
        <meta property="og:description" content="Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. In this paper, an interactive 2D and 3D (hybrid) environment is described, which facilitates collaborative learning and research and utilizes techniques in visualization and VR, therefore enhancing the interpretation of physical problems within these fields. The environment described, termed VizClass, incorporates a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. The environment is particularly appealing for studying critical, complex engineering problems, for example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed evaluation survey was also conducted to determine the merits of the presented environment and the techniques implemented. Results substantiate the plausibility of using these techniques for more general, everyday users. Over 70% of the survey participants believed that the techniques implemented were valuable for engineers."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A hybrid reality environment and its application to the study of earthquake engineering | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0001-7","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Visualization, Multimodal interaction, Human–computer interaction, Display technologies","kwrd":["Virtual_reality","Visualization","Multimodal_interaction","Human–computer_interaction","Display_technologies"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0001-7","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0001-7","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=1;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0001-7">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A hybrid reality environment and its application to the study of earthquake engineering
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0001-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0001-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-10-08" itemprop="datePublished">08 October 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A hybrid reality environment and its application to the study of earthquake engineering</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tara_C_-Hutchinson" data-author-popup="auth-Tara_C_-Hutchinson" data-corresp-id="c1">Tara C. Hutchinson<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Department of Civil and Environmental Engineering, University of California, Irvine, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Falko-Kuester" data-author-popup="auth-Falko-Kuester">Falko Kuester</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Department of Electrical Engineering and Computer Science, University of California, Irvine, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tung_Ju-Hsieh" data-author-popup="auth-Tung_Ju-Hsieh">Tung-Ju Hsieh</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Department of Electrical Engineering and Computer Science, University of California, Irvine, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rebecca-Chadwick" data-author-popup="auth-Rebecca-Chadwick">Rebecca Chadwick</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California" /><meta itemprop="address" content="grid.266093.8, 0000000106687243, Department of Civil and Environmental Engineering, University of California, Irvine, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 9</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">17</span>–<span itemprop="pageEnd">33</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">162 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0001-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. In this paper, an interactive 2D and 3D (hybrid) environment is described, which facilitates collaborative learning and research and utilizes techniques in visualization and VR, therefore enhancing the interpretation of physical problems within these fields. The environment described, termed <i>VizClass</i>, incorporates a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. The environment is particularly appealing for studying critical, complex engineering problems, for example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed evaluation survey was also conducted to determine the merits of the presented environment and the techniques implemented. Results substantiate the plausibility of using these techniques for more general, everyday users. Over 70% of the survey participants believed that the techniques implemented were valuable for engineers.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Though the field of engineering has changed dramatically in the last 20 years, the study of engineering has changed relatively little. Students continue to passively listen to lectures in chalkboard-based environments that afford them little opportunity for visualization, hands-on manipulation, interaction, or creative design. Indeed, almost all of these skills, which are vital for engineering, are learned through on-the-job practice rather than at universities. A continuation of this trend threatens to weaken the relevancy of engineering and computer science study at the university.</p><p>Interactive learning, critical thinking, creative problem-solving and problem-based learning are all critical elements to enhancing engineering education and research. Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. At the University of California, Irvine, a new interactive 2D and 3D (hybrid) reality environment, termed <i>VizClass</i>, is being developed with these objectives in mind (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig1">1</a>). <i>VizClass</i> incorporates both a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial learning by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. Test bed verification of <i>VizClass</i> is targeted towards finite element methods applicable to civil and structural engineering problem solving and investigation. In this context, the environment is particularly appealing for studying critical, complex engineering problems. For example, where time-varying feature modifications and coupling between multiple modes of movement are occurring.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p><i>VizClass</i>—a collaborative research and learning environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. Although earthquake visualization may certainly be conducted on other systems (e.g., within a CAVE<sup>TM</sup> or other immersive environment), here we illustrate the utility with a space where 2-D data (typically time history plots/graphs or equations describing response) and 3-D data (spatially distributed sensors mapped onto the case history building) are presented to the user within a unified space. The example simulation uses field-measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed survey is also conducted to evaluate the merits of the proposed visualization techniques used within this environment. First, a brief review of the literature is provided describing environments that have been or are being developed using related technologies.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous IT-environments</h2><div class="c-article-section__content" id="Sec2-content"><p>In recent years, a number of information technology (IT) environments have been developed at Universities and research laboratories across the United States and internationally. These environments provide infrastructure support for classrooms, office activities, tele-conferencing, and meeting spaces. A common thread, when comparing these environments, is the enhanced visualization and display capabilities provided to support activities held in these spaces. In some cases, multiple tiled 2D display systems are used, while in others, 3D (active or passive) stereo support is provided on high-resolution display systems, allowing for full immersion in the space. Important to this, from a user perspective, is the capability of the environment to provide comfortable human–computer-interaction devices to support user activities. Of particular interest to the work described in this paper, are the <i>eClass, iRoom</i>, and the <i>Office of the Future</i> environments.</p><p>The <i>eClass</i> project (formerly called <i>Classroom2000</i>), has been developed by the Future Computing Environments group at the Georgia Institute of Technology (Abowd <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Abowd GD (1999) Classroom 2000: an experiment with the instrumentation of a living educational environment. IBM Syst J 38(4):508–530" href="/article/10.1007/s10055-005-0001-7#ref-CR2" id="ref-link-section-d10837e401">1999</a>; Abowd et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Abowd GD, Brotherton JA, Bhalodia J (2000) Classroom 2000: a system for capturing and accessing multimedia classroom experiences. In: Conference on Human Factors in Computing Systems CHI 98, May 2000" href="/article/10.1007/s10055-005-0001-7#ref-CR3" id="ref-link-section-d10837e404">2000</a>; Future Computing Environments Group Georgia Institute of Technology <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Future Computing Environments Group Georgia Institute of Technology eClass (2003) (Formerly Classroom2000). &#xA;                    http://www.cc.gatech.edu/fce/eclass/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-005-0001-7#ref-CR8" id="ref-link-section-d10837e407">2003</a>). Since 1995, <i>eClass</i> has supported teaching at Georgia Tech in the areas of Computer Science and Engineering, by providing a digital environment for course lectures and discussion sessions. The environment supports playing, recording, and animating (in real-time) lectures. A java-based client–server system, ZenPad, records and stores lecture video clips, slides, and annotations during each lecture. Lectures collected and compiled by ZenPad provide a detailed record of the discussion, including the original presentation, annotations compiled during the presentation, and a full audio record of the session. <i>eClass</i> has a unique (2D) display layout for lecturers, primarily including a digital (active) LiveBoard, and two projection surfaces for passively displaying information.</p><p>The Interactive Workspace Project (or more commonly termed <i>iRoom</i>) (Stanford University Department of Computer Science <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stanford University Department of Computer Science (2003) iRoom—Stanford Interactive Workspaces Project. &#xA;                    http://graphics.stanford.edu/projects/iwork/room.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-005-0001-7#ref-CR17" id="ref-link-section-d10837e423">2003</a>; Johanson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Johanson B, Fox A, Winograd T (2002) The interactive workspaces project: experiences with ubiquitous computing rooms. IEEE Pervasive Comput 1(2):71–78" href="/article/10.1007/s10055-005-0001-7#ref-CR10" id="ref-link-section-d10837e426">2002</a>) at Stanford University uses a broad array of interactive devices, such as multiple large displays (2D and 3D) and portable (handheld and other) devices to provide an interactive research working space. A software termed <i>iROS</i> (Interactive Room Operating System), is used to provide the underlying middleware architecture integrating the different interaction devices and computational activities together (Ponnekanti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ponnekanti SR, Johanson B, Kiciman E, Fox A (2003) Portability extensibility and robustness in iROS. In: IEEE International Conference on Pervasive Computing and Communications (Percom 2003), Dallas-Fort Worth TX, March 2003, pp 11–19" href="/article/10.1007/s10055-005-0001-7#ref-CR12" id="ref-link-section-d10837e432">2003</a>). The <i>iRoom</i> project brings together interaction at a broad range of scales and using a variety of interfacing techniques (both wired and wireless devices are integrated into the environment). For example, the user can control multiple displays with a handheld device such as an iPAQ, or remotely activate devices during working sessions (such as scanners, printers, etc.).</p><p>The <i>Office of the Future</i> environment (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: Computer Graphics Proceedings SIGGRAPH, Orlando Florida July 1998. ACM" href="/article/10.1007/s10055-005-0001-7#ref-CR13" id="ref-link-section-d10837e445">1998</a>; UNC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="UNC (2003) University of North Carolina at Chapel Hill, Department of Computer Science. Office of the Future Project. &#xA;                    http://www.cs.unc.edu/raskar/Office/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-005-0001-7#ref-CR19" id="ref-link-section-d10837e448">2003</a>) was developed at the University of North Carolina at Chapel Hill to provide an immersive space allowing 3D graphical model display and manipulation for the future office environment. The primary focus of the office of the future project is to capture and display 3D data dynamically and on random surfaces within the space it is constructed in. To do this, researchers have replaced ceiling lights with projection systems and carefully controlled structured light within the room (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: Computer Graphics Proceedings SIGGRAPH, Orlando Florida July 1998. ACM" href="/article/10.1007/s10055-005-0001-7#ref-CR13" id="ref-link-section-d10837e451">1998</a>). Using advanced computer vision techniques, the authors create a flexible display environment, whereby changes in geometry, intensity, and resolution variations across multiple display areas are dynamically controlled.</p><p>These and other systems, are providing state-of-the-art interactive digital workplaces, whether it be for an office, educational, or entirely research environment. Thus far, however, none of these systems have integrated interactive visualization and advanced 3D display components with mathematical solvers. Furthermore, with the exception of the <i>iRoom</i> project, previous IT-spaces that provide advanced visual tools have used <i>either</i> a 2D or a 3D (semi-immersive-type) display paradigm, little emphasis has been given to integrating the two (space scales) into a single environment. In engineering problems, mathematics and modeling are critical to developing and solving a physical problem, engineers first learn the fundamental mathematics defining a boundary value problem (in 2D), and subsequently must develop a physical representation of the problem (likely in 3D). In most cases, the scale of the problem is broad, thus 3D interpretation and spatial correlation is important to understanding the physical constraints imposed. Therefore, in the engineering learning and research environment, it is important that one first interprets the 2D information (equations, algorithms, plots, etc.) and then work with the 3D information (geometric models).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3"><i>VizClass</i>: an environment for expanding engineering thinking</h2><div class="c-article-section__content" id="Sec3-content"><p>A new IT-Classroom is being developed at the University of California, Irvine (UCI), with the primary objective of providing a completely digital, interactive workspace for research and education in the areas of Computer and Civil Engineering. Test bed application courses include computer graphics, scientific visualization, and numerical methods (finite element methods) in structural engineering. The IT-space serves not only as an educational environment, it also provides a space for interdisciplinary research centered around simulation and visualization as well as an IT development test bed advancing system level research and education.</p><p>A 65 m<sup>2</sup> classroom in the Engineering Gateway building on the UCI campus (room EG3131) was allocated for the development of <i>VizClass</i>. In the original room, approximately 20 students passively listened to whiteboard lectures provided at the working front space of the room. Over the past 6 months, EG3131 was reconstructed, redesigned, and remodeled to provide support for a new advanced IT-hardware layout incorporating a range of 2D and 3D displays, driving computational systems, and interactive devices. The layout of <i>VizClass</i>, showing the 2D active display area and the 3D stereo working area is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig2">2</a>a. A photograph showing the working layout is provided in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig2">2</a>b. The following sections describe the advanced display systems, interactive devices, and driving computational systems designed for <i>VizClass</i>. In addition, we describe the challenges faced in implementing such a system.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Details of <i>VizClass</i> layout: <b>a</b> schematic plan and <b>b</b> photograph of students working in the collaborative space</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec4">Advanced display systems</h3><p>The 2D display component consists of three interactive 1.8 m diagonal (72 inch) digital whiteboards (SmartBoards<sup>TM</sup>) connected to individual rendering nodes. The SmartBoards<sup>TM</sup> are compactly designed to fit within a thin 60 cm deep space, by backwards projecting images onto an angled mirror and then forward projecting them onto the optical screens. A single NEC 1060 (2500 ANSI Lumens) projector with a short throw lens is used per screen to provide an on-screen resolution of 1,024×768 pixels. The front of the room layout of 2D displays provides a large laterally tiled screen for users. The digital SmartBoards<sup>TM</sup> have multiple input/output sources—with the primary source being optical on-screen, providing hands-on input for annotating and controlling the environment. Each SmartBoard<sup>TM</sup> is also equipped with a digital pen-tray, providing digitally color-coded pens, supporting <i>`digital ink’</i>, a display surface eraser, and a wireless keyboard with a trackball mouse. These interface devices support input scenarios via direct application of the <i>digital ink</i> from touch events, or query or data access interpreted by mouse or keyboard events. The right side of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig2">2</a>b shows a photograph of one of the 2D interactive SmartBoards<sup>TM</sup> installed in <i>VizClass</i>.</p><p>The 3D display component is designed with two projectors vertically stacked and mounted at floor level projecting from the rear, onto a large 3 m (10 foot) diagonal screen. The projectors are digital light projectors (DLP-based) with a brightness of 3000 ANSI Lumens and resolution of 1,280×1,024. Each projector is equipped with polarizing filters for passive stereo projection. A single PC with an nVidia<sup>TM</sup> 980XGL graphics card using quad-buffering is designated to provide the stereo image pairs and a CYVIZ<sup>TM</sup> 3D stereo converter is subsequently used to assign the left and right eye images to the respective projector. The CYVIZ<sup>TM</sup> converter box is used to allow a broad range of systems with different graphic capabilities to connect to the passive stereo projection system.</p><p>In the workspace area (lecture and discussion space), a 3D surround sound and motion tracking system are under development for capturing lectures and discussions in all modalities. These systems will be integrated and synchronized with work presented on the different advanced display systems.</p><h3 class="c-article__sub-heading" id="Sec5">Device control</h3><p>Control of the entire environment (2D/3D displays, touch-sensitive LCD display, cameras, sound system), is provided by an 8-node PC cluster connected via a Gigabit Ethernet housed in the 3D projection room. System software supporting the environment is primarily being developed in C/C++ and Python and integrated with wxWindows and wxPython, respectively, for GUI development. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig3">3</a> shows the PC cluster and working space for development of the software interface layer.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>PC Clusters in <i>VizClass</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The synchronization of multiple input/output devices is controlled through a keyboard-video-mouse (KVM) matrix switch also located in the 3D projection room. The KVM switch supports high resolution video display with no degradation of the signal. Four wireless keyboards provide access to the three digital whiteboards as well as the 3D display wall. The system can also be configured such that a single keyboard (or other input device) can be seamlessly applied to the different display nodes in the room. For 2D interaction, a broad range of modalities are available on the digital whiteboards via the projection keyboards and mouse emulation. For the 3D space, pinch gloves and electromagnetic tracking can be used to provide the required six-degrees-of-freedom. An example is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig4">4</a>, where data gloves are worn by the user to manipulate a seismic hypocenter model. Access and control to the multiple devices integrated into <i>VizClass</i> is made possible through an interface software layer running on a LCD touch-sensitive display, mounted at eye-level at the entrance to the environment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Example of interacting with the digital whiteboards using datagloves</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec6">Implementation challenges</h3><p>Implementation of such an IT-system has provided a number of important challenges. From our experience, we would like to share the following technical as well as semantic-level challenges observed while developing and testing the environment:
</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Integration of software between multiple devices (and operating systems) requires careful design and interfacing of device drivers, each of which have different attributes.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Organization within the space provided is extremely important. We therefore spent much time at the onset of the project carefully designing the layout for maximum usage by occupants</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Devices we selected are costly. To minimize this, we optimized the resources by seeking educational promotions and working closely with hardware vendors on new designs (e.g., the customize 3D stereo system).</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Retrofitting the class infrastructure is challenging, due to existing utilities (e.g., air-conditioning, networking, power). While this may seem to be a small detail, it requires careful thought and plays an important role in the design and overall layout of any space.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>Since the main touch panel provides the login ‘tone’ for the space, proper sequencing of devices and prioritization of device cues requires important consideration.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">6.</span>
                      
                        <p>Finally, user comfort within the space, and acquaintance with the different hardware requires usage time. Most importantly, the typical instructor has developed habits, such as using one’s hands to erase a chalkboard, which simply are not supported on digital whiteboards.</p>
                      
                    </li>
                  </ol>
                        </div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Case study example: seismic response of a building in  Van Nuys, California</h2><div class="c-article-section__content" id="Sec7-content"><p>Temporal loading plays a large role in the extreme loading conditions civil engineers must design for. New visual tools are desirable to allow articulation of these extreme movements, particularly considering temporal variations. Civil engineers have begun to develop such tools, for example, work described by Cross et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Cross B, Smith S, Kennedy C, Arbogast S (2002) Virtual reality retrofit demonstrations. In: National Conference on Earthquake Engineering, Boston, MA" href="/article/10.1007/s10055-005-0001-7#ref-CR6" id="ref-link-section-d10837e706">2002</a>) uses the virtual reality modeling language (VRML) to develop application tools for visualizing the earthquake retrofitting of structures. Similarly, Haque (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Haque ME (2001) Web-based visualization techniques for structural design education American Society for Engineering Education (ASEE) Annual Conference and Exposition" href="/article/10.1007/s10055-005-0001-7#ref-CR9" id="ref-link-section-d10837e709">2001</a>) developed VRML-based structural design modules allowing students to immerse in the 3D Internet-based framework.</p><p>In this section, we describe visualization and VR techniques used to enhance our perception of the time-varying movements of a case study building subjected to several earthquake records. The techniques developed are designed specifically for implementation within the hybrid <i>VizClass</i> environment. The studied building is a seven-story reinforced concrete structure located in central San Fernando Valley, California. This structure serves as an excellent case study building, since the structure itself has experienced 12 large or moderate earthquakes, 2 of which, have caused substantial structural damage. Moreover, numerous seismic sensors were placed throughout the building and recorded these events.</p><h3 class="c-article__sub-heading" id="Sec8">Building instrumentation and measured earthquake response</h3><p>The California Strong Motion Instrumentation Program (CSMIP) (California Geological Survey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="California Geological Survey (2003) California Strong Motion Instrumentation Program (CSMIP). &#xA;                    http://www.consrv.ca.gov/cgs/smip/about.htm&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-005-0001-7#ref-CR5" id="ref-link-section-d10837e725">2003</a>) manages the seismic instrumentation installed throughout this building. Initially, three tri-axial accelerometers were placed on the first, fourth, and roof levels, measuring the 1971 San Fernando earthquake only. Instrumentation was subsequently upgraded throughout the structure, resulting in a finer distribution of sensors. The current system of accelerometers includes one measuring the vertical component of motion on the first floor, ten measuring the north–south direction on floors one, two, three, six, and the roof, and five measuring the east–west direction on floors one, two, three, six, and the roof level. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig5">5</a> shows the instrumentation layout throughout the structure.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Location of seismographs throughout the structure (upgraded instrumentation after the 1971 San Fernando earthquake, after Trifunac et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Trifunac MD, Ivanovic SS, Todorovska MI (1999) Instrumented 7-storey reinforced concrete building in Van Nuys California: description of the damage from the 1994 Northridge earthquake and strong motion data. Technical Report, 1999" href="/article/10.1007/s10055-005-0001-7#ref-CR18" id="ref-link-section-d10837e741">1999</a>) (all units in meters)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>By the end of 1994, the instrumentation system captured strong motion records from a total of nine earthquakes and three aftershocks. Earthquake recordings came from the following events; the 1971 San Fernando earthquake, the 1987 Whittier-Narrows earthquake, a Whittier-Narrows aftershock, the 1988 Pasadena earthquake, the 1989 earthquakes from Montebello and Malibu, the 1991 Sierra Madre earthquake, the 1992 Landers and Big Bear earthquakes, and the 1994 Northridge earthquake along with two aftershocks from this event. The <i>M</i>
                           <sub>
                    <i>w</i>
                  </sub> 6.6 San Fernando earthquake in 1971 was the first earthquake to cause damage to the structure. Although the magnitude would suggest that this earthquake was moderate, it was one of the most devastating earthquakes in the history of California. In total, the earthquake caused 65 deaths and over $500 million in property damage (SCEDC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="SCEDC (2003) Southern California Earthquake Data Center (SCEDC). &#xA;                    http://www.scecdc.scec.org/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-005-0001-7#ref-CR14" id="ref-link-section-d10837e765">2003</a>). The <i>M</i>
                           <sub>
                    <i>w</i>
                  </sub> 6.7 Northridge earthquake in 1994, was the first earthquake to cause extensive damage to the Van Nuys building. In comparison to the San Fernando earthquake of 1971, the Northridge earthquake caused much more economic damage, estimated at about $50 billion, and resulting in 33 deaths (CUREE  <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="CUREE (1998) Conference and workshop on research on the Northridge, California Earthquake of January 17, 1994. Consortium of Universities for research in earthquake engineering. Sponsored by the National Earthquake Hazards Reduction Program (NEHRP)" href="/article/10.1007/s10055-005-0001-7#ref-CR7" id="ref-link-section-d10837e778">1998</a>).</p><p>Observations after the San Fernando earthquake indicate minor structural damage was found at the northeast corner of the building visible as cracks in the spandrel beam-to-column connections. There was also significant damage to the exterior plaster and partitions. However, most damage to the building was nonstructural, such as damage to the dry wall partitions, bathroom tiles and pluming fixtures. The total cost of repairs from damage due to the San Fernando earthquake was about $143,000 U.S. (Trifunac et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Trifunac MD, Ivanovic SS, Todorovska MI (1999) Instrumented 7-storey reinforced concrete building in Van Nuys California: description of the damage from the 1994 Northridge earthquake and strong motion data. Technical Report, 1999" href="/article/10.1007/s10055-005-0001-7#ref-CR18" id="ref-link-section-d10837e784">1999</a>). The Northridge earthquake caused significant damage to this building, both structural and nonstructural, rendering the building unsafe for occupancy. All exterior columns in the longitudinal direction of the building on the fourth floor developed large shear cracks. On the South facing wall, there were wide shear cracks in the columns. Analysis of response records concur with visual observations, indicating that the building behaved roughly in the linear range during the 1971 San Fernando earthquake, and in the nonlinear range during the 1994 Northridge earthquake.</p><h3 class="c-article__sub-heading" id="Sec9">Visualizing the case study building</h3><p>Since the greatest damage to the building was observed during the 1994 Northridge earthquake, the example results presented in this paper will focus on this specific event. The following sections will describe the model development and the visual techniques applied within the hybrid environment. Visualization techniques such as shade (color) coding and interpolation, texture mapping, temporal simulation, and integration within the VR framework were developed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Model development</h4><p>A 3D model of the Van Nuys building was constructed using the 3DStudioMax (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="3dMax (2003) 3D Studio Max Software Platform. &#xA;                    http://www.3dmax.com/&#xA;                    &#xA;                  ,2003." href="/article/10.1007/s10055-005-0001-7#ref-CR1" id="ref-link-section-d10837e801">2003</a>) software platform and exported to Wavefront OBJ format for subsequent visualization in our VR framework. Major components of the structure were subdivided into groups of solid objects, such as columns, beams, floors (including the roof level), panelling, and facia. This allows groups of objects to have common texture skins and to be independently manipulated. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig6">6</a>a shows the geometric model of the Van Nuys building. Different texture skins were collected using a high resolution digital camera. These were then used to create a photorealistic representation of the undamaged, damaged, and repaired structure. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig6">6</a>b shows the geometric model with the undamaged texture skins. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig6">6</a>c shows an example of texture skins applied over damaged beam-column regions along the longitudinal exterior of the building. At run-time, local damage texture skins can be appropriately triggered (to blend from undamaged to damaged at the correct time during the motion) and a more realistic sense of the state of the entire structure, relative to this damage cumulation is expressed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>3D model of the Van Nuys building: <b>a</b> without textures, <b>b</b> with textures (undamaged), and <b>c</b> close-up view with textures (showing damaged beam-column joints during 1994 Northridge earthquake)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Visualizing the field measured displacement data</h4><p>The field measured datasets are discretely mapped onto the geometric model and used to generate temporal simulations reproducing the deformation patterns observed. Records from the 1994 Northridge earthquake consist of 16 channels of seismograms measuring acceleration in both horizontal directions, and select measurements in the vertical direction. Accelerations can be double-integrated and processed to obtain displacement records. Since sensors are not placed at each floor level (as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig5">5</a>), or at each extreme edge of the building, to fully describe the 3-D movement of the structure, interpolation was applied. Floor levels are structurally rigid and no local damage to the members was observed, therefore, the assumption of linear interpolation across the plan of each of the floors is reasonable.</p><p>Vertically, from floor to floor, the concept of structural modal parametrization was applied to estimate the unknown floor level displacements. Assuming the first mode response of the structure dominates, an assumed modal shape (varying with height <i>z</i> and time <i>t</i>), ϕ(<i>z</i>,<i>t</i>), can be determined. Knowing the period, one can estimate the stiffness, and verify this assumed modal shape. Analyses indicate a linear modal shape is reasonable in longitudinal and transverse directions.</p><p>Softer, lateral restraining elements, such as the columns, required more refined and realistic interpolation of movement at each discrete column and along their height. To calculate 1D of movement, elastic beam theory is assumed and the technique of successive integration applied. Elastic Euler–Bernoulli beam theory states:
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$v(x,t) = \int\int\frac{{M(x,t)}}{{{\text{EI}}}}{\text{d}}x{\text{d}}x,$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p> where <i>EI</i> = the flexural rigidity of the column, <i>M</i>(<i>x</i>,<i>t</i>) = the moment along the height (local <i>x</i>) of the column, and <i>v</i>(<i>x</i>,<i>t</i>) = the transverse displacement. Both the moment <i>M</i>(<i>x</i>,<i>t</i>) and displacement <i>v</i>(<i>x</i>,<i>t</i>) are varying in time. Through the technique of successive integration, the lateral displacement vertical profile <i>v</i>(<i>x</i>), may be determined, for each time increment <i>t</i>. The constants of integration may be determined using the measured boundary conditions and the vertically varying moments <i>M</i>(<i>x</i>) are assumed to varying linearly along the height.</p><p>Each seismogram consists of 3,000 sample points with a sample spacing of Δ<i>t</i>=0.02 s, for a total duration of the time-varying visualization of 60 s. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig7">7</a> shows a sequence of time steps and snapshots of the fully deformed building model, based on the assumptions regarding the unknown deformation points, as described. Textured skins are removed from this example to highlight the primary structural components (beams, columns, panels). The ground floor level in this case is fixed and all subsequent floors are shown relative to ground surface. It should be noted that deformations of the building are amplified by 20 to enhance their visual perspective.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Image sequence showing building deformations between time <i>t</i>=7.0–9.2 s (sub-sampled to a Δ<i>t</i>=0.2 s)—1994 Northridge earthquake</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig7">7</a> clearly shows the extreme in-floor level torsion the structure undergoes during 3D oscillation, as well as the large (dominant) longitudinal (long axis of the building) displacement demand. Displacement measurements indicate peak values in the north–south direction of 17 cm on the east end and 23 cm on the west end. This significant difference between the east and west-end displacements indicated that the building suffered large torsional movements during the earthquake, which concurs with the clear depiction illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig7">7</a>.</p><p>As observed in the field after this earthquake, the longitudinal load-resisting system was severely damaged, thus significantly softening the structure in this direction, and the 3D interactive visualization clearly shows this. In addition, between time steps shown, a sharp reversal in the direction of movement can be observed (e.g., between parts e, f, and g of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig7">7</a>). The visualization provides a unique opportunity to show an overview of the structural response of the building for a given ground motion history. In addition, the articulation of the spatial transitions assists in locating the building components, which may have been damaged on a local level.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig8">8</a> shows a sequence of time steps and snapshots of the fully deformed building model using shade (color) coding applied to the components of the model. In the real simulation for the students, red is used to represent the maximum deformations (over the entire record), while blue is used to represent minimum values. Interpolation is applied across the color spectrum and mapped onto the model. Color provides an intuitive, rapid assessment of the current state of the structure during time. For the purposes of black and white publication, herein, we use a gray scale shading. Nonetheless, one can quickly observe from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig8">8</a> the maximum deformation occurs at time <i>t</i>=7.38 s.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Image sequence showing building deformations (<i>gray scale shading</i> represents amplitude) between time <i>t</i>=6.96–7.38 s (sub-sampled to a Δ<i>t</i>=0.06 s)—1994 Northridge earthquake</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Path history using 3D sensor traces</h4><p>Discrete point-based displacement histories can be articulated using the concept of 3D sensor traces. In this case, we render a spherical proxy and use it to represent the original position of the sensor. A second spherical proxy is rendered and used to represent the current time location of the sensor (during the animation). Different colors or opacities can be used to differentiate between the original and current position sensors. During the animation, a trace line is drawn and retains the sensors path to represent the history of that sensors movement. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig9">9</a> shows a sequence of screen shots of the 3D sensor traces at the roof of the building, where a wire sphere is used to represent the original sensor location and a solid sphere is used to represent the current sensor location. The traces reveal the displacement path from time <i>t</i>=2.0–8.0 s.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>3D sensor traces between time <i>t</i>=2.0–8.0 s (sub-sampled to a Δ<i>t</i>=2.0 s)—1994 Northridge earthquake (<i>wire sphere</i> represents the original sensor location, <i>solid sphere</i> represents the current sensor location)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Bounding volumes to articulate maxima</h4><p>A semi-transparent bounding volume is rendered during the animation and used to envelop maxima values experienced by the structure. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig10">10</a> shows an example of the semi-transparent reference planes at a snapshot in time. This allows the user to visually identify axis (or hot spots) of extreme values immediately. In a practical structural design situation, this feature is critical to identifying placement of neighboring buildings, particularly in tightly spaced urban areas, to avoid collisions. The example shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig10">10</a> is applied to maximum deformations, however, this could just as easily be applied to maximum accelerations or velocities imposed on the model.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Transparent reference volume used to represent the maximum displacement of the building model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec14">Interacting with the case study building</h3><p>Interaction with the VR-based simulation of the building is possible using spatially tracked pinch gloves or the digital whiteboards (as described previously). The pinch gloves
<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> are a digital input device with electronic touch sensors attached on the tips of the fingers. Interaction capabilities provided using these gloves include scaling the building, moving throughout the scene, arbitrarily placing ‘virtual sensors’, bringing up waveforms (at virtual or real sensor locations), and controlling the time of the animation sequence (e.g., stop/pause, rewind, fastforward). The level of complexity of the interaction depends upon the interaction modality desired (2D, 2.5D, or 3D). In the future, we plan to integrate a gyroscopic mouse into the interaction capabilities, with the same functionality as the pinch glove devices, at significantly reduced costs.</p><p>The concept of ‘virtual sensors’ allows the user to place a sensor anywhere inside the building and reveal the corresponding interpolated seismic waveform. A spherical proxy is rendered at the virtual sensor location, while the user uses finger pinching to grab the virtual sensor and move it toward the desired location. This approach provides an efficient way to explore the data and to intuitively understand the structure’s seismic response.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Evaluation of visualization techniques</h2><div class="c-article-section__content" id="Sec15-content"><p>The effectiveness of the implemented techniques are investigated by conducting a detailed evaluative survey. A specially designed survey, including a visual demonstration, was developed and presented to groups of both Civil Engineering and Computer Science students to solicit specific feedback. The objective of the survey was to determine the effectiveness of the implemented techniques, within the VizClass environment, to strengthen a user’s understanding of the salient characteristics of the response of structures to earthquake motions.</p><p>Reviewing the literature regarding evaluations conducted for VR and visualization applications, one finds that there are a variety of approaches for conducting such surveys (e.g., Lin and Loftin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Lin C-R, Loftin RB (1998) Application of virtual reality in the interpretation of geoscience data. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 187–194" href="/article/10.1007/s10055-005-0001-7#ref-CR11" id="ref-link-section-d10837e1139">1998</a>; Witmer and Singer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Witmer BG, Singer MJ (1998) measuring presence in virtual environments: a presence questionnaire. Presence 7(3):225–240" href="/article/10.1007/s10055-005-0001-7#ref-CR20" id="ref-link-section-d10837e1142">1998</a>; Bouchlaghem et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Bouchlaghem N, Sher W, Beacham N (2000) Computer imagery and visualization in civil engineering education. J Comput Civil Eng 14(2):134–140" href="/article/10.1007/s10055-005-0001-7#ref-CR4" id="ref-link-section-d10837e1145">2000</a>; Sebok et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sebok A, Nystad E, Helgar S (2004) Navigation in desktop virtual environments: an evaluation and recommendations for supporting usability. Virtual Reality 8(1):26–40" href="/article/10.1007/s10055-005-0001-7#ref-CR15" id="ref-link-section-d10837e1148">2004</a>; Smith and Marsh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Smith SP, Marsh T (2004) Evaluating design guidelines for reducing user disorientation in a desktop virtual environment. Virtual Reality 8(1):55–62" href="/article/10.1007/s10055-005-0001-7#ref-CR16" id="ref-link-section-d10837e1151">2004</a>). For example, Lin and Loftin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Lin C-R, Loftin RB (1998) Application of virtual reality in the interpretation of geoscience data. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 187–194" href="/article/10.1007/s10055-005-0001-7#ref-CR11" id="ref-link-section-d10837e1155">1998</a>) presented a VR system to analyze geoscience data and subsequently invited a group of geoscientists and software developers to complete written questionnaires after they had evaluated the VR system. Results showed that the interpretation of geoscience data could indeed be enhanced in VR environments. Work within the educational realm includes that by Bouchlaghem et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Bouchlaghem N, Sher W, Beacham N (2000) Computer imagery and visualization in civil engineering education. J Comput Civil Eng 14(2):134–140" href="/article/10.1007/s10055-005-0001-7#ref-CR4" id="ref-link-section-d10837e1158">2000</a>), who presented a browser-based system to educate Civil Engineering students with visualization tools. Evaluation of the system indicated that instructors observed better student participation in lectures and less time was required to complete tutorial exercises. Sebok et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sebok A, Nystad E, Helgar S (2004) Navigation in desktop virtual environments: an evaluation and recommendations for supporting usability. Virtual Reality 8(1):26–40" href="/article/10.1007/s10055-005-0001-7#ref-CR15" id="ref-link-section-d10837e1161">2004</a>) conducted a survey to determine which factors affect the efficiency of a VR system. Results from their study suggested that the following factors play an important role: (a) real-world constraints, (b) specialized navigation techniques, and (c) feedback regarding location and direction of travel. These and other studies substantiate that user feedback can certainly be useful to evaluate the usefulness of a VR system.</p><h3 class="c-article__sub-heading" id="Sec16">Scope of our evaluation study</h3><p>In our study, students from two senior-level undergraduate courses, Structural Design (CEE 151C) and Computer Graphics (EECS 104), were selected to participate in a written survey. The survey (presented in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-005-0001-7#Sec22">Appendix</a>) contained three different types of questions: (a) demographic, (b) identification, and (c) qualitative. The survey was designed to be partially interactive, as is the nature of the implemented techniques.</p><p>Participating students were divided into several small groups (between 5–10 students) and each group was given a live demonstration of the Van Nuys building visualization described in this paper. The total duration for each small group survey (demonstration and completion of written survey forms) was approximately 25 min. In the beginning of the demo, a 5-min introduction presentation was given followed by a 10-min interactive live demo using the 3D immersive display system. The students were asked to answer identification questions during the live demo, and subsequently instructed to individually complete the remaining qualitative questions.</p><h3 class="c-article__sub-heading" id="Sec17">Evaluation results</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Sample population and demographics</h4><p>A total of 92 students participated in this survey, with 42 students from the Civil and Environmental Engineering (CEE) Department and 48 students from the Electrical Engineering and Computer Science (EECS) Department. Most of the participating students are undergraduate students, with only two graduate students participating. Only 10% of students intend to continue their study in graduate schools. Additional demographic data is provided in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig11">11</a>, where part (a) shows their academic year and part (b) indicates the level of past engineering experience of the students. Note that most students participating in the survey are seniors, and approximately 50% of the students have had internships/research experience.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Demographic background of the participating students: <b>a</b> academic year and <b>b</b> engineering experience</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Identification questions</h4><p>In this user survey, different visualization techniques are presented to the students, and the effectiveness of the techniques are evaluated by determining if the students can identify important engineering features from the visualization itself. For example, both a linear and a nonlinear shading is encoded (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig12">12</a>) to represent the amplitude of displacement (in time). The nonlinear shading was used to tone down the effect of lower displacement amplitudes, while amplifying for the user the effects of higher displacement amplitudes. As a result, larger displacements are emphasized. Recall that in the real simulations for the students, colors were used, rather than gray scale shading.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Definition of linear and nonlinear shading as mapped to displacement during the Van Nuys building simulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig13">13</a> presents the results of the identification questions. From these results, it can be seen that considering linear coding, 70% of the students correctly identified when the maximum displacement occurred in the simulation (part a, correct answer event 2), whereas, 89% of the students correctly responded when nonlinear shading is used (part b, correct answer between 8.0 and 8.9 s). It is interesting to note that the percentage of CEE and EECS students that correctly responded to the identification questions is nearly similar. This indicates that the non-CE student has ample success in interpreting the response characteristics using the implemented techniques, as does the CEE student, whom presumably may have been exposed to earthquake engineering at least in a fundamental sense in the past. From this example, it is also clear that a data presentation scheme that is designed to the specific application of the user will best increase the observers’ understanding of the presented data. Critical information can be emphasized by carefully highlighting parameters of particular interest when it is visually displayed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Results of the identification questions: <b>a</b> <i>linear gray scale shading</i> and <b>b</b> <i>nonlinear gray scale shading</i> (Note that color was used in the actual simulations)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Qualitative questions</h4><p>Qualitative questions were designed to evaluate the effectiveness of individual techniques, after the students have observed the entire demonstration and responded to the demographic and identification questions. An effort was made to design these questions to be comparative, such that relative effectiveness of different techniques and equipment used to present the visualization results, could be evaluated.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig14">14</a> shows sample results from the qualitative questions. Overall, the majority of all students felt that all techniques were effective. Part (a) of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig14">14</a>, indicates that more than 80% of the students believe that the shade (color) rendering is very effective. From Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig14">14</a>b, more than 70% of the students believe that the boundary box technique is very effective. It is interesting to note that more EECS students believe the boundary box is very effective. From Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig14">14</a>c, d, more than 60% of students think sensor traces are very effective and prefer the 3D stereo rendering.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Results of the qualitative questions: <b>a</b> effectiveness of the shade (<i>color</i>) rendering, <b>b</b> effectiveness of the boundary box, <b>c</b> effectiveness of the sensor traces and <b>d</b> effectiveness of the 3D immersive display</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig15">15</a> presents comparative question results. Part (a) indicates that more than 50% of the students believe shade (color)-rendering is the most effective computer graphics technique implemented to increase their understanding of a building’s behavior during an earthquake. It is interesting to note that the students were most attracted to visual cue enhancement (shade (color)-rendering, bounding box, or sensor traces), rather than the use of 3D immersion (less than 5% of the students felt that the immersive display increased their understanding the most). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0001-7#Fig15">15</a>b shows that more than 70% of the total students believe that, in general, the visualization tools presented are helpful for engineers. It also shows that slightly more EECS (than CEE) students believe the implemented techniques are very valuable. This may be due to the relatively limited exposure of an EECS student with building response and structural dynamics. CEE students, who are trained to calculate structural response parameters, may have tended to readily verify what they are seeing with their structural engineering knowledge.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Students’ opinions toward the implemented technique: <b>a</b> which technique increased understanding the most? and <b>b</b> how valuable is the implemented techniques for engineers?</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Summary and conclusions</h2><div class="c-article-section__content" id="Sec21-content"><p>In this paper, we describe a new interactive learning and research environment, we have termed <i>VizClass</i>. <i>VizClass</i> is a hybrid reality environment in the sense that it incorporates a specially designed lecture room and laboratory, integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display.</p><p>The environment is particularly appealing for studying critical, complex engineering problems. For example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. In this paper, we show an example of the use of <i>VizClass</i>, through a case study building, where notable dynamic movements are measured and associated damage to the structure was observed. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. Simple interpolation schemes are applied to visualize the time-varying deformation of the building, study the 3D displacement history (using 3D traces), and rapidly assess maxima values (using bounding boxes). Interaction paradigms not otherwise possible in conventional modeling are integrated allowing the user to manipulate the model and the data with various sources (spatially tracked gloves and interactive whiteboards with keyboard/mouse interfacing).</p><p>A detailed evaluation survey considering 92 mostly undergraduate students in CEE and EECS was conducted to determine the merits of the presented environment and the techniques implemented. The survey was designed to evaluate the students’ ability to identify key building response features, as well as to evaluate the relative effectiveness of the different techniques. Results indicate that CEE and EECS students have equal success in identifying key features of the building response, using the implemented techniques, even though CEE students may have had prior exposure to earthquake engineering in academic course work or research. This substantiates the plausibility of using these techniques for more general, everyday users. Students surveyed also indicated that, of the different techniques implemented, shade (color) rendering was by far the most effective, with bounding boxes ranked second in effectiveness. Over 70% of the students believed that the techniques implemented were valuable for engineers.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>We are using Fakespace PinchGloves with Ascension Nest of Birds electromagnetic trackers.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="3dMax (2003) 3D Studio Max Software Platform. http://www.3dmax.com/,2003." /><p class="c-article-references__text" id="ref-CR1">3dMax (2003) 3D Studio Max Software Platform. <a href="http://www.3dmax.com/">http://www.3dmax.com/</a>,2003.</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GD. Abowd, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Abowd GD (1999) Classroom 2000: an experiment with the instrumentation of a living educational environment. IB" /><p class="c-article-references__text" id="ref-CR2">Abowd GD (1999) Classroom 2000: an experiment with the instrumentation of a living educational environment. IBM Syst J 38(4):508–530</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1147%2Fsj.384.0508" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Classroom%202000%3A%20an%20experiment%20with%20the%20instrumentation%20of%20a%20living%20educational%20environment&amp;journal=IBM%20Syst%20J&amp;volume=38&amp;issue=4&amp;pages=508-530&amp;publication_year=1999&amp;author=Abowd%2CGD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Abowd GD, Brotherton JA, Bhalodia J (2000) Classroom 2000: a system for capturing and accessing multimedia cla" /><p class="c-article-references__text" id="ref-CR3">Abowd GD, Brotherton JA, Bhalodia J (2000) Classroom 2000: a system for capturing and accessing multimedia classroom experiences. In: Conference on Human Factors in Computing Systems CHI 98, May 2000</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Bouchlaghem, W. Sher, N. Beacham, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Bouchlaghem N, Sher W, Beacham N (2000) Computer imagery and visualization in civil engineering education. J C" /><p class="c-article-references__text" id="ref-CR4">Bouchlaghem N, Sher W, Beacham N (2000) Computer imagery and visualization in civil engineering education. J Comput Civil Eng 14(2):134–140</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1061%2F%28ASCE%290887-3801%282000%2914%3A2%28134%29" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20imagery%20and%20visualization%20in%20civil%20engineering%20education&amp;journal=J%20Comput%20Civil%20Eng&amp;volume=14&amp;issue=2&amp;pages=134-140&amp;publication_year=2000&amp;author=Bouchlaghem%2CN&amp;author=Sher%2CW&amp;author=Beacham%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="California Geological Survey (2003) California Strong Motion Instrumentation Program (CSMIP). http://www.consr" /><p class="c-article-references__text" id="ref-CR5">California Geological Survey (2003) California Strong Motion Instrumentation Program (CSMIP). <a href="http://www.consrv.ca.gov/cgs/smip/about.htm">http://www.consrv.ca.gov/cgs/smip/about.htm</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cross B, Smith S, Kennedy C, Arbogast S (2002) Virtual reality retrofit demonstrations. In: National Conferenc" /><p class="c-article-references__text" id="ref-CR6">Cross B, Smith S, Kennedy C, Arbogast S (2002) Virtual reality retrofit demonstrations. In: National Conference on Earthquake Engineering, Boston, MA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="CUREE (1998) Conference and workshop on research on the Northridge, California Earthquake of January 17, 1994." /><p class="c-article-references__text" id="ref-CR7">CUREE (1998) Conference and workshop on research on the Northridge, California Earthquake of January 17, 1994. Consortium of Universities for research in earthquake engineering. Sponsored by the National Earthquake Hazards Reduction Program (NEHRP)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Future Computing Environments Group Georgia Institute of Technology eClass (2003) (Formerly Classroom2000). ht" /><p class="c-article-references__text" id="ref-CR8">Future Computing Environments Group Georgia Institute of Technology eClass (2003) (Formerly Classroom2000). <a href="http://www.cc.gatech.edu/fce/eclass/">http://www.cc.gatech.edu/fce/eclass/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Haque ME (2001) Web-based visualization techniques for structural design education American Society for Engine" /><p class="c-article-references__text" id="ref-CR9">Haque ME (2001) Web-based visualization techniques for structural design education American Society for Engineering Education (ASEE) Annual Conference and Exposition</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Johanson, A. Fox, T. Winograd, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Johanson B, Fox A, Winograd T (2002) The interactive workspaces project: experiences with ubiquitous computing" /><p class="c-article-references__text" id="ref-CR10">Johanson B, Fox A, Winograd T (2002) The interactive workspaces project: experiences with ubiquitous computing rooms. IEEE Pervasive Comput 1(2):71–78</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMPRV.2002.1012339" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20interactive%20workspaces%20project%3A%20experiences%20with%20ubiquitous%20computing%20rooms&amp;journal=IEEE%20Pervasive%20Comput&amp;volume=1&amp;issue=2&amp;pages=71-78&amp;publication_year=2002&amp;author=Johanson%2CB&amp;author=Fox%2CA&amp;author=Winograd%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lin C-R, Loftin RB (1998) Application of virtual reality in the interpretation of geoscience data. In: Proceed" /><p class="c-article-references__text" id="ref-CR11">Lin C-R, Loftin RB (1998) Application of virtual reality in the interpretation of geoscience data. In: Proceedings of the ACM symposium on Virtual reality software and technology, pp 187–194</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ponnekanti SR, Johanson B, Kiciman E, Fox A (2003) Portability extensibility and robustness in iROS. In: IEEE " /><p class="c-article-references__text" id="ref-CR12">Ponnekanti SR, Johanson B, Kiciman E, Fox A (2003) Portability extensibility and robustness in iROS. In: IEEE International Conference on Pervasive Computing and Communications (Percom 2003), Dallas-Fort Worth TX, March 2003, pp 11–19</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to i" /><p class="c-article-references__text" id="ref-CR13">Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: Computer Graphics Proceedings SIGGRAPH, Orlando Florida July 1998. ACM</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SCEDC (2003) Southern California Earthquake Data Center (SCEDC). http://www.scecdc.scec.org/&#xA;                 " /><p class="c-article-references__text" id="ref-CR14">SCEDC (2003) Southern California Earthquake Data Center (SCEDC). <a href="http://www.scecdc.scec.org/">http://www.scecdc.scec.org/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Sebok, E. Nystad, S. Helgar, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Sebok A, Nystad E, Helgar S (2004) Navigation in desktop virtual environments: an evaluation and recommendatio" /><p class="c-article-references__text" id="ref-CR15">Sebok A, Nystad E, Helgar S (2004) Navigation in desktop virtual environments: an evaluation and recommendations for supporting usability. Virtual Reality 8(1):26–40</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-004-0133-1" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigation%20in%20desktop%20virtual%20environments%3A%20an%20evaluation%20and%20recommendations%20for%20supporting%20usability&amp;journal=Virtual%20Reality&amp;volume=8&amp;issue=1&amp;pages=26-40&amp;publication_year=2004&amp;author=Sebok%2CA&amp;author=Nystad%2CE&amp;author=Helgar%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SP. Smith, T. Marsh, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Smith SP, Marsh T (2004) Evaluating design guidelines for reducing user disorientation in a desktop virtual en" /><p class="c-article-references__text" id="ref-CR16">Smith SP, Marsh T (2004) Evaluating design guidelines for reducing user disorientation in a desktop virtual environment. Virtual Reality 8(1):55–62</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-004-0137-x" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20design%20guidelines%20for%20reducing%20user%20disorientation%20in%20a%20desktop%20virtual%20environment&amp;journal=Virtual%20Reality&amp;volume=8&amp;issue=1&amp;pages=55-62&amp;publication_year=2004&amp;author=Smith%2CSP&amp;author=Marsh%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stanford University Department of Computer Science (2003) iRoom—Stanford Interactive Workspaces Project. http:" /><p class="c-article-references__text" id="ref-CR17">Stanford University Department of Computer Science (2003) iRoom—Stanford Interactive Workspaces Project. <a href="http://graphics.stanford.edu/projects/iwork/room.html">http://graphics.stanford.edu/projects/iwork/room.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Trifunac MD, Ivanovic SS, Todorovska MI (1999) Instrumented 7-storey reinforced concrete building in Van Nuys " /><p class="c-article-references__text" id="ref-CR18">Trifunac MD, Ivanovic SS, Todorovska MI (1999) Instrumented 7-storey reinforced concrete building in Van Nuys California: description of the damage from the 1994 Northridge earthquake and strong motion data. Technical Report, 1999</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="UNC (2003) University of North Carolina at Chapel Hill, Department of Computer Science. Office of the Future P" /><p class="c-article-references__text" id="ref-CR19">UNC (2003) University of North Carolina at Chapel Hill, Department of Computer Science. Office of the Future Project. <a href="http://www.cs.unc.edu/raskar/Office/">http://www.cs.unc.edu/raskar/Office/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BG. Witmer, MJ. Singer, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Witmer BG, Singer MJ (1998) measuring presence in virtual environments: a presence questionnaire. Presence 7(3" /><p class="c-article-references__text" id="ref-CR20">Witmer BG, Singer MJ (1998) measuring presence in virtual environments: a presence questionnaire. Presence 7(3):225–240</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565686" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20presence%20in%20virtual%20environments%3A%20a%20presence%20questionnaire&amp;journal=Presence&amp;volume=7&amp;issue=3&amp;pages=225-240&amp;publication_year=1998&amp;author=Witmer%2CBG&amp;author=Singer%2CMJ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0001-7-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This research is supported by the National Science Foundation, under Grant Number EIA-0203528 and the Holmes Fellowship Foundation. The last author was supported by the Pacific Earthquake Engineering Research (PEER) Internship program during the study. The Van Nuys building owners graciously provided building drawings and allowing us access to the structure. Their support is greatly appreciated. The assistance of the students in Civil Engineering and Computer Science that participated in the evaluation survey is also appreciated.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Civil and Environmental Engineering, University of California, Irvine, USA</p><p class="c-article-author-affiliation__authors-list">Tara C. Hutchinson &amp; Rebecca Chadwick</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Electrical Engineering and Computer Science, University of California, Irvine, USA</p><p class="c-article-author-affiliation__authors-list">Falko Kuester &amp; Tung-Ju Hsieh</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Tara_C_-Hutchinson"><span class="c-article-authors-search__title u-h3 js-search-name">Tara C. Hutchinson</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tara C.+Hutchinson&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tara C.+Hutchinson" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tara C.+Hutchinson%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Falko-Kuester"><span class="c-article-authors-search__title u-h3 js-search-name">Falko Kuester</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Falko+Kuester&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Falko+Kuester" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Falko+Kuester%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tung_Ju-Hsieh"><span class="c-article-authors-search__title u-h3 js-search-name">Tung-Ju Hsieh</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tung-Ju+Hsieh&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tung-Ju+Hsieh" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tung-Ju+Hsieh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Rebecca-Chadwick"><span class="c-article-authors-search__title u-h3 js-search-name">Rebecca Chadwick</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Rebecca+Chadwick&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rebecca+Chadwick" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rebecca+Chadwick%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0001-7/email/correspondent/c1/new">Tara C. Hutchinson</a>.</p></div></div></section><section aria-labelledby="appendices"><div class="c-article-section" id="appendices-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="appendices">Appendix</h2><div class="c-article-section__content" id="appendices-content"><h3 class="c-article__sub-heading u-visually-hidden" id="App1">Appendix</h3><h3 class="c-article__sub-heading" id="Sec23">Sample of evaluation questionnaire</h3>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Demographic questions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig17_HTML.gif?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig17_HTML.gif" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Identification questions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig18_HTML.gif?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-005-0001-7/MediaObjects/10055_2005_1_Fig18_HTML.gif" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Qualitative questions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0001-7/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  </div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20hybrid%20reality%20environment%20and%20its%20application%20to%20the%20study%20of%20earthquake%20engineering&amp;author=Tara%20C.%20Hutchinson%20et%20al&amp;contentID=10.1007%2Fs10055-005-0001-7&amp;publication=1359-4338&amp;publicationDate=2005-10-08&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Hutchinson, T.C., Kuester, F., Hsieh, T. <i>et al.</i> A hybrid reality environment and its application to the study of earthquake engineering.
                    <i>Virtual Reality</i> <b>9, </b>17–33 (2005). https://doi.org/10.1007/s10055-005-0001-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0001-7.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2003-12-22">22 December 2003</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-05-19">19 May 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-10-08">08 October 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12">December 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0001-7" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0001-7</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visualization</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Display technologies</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0001-7.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=1;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

