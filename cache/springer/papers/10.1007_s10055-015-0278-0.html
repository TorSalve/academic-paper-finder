<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Locating virtual sound sources at arbitrary distances in real-time bin"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="A real-time system for sound spatialization via headphones is presented. Conventional headphone spatialization techniques effectively place sources on the surface of a virtual sphere around the..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/19/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Locating virtual sound sources at arbitrary distances in real-time binaural reproduction"/>

    <meta name="dc.source" content="Virtual Reality 2015 19:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2015-10-13"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2015 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="A real-time system for sound spatialization via headphones is presented. Conventional headphone spatialization techniques effectively place sources on the surface of a virtual sphere around the listener. In the new system, sources can be spatialized at different distances from a listener by interpolating head-related impulse responses (HRIRs) measured between 20 and 160&#160;cm. These HRIRs are stored in different databases depending on the audio sampling rate. To ease the real-time constraints, users can choose the number of HRIR taps used in the convolution, and an alternative interpolation technique (simplex interpolation) was implemented instead of trilinear interpolation. Subjective tests showed that such simplifications yield satisfactory spatialization for some angles and distances."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2015-10-13"/>

    <meta name="prism.volume" content="19"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="201"/>

    <meta name="prism.endingPage" content="212"/>

    <meta name="prism.copyright" content="2015 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-015-0278-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-015-0278-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-015-0278-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-015-0278-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Locating virtual sound sources at arbitrary distances in real-time binaural reproduction"/>

    <meta name="citation_volume" content="19"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2015/11"/>

    <meta name="citation_online_date" content="2015/10/13"/>

    <meta name="citation_firstpage" content="201"/>

    <meta name="citation_lastpage" content="212"/>

    <meta name="citation_article_type" content="S.I. : Spatial Sound"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-015-0278-0"/>

    <meta name="DOI" content="10.1007/s10055-015-0278-0"/>

    <meta name="citation_doi" content="10.1007/s10055-015-0278-0"/>

    <meta name="description" content="A real-time system for sound spatialization via headphones is presented. Conventional headphone spatialization techniques effectively place sources on the "/>

    <meta name="dc.creator" content="Juli&#225;n Villegas"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Algazi V, Duda R, Thompson D, Avendano C (2001) The cipic hrtf database. In: Proceedings of Ieee Workshop on the applications of signal processing to audio and acoustics, pp 99&#8211;102. 
                    http://earlab.bu.edu/databases/collections/cipic
                    
                  
                        "/>

    <meta name="citation_reference" content="Apple Inc. (2015) iTunes connect developer guide. Apple Inc., Cupertino, CA (USA). 
                    https://developer.apple.com/library/ios/documentation/LanguagesUtilities/Conceptual/iTunesConnect_Guide/iTunesConnect_Guide.pdf
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Perception of the relative distances of nearby sound sources; citation_author=DH Ashmead, D Leroy, RD Odom; citation_volume=47; citation_issue=4; citation_publication_date=1990; citation_pages=326-331; citation_doi=10.3758/BF03210871; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Multichannel spatial auditory display for speech communications; citation_author=DR Begault, T Erbe; citation_volume=42; citation_issue=10; citation_publication_date=1994; citation_pages=819-826; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Ubiquitous Comput; citation_title=Using 3D sound to improve the effectiveness of the advanced driver assistance systems; citation_author=F Bellotti, R Berta, A Gloria, M Margarone; citation_volume=6; citation_issue=3; citation_publication_date=2002; citation_pages=155-163; citation_doi=10.1007/s007790200016; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Acoustic control by wave field synthesis; citation_author=AJ Berkhout, D Vries, P Vogel; citation_volume=93; citation_publication_date=1993; citation_pages=2764-2778; citation_doi=10.1121/1.405852; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_title=Spatial hearing: the psychophysics of human sound localization; citation_publication_date=1997; citation_id=CR7; citation_author=J Blauert; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Arch Acoust; citation_title=Providing surround sound with loudspeakers: a synopsis of current methods; citation_author=J Blauert, R Rabenstein; citation_volume=37; citation_issue=1; citation_publication_date=2012; citation_pages=5-18; citation_doi=10.2478/v10168-012-0002-y; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Auditory distance perception in rooms; citation_author=AW Bronkhorst, T Houtgast; citation_volume=397; citation_issue=6719; citation_publication_date=1999; citation_pages=517-520; citation_doi=10.1038/17374; citation_id=CR9"/>

    <meta name="citation_reference" content="Br&#252;el PV, Frederiksen E, Rasmussen G (1962) Artificial ears for the calibration of earphones of the external type. Technical Report&#160;1, Br&#252;el and Kj&#230;r, N&#230;rum, Denmark"/>

    <meta name="citation_reference" content="citation_journal_title=Int Conf Nat Comput; citation_title=Head-related impulse response interpolation in virtual sound system; citation_author=L Chen, H Hu, Z Wu; citation_volume=6; citation_publication_date=2008; citation_pages=162-166; citation_id=CR11"/>

    <meta name="citation_reference" content="Cohen M, Villegas J (2015) Applications of audio augmented reality. Wearware, everyware, anyware, and awareware, chapter&#160;13. In: Fundamentals of wearable computers and augmented reality, 2nd edn. CRC Press, Boca Raton, pp 309&#8211;329"/>

    <meta name="citation_reference" content="Doukhan D, S&#233;d&#232;s A (2009) CW\_binaural
                    
                      
                    
                    $$^{\sim }$$
                    
                      
                        
                      
                    
                  : a binaural synthesis external for pure data. In: Proceedings of 3 Pure-data International Convention"/>

    <meta name="citation_reference" content="Estrella J (2010) On the extraction of interaural time differences from binaural room impulse responses. Master&#8217;s thesis, Technische Universit&#228;t Berlin"/>

    <meta name="citation_reference" content="Gamper H (2014) Enabling technologies for audio augmented reality systems. PhD thesis, Aalto University"/>

    <meta name="citation_reference" content="Gardner WG (1995) Transaural 3-d audio. Technical report, MIT Media Laboratory, Perceptual Computing Section"/>

    <meta name="citation_reference" content="citation_title=3-D audio using loudspeakers; citation_publication_date=1998; citation_id=CR17; citation_author=WG Gardner; citation_publisher=Springer Science &amp; Business Media"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust. Soc Am; citation_title=Hrtf measurements of a kemar; citation_author=WG Gardner, KD Martin; citation_volume=97; citation_issue=6; citation_publication_date=1995; citation_pages=3907-3908; citation_doi=10.1121/1.412407; citation_id=CR18"/>

    <meta name="citation_reference" content="Geier M, Spors S (2012) Spatial audio with the soundscape renderer. In: 27 Tonmeistertagung&#8211;VDT International Convention"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Periphony: with-height sound reproduction; citation_author=MA Gerzon; citation_volume=21; citation_issue=1; citation_publication_date=1973; citation_pages=2-10; citation_id=CR20"/>

    <meta name="citation_reference" content="Grosjean P, Denis K (2013) mlearning: Machine learning algorithms with unified interface and confusion matrices. R package version 1.0-0"/>

    <meta name="citation_reference" content="citation_journal_title=J Audio Eng Soc; citation_title=Digital signal processing tools for loudspeaker evaluation and discrete-time crossover design; citation_author=MO Hawksford; citation_volume=45; citation_issue=1/2; citation_publication_date=1997; citation_pages=37-62; citation_id=CR22"/>

    <meta name="citation_reference" content="Hemingway P (2002) n-Simplex interpolation. Technical report, Hewlett-Packard Laboratories Bristol"/>

    <meta name="citation_reference" content="Hosoe S, Nishino T, Itou K, Takeda K (2005) Measurement of head-related transfer functions in the proximal region. Forum Acusticum, pp 2539&#8211;2542"/>

    <meta name="citation_reference" content="Ikei Y, Yamazaki H, Hirota K, Hirose M (2006) vCocktail: multiplexed-voice menu presentation method for wearable computers. In: Proceedings of Virtual Reality Conference, pp 183&#8211;190"/>

    <meta name="citation_reference" content="Jot J-M, Larcher V, Warusfel O (1995) Digital signal processing issues in the context of binaural and transaural stereophony. In: Proceedings of 98 Audio Engineering Society Convention"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=A psychophysical evaluation of near-field head-related transfer functions synthesized using a distance variation function; citation_author=A Kan, C Jin, A Schaik; citation_volume=125; citation_issue=4; citation_publication_date=2009; citation_pages=2233-2242; citation_doi=10.1121/1.3081395; citation_id=CR27"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Acust United Acust; citation_title=Distance perception in interactive virtual acoustic environments using first and higher order ambisonic sound fields; citation_author=G Kearney, M Gorzel, H Rice, F Boland; citation_volume=98; citation_issue=1; citation_publication_date=2012; citation_pages=61-71; citation_doi=10.3813/AAA.918492; citation_id=CR28"/>

    <meta name="citation_reference" content="Kim S, Ikeda M, Takahashi A, Ono Y, Martens WL (2009) Virtual ceiling speaker: elevating auditory imagery in a 5-channel reproduction. In: Audio Engineering Society Convention 127. Audio Engineering Society"/>

    <meta name="citation_reference" content="Majdak P, Iwaya Y, Carpentier T, Nicol R, Parmentier M, Roginska A, Suzuki Y, Watanabe K, Wierstorf H, Ziegelwanger H et al. (2013) Spatially oriented format for acoustics: a data exchange format representing head-related transfer functions. In: Proceedings of 134 Audio Engineering Society Convention"/>

    <meta name="citation_reference" content="citation_journal_title=Acoust Sci Technol; citation_title=Perceptual evaluation of filters controlling source direction: customized and generalized hrtfs for binaural synthesis; citation_author=WL Martens; citation_volume=24; citation_issue=5; citation_publication_date=2003; citation_pages=220-232; citation_doi=10.1250/ast.24.220; citation_id=CR31"/>

    <meta name="citation_reference" content="McGee R, Wright M (2011) Sound element spatializer. Master&#8217;s thesis, University of Michigan"/>

    <meta name="citation_reference" content="McKinley RL, Ericson MA (1997) Binaural and spatial hearing in real and virtual environments, chapter. In: Flight demonstration of a 3-D auditory display. Lawrence Erlbaum Assoc., Inc., Mahwah, NJ, USA, pp 683&#8211;699"/>

    <meta name="citation_reference" content="Murphy D, Neff F (2011) Game sound technology and player interaction: concepts and developments, chapter. In: Spatial sound for computer games and virtual reality. Information Science Reference, pp 287&#8211;312"/>

    <meta name="citation_reference" content="Musil T, Noisternig M, H&#246;ldrich R (2005) A library for realtime 3D binaural sound reproduction in Pure Data (pd). In: Proceedings of International Conference on Digital Audio Effects"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors J Hum Factors Ergon Soc; citation_title=Effects of localized auditory information on visual target detection performance using a helmet-mounted display; citation_author=WT Nelson, LJ Hettinger, JA Cunningham, BJ Brickman, MW Haas, RL McKinley; citation_volume=40; citation_issue=3; citation_publication_date=1998; citation_pages=452-460; citation_doi=10.1518/001872098779591304; citation_id=CR36"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Circuits Syst II Exp Br; citation_title=Minimum-phase fir filter design using real cepstrum; citation_author=S-C Pei, H-S Lin; citation_volume=53; citation_issue=10; citation_publication_date=2006; citation_pages=1113-1117; citation_doi=10.1109/TCSII.2006.882193; citation_id=CR37"/>

    <meta name="citation_reference" content="citation_journal_title=J Neurosci Methods; citation_title=PsychoPy-psychophysics software in python; citation_author=JW Peirce; citation_volume=162; citation_issue=1&#8211;2; citation_publication_date=2007; citation_pages=8-13; citation_doi=10.1016/j.jneumeth.2006.11.017; citation_id=CR38"/>

    <meta name="citation_reference" content="Penha R, Oliveira J (2013) Spatium, tools for sound spatialization. In: Proceedings of the Sound and Music Computing Conference"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Acust United Acust; citation_title=Calculation of head-related transfer functions for arbitrary field points using spherical harmonics decomposition; citation_author=M Pollow, K-V Nguyen, O Warusfel, T Carpentier, M M&#252;ller-Trapet, M Vorl&#228;nder, M Noisternig; citation_volume=98; citation_issue=1; citation_publication_date=2012; citation_pages=72-82; citation_doi=10.3813/AAA.918493; citation_id=CR40"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Speech Lang Process; citation_title=Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap; citation_author=T Qu, Z Xiao, M Gong, Y Huang, X Li, X Wu; citation_volume=17; citation_issue=6; citation_publication_date=2009; citation_pages=1124-1132; citation_doi=10.1109/TASL.2009.2020532; citation_id=CR41"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Audio Electroacoust; citation_title=Ieee recommended practice for speech quality measurements; citation_author=EH Rothauser, WD Chapman, N Guttman, HR Silbiger, MHL Hecker, GE Urbanek, KS Nordby, M Weinstock; citation_volume=17; citation_issue=3; citation_publication_date=1969; citation_pages=225-246; citation_doi=10.1109/TAU.1969.1162058; citation_id=CR42"/>

    <meta name="citation_reference" content="Sanuki W, Villegas J, Cohen M (2014) Spatial sound for mobile navigation systems. In: Proceedings of 136 Audio Engineering Society Convention"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Improved quasi-stereophony and &#8220;colorless&#8221; artificial reverberation; citation_author=MR Schroeder; citation_volume=33; citation_issue=8; citation_publication_date=1961; citation_pages=1061-1064; citation_doi=10.1121/1.1908892; citation_id=CR44"/>

    <meta name="citation_reference" content="S&#232;des A, Guillot P, Paris E (2014) The HOA library, review and prospect. In: Proceedings of the joint ICMC-SMC Conference, pp 855&#8211;860"/>

    <meta name="citation_reference" content="Smith J, Lee N (2008) Computational acoustic modeling with digital delay. Center for Computer Research in Music and Acoustics, Stanford University. 
                    https://ccrma.stanford.edu/realsimple/Delay/
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Appl Acoust; citation_title=Spatial sound resolution of an interpolated hrir library; citation_author=J Sodnik, R Su&#353;nik, M &#352;tular, S Toma&#382;i&#269;; citation_volume=66; citation_issue=11; citation_publication_date=2005; citation_pages=1219-1234; citation_doi=10.1016/j.apacoust.2005.04.003; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Spatial sound with loudspeakers and its perception: a review of the current state; citation_author=S Spors, H Wierstorf, A Raake, F Melchior, M Frank, F Zotter; citation_volume=101; citation_issue=9; citation_publication_date=2013; citation_pages=1920-1938; citation_doi=10.1109/JPROC.2013.2264784; citation_id=CR48"/>

    <meta name="citation_reference" content="Villegas J, Cohen M (2010a) &#8220;Gabriel&#8221;: geo-aware broadcasting for in-vehicle entertainment and larger safety. In: Proceedings of 135 Audio Engineering Society International Convention"/>

    <meta name="citation_reference" content="Villegas, J. and Cohen, M. (2010b) Hrir
                           
                    ~
                  : modulating range in headphone-reproduced spatial audio. In: Proceedings of 9 International Conference on VR Continuum and Its Applications in Industry"/>

    <meta name="citation_reference" content="Villegas J, Cohen M (2013) Real-time head-related impulse response filtering with distance control. In: Proceedings of 135 Audio Engineering Society Convention"/>

    <meta name="citation_reference" content="Warusfel O (2003) listen hrtf database. 
                    http://recherche.ircam.fr/equipes/salles/listen/
                    
                  
                        "/>

    <meta name="citation_reference" content="Wenzel EM, Foster SH (1993) Perceptual consequences of interpolating head-related transfer functions during spatial synthesis. In: Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp 102&#8211;105"/>

    <meta name="citation_reference" content="Wierstorf H, Geier M, Spors S (2011) A free database of head related impulse response measurements in the horizontal plane with multiple distances. In: Proceedings of 130 Audio Engineering Society Convention"/>

    <meta name="citation_reference" content="citation_journal_title=Organ Sound; citation_title=Open sound control: an enabling technology for musical networking; citation_author=M Wright; citation_volume=10; citation_publication_date=2005; citation_pages=193-200; citation_doi=10.1017/S1355771805000932; citation_id=CR55"/>

    <meta name="citation_reference" content="Xiang P, Camargo D, Puckette M (2005) Experiments on spatial gestures in binaural sound display. In: Proceedings of 11 International Conference on Auditory Display"/>

    <meta name="citation_reference" content="citation_journal_title=Acta Acust United Acust; citation_title=Auditory distance perception in humans: a summary of past and present research; citation_author=P Zahorik, DS Brungart, AW Bronkhorst; citation_volume=91; citation_issue=3; citation_publication_date=2005; citation_pages=409-420; citation_id=CR57"/>

    <meta name="citation_author" content="Juli&#225;n Villegas"/>

    <meta name="citation_author_email" content="julian@u-aizu.ac.jp"/>

    <meta name="citation_author_institution" content="Computer Arts Laboratory, University of Aizu, Aizu-Wakamatsu, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-015-0278-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2015/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-015-0278-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Locating virtual sound sources at arbitrary distances in real-time binaural reproduction"/>
        <meta property="og:description" content="A real-time system for sound spatialization via headphones is presented. Conventional headphone spatialization techniques effectively place sources on the surface of a virtual sphere around the listener. In the new system, sources can be spatialized at different distances from a listener by interpolating head-related impulse responses (HRIRs) measured between 20 and 160&amp;nbsp;cm. These HRIRs are stored in different databases depending on the audio sampling rate. To ease the real-time constraints, users can choose the number of HRIR taps used in the convolution, and an alternative interpolation technique (simplex interpolation) was implemented instead of trilinear interpolation. Subjective tests showed that such simplifications yield satisfactory spatialization for some angles and distances."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Locating virtual sound sources at arbitrary distances in real-time binaural reproduction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-015-0278-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Headphone reproduction, Binaural hearing, Localization of virtual sound sources, Pure-data","kwrd":["Headphone_reproduction","Binaural_hearing","Localization_of_virtual_sound_sources","Pure-data"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-015-0278-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-015-0278-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=278;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-015-0278-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Locating virtual sound sources at arbitrary distances in real-time binaural reproduction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0278-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0278-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">S.I. : Spatial Sound</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2015-10-13" itemprop="datePublished">13 October 2015</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Locating virtual sound sources at arbitrary distances in real-time binaural reproduction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Juli_n-Villegas" data-author-popup="auth-Juli_n-Villegas" data-corresp-id="c1">Julián Villegas<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Aizu" /><meta itemprop="address" content="grid.265880.1, 0000000417630236, Computer Arts Laboratory, University of Aizu, Aizu-Wakamatsu, 965-8580, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 19</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">201</span>–<span itemprop="pageEnd">212</span>(<span data-test="article-publication-year">2015</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">547 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-015-0278-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>A real-time system for sound spatialization via headphones is presented. Conventional headphone spatialization techniques effectively place sources on the surface of a virtual sphere around the listener. In the new system, sources can be spatialized at different distances from a listener by interpolating head-related impulse responses (HRIRs) measured between 20 and 160 cm. These HRIRs are stored in different databases depending on the audio sampling rate. To ease the real-time constraints, users can choose the number of HRIR taps used in the convolution, and an alternative interpolation technique (simplex interpolation) was implemented instead of trilinear interpolation. Subjective tests showed that such simplifications yield satisfactory spatialization for some angles and distances.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Spatialization of sound sources in virtual and augmented environments is an active topic of research with applications in gaming (Murphy and Neff <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Murphy D, Neff F (2011) Game sound technology and player interaction: concepts and developments, chapter. In: Spatial sound for computer games and virtual reality. Information Science Reference, pp 287–312" href="/article/10.1007/s10055-015-0278-0#ref-CR34" id="ref-link-section-d1289e336">2011</a>), security, and safety (Bellotti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bellotti F, Berta R, De Gloria A, Margarone M (2002) Using 3D sound to improve the effectiveness of the advanced driver assistance systems. Pers Ubiquitous Comput 6(3):155–163" href="/article/10.1007/s10055-015-0278-0#ref-CR5" id="ref-link-section-d1289e339">2002</a>), among others [see Cohen and Villegas (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Cohen M, Villegas J (2015) Applications of audio augmented reality. Wearware, everyware, anyware, and awareware, chapter 13. In: Fundamentals of wearable computers and augmented reality, 2nd edn. CRC Press, Boca Raton, pp 309–329" href="/article/10.1007/s10055-015-0278-0#ref-CR12" id="ref-link-section-d1289e342">2015</a>) for an extended review]. In most cases, spatial sound reinforces actions portrayed on in a screen, but in some applications, the aural modality exceeds the importance of the visuals. Consider, for example, “Papa Sangre II,” a game developed by Somethin’ Else, self-described as “a survival horror game told entirely through sound,” where the main way of interaction is via spatial sound.<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> Besides enhancing the immersion experience, sound spatialization is especially important in virtual environments when the visual modality is already saturated with information. By presenting some of that information aurally, virtual reality (VR) practitioners can ease decision-making processes, enrich the environment, etc. For instance, Ikei et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ikei Y, Yamazaki H, Hirota K, Hirose M (2006) vCocktail: multiplexed-voice menu presentation method for wearable computers. In: Proceedings of Virtual Reality Conference, pp 183–190" href="/article/10.1007/s10055-015-0278-0#ref-CR25" id="ref-link-section-d1289e358">2006</a>) reported a 99.7 % recognition rate of <span class="u-small-caps">gui</span> menu items, otherwise cramped in a small screen, using a technique which virtually separates them in azimuth. In the same vein, spatial audio has been used successfully in places such as airport controller rooms (Begault and Erbe <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Begault DR, Erbe T (1994) Multichannel spatial auditory display for speech communications. J Audio Eng Soc 42(10):819–826" href="/article/10.1007/s10055-015-0278-0#ref-CR4" id="ref-link-section-d1289e365">1994</a>) or aircraft cockpits (Nelson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Nelson WT, Hettinger LJ, Cunningham JA, Brickman BJ, Haas MW, McKinley RL (1998) Effects of localized auditory information on visual target detection performance using a helmet-mounted display. Hum Factors J Hum Factors Ergon Soc 40(3):452–460" href="/article/10.1007/s10055-015-0278-0#ref-CR36" id="ref-link-section-d1289e368">1998</a>) where aircrafts simultaneously approaching from different locations can pose hazardous situations. In the latter cases, adding sound spatialization to the displayed information yielded higher target detection performance as well as lower workload ratings compared to cases where only visualization was used. The same distribution of information across sensory modalities can be used in navigation systems used by drivers (Villegas and Cohen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010a" title="Villegas J, Cohen M (2010a) “Gabriel”: geo-aware broadcasting for in-vehicle entertainment and larger safety. In: Proceedings of 135 Audio Engineering Society International Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR49" id="ref-link-section-d1289e371">2010a</a>) and pedestrians (Sanuki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Sanuki W, Villegas J, Cohen M (2014) Spatial sound for mobile navigation systems. In: Proceedings of 136 Audio Engineering Society Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR43" id="ref-link-section-d1289e374">2014</a>) to lower the incidence of accidents caused by smartphone users who engage in retrieving information from their gadgets’ screens while moving through a busy place.</p><p>For reasons discussed later, many of these applications are only capable of directionalizing sounds at a fixed distance from the listener. Neglecting this third dimension diminishes the realism and possibilities of VR systems. The current study describes a Pure-data-based HRIR convolution system, easily integrable with other VR systems, that allows sound spatialization for arbitrary values of distance (<i>d</i>), elevation (<span class="mathjax-tex">\(\phi\)</span>), and azimuth (<span class="mathjax-tex">\(\theta\)</span>), via headphone reproduction. In what follows, a short review of different auralization techniques available for loudspeaker and headphone reproduction is presented, along with a summary some of the main HRIR databases currently available. Then, a description of how the proposed system was implemented precedes a section where results of its subjective evaluation are discussed; finally, integration of the developed tool with VR applications is briefly discussed.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background</h2><div class="c-article-section__content" id="Sec2-content"><p>When confronted with including sound spatialization in virtual environments, developers often need to choose between loudspeaker or headphone reproduction systems depending on the requirements of the application. Collocated users may enjoy a common loudspeaker array installation, while mobile application users are often required to use headphones for a better experience. For each alternative, several techniques to create an aural image have been proposed. A non-exhaustive list of solutions capable of expressing azimuth, elevation, and distance is presented in the following subsections.</p><h3 class="c-article__sub-heading" id="Sec3">Loudspeaker techniques</h3><p>Due to the nature of sound propagation, it is difficult to precisely spatialize virtual sounds (localize them at an arbitrary distance and direction around the listener) using loudspeaker systems, especially in areas between transducers (Gardner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gardner WG (1998) 3-D audio using loudspeakers. Springer Science &amp; Business Media, Berlin" href="/article/10.1007/s10055-015-0278-0#ref-CR17" id="ref-link-section-d1289e438">1998</a>). However, some techniques can be very effective in such a task, that is, the case of cross-talk cancelation, ambisonics, and wave field synthesis techniques (Spors et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Spors S, Wierstorf H, Raake A, Melchior F, Frank M, Zotter F (2013) Spatial sound with loudspeakers and its perception: a review of the current state. Proc IEEE 101(9):1920–1938" href="/article/10.1007/s10055-015-0278-0#ref-CR48" id="ref-link-section-d1289e441">2013</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec4">Cross-talk cancelation techniques</h4><p>This technique is based on the convolution of monophonic sounds with HRIRs and additional filtering of contralateral loudspeaker signals on ipsilateral loudspeaker signals (Schroeder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1961" title="Schroeder MR (1961) Improved quasi-stereophony and “colorless” artificial reverberation. J Acoust Soc Am 33(8):1061–1064" href="/article/10.1007/s10055-015-0278-0#ref-CR44" id="ref-link-section-d1289e451">1961</a>). Cross-talk cancelation spatializers rely on a known position (location plus orientation) of a listener’s head, which can be fairly inferred in applications such as moving picture reproduction, but not so for music reproduction, gaming, etc. When the position of the user’s head is uncertain, additional head-tracking systems are often recommended. Note that although it is possible to track several heads simultaneously, reproducing a sound field accord with each listener position is a very difficult task using this technique.</p><p>Traditionally, implementations of cross-talk cancelation techniques have used loudspeakers located symmetrically in front of the user, and distance cues are simulated with attenuation of the signals (Gardner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gardner WG (1995) Transaural 3-d audio. Technical report, MIT Media Laboratory, Perceptual Computing Section" href="/article/10.1007/s10055-015-0278-0#ref-CR16" id="ref-link-section-d1289e457">1995</a>), but cross-talk cancelation techniques have been successfully used in more complex loudspeaker arrays (e.g., 5.1 surround systems) (Kim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kim S, Ikeda M, Takahashi A, Ono Y, Martens WL (2009) Virtual ceiling speaker: elevating auditory imagery in a 5-channel reproduction. In: Audio Engineering Society Convention 127. Audio Engineering Society" href="/article/10.1007/s10055-015-0278-0#ref-CR29" id="ref-link-section-d1289e460">2009</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec5">Ambisonics</h4><p>A sound field (a region containing sound waves) measured at single point can be decomposed into spherical harmonics in a similar way in which a Fourier analysis decomposes a periodic signal (Gerzon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1973" title="Gerzon MA (1973) Periphony: with-height sound reproduction. J Audio Eng Soc 21(1):2–10" href="/article/10.1007/s10055-015-0278-0#ref-CR20" id="ref-link-section-d1289e471">1973</a>). When many spherical harmonics are considered (higher-order ambisonics—HOA), robust spatialization can be achieved over a broad sweet spot (the zone where the spatialization is more accurate) with decreasing localization blur as the number of spherical harmonic increases (Blauert and Rabenstein <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Blauert J, Rabenstein R (2012) Providing surround sound with loudspeakers: a synopsis of current methods. Arch Acoust 37(1):5–18" href="/article/10.1007/s10055-015-0278-0#ref-CR8" id="ref-link-section-d1289e474">2012</a>). Although a complete 3D representation is possible with this technique, most HOA applications have been limited to two-dimensional spaces (e.g., a circle in the same plane as the listener ears), and the aural images obtained appear to be at loudspeaker distance unless some kind of compensation is used (attenuation, direct-to-reverberant energy ratio manipulations, etc.).</p><p>Some promising real-time spatializers that offer HOA with distance control include “Sound Element Spatializer” (SES) (McGee and Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="McGee R, Wright M (2011) Sound element spatializer. Master’s thesis, University of Michigan" href="/article/10.1007/s10055-015-0278-0#ref-CR32" id="ref-link-section-d1289e480">2011</a>) and “spatium” (Penha and Oliveira <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Penha R, Oliveira J (2013) Spatium, tools for sound spatialization. In: Proceedings of the Sound and Music Computing Conference" href="/article/10.1007/s10055-015-0278-0#ref-CR39" id="ref-link-section-d1289e483">2013</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Wave field synthesis</h4><p>According to the Huygens–Fresnel principle, measurement points (microphones) in a sound field can be exchanged with secondary sound sources (loudspeakers). The superposition of individual sound fields produced by the loudspeakers can be controlled (basically, individually amplified and delayed) to reproduce an original sound field which can be real or synthetic (Berkhout et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93:2764–2778" href="/article/10.1007/s10055-015-0278-0#ref-CR6" id="ref-link-section-d1289e494">1993</a>). As in the case of ambisonics, <span class="u-small-caps">wfs</span>’ localization blur decreases with larger number of loudspeakers. Apparent distance in <span class="u-small-caps">wfs</span> systems can be manipulated by means of a technique based on direct-to-reverberant energy ratios (Bronkhorst and Houtgast <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bronkhorst AW, Houtgast T (1999) Auditory distance perception in rooms. Nature 397(6719):517–520" href="/article/10.1007/s10055-015-0278-0#ref-CR9" id="ref-link-section-d1289e503">1999</a>). Spatializations based on <span class="u-small-caps">wfs</span> are usually comparable to those achieved with HOA; however, they seem to be more computationally demanding. The SoundScape Renderer (Geier and Spors <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Geier M, Spors S (2012) Spatial audio with the soundscape renderer. In: 27 Tonmeistertagung–VDT International Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR19" id="ref-link-section-d1289e510">2012</a>) and SES are spatialization tools that offer <span class="u-small-caps">wfs</span> rendering among their options.</p><h3 class="c-article__sub-heading" id="Sec7">Headphone techniques</h3><p>The aforementioned HOA can also be used to display spatial sound via headphones. To this end, HRIRs representing the positions of the real loudspeakers are used to project the ambisonic signals at each side of the headphones. In such systems, the relative distance of a sound source can be conveyed by using monaural cues or first-order ambisonics (Kearney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kearney G, Gorzel M, Rice H, Boland F (2012) Distance perception in interactive virtual acoustic environments using first and higher order ambisonic sound fields. Acta Acust United Acust 98(1):61–71" href="/article/10.1007/s10055-015-0278-0#ref-CR28" id="ref-link-section-d1289e525">2012</a>). A free implementation of HOA in Pure-data was introduced by Sèdes et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Sèdes A, Guillot P, Paris E (2014) The HOA library, review and prospect. In: Proceedings of the joint ICMC-SMC Conference, pp 855–860" href="/article/10.1007/s10055-015-0278-0#ref-CR45" id="ref-link-section-d1289e528">2014</a>). In its current version (v2.2), this set of libraries allows the rendering of 2D and 3D HOA via loudspeakers or headphones.</p><p>Other systems, concerned only with lateralization of virtual sound sources, are usually based on level or time differences between the ears. Some of them incorporate intensity attenuation to simulate changes in the distance between a source and a listener; some others also include primitive Doppler shifts to create the illusion of a moving source or listener. More accurate systems allow positioning virtual sources at arbitrary elevation and azimuth. Systems based on HRIR are often used to that end in headphone-based reproduction.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">HRIR-based solutions</h4><p>In HRIR-based systems, a monophonic signal is convolved with a captured impulse response associated with the desired location; if the desired location does not exist among the available impulse responses, interpolation between the closest matches is used. The capturing process is performed painstakingly in anechoic chambers using a loudspeaker as a point source and microphones at the ears of real subjects or head and torso simulators. As a result, the number of locations from where the impulses are recorded are rather discrete in every dimension (distance, azimuth, and elevation). Furthermore, simulating point sources with loudspeakers imposes restrictions on the distance at which the impulse response measurements can be taken. These issues may explain why most available HRIR databases are only useful for rendering aural images at arbitrary azimuth and elevation, but at a relative large fixed distance (range), i.e., along the surface of a single sphere.</p><p>Recent developments in HRIR capturing techniques have allowed recording HRIRs as close as 10 cm (Hosoe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hosoe S, Nishino T, Itou K, Takeda K (2005) Measurement of head-related transfer functions in the proximal region. Forum Acusticum, pp 2539–2542" href="/article/10.1007/s10055-015-0278-0#ref-CR24" id="ref-link-section-d1289e544">2005</a>), but this improvement has not been broadly reflected in synthetic soundscapes. The potential benefits of using a sound spatializer like the one presented here include correct simulation of sounds in the near field, and an accurate simulation of approaching and receding sources over headphone reproduction in a similar fashion to some implementations of HOA and <span class="u-small-caps">wfs</span> over loudspeakers.</p><p>Spectral and temporal cues given by distance, elevation, and azimuth changes in a sound source with respect to a listener are naturally captured by HRIRs. Among the surveyed HRIR databases, the MIT (Gardner and Martin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Gardner WG, Martin KD (1995) Hrtf measurements of a kemar. J Acoust. Soc Am 97(6):3907–3908" href="/article/10.1007/s10055-015-0278-0#ref-CR18" id="ref-link-section-d1289e553">1995</a>), <span class="u-small-caps">cipic</span> (Algazi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Algazi V, Duda R, Thompson D, Avendano C (2001) The cipic hrtf database. In: Proceedings of Ieee Workshop on the applications of signal processing to audio and acoustics, pp 99–102. &#xA;                    http://earlab.bu.edu/databases/collections/cipic&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0278-0#ref-CR1" id="ref-link-section-d1289e559">2001</a>), and LISTEN (Warusfel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Warusfel O (2003) listen hrtf database. &#xA;                    http://recherche.ircam.fr/equipes/salles/listen/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0278-0#ref-CR52" id="ref-link-section-d1289e562">2003</a>) HRIR databases are perhaps the most widespread.</p><p>The MIT database comprises 710 HRIRs, recorded at 140 cm from the center of the head of a Knowles Electronics Mannequin for Acoustics Research (KEMAR). The <span class="u-small-caps">Cipic</span> database includes more than 2500 high spatial resolution HRIR measurements for 43 subjects and a KEMAR. In this database, HRIRs were captured at a fixed range of 100 cm. The LISTEN HRTF database consists of 187 HRIR measurements for 50 subjects recorded at a distance of 195 cm. All of the aforementioned sets were captured at a sampling rate of 44.1 kHz. They also share the characteristic that only a single distance was used in the recordings. Note that the last two databases are more sophisticated since they allow a user to select an HRIR suitable for a subject’s head and pinna size, improving the externalization and accuracy of virtual source localization.</p><p>With the above-mentioned databases, it is still possible to derive HRIRs at arbitrary distances in real time by decomposing existing HRIRs into spherical harmonics (Pollow et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Pollow M, Nguyen K-V, Warusfel O, Carpentier T, Müller-Trapet M, Vorländer M, Noisternig M (2012) Calculation of head-related transfer functions for arbitrary field points using spherical harmonics decomposition. Acta Acust United Acust 98(1):72–82" href="/article/10.1007/s10055-015-0278-0#ref-CR40" id="ref-link-section-d1289e575">2012</a>), or computing a distance variation function (DVF) (Kan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kan A, Jin C, van Schaik A (2009) A psychophysical evaluation of near-field head-related transfer functions synthesized using a distance variation function. J Acoust Soc Am 125(4):2233–2242" href="/article/10.1007/s10055-015-0278-0#ref-CR27" id="ref-link-section-d1289e578">2009</a>). According to results reported by Kan et al., distance judgements of stimuli processed with a DVF (derived from a rigid sphere model of the head) were more accurate than those of stimuli processed exclusively with intensity attenuation. However, in both cases, an overestimation of the actual distance was observed for sources synthesized to be closer to the listeners. Whether this overestimation is caused by the derivation of the DVF or by other factors such as perceptual bias (Zahorik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Zahorik P, Brungart DS, Bronkhorst AW (2005) Auditory distance perception in humans: a summary of past and present research. Acta Acust United Acust 91(3):409–420" href="/article/10.1007/s10055-015-0278-0#ref-CR57" id="ref-link-section-d1289e581">2005</a>) is a current matter of investigation.</p><p>HRIR datasets including measurements at different distances have been made available recently, including those of Hosoe et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Hosoe S, Nishino T, Itou K, Takeda K (2005) Measurement of head-related transfer functions in the proximal region. Forum Acusticum, pp 2539–2542" href="/article/10.1007/s10055-015-0278-0#ref-CR24" id="ref-link-section-d1289e587">2005</a>) with HRIRs measured in the range of 10–100 cm, every 10 cm, at a sampling rate of 48 kHz; Wierstorf et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wierstorf H, Geier M, Spors S (2011) A free database of head related impulse response measurements in the horizontal plane with multiple distances. In: Proceedings of 130 Audio Engineering Society Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR54" id="ref-link-section-d1289e590">2011</a>) who captured HRIRs at distances of 0.5, 1.0, 2.0,  and 3.0 m using a sampling rate of 44.1 kHz; and the database published by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e593">2009</a>). This last was used for building the system described here.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Impulse response locations in the implemented solution. Distance versus elevation (<i>left panel</i>) and elevation against azimuth (<i>right panel</i>). Measurements originally found in the database of Qu et al. are shown as <i>degree symbol</i> marks; interpolated measurements in the regularization stage, as <i>plus symbol</i> marks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e630">2009</a>) published a database obtained by using a spark gap as a sound source. At a sampling frequency of 65.536 kHz, they captured 6344 HRIRs around a KEMAR, varying the elevation from <span class="mathjax-tex">\(-40^{\circ }\)</span> to <span class="mathjax-tex">\(90^{\circ }\)</span> in <span class="mathjax-tex">\(10^{\circ }\)</span> steps, at distances of 20, 30, 40, 50, 75, 100, 130,  and 160 cm. The number of measurements depended on the elevation angle, whereas for elevations below <span class="mathjax-tex">\(60^{\circ },\)</span> they took 72 samples (i.e., uniformly distributed every <span class="mathjax-tex">\(5^{\circ }\)</span> in azimuth), and for higher elevations, the number of measurements decreased (namely 36, 24, 12,  and 1 samples for elevations of <span class="mathjax-tex">\(60^{\circ }, 70^{\circ }, 80^{\circ },\)</span> and <span class="mathjax-tex">\(90^{\circ }\)</span>, respectively). These locations are shown as "degree symbol" marks in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig1">1</a>.</p><p>The database of Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e842">2009</a>) is available as a compressed directory tree comprising binary files for each measurement. Each file contains 2048 double precision floats, the first half corresponding to the impulse response of the left ear and the second half for the right ear.</p><p>Some of the databases mentioned above have been used in various software applications. For the sake of brevity, the discussion here is limited to those available in Pure-data (Pd),<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> the real-time visual programming language used for the implementation.</p><p>
                              <span class="u-monospace">Earplug</span>
                              <sup><span class="u-monospace">~</span></sup> (Xiang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Xiang P, Camargo D, Puckette M (2005) Experiments on spatial gestures in binaural sound display. In: Proceedings of 11 International Conference on Auditory Display" href="/article/10.1007/s10055-015-0278-0#ref-CR56" id="ref-link-section-d1289e871">2005</a>) is the de facto binaural filter for sound spatialization in Pure-data. It uses the compact set of the MIT database. Musil et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Musil T, Noisternig M, Höldrich R (2005) A library for realtime 3D binaural sound reproduction in Pure Data (pd). In: Proceedings of International Conference on Digital Audio Effects" href="/article/10.1007/s10055-015-0278-0#ref-CR35" id="ref-link-section-d1289e874">2005</a>) created a library for binaural sound based on ambisonics. Besides the non-individualized MIT dataset, the authors claim that their library allows users to select entries from the <span class="u-small-caps">cipic</span> database. More recently, Doukhan and Sédès (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Doukhan D, Sédès A (2009) CW\_binaural&#xA;                    &#xA;                      &#xA;                    &#xA;                    $$^{\sim }$$&#xA;                    &#xA;                      &#xA;                        &#xA;                          &#xA;                          ∼&#xA;                        &#xA;                      &#xA;                    &#xA;                  : a binaural synthesis external for pure data. In: Proceedings of 3 Pure-data International Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR13" id="ref-link-section-d1289e881">2009</a>) introduced a spatialization library based on the LISTEN database. Users can select a desired subject <span class="u-small-caps">id</span> from this database, the number of taps for the convolution filtering, and the kind of filtering to use.</p><p>These extensions inherit limitations of the databases they use, so distance cues are usually missed or added by monaural changes in the level of the signals (e.g., distance dependent attenuation). Furthermore, differences between sampling rates of the application and the database become an issue.</p><p>The purpose of this research was to cover the gap left by existing HRIR-based spatializers, creating a real-time system capable of localizing sounds at an arbitrary direction and distance. In its design, the following features were considered important:</p><ul class="u-list-style-bullet">
                      <li>
                        <p>
                                       <i>Interoperability</i> The system is dedicated to spatializing monaural audio streams. It is also capable of receiving real-time parameters (distance, elevation, and azimuth) from other applications via the Open Sound Control (OSC) protocol (Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wright M (2005) Open sound control: an enabling technology for musical networking. Organ Sound 10:193–200" href="/article/10.1007/s10055-015-0278-0#ref-CR55" id="ref-link-section-d1289e903">2005</a>). The use of OSC allows integration with other tools for tasks such as video rendering/playback with no additional coding. In the same manner, low latency audio servers (such as Jack OS X) can be used to forward the spatialized streams to other applications for further treatment.</p>
                      </li>
                      <li>
                        <p>
                                       <i>Scalability</i> The system runs at different sampling rates with a selectable number of taps for convolution, so it can run on limited-power processors or for a number of sound sources, the practical upper limits established by the hardware used.</p>
                      </li>
                      <li>
                        <p>
                                       <i>Availability</i> Source code, databases, and compiled versions of the solution are available for popular operating systems in public repositories under a GNU license.</p>
                      </li>
                    </ul>
                           </div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Implementation</h2><div class="c-article-section__content" id="Sec9-content"><p>The binaural filter was deployed as a Pure-data object named <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup>. Executables for Mac OS X <span class="mathjax-tex">\(\ge\)</span> 10.9.5, Windows <span class="mathjax-tex">\(\ge\)</span> 7, source code, and databases needed to run this object are freely available from <a href="http://arts.u-aizu.ac.jp/research/pd-hrir">http://arts.u-aizu.ac.jp/research/pd-hrir</a>.</p><p>For the Pure-data implementation, only the right hemisphere measurements of the original database were stored, in a SQLite3 database.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overall integration of <span class="u-monospace">hrir</span>
                                    <sup><span class="u-monospace">~</span></sup> in Pure-data. The newly developed components are shown within the <i>dashed frame</i>. The SQLite database was created to store the HRIR measurements</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>
                        <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> (like most Pd extensions) is built as a C program that interacts with the main engine of Pure-data. The main engine is a notional module that handles input and output signals, accesses the file system, shares resources among objects within a program, as well as interpreting programs. Along with the <span class="u-small-caps">gui</span> and watchdog routines, the main engine constitutes the principal components of Pure-data. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig2">2</a> illustrates how the newly created extension fits into the Pure-data architecture.</p><p>The overall process to create <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> included diffuse-field equalization of the original database, homogenizing the database to have the same number of measurements regardless of elevation, resampling the HRIRs to the most commonly used sampling rates, storing the resampled HRIRs in the respective SQLite3 database, and creating the C program for the Pure-data object.</p><h3 class="c-article__sub-heading" id="Sec10">Diffuse-field equalization</h3><p>Along with the influence of head and torso on incoming sounds, the <span class="u-small-caps">hrir</span> recordings also manifest the influence of the measurement apparatus, including loudspeaker and KEMAR microphones as well as the mannequin’s ear canal resonances. Thus, using the original HRIRs would result in a quality degradation of the virtual imagery. These incidental features were removed by means of diffuse-field equalization: The average magnitude spectrum of all HRIR measurements was used to compute an inverse minimum phase filter that, once convolved with the original measurements, effectively removed features that are invariant with direction (e.g., the recording apparatus). The diffuse-field average and its inverse are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig3">3</a>. Although the minimum phase filter <span class="mathjax-tex">\(h_{m}\)</span> of an <i>n</i> sequence (e.g., left ear HRIR) can be estimated by the real cepstrum of the sequence (Pei and Lin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Pei S-C, Lin H-S (2006) Minimum-phase fir filter design using real cepstrum. IEEE Trans Circuits Syst II Exp Br 53(10):1113–1117" href="/article/10.1007/s10055-015-0278-0#ref-CR37" id="ref-link-section-d1289e1088">2006</a>), the method described by Hawksford (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hawksford MO (1997) Digital signal processing tools for loudspeaker evaluation and discrete-time crossover design. J Audio Eng Soc 45(1/2):37–62" href="/article/10.1007/s10055-015-0278-0#ref-CR22" id="ref-link-section-d1289e1092">1997</a>) was used instead:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} h_{m}(n) =\mathfrak {R}\bigg ( \mathscr {F}^{-1}\Big [ \exp \Big ( \mathscr {H}\big ( \log (\, |\mathscr {F}(n)\, |\, )\big )^{*} \Big ) \Big ] \bigg ), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(\mathscr {F}\)</span> and <span class="mathjax-tex">\(\mathscr {H}\)</span> are the Fourier and Hilbert transformations, respectively.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Diffuse-field average and its inverse used for equalization. Note that the original recordings are high-pass filtered with a cutoff frequency of 70 Hz</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec11">Regularization</h3><p>A regular grid—spaced <span class="mathjax-tex">\(5^{\circ }\)</span> in azimuth, <span class="mathjax-tex">\(10^{\circ }\)</span> in elevation and 10 cm in distance—was built off-line to ease the real-time implementation. The additional locations are shown by "plus symbol" marks in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig1">1</a>. Missing HRIRs were computed by a linear interpolation between the two closest HRIRs in the desired dimension, as suggested by Wenzel and Foster (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Wenzel EM, Foster SH (1993) Perceptual consequences of interpolating head-related transfer functions during spatial synthesis. In: Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp 102–105" href="/article/10.1007/s10055-015-0278-0#ref-CR53" id="ref-link-section-d1289e1388">1993</a>). In this simple approach, HRIRs are first converted to HRTFs (head-related transfer functions) via Fourier transformations; the interpolated HRTF <span class="mathjax-tex">\(|\hat{H}(\omega _i) |\)</span> at the missing location <span class="mathjax-tex">\(p_m\)</span> is computed as</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} |\hat{H}(\omega _i) |= w_1|H_1(\omega _i) |+ w_2|H_2(\omega _i) |\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where the HRTF bins are represented by <i>i</i> (<span class="mathjax-tex">\(1,\dots ,1024\)</span>), <span class="mathjax-tex">\(|H_1(\omega _i) |\)</span> and <span class="mathjax-tex">\(|H_2(\omega _i) |\)</span> are the HRTFs corresponding to <span class="mathjax-tex">\(p_1\)</span> and <span class="mathjax-tex">\(p_2\)</span>, the circumscribing locations in the desired dimension (i.e., azimuth or distance only), and</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} w_{1} = \frac{\Delta ({p_2},{p_m})}{\Delta ({p_2},{p_1})}, \quad w_{2}=1-w_{1}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>i.e., <span class="mathjax-tex">\(w_1\)</span> and <span class="mathjax-tex">\(w_2\)</span> are the normalized distances between <span class="mathjax-tex">\(p_2\)</span> and <span class="mathjax-tex">\(p_m\)</span>, and <span class="mathjax-tex">\(p_1\)</span> and <span class="mathjax-tex">\(p_m\)</span>, respectively.</p><p>HRIRs at the interpolated locations were found by applying the inverse Fourier transformation to <span class="mathjax-tex">\(|\hat{H}(\omega _i) |\)</span>. This simple interpolation method produced no apparent artifacts. A more sophisticated approach could have been used: decomposing each HRIR into a minimum phase filter and an interaural time difference (ITD), then interpolating the minimum phase filters, and reconstructing the HRIR as discussed by Sodnik et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sodnik J, Sušnik R, Štular M, Tomažič S (2005) Spatial sound resolution of an interpolated hrir library. Appl Acoust 66(11):1219–1234" href="/article/10.1007/s10055-015-0278-0#ref-CR47" id="ref-link-section-d1289e2162">2005</a>). Such refinement was not introduced since the computation of ITDs is not trivial and may introduce undesirable errors in the interpolated HRIRs (Estrella <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Estrella J (2010) On the extraction of interaural time differences from binaural room impulse responses. Master’s thesis, Technische Universität Berlin" href="/article/10.1007/s10055-015-0278-0#ref-CR14" id="ref-link-section-d1289e2165">2010</a>).</p><h3 class="c-article__sub-heading" id="Sec12">Storage</h3><p>To ease the integration of <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> with other audio applications, the diffused field equalized HRIRs were resampled to the most common sampling rates currently used in audio: 8, 16, 22.05, 44.1, 48, 96, and 192 kHz. A different database was created for each sampling rate. HRIRs, with their respective parameters (distance, elevation, and azimuth), were stored in a single SQLite3 database table. Indexes were created on the HRIR parameters to speed up their retrieval. The actual HRIR taps were stored as binary large objects (BLOBs) of single precision floats.</p><p>With the float precision reduction, along with halving the number of stored HRIRs (by keeping only the right hemisphere measurements), a typical application at a sampling rate of 44.1 kHz requires an extra memory footprint of about 44 MB to use <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup>. This represents less than half of the original database size, and although apparently large, this figure is still below the current thresholds imposed for some mobile phone executable files, e.g., 60 MB for iOS applications (Apple Inc. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Apple Inc. (2015) iTunes connect developer guide. Apple Inc., Cupertino, CA (USA). &#xA;                    https://developer.apple.com/library/ios/documentation/LanguagesUtilities/Conceptual/iTunesConnect_Guide/iTunesConnect_Guide.pdf&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0278-0#ref-CR2" id="ref-link-section-d1289e2193">2015</a>).</p><h3 class="c-article__sub-heading" id="Sec13">Real-time processing</h3><p>When <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> is instantiated within Pure-data, it performs the convolution of an incoming monophonic audio connected to its (top) leftmost inlet with an HRIR extracted from the database, or synthesized via interpolation as detailed in the following section. As the sole object argument, users can set the number of filter taps for the convolution to any power of two less than the existing taps in the database (e.g., 689, 750, 1500, for sampling rates of 44.1, 48, 96 kHz, respectively). This advisory information is presented to the user via the Pure-data console when <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> is instantiated. Azimuth, elevation, and distance can be set using the remaining inlets in that order, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig4">4</a>. These parameters are internally represented as integers to maximize correspondences with those stored in the database.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A simple application based on <span class="u-monospace">hrir</span>
                                       <sup><span class="u-monospace">~</span></sup>. White noise is spatialized to be at <span class="mathjax-tex">\(\theta = 13^{\circ },\phi = -8^{\circ },\)</span> and <span class="mathjax-tex">\(d = 34\)</span> cm by convolving it with 256 taps of the synthesized HRIR for that location. The resulting binaural signal is sent to the computer sound card for reproduction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>
                                       <i>n</i>-Simplex interpolation. No interpolation is needed when the desired location <span class="mathjax-tex">\(p_m\)</span> (★) coincides with a database measurement. Corresponding <i>color line segments</i>, <i>triangle areas</i>, and <i>tetrahedron volumes</i> are used to compute the weighting factors of each<i> color</i> coded location, for 1-, 2-, and 3-simplex interpolation, respectively. To simplify the visualization, only three tetrahedra are shown in the 3-simplex interpolation case (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>By default, <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> will connect to the corresponding database according to the sampling frequency being used in Pure-data. If such database does not exist in the same directory of the application, it will try to connect to the 44.1 kHz database, warning the user of inaccurate results. If no databases are found, the spatialization is bypassed.</p><p>Convolutions are performed every <span class="u-small-caps">dsp</span> block (64 samples by default in Pure-data) in the time domain. <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> imposes an upper limit of 8192 (<span class="mathjax-tex">\(2^{13}\)</span>) samples to which users can increase the size of the blocks. Note that, as in the case of the number of taps for convolution, in Pure-data changes on the processing block size must be done in powers of two. To eliminate transient artifacts between blocks, a commutation technique (Jot et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Jot J-M, Larcher V, Warusfel O (1995) Digital signal processing issues in the context of binaural and transaural stereophony. In: Proceedings of 98 Audio Engineering Society Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR26" id="ref-link-section-d1289e2431">1995</a>) based on equal power cross-fading was implemented between the HRIRs of subsequent blocks.</p><h3 class="c-article__sub-heading" id="Sec14">Interpolation</h3><p>Trilinear interpolation, or the linear interpolation of the closest measurements bounding a desired location <span class="mathjax-tex">\(p_m\)</span> (usually eight), could be used for approximating the HRIR of <span class="mathjax-tex">\(p_m\)</span> when it is missing in the database, but this approach is computationally demanding and not suitable for real-time implementations. Instead, taking advantage of regularity of the built database, an <i>n</i>-simplex interpolation technique (Hemingway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Hemingway P (2002) n-Simplex interpolation. Technical report, Hewlett-Packard Laboratories Bristol" href="/article/10.1007/s10055-015-0278-0#ref-CR23" id="ref-link-section-d1289e2493">2002</a>) was deployed. Note that when the HRIR measurements are not regularly distributed in space, other interpolation strategies such as those based on Delaunay triangulation (Gamper <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Gamper H (2014) Enabling technologies for audio augmented reality systems. PhD thesis, Aalto University" href="/article/10.1007/s10055-015-0278-0#ref-CR15" id="ref-link-section-d1289e2496">2014</a>) may be necessary.</p><p>The general interpolation process is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig5">5</a>: 
If the three parameters (distance, elevation, and azimuth) exist in the database, no interpolation is required, and the stored HRIR is used for convolution (0-simplex interpolation). If two of the parameters have matches in the database, a linear interpolation (i.e., 1-simplex interpolation) is carried out between the two closest HRIRs stored in the database which only differ with <span class="mathjax-tex">\(p_m\)</span> in the mismatched dimension; the distance between <span class="mathjax-tex">\(p_m\)</span> and a given measurement location is used as the weighting factor of the other measurement, as shown in Eq.  <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-015-0278-0#Equ3">3</a>. If only one parameter exists, a barycentric interpolation (or 2-simplex interpolation) is conducted between the HRIRs of the three closest locations to <span class="mathjax-tex">\(p_m\)</span>; the triangular areas defined by these locations are used as weighting factors as shown in the figure. Finally, when none of the parameters exist in the database, the tetrahedral volumes defined by the closest four measurements and <span class="mathjax-tex">\(p_m\)</span> are used as the interpolation weights (i.e., a 3-simplex interpolation) for the existing HRIRs. This procedure is followed unless the location has elevation <span class="mathjax-tex">\(\phi _m &gt;80^{\circ }\)</span>, in which case the two HRIRs elevated <span class="mathjax-tex">\(80^{\circ }\)</span> are linearly combined, after which a 2-simplex interpolation between <span class="mathjax-tex">\(p_m\)</span>, the newly found HRIR, and the zenithal HRIRs closest to <span class="mathjax-tex">\(p_m\)</span> is performed.</p><p>In the Pure-data object, HRIRs are interpolated in the time domain. Previous versions of <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> (Villegas and Cohen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Villegas J, Cohen M (2013) Real-time head-related impulse response filtering with distance control. In: Proceedings of 135 Audio Engineering Society Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR51" id="ref-link-section-d1289e2724">2013</a>) featured time domain interpolation of the minimum phase filter versions of the HRIRs, followed by interpolation of the ITDs, which were finally expressed using a delay line (Smith and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Smith J, Lee N (2008) Computational acoustic modeling with digital delay. Center for Computer Research in Music and Acoustics, Stanford University. &#xA;                    https://ccrma.stanford.edu/realsimple/Delay/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-015-0278-0#ref-CR46" id="ref-link-section-d1289e2727">2008</a>). The current simplification eases the real-time computations but could yield artifacts mainly due to interaural time differences between two HRIRs (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen L, Hu H, Wu Z (2008) Head-related impulse response interpolation in virtual sound system. Int Conf Nat Comput 6:162–166" href="/article/10.1007/s10055-015-0278-0#ref-CR11" id="ref-link-section-d1289e2730">2008</a>). Fortunately, the use of locations relatively close to each other seems to mitigate such errors, as suggested by Wenzel and Foster (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Wenzel EM, Foster SH (1993) Perceptual consequences of interpolating head-related transfer functions during spatial synthesis. In: Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp 102–105" href="/article/10.1007/s10055-015-0278-0#ref-CR53" id="ref-link-section-d1289e2734">1993</a>) and the subjective evaluation described below.</p><p>Besides the interpolation simplification, other simplifications were applied: Eight circumscribing measurements (a volume element in spherical coordinates) are regarded as cubes, so linear interpolation is used instead of spherical linear interpolation (SLERP) for azimuth and elevation. Also, if <span class="mathjax-tex">\(p_m\)</span> lays over one main diagonal of this cube, a 3-simplex interpolation (as opposed to a 1-simplex) is used.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Subjective evaluation</h2><div class="c-article-section__content" id="Sec15-content"><p>A subjective experiment was performed to evaluate the impact of the implemented simplifications and the spatialization capabilities of <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup>. Concretely, it is important to assess:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>How well subjects were able to identify the intended origin of a virtual sound source,</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>How the auralization performed with <span class="u-monospace">hrir</span>
                                    <sup><span class="u-monospace">~</span></sup> related to that performed with the original database, and</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>The grade of confusion between different localizations.</p>
                    
                  </li>
                </ol>
                     <h3 class="c-article__sub-heading" id="Sec16">Method</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Participants</h4><p>A cohort of eight volunteers from the University of Aizu (six were males, two were females) participated in the experiment. They were on average 24 years old (SD <span class="mathjax-tex">\(=6\)</span>) and had normal thresholds of hearing, i.e., hearing thresholds ≤20 dB (HL) in the range of 0.125–8 kHz. This was verified with a Maico MA25 audiometer. Permission for performing this experiment was obtained following the University of Aizu ethics procedure.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Apparatus</h4><p>The experiment took place in a quiet room (noise criterion <span class="mathjax-tex">\(NC=25\)</span> at 8 kHz) where participants individually listened to the stimuli through Sennheiser HD650 open headphones. The headphones were directly connected to the audio output of a <span class="mathjax-tex">\(27^{\prime \prime }\)</span> iMac computer running OS X 10.9.5. The self-paced experiment was programmed in PsychoPy2 (Peirce <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Peirce JW (2007) PsychoPy-psychophysics software in python. J Neurosci Methods 162(1–2):8–13" href="/article/10.1007/s10055-015-0278-0#ref-CR38" id="ref-link-section-d1289e2918">2007</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Stimuli</h4><p>A monophonic recording of a male uttering “The birch canoe slid on the smooth planks” (Rothauser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1969" title="Rothauser EH, Chapman WD, Guttman N, Silbiger HR, Hecker MHL, Urbanek GE, Nordby KS, Weinstock M (1969) Ieee recommended practice for speech quality measurements. IEEE Trans Audio Electroacoust 17(3):225–246" href="/article/10.1007/s10055-015-0278-0#ref-CR42" id="ref-link-section-d1289e2929">1969</a>) was used in this experiment. This sentence (2.27 s long) was sampled at 44.1 kHz with 24-bit resolution.</p><p>The stimuli was generated in Pure-data by filtering the speech recording with an <span class="u-monospace">hrir</span>
                              <sup><span class="u-monospace">~</span></sup> object featuring 512 HRIR taps at 44.1 kHz. The resulting sounds were stored in 2-channel WAV files.</p><p>Spatialization was carried out independently for each parameter: Distance was tested at 20, 60, 100, and 140 cm with fixed azimuth <span class="mathjax-tex">\(\theta = 45^{\circ }\)</span> and elevation <span class="mathjax-tex">\(\phi =0^{\circ }\)</span>. Five elevation angles separated <span class="mathjax-tex">\(30^{\circ }\)</span> from each other, starting from <span class="mathjax-tex">\(\phi = -30^{\circ }\)</span>, were also tested. For these stimuli, azimuth and distance were fixed at <span class="mathjax-tex">\(\theta = 0^{\circ }\)</span> and <span class="mathjax-tex">\(d = 50\)</span> cm. Azimuth (with fixed elevation <span class="mathjax-tex">\(\phi = 0^{\circ }\)</span> and distance <span class="mathjax-tex">\(d = 50\)</span> cm) was tested in <span class="mathjax-tex">\(30^{\circ }\)</span> intervals starting from the front of the subject (<span class="mathjax-tex">\(\theta = 0^{\circ }\)</span>), for a total of 12 angles. These azimuth angles were chosen to aid comparison with the results presented by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e3240">2009</a>).</p><p>Similar stimuli were generated for practice trials: The speech of the same male speaker uttering a different sentence (about the same length) was spatialized to change in azimuth, elevation, or distance, with some variation from the stimuli used in the actual experiment (e.g., a larger distance for azimuth, a different azimuth for distance). Responses of practice trials were excluded for the result analysis.</p><p>The overall intensity level was calibrated so that a stimulus spatialized to be at 50 cm in front of the listener (i.e., <span class="mathjax-tex">\(\theta =0^{^{\circ }}, \phi = 0^{^{\circ }},\;d= 50\)</span> cm) yielded <span class="mathjax-tex">\(67.5\pm 0.5\)</span> dB(A) at each ear, as verified with a Brüel and Kjær artificial ear type 4153 (Brüel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1962" title="Brüel PV, Frederiksen E, Rasmussen G (1962) Artificial ears for the calibration of earphones of the external type. Technical Report 1, Brüel and Kjær, Nærum, Denmark" href="/article/10.1007/s10055-015-0278-0#ref-CR10" id="ref-link-section-d1289e3344">1962</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Confusion matrices for each parameter tested in the experiment. Elevation and distance on<i> top</i>, azimuth (without and with front–back correction) at the<i> bottom</i>. A given row in a matrix shows the distribution of responses in percentage across all the tested levels. <i>Darker colors</i> correspond to better identification, perfect results would have 100 % in the matrix diagonal</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Procedure</h4><p>The experiment comprised three blocks (one for each spherical coordinate), randomly sorted for each participant. Practice sessions were held immediately before each block. Visual feedback indicating correct answers was provided after the subject’s responses in the practice trials, but not in the actual experiment.</p><p>Participants were subjected to seven repetitions of the stimuli in both practice and real sessions. Hence, subjects heard a total of 294 trials during the experiment, equally divided into 147 trials per practice and real sessions (84 trials for azimuth, 35 trials for elevation, and 28 trials for distance). Stimuli within practice and real sessions were sorted into random permutation order for each participant.</p><p>In a given trial, after the stimulus was completely reproduced, a visual scale was presented to the participants from which they were asked to select an answer. There were discrete marks in the scale such that it was impossible to choose values between them.</p><p>All the instructions for the experiment were displayed on the computer screen. Prior to engaging in the actual experiment, participants were encouraged to raise their hand for oral assistance. Participants were also encouraged to take short breaks between blocks. On average, all subjects completed the experiment in about 30 min.</p><h3 class="c-article__sub-heading" id="Sec21">Results</h3><p>Responses for distance (<i>d</i>), elevation (<span class="mathjax-tex">\(\phi\)</span>), and azimuth without and with front–back correction (<span class="mathjax-tex">\(\theta\)</span> and <span class="mathjax-tex">\(\theta _c\)</span>, respectively) were analyzed independently through confusion matrices, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig6">6</a>. Front–back confusions, i.e., judging a sound synthesized to be in front of the listener as coming from the back (or vice versa), were corrected by reassigning incorrect responses to the expected ones, only when they were symmetrically located around the interaural axis.</p><p>Accuracy of subjective judgements was estimated with micro-averaged F-scores (the harmonic mean of subjective precision and recall), as implemented by Grosjean and Denis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Grosjean P, Denis K (2013) mlearning: Machine learning algorithms with unified interface and confusion matrices. R package version 1.0-0" href="/article/10.1007/s10055-015-0278-0#ref-CR21" id="ref-link-section-d1289e3475">2013</a>) and summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0278-0#Tab1">1</a>. Pearson’s correlation coefficients were computed between reported and actual values. A significant positive correlation (<span class="mathjax-tex">\(p&lt;.001\)</span>) was found in all cases, as shown in the same table. Furthermore, Pearson’s correlation coefficients between the target and reported locations were also computed for each subject, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig7">7</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Global statistics on the experiment results</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-015-0278-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Pearson’s correlation coefficients between the target and perceived locations for each of the eight participants</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Best results were obtained for distance estimation. The loss of 44 percent points on the error rate with respect to the results reported by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e3936">2009</a>) could be explained by changes in intensity levels associated with distance. Whereas in this experiment these differences were preserved, they were equalized in the experiment conducted by Qu et al. to only observe binaural effects. Including level differences in this experiment is justified by the interest of assessing the efficacy of spatializations based on <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup>. The ratio between the error rate found in this experiment and that reported in (Qu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e3946">2009</a>) (i.e., <span class="mathjax-tex">\(29\,\% = 18\,\%/62\,\%\)</span>) is comparable to the ratio of distance discrimination thresholds for stimuli with and without intensity cues reported by Ashmead et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Ashmead DH, Leroy D, Odom RD (1990) Perception of the relative distances of nearby sound sources. Percept Psychophys 47(4):326–331" href="/article/10.1007/s10055-015-0278-0#ref-CR3" id="ref-link-section-d1289e3994">1990</a>), (<span class="mathjax-tex">\(37.5\%=6\% /16\%\)</span>), the methodological differences notwithstanding.</p><p>In contrast to judgements of distance, the error rate in elevation responses was high. A gain of 8.2 percent points over the results reported by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e4038">2009</a>) may respond to the lack of ITD variation in the current experiment. Stimuli in (Qu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e4041">2009</a>) was presented at the same distance as in this experiment, but at <span class="mathjax-tex">\(\theta = 90^{\circ }\)</span>, so any changes in elevation in their case also caused changes in the interaural time difference. Another possible cause for the poor elevation results is the up and down confusions. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig7">7</a>, a negative correlation between target and perceived elevations was found for a subject (<span class="mathjax-tex">\(\rho = -.29\)</span>). However, removing data from this participant in the elevation analysis resulted in marginal improvements compared to those reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-015-0278-0#Tab1">1</a>: A global correlation of .508, <i>F</i>-score of .466, and error rate of 55.1%. Note that the elevation error rate found in this experiment with all the subjective data is similar to those reported in the literature. For example, McKinley and Ericson (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="McKinley RL, Ericson MA (1997) Binaural and spatial hearing in real and virtual environments, chapter. In: Flight demonstration of a 3-D auditory display. Lawrence Erlbaum Assoc., Inc., Mahwah, NJ, USA, pp 683–699" href="/article/10.1007/s10055-015-0278-0#ref-CR33" id="ref-link-section-d1289e4117">1997</a>) reported that the minimum audible angle (<span class="u-small-caps">maa</span>) for elevations on the median plane was between <span class="mathjax-tex">\(30^{\circ }\)</span> and <span class="mathjax-tex">\(35^{\circ }\)</span> when the stimuli were convolved with a mannequin HRIR. In the same vein, Gardner (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gardner WG (1998) 3-D audio using loudspeakers. Springer Science &amp; Business Media, Berlin" href="/article/10.1007/s10055-015-0278-0#ref-CR17" id="ref-link-section-d1289e4172">1998</a>) reported large variances in the estimation of elevation of KEMAR-based spatialized sounds on the median plane. Interestingly, subjective responses in the present experiment seem to have a bias toward <span class="mathjax-tex">\(30^{\circ }\)</span> of elevation, which is somewhat more elevated than the bias reported by Gardner in the same study: Medial elevation responses in his case were clustered around <span class="mathjax-tex">\(0^{\circ }\)</span>.</p><p>Azimuth results are directly comparable to those reported by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e4227">2009</a>). In their case, error rates were of 63 and 45 % for responses with and without front–back confusion, respectively. These figures are slightly lower than those reported here, 5 and 7 % points, respectively. These differences may be due to differences in material, sampling rates, and convolution engines. About 16 % of the subjective responses were confusions; front–back confusions (≈10 %) occurred more often than back–front confusions (≈6 %), in agreement with what has been reported by many other researchers (Blauert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Blauert J (1997) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge" href="/article/10.1007/s10055-015-0278-0#ref-CR7" id="ref-link-section-d1289e4230">1997</a>, pp. 104ff). Likewise, the azimuth judgements seem to be biased toward the back hemisphere, even when confusions were resolved: out of 672 azimuth responses, <span class="mathjax-tex">\(\theta = 120^{\circ }\)</span> and <span class="mathjax-tex">\(\theta = 240^{\circ }\)</span> were selected 90 and 87 times, as illustrated by their colored vertical entries in the bottom-right matrix of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig6">6</a>. This finding suggests that participants found somewhat difficult to judge the apparent azimuth of the stimuli (Blauert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Blauert J (1997) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge" href="/article/10.1007/s10055-015-0278-0#ref-CR7" id="ref-link-section-d1289e4301">1997</a>).</p><h3 class="c-article__sub-heading" id="Sec22">Discussion</h3><p>The experiment results suggest that even with the simplifications imposed for the real-time implementation, the subjective results are comparable to results obtained by Qu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132" href="/article/10.1007/s10055-015-0278-0#ref-CR41" id="ref-link-section-d1289e4312">2009</a>) and other researchers. In general, judgements of localizations obtained with <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> were biased toward the back hemisphere (at least for some angles) and were somewhat elevated. The actual pinna sizes of the subjects and the pinnae simulators used for the recordings are unknown, but it is possible that subjects with pinnae larger than the KEMAR simulators shifted their elevation judgements upwards (Martens <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Martens WL (2003) Perceptual evaluation of filters controlling source direction: customized and generalized hrtfs for binaural synthesis. Acoust Sci Technol 24(5):220–232" href="/article/10.1007/s10055-015-0278-0#ref-CR31" id="ref-link-section-d1289e4322">2003</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-015-0278-0/MediaObjects/10055_2015_278_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Integration of <span class="u-monospace">hrir</span>
                                       <sup><span class="u-monospace">~</span></sup> in a simple VR application. Each component may run in a different computer if necessary</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-015-0278-0/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Location confusion, bias, and blur issues can be minimized by training, familiarity with the sound sources, and by allowing movements of sound sources and the head of the listener in virtual environments, as naturally happens in real environments. Hence, spatializations based on <span class="u-monospace">hrir</span>
                           <sup><span class="u-monospace">~</span></sup> are expected to give satisfactory results for many virtual environments.</p></div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Integration</h2><div class="c-article-section__content" id="Sec23-content"><p>
                        <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> can be easily integrated in VR ecosystems. Considering the Model–View–Controller (MVC) paradigm depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-015-0278-0#Fig8">8</a>, data coming from a GUI (or sensors) can be utilized to control the sound spatialization. In simple applications such as the <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> documentation program, the three <span class="u-small-caps">mvc</span> parts can be hosted by the same machine and even by the same program (i.e., Pure-data), but when necessary, they can be distributed across different hosts or platforms. For example, one can use the outputs of a gyroscope and an accelerometer included in a inertial measurement unit—IMU (wirelessly transmitted by an Arduino micro-controller) to track the head of a user, robot, etc., and modify a VR scene (running in a different computer); in turn, the spatialized sound can be streamed over the Internet to one or multiple listeners.</p><p>Although several protocols can be used to transmit control data, the Open Sound Control (OSC) protocol (Wright <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wright M (2005) Open sound control: an enabling technology for musical networking. Organ Sound 10:193–200" href="/article/10.1007/s10055-015-0278-0#ref-CR55" id="ref-link-section-d1289e4394">2005</a>) is recommended because of its portability, reliability, and simplicity. With this protocol, one mainly needs to specify the URL of a machine hosting the model (and a corresponding port) in the controller and enable such port in the receiving end.<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup>
                     </p><p>Note that the machine hosting the model part is not necessarily a desktop or laptop computer. Pure-data (and <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup>) can be installed in several architectures, including Raspberry Pi computers, smartphones, and tablets (via libpd and PdDroidParty extensions).</p><p>These integration possibilities have been explored to spatialize sounds of three-dimensional Lissajous figures built in Quartz Composer (Villegas and Cohen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Villegas, J. and Cohen, M. (2010b) Hrir&#xA;                           ~: modulating range in headphone-reproduced spatial audio. In: Proceedings of 9 International Conference on VR Continuum and Its Applications in Industry" href="/article/10.1007/s10055-015-0278-0#ref-CR50" id="ref-link-section-d1289e4423">2010b</a>), as well as for head-tracked binaural systems.</p><p>In the latter case, user head rotations (pitch, yaw, and roll) were captured using a worn iPhone and wirelessly transmitted to a desktop computer within the same local area network (LAN). The gyroscopic data were used to decouple the virtual location of a sound source and the orientation of the listener’s head, stabilizing the spatial imagery. These spatializations run on modern Mac computers at a sampling rate of 44.1 kHz with a output buffer of 12 ms. Although no appreciable audio dropouts or other real-time artifacts were observed, network latencies and <span class="u-small-caps">cpu</span> peak loads could become a problem in distributed or large applications. In such cases, isolating the traffic of other computers, instantiating Pure-data without its graphical interface, and reducing the number of taps used for the convolution could ameliorate such issues.</p><p>On a final note, with the recent release of the Audio Spatializer SDK for Unity 5.2,<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup>, there is the possibility of using (or porting) <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> into this popular game engine in the future.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusions and Future work</h2><div class="c-article-section__content" id="Sec24-content"><p>
                        <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup>, a real-time system that allows arbitrary spatialization of monophonic sounds in the three spherical coordinates, was introduced. The system, based on the Pure-data programming language, yields subjective results comparable to those obtained with other deferred processing (non-real-time) systems. Simplifications such as real-time <i>n</i>-simplex interpolation to compute missing HRIRs were made to meet real-time requirements. Such shortcuts were not clearly reflected in the overall quality of the spatialized sounds. Spatializations based on <span class="u-monospace">hrir</span>
                        <sup><span class="u-monospace">~</span></sup> were tested in several software applications, but a formal scalability and performance evaluation is postponed for a future study. Also, the original HRIR databases as well as those obtained for different sampling frequencies are being formatted using Spatially Oriented Format for Acoustics (SOFA) (Majdak et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Majdak P, Iwaya Y, Carpentier T, Nicol R, Parmentier M, Roginska A, Suzuki Y, Watanabe K, Wierstorf H, Ziegelwanger H et al. (2013) Spatially oriented format for acoustics: a data exchange format representing head-related transfer functions. In: Proceedings of 134 Audio Engineering Society Convention" href="/article/10.1007/s10055-015-0278-0#ref-CR30" id="ref-link-section-d1289e4483">2013</a>).</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>
                              <a href="http://www.papasangre.com">www.papasangre.com</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>
                                    <a href="http://puredata.info">http://puredata.info</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>
                              <a href="http://opensoundcontrol.org">http://opensoundcontrol.org</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>
                              <a href="http://docs.unity3d.com/Manual/AudioSpatializerSDK.html">http://docs.unity3d.com/Manual/AudioSpatializerSDK.html</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Algazi V, Duda R, Thompson D, Avendano C (2001) The cipic hrtf database. In: Proceedings of Ieee Workshop on t" /><p class="c-article-references__text" id="ref-CR1">Algazi V, Duda R, Thompson D, Avendano C (2001) The <span class="u-small-caps">cipic hrtf</span> database. In: Proceedings of <span class="u-small-caps">Ieee</span> Workshop on the applications of signal processing to audio and acoustics, pp 99–102. <a href="http://earlab.bu.edu/databases/collections/cipic">http://earlab.bu.edu/databases/collections/cipic</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Apple Inc. (2015) iTunes connect developer guide. Apple Inc., Cupertino, CA (USA). https://developer.apple.com" /><p class="c-article-references__text" id="ref-CR2">Apple Inc. (2015) iTunes connect developer guide. Apple Inc., Cupertino, CA (USA). <a href="https://developer.apple.com/library/ios/documentation/LanguagesUtilities/Conceptual/iTunesConnect_Guide/iTunesConnect_Guide.pdf">https://developer.apple.com/library/ios/documentation/LanguagesUtilities/Conceptual/iTunesConnect_Guide/iTunesConnect_Guide.pdf</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DH. Ashmead, D. Leroy, RD. Odom, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Ashmead DH, Leroy D, Odom RD (1990) Perception of the relative distances of nearby sound sources. Percept Psyc" /><p class="c-article-references__text" id="ref-CR3">Ashmead DH, Leroy D, Odom RD (1990) Perception of the relative distances of nearby sound sources. Percept Psychophys 47(4):326–331</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3758%2FBF03210871" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perception%20of%20the%20relative%20distances%20of%20nearby%20sound%20sources&amp;journal=Percept%20Psychophys&amp;volume=47&amp;issue=4&amp;pages=326-331&amp;publication_year=1990&amp;author=Ashmead%2CDH&amp;author=Leroy%2CD&amp;author=Odom%2CRD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DR. Begault, T. Erbe, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Begault DR, Erbe T (1994) Multichannel spatial auditory display for speech communications. J Audio Eng Soc 42(" /><p class="c-article-references__text" id="ref-CR4">Begault DR, Erbe T (1994) Multichannel spatial auditory display for speech communications. J Audio Eng Soc 42(10):819–826</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multichannel%20spatial%20auditory%20display%20for%20speech%20communications&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=42&amp;issue=10&amp;pages=819-826&amp;publication_year=1994&amp;author=Begault%2CDR&amp;author=Erbe%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Bellotti, R. Berta, A. Gloria, M. Margarone, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Bellotti F, Berta R, De Gloria A, Margarone M (2002) Using 3D sound to improve the effectiveness of the advanc" /><p class="c-article-references__text" id="ref-CR5">Bellotti F, Berta R, De Gloria A, Margarone M (2002) Using 3D sound to improve the effectiveness of the advanced driver assistance systems. Pers Ubiquitous Comput 6(3):155–163</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs007790200016" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%203D%20sound%20to%20improve%20the%20effectiveness%20of%20the%20advanced%20driver%20assistance%20systems&amp;journal=Pers%20Ubiquitous%20Comput&amp;volume=6&amp;issue=3&amp;pages=155-163&amp;publication_year=2002&amp;author=Bellotti%2CF&amp;author=Berta%2CR&amp;author=Gloria%2CA&amp;author=Margarone%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Berkhout, D. Vries, P. Vogel, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93:2764–2778" /><p class="c-article-references__text" id="ref-CR6">Berkhout AJ, de Vries D, Vogel P (1993) Acoustic control by wave field synthesis. J Acoust Soc Am 93:2764–2778</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.405852" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Acoustic%20control%20by%20wave%20field%20synthesis&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=93&amp;pages=2764-2778&amp;publication_year=1993&amp;author=Berkhout%2CAJ&amp;author=Vries%2CD&amp;author=Vogel%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Blauert, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Blauert J (1997) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR7">Blauert J (1997) Spatial hearing: the psychophysics of human sound localization. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20hearing%3A%20the%20psychophysics%20of%20human%20sound%20localization&amp;publication_year=1997&amp;author=Blauert%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Blauert, R. Rabenstein, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Blauert J, Rabenstein R (2012) Providing surround sound with loudspeakers: a synopsis of current methods. Arch" /><p class="c-article-references__text" id="ref-CR8">Blauert J, Rabenstein R (2012) Providing surround sound with loudspeakers: a synopsis of current methods. Arch Acoust 37(1):5–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2478%2Fv10168-012-0002-y" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Providing%20surround%20sound%20with%20loudspeakers%3A%20a%20synopsis%20of%20current%20methods&amp;journal=Arch%20Acoust&amp;volume=37&amp;issue=1&amp;pages=5-18&amp;publication_year=2012&amp;author=Blauert%2CJ&amp;author=Rabenstein%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AW. Bronkhorst, T. Houtgast, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Bronkhorst AW, Houtgast T (1999) Auditory distance perception in rooms. Nature 397(6719):517–520" /><p class="c-article-references__text" id="ref-CR9">Bronkhorst AW, Houtgast T (1999) Auditory distance perception in rooms. Nature 397(6719):517–520</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F17374" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20distance%20perception%20in%20rooms&amp;journal=Nature&amp;volume=397&amp;issue=6719&amp;pages=517-520&amp;publication_year=1999&amp;author=Bronkhorst%2CAW&amp;author=Houtgast%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brüel PV, Frederiksen E, Rasmussen G (1962) Artificial ears for the calibration of earphones of the external t" /><p class="c-article-references__text" id="ref-CR10">Brüel PV, Frederiksen E, Rasmussen G (1962) Artificial ears for the calibration of earphones of the external type. Technical Report 1, Brüel and Kjær, Nærum, Denmark</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Chen, H. Hu, Z. Wu, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Chen L, Hu H, Wu Z (2008) Head-related impulse response interpolation in virtual sound system. Int Conf Nat Co" /><p class="c-article-references__text" id="ref-CR11">Chen L, Hu H, Wu Z (2008) Head-related impulse response interpolation in virtual sound system. Int Conf Nat Comput 6:162–166</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Head-related%20impulse%20response%20interpolation%20in%20virtual%20sound%20system&amp;journal=Int%20Conf%20Nat%20Comput&amp;volume=6&amp;pages=162-166&amp;publication_year=2008&amp;author=Chen%2CL&amp;author=Hu%2CH&amp;author=Wu%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen M, Villegas J (2015) Applications of audio augmented reality. Wearware, everyware, anyware, and awarewar" /><p class="c-article-references__text" id="ref-CR12">Cohen M, Villegas J (2015) Applications of audio augmented reality. Wearware, everyware, anyware, and awareware, chapter 13. In: Fundamentals of wearable computers and augmented reality, 2nd edn. CRC Press, Boca Raton, pp 309–329</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Doukhan D, Sédès A (2009) CW\_binaural\(^{\sim }\): a binaural synthesis external for pure data. In: Proceedin" /><p class="c-article-references__text" id="ref-CR13">Doukhan D, Sédès A (2009) CW\_binaural<span class="mathjax-tex">\(^{\sim }\)</span>: a binaural synthesis external for pure data. In: Proceedings of 3 Pure-data International Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Estrella J (2010) On the extraction of interaural time differences from binaural room impulse responses. Maste" /><p class="c-article-references__text" id="ref-CR14">Estrella J (2010) On the extraction of interaural time differences from binaural room impulse responses. Master’s thesis, Technische Universität Berlin</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gamper H (2014) Enabling technologies for audio augmented reality systems. PhD thesis, Aalto University" /><p class="c-article-references__text" id="ref-CR15">Gamper H (2014) Enabling technologies for audio augmented reality systems. PhD thesis, Aalto University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gardner WG (1995) Transaural 3-d audio. Technical report, MIT Media Laboratory, Perceptual Computing Section" /><p class="c-article-references__text" id="ref-CR16">Gardner WG (1995) Transaural 3-d audio. Technical report, MIT Media Laboratory, Perceptual Computing Section</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="WG. Gardner, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Gardner WG (1998) 3-D audio using loudspeakers. Springer Science &amp; Business Media, Berlin" /><p class="c-article-references__text" id="ref-CR17">Gardner WG (1998) 3-D audio using loudspeakers. Springer Science &amp; Business Media, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3-D%20audio%20using%20loudspeakers&amp;publication_year=1998&amp;author=Gardner%2CWG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WG. Gardner, KD. Martin, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Gardner WG, Martin KD (1995) Hrtf measurements of a kemar. J Acoust. Soc Am 97(6):3907–3908" /><p class="c-article-references__text" id="ref-CR18">Gardner WG, Martin KD (1995) Hrtf measurements of a kemar. J Acoust. Soc Am 97(6):3907–3908</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.412407" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hrtf%20measurements%20of%20a%20kemar&amp;journal=J%20Acoust.%20Soc%20Am&amp;volume=97&amp;issue=6&amp;pages=3907-3908&amp;publication_year=1995&amp;author=Gardner%2CWG&amp;author=Martin%2CKD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Geier M, Spors S (2012) Spatial audio with the soundscape renderer. In: 27 Tonmeistertagung–VDT International " /><p class="c-article-references__text" id="ref-CR19">Geier M, Spors S (2012) Spatial audio with the soundscape renderer. In: 27 Tonmeistertagung–VDT International Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Gerzon, " /><meta itemprop="datePublished" content="1973" /><meta itemprop="headline" content="Gerzon MA (1973) Periphony: with-height sound reproduction. J Audio Eng Soc 21(1):2–10" /><p class="c-article-references__text" id="ref-CR20">Gerzon MA (1973) Periphony: with-height sound reproduction. J Audio Eng Soc 21(1):2–10</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Periphony%3A%20with-height%20sound%20reproduction&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=21&amp;issue=1&amp;pages=2-10&amp;publication_year=1973&amp;author=Gerzon%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grosjean P, Denis K (2013) mlearning: Machine learning algorithms with unified interface and confusion matrice" /><p class="c-article-references__text" id="ref-CR21">Grosjean P, Denis K (2013) mlearning: Machine learning algorithms with unified interface and confusion matrices. R package version 1.0-0</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MO. Hawksford, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Hawksford MO (1997) Digital signal processing tools for loudspeaker evaluation and discrete-time crossover des" /><p class="c-article-references__text" id="ref-CR22">Hawksford MO (1997) Digital signal processing tools for loudspeaker evaluation and discrete-time crossover design. J Audio Eng Soc 45(1/2):37–62</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Digital%20signal%20processing%20tools%20for%20loudspeaker%20evaluation%20and%20discrete-time%20crossover%20design&amp;journal=J%20Audio%20Eng%20Soc&amp;volume=45&amp;issue=1%2F2&amp;pages=37-62&amp;publication_year=1997&amp;author=Hawksford%2CMO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hemingway P (2002) n-Simplex interpolation. Technical report, Hewlett-Packard Laboratories Bristol" /><p class="c-article-references__text" id="ref-CR23">Hemingway P (2002) n-Simplex interpolation. Technical report, Hewlett-Packard Laboratories Bristol</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hosoe S, Nishino T, Itou K, Takeda K (2005) Measurement of head-related transfer functions in the proximal reg" /><p class="c-article-references__text" id="ref-CR24">Hosoe S, Nishino T, Itou K, Takeda K (2005) Measurement of head-related transfer functions in the proximal region. Forum Acusticum, pp 2539–2542</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ikei Y, Yamazaki H, Hirota K, Hirose M (2006) vCocktail: multiplexed-voice menu presentation method for wearab" /><p class="c-article-references__text" id="ref-CR25">Ikei Y, Yamazaki H, Hirota K, Hirose M (2006) vCocktail: multiplexed-voice menu presentation method for wearable computers. In: Proceedings of Virtual Reality Conference, pp 183–190</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jot J-M, Larcher V, Warusfel O (1995) Digital signal processing issues in the context of binaural and transaur" /><p class="c-article-references__text" id="ref-CR26">Jot J-M, Larcher V, Warusfel O (1995) Digital signal processing issues in the context of binaural and transaural stereophony. In: Proceedings of 98 Audio Engineering Society Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Kan, C. Jin, A. Schaik, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Kan A, Jin C, van Schaik A (2009) A psychophysical evaluation of near-field head-related transfer functions sy" /><p class="c-article-references__text" id="ref-CR27">Kan A, Jin C, van Schaik A (2009) A psychophysical evaluation of near-field head-related transfer functions synthesized using a distance variation function. J Acoust Soc Am 125(4):2233–2242</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.3081395" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20psychophysical%20evaluation%20of%20near-field%20head-related%20transfer%20functions%20synthesized%20using%20a%20distance%20variation%20function&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=125&amp;issue=4&amp;pages=2233-2242&amp;publication_year=2009&amp;author=Kan%2CA&amp;author=Jin%2CC&amp;author=Schaik%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Kearney, M. Gorzel, H. Rice, F. Boland, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Kearney G, Gorzel M, Rice H, Boland F (2012) Distance perception in interactive virtual acoustic environments " /><p class="c-article-references__text" id="ref-CR28">Kearney G, Gorzel M, Rice H, Boland F (2012) Distance perception in interactive virtual acoustic environments using first and higher order ambisonic sound fields. Acta Acust United Acust 98(1):61–71</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3813%2FAAA.918492" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distance%20perception%20in%20interactive%20virtual%20acoustic%20environments%20using%20first%20and%20higher%20order%20ambisonic%20sound%20fields&amp;journal=Acta%20Acust%20United%20Acust&amp;volume=98&amp;issue=1&amp;pages=61-71&amp;publication_year=2012&amp;author=Kearney%2CG&amp;author=Gorzel%2CM&amp;author=Rice%2CH&amp;author=Boland%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kim S, Ikeda M, Takahashi A, Ono Y, Martens WL (2009) Virtual ceiling speaker: elevating auditory imagery in a" /><p class="c-article-references__text" id="ref-CR29">Kim S, Ikeda M, Takahashi A, Ono Y, Martens WL (2009) Virtual ceiling speaker: elevating auditory imagery in a 5-channel reproduction. In: Audio Engineering Society Convention 127. Audio Engineering Society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Majdak P, Iwaya Y, Carpentier T, Nicol R, Parmentier M, Roginska A, Suzuki Y, Watanabe K, Wierstorf H, Ziegelw" /><p class="c-article-references__text" id="ref-CR30">Majdak P, Iwaya Y, Carpentier T, Nicol R, Parmentier M, Roginska A, Suzuki Y, Watanabe K, Wierstorf H, Ziegelwanger H et al. (2013) Spatially oriented format for acoustics: a data exchange format representing head-related transfer functions. In: Proceedings of 134 Audio Engineering Society Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WL. Martens, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Martens WL (2003) Perceptual evaluation of filters controlling source direction: customized and generalized hr" /><p class="c-article-references__text" id="ref-CR31">Martens WL (2003) Perceptual evaluation of filters controlling source direction: customized and generalized hrtfs for binaural synthesis. Acoust Sci Technol 24(5):220–232</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1250%2Fast.24.220" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20evaluation%20of%20filters%20controlling%20source%20direction%3A%20customized%20and%20generalized%20hrtfs%20for%20binaural%20synthesis&amp;journal=Acoust%20Sci%20Technol&amp;volume=24&amp;issue=5&amp;pages=220-232&amp;publication_year=2003&amp;author=Martens%2CWL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McGee R, Wright M (2011) Sound element spatializer. Master’s thesis, University of Michigan" /><p class="c-article-references__text" id="ref-CR32">McGee R, Wright M (2011) Sound element spatializer. Master’s thesis, University of Michigan</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McKinley RL, Ericson MA (1997) Binaural and spatial hearing in real and virtual environments, chapter. In: Fli" /><p class="c-article-references__text" id="ref-CR33">McKinley RL, Ericson MA (1997) Binaural and spatial hearing in real and virtual environments, chapter. In: Flight demonstration of a 3-D auditory display. Lawrence Erlbaum Assoc., Inc., Mahwah, NJ, USA, pp 683–699</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Murphy D, Neff F (2011) Game sound technology and player interaction: concepts and developments, chapter. In: " /><p class="c-article-references__text" id="ref-CR34">Murphy D, Neff F (2011) Game sound technology and player interaction: concepts and developments, chapter. In: Spatial sound for computer games and virtual reality. Information Science Reference, pp 287–312</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Musil T, Noisternig M, Höldrich R (2005) A library for realtime 3D binaural sound reproduction in Pure Data (p" /><p class="c-article-references__text" id="ref-CR35">Musil T, Noisternig M, Höldrich R (2005) A library for realtime 3D binaural sound reproduction in Pure Data (pd). In: Proceedings of International Conference on Digital Audio Effects</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WT. Nelson, LJ. Hettinger, JA. Cunningham, BJ. Brickman, MW. Haas, RL. McKinley, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Nelson WT, Hettinger LJ, Cunningham JA, Brickman BJ, Haas MW, McKinley RL (1998) Effects of localized auditory" /><p class="c-article-references__text" id="ref-CR36">Nelson WT, Hettinger LJ, Cunningham JA, Brickman BJ, Haas MW, McKinley RL (1998) Effects of localized auditory information on visual target detection performance using a helmet-mounted display. Hum Factors J Hum Factors Ergon Soc 40(3):452–460</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872098779591304" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Effects%20of%20localized%20auditory%20information%20on%20visual%20target%20detection%20performance%20using%20a%20helmet-mounted%20display&amp;journal=Hum%20Factors%20J%20Hum%20Factors%20Ergon%20Soc&amp;volume=40&amp;issue=3&amp;pages=452-460&amp;publication_year=1998&amp;author=Nelson%2CWT&amp;author=Hettinger%2CLJ&amp;author=Cunningham%2CJA&amp;author=Brickman%2CBJ&amp;author=Haas%2CMW&amp;author=McKinley%2CRL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S-C. Pei, H-S. Lin, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Pei S-C, Lin H-S (2006) Minimum-phase fir filter design using real cepstrum. IEEE Trans Circuits Syst II Exp B" /><p class="c-article-references__text" id="ref-CR37">Pei S-C, Lin H-S (2006) Minimum-phase fir filter design using real cepstrum. IEEE Trans Circuits Syst II Exp Br 53(10):1113–1117</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTCSII.2006.882193" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Minimum-phase%20fir%20filter%20design%20using%20real%20cepstrum&amp;journal=IEEE%20Trans%20Circuits%20Syst%20II%20Exp%20Br&amp;volume=53&amp;issue=10&amp;pages=1113-1117&amp;publication_year=2006&amp;author=Pei%2CS-C&amp;author=Lin%2CH-S">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JW. Peirce, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Peirce JW (2007) PsychoPy-psychophysics software in python. J Neurosci Methods 162(1–2):8–13" /><p class="c-article-references__text" id="ref-CR38">Peirce JW (2007) PsychoPy-psychophysics software in python. J Neurosci Methods 162(1–2):8–13</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jneumeth.2006.11.017" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=PsychoPy-psychophysics%20software%20in%20python&amp;journal=J%20Neurosci%20Methods&amp;volume=162&amp;issue=1%E2%80%932&amp;pages=8-13&amp;publication_year=2007&amp;author=Peirce%2CJW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Penha R, Oliveira J (2013) Spatium, tools for sound spatialization. In: Proceedings of the Sound and Music Com" /><p class="c-article-references__text" id="ref-CR39">Penha R, Oliveira J (2013) Spatium, tools for sound spatialization. In: Proceedings of the Sound and Music Computing Conference</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Pollow, K-V. Nguyen, O. Warusfel, T. Carpentier, M. Müller-Trapet, M. Vorländer, M. Noisternig, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Pollow M, Nguyen K-V, Warusfel O, Carpentier T, Müller-Trapet M, Vorländer M, Noisternig M (2012) Calculation " /><p class="c-article-references__text" id="ref-CR40">Pollow M, Nguyen K-V, Warusfel O, Carpentier T, Müller-Trapet M, Vorländer M, Noisternig M (2012) Calculation of head-related transfer functions for arbitrary field points using spherical harmonics decomposition. Acta Acust United Acust 98(1):72–82</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3813%2FAAA.918493" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Calculation%20of%20head-related%20transfer%20functions%20for%20arbitrary%20field%20points%20using%20spherical%20harmonics%20decomposition&amp;journal=Acta%20Acust%20United%20Acust&amp;volume=98&amp;issue=1&amp;pages=72-82&amp;publication_year=2012&amp;author=Pollow%2CM&amp;author=Nguyen%2CK-V&amp;author=Warusfel%2CO&amp;author=Carpentier%2CT&amp;author=M%C3%BCller-Trapet%2CM&amp;author=Vorl%C3%A4nder%2CM&amp;author=Noisternig%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Qu, Z. Xiao, M. Gong, Y. Huang, X. Li, X. Wu, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured w" /><p class="c-article-references__text" id="ref-CR41">Qu T, Xiao Z, Gong M, Huang Y, Li X, Wu X (2009) Distance-dependent head-related transfer functions measured with high spatial resolution using a spark gap. IEEE Trans Audio Speech Lang Process 17(6):1124–1132</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTASL.2009.2020532" aria-label="View reference 41">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distance-dependent%20head-related%20transfer%20functions%20measured%20with%20high%20spatial%20resolution%20using%20a%20spark%20gap&amp;journal=IEEE%20Trans%20Audio%20Speech%20Lang%20Process&amp;volume=17&amp;issue=6&amp;pages=1124-1132&amp;publication_year=2009&amp;author=Qu%2CT&amp;author=Xiao%2CZ&amp;author=Gong%2CM&amp;author=Huang%2CY&amp;author=Li%2CX&amp;author=Wu%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EH. Rothauser, WD. Chapman, N. Guttman, HR. Silbiger, MHL. Hecker, GE. Urbanek, KS. Nordby, M. Weinstock, " /><meta itemprop="datePublished" content="1969" /><meta itemprop="headline" content="Rothauser EH, Chapman WD, Guttman N, Silbiger HR, Hecker MHL, Urbanek GE, Nordby KS, Weinstock M (1969) Ieee r" /><p class="c-article-references__text" id="ref-CR42">Rothauser EH, Chapman WD, Guttman N, Silbiger HR, Hecker MHL, Urbanek GE, Nordby KS, Weinstock M (1969) Ieee recommended practice for speech quality measurements. IEEE Trans Audio Electroacoust 17(3):225–246</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTAU.1969.1162058" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ieee%20recommended%20practice%20for%20speech%20quality%20measurements&amp;journal=IEEE%20Trans%20Audio%20Electroacoust&amp;volume=17&amp;issue=3&amp;pages=225-246&amp;publication_year=1969&amp;author=Rothauser%2CEH&amp;author=Chapman%2CWD&amp;author=Guttman%2CN&amp;author=Silbiger%2CHR&amp;author=Hecker%2CMHL&amp;author=Urbanek%2CGE&amp;author=Nordby%2CKS&amp;author=Weinstock%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sanuki W, Villegas J, Cohen M (2014) Spatial sound for mobile navigation systems. In: Proceedings of 136 Audio" /><p class="c-article-references__text" id="ref-CR43">Sanuki W, Villegas J, Cohen M (2014) Spatial sound for mobile navigation systems. In: Proceedings of 136 Audio Engineering Society Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MR. Schroeder, " /><meta itemprop="datePublished" content="1961" /><meta itemprop="headline" content="Schroeder MR (1961) Improved quasi-stereophony and “colorless” artificial reverberation. J Acoust Soc Am 33(8)" /><p class="c-article-references__text" id="ref-CR44">Schroeder MR (1961) Improved quasi-stereophony and “colorless” artificial reverberation. J Acoust Soc Am 33(8):1061–1064</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.1908892" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Improved%20quasi-stereophony%20and%20%E2%80%9Ccolorless%E2%80%9D%20artificial%20reverberation&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=33&amp;issue=8&amp;pages=1061-1064&amp;publication_year=1961&amp;author=Schroeder%2CMR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sèdes A, Guillot P, Paris E (2014) The HOA library, review and prospect. In: Proceedings of the joint ICMC-SMC" /><p class="c-article-references__text" id="ref-CR45">Sèdes A, Guillot P, Paris E (2014) The HOA library, review and prospect. In: Proceedings of the joint ICMC-SMC Conference, pp 855–860</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smith J, Lee N (2008) Computational acoustic modeling with digital delay. Center for Computer Research in Musi" /><p class="c-article-references__text" id="ref-CR46">Smith J, Lee N (2008) Computational acoustic modeling with digital delay. Center for Computer Research in Music and Acoustics, Stanford University. <a href="https://ccrma.stanford.edu/realsimple/Delay/">https://ccrma.stanford.edu/realsimple/Delay/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Sodnik, R. Sušnik, M. Štular, S. Tomažič, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Sodnik J, Sušnik R, Štular M, Tomažič S (2005) Spatial sound resolution of an interpolated hrir library. Appl " /><p class="c-article-references__text" id="ref-CR47">Sodnik J, Sušnik R, Štular M, Tomažič S (2005) Spatial sound resolution of an interpolated hrir library. Appl Acoust 66(11):1219–1234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.apacoust.2005.04.003" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20sound%20resolution%20of%20an%20interpolated%20hrir%20library&amp;journal=Appl%20Acoust&amp;volume=66&amp;issue=11&amp;pages=1219-1234&amp;publication_year=2005&amp;author=Sodnik%2CJ&amp;author=Su%C5%A1nik%2CR&amp;author=%C5%A0tular%2CM&amp;author=Toma%C5%BEi%C4%8D%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Spors, H. Wierstorf, A. Raake, F. Melchior, M. Frank, F. Zotter, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Spors S, Wierstorf H, Raake A, Melchior F, Frank M, Zotter F (2013) Spatial sound with loudspeakers and its pe" /><p class="c-article-references__text" id="ref-CR48">Spors S, Wierstorf H, Raake A, Melchior F, Frank M, Zotter F (2013) Spatial sound with loudspeakers and its perception: a review of the current state. Proc IEEE 101(9):1920–1938</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJPROC.2013.2264784" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20sound%20with%20loudspeakers%20and%20its%20perception%3A%20a%20review%20of%20the%20current%20state&amp;journal=Proc%20IEEE&amp;volume=101&amp;issue=9&amp;pages=1920-1938&amp;publication_year=2013&amp;author=Spors%2CS&amp;author=Wierstorf%2CH&amp;author=Raake%2CA&amp;author=Melchior%2CF&amp;author=Frank%2CM&amp;author=Zotter%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Villegas J, Cohen M (2010a) “Gabriel”: geo-aware broadcasting for in-vehicle entertainment and larger safety. " /><p class="c-article-references__text" id="ref-CR49">Villegas J, Cohen M (2010a) “Gabriel”: geo-aware broadcasting for in-vehicle entertainment and larger safety. In: Proceedings of 135 Audio Engineering Society International Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Villegas, J. and Cohen, M. (2010b) Hrir&#xA;                           ~: modulating range in headphone-reproduced" /><p class="c-article-references__text" id="ref-CR50">Villegas, J. and Cohen, M. (2010b) <span class="u-monospace">Hrir</span>
                           <sup><span class="u-monospace">~</span></sup>: modulating range in headphone-reproduced spatial audio. In: Proceedings of 9 International Conference on VR Continuum and Its Applications in Industry</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Villegas J, Cohen M (2013) Real-time head-related impulse response filtering with distance control. In: Procee" /><p class="c-article-references__text" id="ref-CR51">Villegas J, Cohen M (2013) Real-time head-related impulse response filtering with distance control. In: Proceedings of 135 Audio Engineering Society Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Warusfel O (2003) listen hrtf database. http://recherche.ircam.fr/equipes/salles/listen/&#xA;                     " /><p class="c-article-references__text" id="ref-CR52">Warusfel O (2003) <span class="u-small-caps">listen hrtf</span> database. <a href="http://recherche.ircam.fr/equipes/salles/listen/">http://recherche.ircam.fr/equipes/salles/listen/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wenzel EM, Foster SH (1993) Perceptual consequences of interpolating head-related transfer functions during sp" /><p class="c-article-references__text" id="ref-CR53">Wenzel EM, Foster SH (1993) Perceptual consequences of interpolating head-related transfer functions during spatial synthesis. In: Proceedings of IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, pp 102–105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wierstorf H, Geier M, Spors S (2011) A free database of head related impulse response measurements in the hori" /><p class="c-article-references__text" id="ref-CR54">Wierstorf H, Geier M, Spors S (2011) A free database of head related impulse response measurements in the horizontal plane with multiple distances. In: Proceedings of 130 Audio Engineering Society Convention</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Wright, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Wright M (2005) Open sound control: an enabling technology for musical networking. Organ Sound 10:193–200" /><p class="c-article-references__text" id="ref-CR55">Wright M (2005) Open sound control: an enabling technology for musical networking. Organ Sound 10:193–200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1017%2FS1355771805000932" aria-label="View reference 55">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 55 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Open%20sound%20control%3A%20an%20enabling%20technology%20for%20musical%20networking&amp;journal=Organ%20Sound&amp;volume=10&amp;pages=193-200&amp;publication_year=2005&amp;author=Wright%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xiang P, Camargo D, Puckette M (2005) Experiments on spatial gestures in binaural sound display. In: Proceedin" /><p class="c-article-references__text" id="ref-CR56">Xiang P, Camargo D, Puckette M (2005) Experiments on spatial gestures in binaural sound display. In: Proceedings of 11 International Conference on Auditory Display</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Zahorik, DS. Brungart, AW. Bronkhorst, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Zahorik P, Brungart DS, Bronkhorst AW (2005) Auditory distance perception in humans: a summary of past and pre" /><p class="c-article-references__text" id="ref-CR57">Zahorik P, Brungart DS, Bronkhorst AW (2005) Auditory distance perception in humans: a summary of past and present research. Acta Acust United Acust 91(3):409–420</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20distance%20perception%20in%20humans%3A%20a%20summary%20of%20past%20and%20present%20research&amp;journal=Acta%20Acust%20United%20Acust&amp;volume=91&amp;issue=3&amp;pages=409-420&amp;publication_year=2005&amp;author=Zahorik%2CP&amp;author=Brungart%2CDS&amp;author=Bronkhorst%2CAW">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-015-0278-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was funded by the Competitive Research Funds (P-14) of the University of Aizu.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Computer Arts Laboratory, University of Aizu, Aizu-Wakamatsu, 965-8580, Japan</p><p class="c-article-author-affiliation__authors-list">Julián Villegas</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Juli_n-Villegas"><span class="c-article-authors-search__title u-h3 js-search-name">Julián Villegas</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Juli%C3%A1n+Villegas&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Juli%C3%A1n+Villegas" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Juli%C3%A1n+Villegas%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-015-0278-0/email/correspondent/c1/new">Julián Villegas</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Locating%20virtual%20sound%20sources%20at%20arbitrary%20distances%20in%20real-time%20binaural%20reproduction&amp;author=Juli%C3%A1n%20Villegas&amp;contentID=10.1007%2Fs10055-015-0278-0&amp;publication=1359-4338&amp;publicationDate=2015-10-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-015-0278-0" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-015-0278-0" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Villegas, J. Locating virtual sound sources at arbitrary distances in real-time binaural reproduction.
                    <i>Virtual Reality</i> <b>19, </b>201–212 (2015). https://doi.org/10.1007/s10055-015-0278-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-015-0278-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-02-05">05 February 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-09-30">30 September 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-10-13">13 October 2015</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2015-11">November 2015</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-015-0278-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-015-0278-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Headphone reproduction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Binaural hearing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Localization of virtual sound sources</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Pure-data</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-015-0278-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=278;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

