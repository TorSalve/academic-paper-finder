<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Augmenting moving planar surfaces robustly with video projection and d"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even when they move, but the projection can..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Augmenting moving planar surfaces robustly with video projection and direct image alignment"/>

    <meta name="dc.source" content="Virtual Reality 2012 17:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-04-11"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even when they move, but the projection can severely alter their appearances to the point where traditional computer vision algorithms fail. Current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light. They cannot directly align projector images with real-world surfaces, even though this may be the actual goal. We propose instead to model the light emitted by projectors and reflected into cameras and to consider the displayed content as additional information useful for direct alignment. Using a color camera, our implemented software successfully tracks with subpixel accuracy a planar surface of diffuse reflectance properties at an average of eight frames per second on commodity hardware, providing a solid base for future enhancements."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-04-11"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="157"/>

    <meta name="prism.endingPage" content="168"/>

    <meta name="prism.copyright" content="2012 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-012-0210-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-012-0210-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-012-0210-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-012-0210-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Augmenting moving planar surfaces robustly with video projection and direct image alignment"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2013/06"/>

    <meta name="citation_online_date" content="2012/04/11"/>

    <meta name="citation_firstpage" content="157"/>

    <meta name="citation_lastpage" content="168"/>

    <meta name="citation_article_type" content="SI: Mixed and Augmented Reality"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-012-0210-9"/>

    <meta name="DOI" content="10.1007/s10055-012-0210-9"/>

    <meta name="citation_doi" content="10.1007/s10055-012-0210-9"/>

    <meta name="description" content="Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even wh"/>

    <meta name="dc.creator" content="Samuel Audet"/>

    <meta name="dc.creator" content="Masatoshi Okutomi"/>

    <meta name="dc.creator" content="Masayuki Tanaka"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Audet S, Okutomi M (2009) A user-friendly method to geometrically calibrate projector-camera systems. In: 2009 IEEE conference on computer vision and pattern recognition (CVPR 2009)&#8212;workshops (Procams 2009), IEEE computer society, &#169; 2009 IEEE, pp 47&#8211;54"/>

    <meta name="citation_reference" content="Audet S, Okutomi M, Tanaka M (2010) Direct image alignment of projector-camera systems with planar surfaces. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR 2010), IEEE computer society, &#169; 2010 IEEE"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Lucas-Kanade 20&#160;years on: a unifying framework; citation_author=S Baker, I Matthews; citation_volume=56; citation_issue=1; citation_publication_date=2004; citation_pages=221-255; citation_doi=10.1023/B:VISI.0000011205.11775.fd; citation_id=CR3"/>

    <meta name="citation_reference" content="Baker S, Datta A, Kanade T (2006) Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA"/>

    <meta name="citation_reference" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: 2001 IEEE and ACM international symposium on augmented reality (ISAR 2001), IEEE computer society, p 207"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Groupwise geometric and photometric direct image registration; citation_author=A Bartoli; citation_volume=30; citation_issue=12; citation_publication_date=2008; citation_pages=2098-2108; citation_doi=10.1109/TPAMI.2008.22; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_title=Spatial augmented reality: merging real and virtual worlds; citation_publication_date=2005; citation_id=CR7; citation_author=O Bimber; citation_author=R Raskar; citation_publisher=A. K. Peters, Ltd."/>

    <meta name="citation_reference" content="citation_title=Learning OpenCV: computer vision with the OpenCV library; citation_publication_date=2008; citation_id=CR8; citation_author=G Bradski; citation_author=A Kaehler; citation_publisher=O&#8217;Reilly"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Range imaging with adaptive color structured light; citation_author=D Caspi, N Kiryati, J Shamir; citation_volume=20; citation_issue=5; citation_publication_date=1998; citation_pages=470-480; citation_doi=10.1109/34.682177; citation_id=CR9"/>

    <meta name="citation_reference" content="Chen X, Yang X, Xiao S, Li M (2008) Color mixing property of a projector-camera system. In: Fifth international workshop on projector-camera systems (Procams 2008), ACM, pp 1&#8211;6"/>

    <meta name="citation_reference" content="citation_title=Multiple view geometry in computer vision, second edition; citation_publication_date=2004; citation_id=CR11; citation_author=R Hartley; citation_author=A Zisserman; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="International Electrotechnical Commission (1999) IEC 61966-2-1 (1999-10-18): multimedia systems and equipment&#8212;colour measurement and management&#8212;part 2-1: colour management&#8212;default RGB colour space&#8212;sRGB"/>

    <meta name="citation_reference" content="Johnson T, Fuchs H (2007) Real-time projector tracking on complex geometry using ordinary imagery. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007)&#8212;workshops (Procams 2007), IEEE computer society, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: towards spontaneous and natural interaction in semi-immersive virtual environments. In: 2000 IEEE virtual reality conference (VR 2000), IEEE computer society, pp 13&#8211;20"/>

    <meta name="citation_reference" content="Moritani T, Hiura S, Sato K (2006) Real-time object tracking without feature extraction. In: 2006 IEEE conference on pattern recognition (ICPR 2006), IEEE computer society, Los Alamitos, CA, USA, vol 1, pp 747&#8211;750"/>

    <meta name="citation_reference" content="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: 25th international conference on computer graphics and interactive techniques (SIGGRAPH 98). ACM Press, New York, pp 179&#8211;188"/>

    <meta name="citation_reference" content="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2003) iLamps: geometrically aware and self-configuring projectors. In: ACM transactions on graphics (SIGGRAPH 2003), ACM, pp 809&#8211;818"/>

    <meta name="citation_reference" content="Silveira G, Malis E (2007) Real-time visual tracking under arbitrary illumination changes. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society"/>

    <meta name="citation_reference" content="Sturm P (2000) Algorithms for plane-based pose estimation. In: 2000 IEEE conference on computer vision and pattern recognition (CVPR 2000), IEEE computer society, Hilton Head Island, South Carolina, USA, pp 1706&#8211;1711"/>

    <meta name="citation_reference" content="Sugimoto S, Okutomi M (2007) A direct and efficient method for piecewise-planar surface reconstruction from stereo images. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Comput Vis; citation_title=Tele-graffiti: a camera-projector based remote sketching system with hand-based user interface and automatic session summarization; citation_author=N Takao, J Shi, S Baker; citation_volume=53; citation_issue=2; citation_publication_date=2003; citation_pages=115-133; citation_doi=10.1023/A:1023084706295; citation_id=CR21"/>

    <meta name="citation_reference" content="Triggs B (1998) Autocalibration from planar scenes. In: 5th European conference on computer vision (ECCV &#8217;98), vol I, Springer, Berlin, pp 89&#8211;105"/>

    <meta name="citation_reference" content="Wagner D, Schmalstieg D (2007) ARToolKitPlus for pose tracking on mobile devices. In: 12th computer vision winter workshop (CVWW&#8217;07), Graz University of Technology, St. Lambrecht, Austria"/>

    <meta name="citation_reference" content="Xiao J, Baker S, Matthews I, Kanade T (2004) Real-time combined 2D+3D active appearance models. In: 2004 IEEE conference on computer vision and pattern recognition (CVPR 2004), IEEE computer society, Washinton, DC, USA, vol 2, pp 535&#8211;542"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A flexible new technique for camera calibration; citation_author=Z Zhang; citation_volume=22; citation_issue=11; citation_publication_date=2000; citation_pages=1330-1334; citation_doi=10.1109/34.888718; citation_id=CR25"/>

    <meta name="citation_author" content="Samuel Audet"/>

    <meta name="citation_author_email" content="saudet@ok.ctrl.titech.ac.jp"/>

    <meta name="citation_author_institution" content="Tokyo Institute of Technology, Tokyo, Japan"/>

    <meta name="citation_author" content="Masatoshi Okutomi"/>

    <meta name="citation_author_email" content="mxo@ctrl.titech.ac.jp"/>

    <meta name="citation_author_institution" content="Tokyo Institute of Technology, Tokyo, Japan"/>

    <meta name="citation_author" content="Masayuki Tanaka"/>

    <meta name="citation_author_email" content="mtanaka@ctrl.titech.ac.jp"/>

    <meta name="citation_author_institution" content="Tokyo Institute of Technology, Tokyo, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-012-0210-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-012-0210-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Augmenting moving planar surfaces robustly with video projection and direct image alignment"/>
        <meta property="og:description" content="Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even when they move, but the projection can severely alter their appearances to the point where traditional computer vision algorithms fail. Current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light. They cannot directly align projector images with real-world surfaces, even though this may be the actual goal. We propose instead to model the light emitted by projectors and reflected into cameras and to consider the displayed content as additional information useful for direct alignment. Using a color camera, our implemented software successfully tracks with subpixel accuracy a planar surface of diffuse reflectance properties at an average of eight frames per second on commodity hardware, providing a solid base for future enhancements."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Augmenting moving planar surfaces robustly with video projection and direct image alignment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-012-0210-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Augmented reality, Vision-based tracking, Projector-camera systems, Video projection, Image alignment","kwrd":["Augmented_reality","Vision-based_tracking","Projector-camera_systems","Video_projection","Image_alignment"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-012-0210-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-012-0210-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-5663397ef2.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-177af7d19e.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=210;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-012-0210-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Augmenting moving planar surfaces robustly with video projection and direct image alignment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0210-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0210-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Mixed and Augmented Reality</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-04-11" itemprop="datePublished">11 April 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Augmenting moving planar surfaces robustly with video projection and direct image alignment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Samuel-Audet" data-author-popup="auth-Samuel-Audet" data-corresp-id="c1">Samuel Audet<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tokyo Institute of Technology" /><meta itemprop="address" content="grid.32197.3e, 0000000121792105, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Masatoshi-Okutomi" data-author-popup="auth-Masatoshi-Okutomi">Masatoshi Okutomi</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tokyo Institute of Technology" /><meta itemprop="address" content="grid.32197.3e, 0000000121792105, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Masayuki-Tanaka" data-author-popup="auth-Masayuki-Tanaka">Masayuki Tanaka</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Tokyo Institute of Technology" /><meta itemprop="address" content="grid.32197.3e, 0000000121792105, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">157</span>–<span itemprop="pageEnd">168</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">398 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-012-0210-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even when they move, but the projection can severely alter their appearances to the point where traditional computer vision algorithms fail. Current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light. They cannot directly align projector images with real-world surfaces, even though this may be the actual goal. We propose instead to model the light emitted by projectors and reflected into cameras and to consider the displayed content as additional information useful for direct alignment. Using a color camera, our implemented software successfully tracks with subpixel accuracy a planar surface of diffuse reflectance properties at an average of eight frames per second on commodity hardware, providing a solid base for future enhancements.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Traditional applications of augmented reality superimpose generated images onto the real world through goggles or monitors held between objects of interest and the user. The display must usually follow objects, and developers often choose cameras to perform tracking, as computer vision methods are flexible and non intrusive. Since the augmented objects remain in reality unchanged, we do not need to change the image processing algorithms either. One can directly apply existing computer vision techniques for tracking or other purposes. However, when using video projectors to display computer graphics or data onto surfaces, as exemplified in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig1">1</a>, the appearance of the objects may be severely affected, requiring new methods.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Snapshot of our demo video showing the patterns of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig9">9</a>, where the system has aligned the projector displayed pattern with the printed one (alongside a chronometer, a video animation, and a red virtual ball)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec2">Previous methods</h3><p>To avoid complications, current applications either exploit information channels that do not overlap with the displayed content or make assumptions that restrict their usefulness. Some use fiducial markers, such as iLamps (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2003) iLamps: geometrically aware and self-configuring projectors. In: ACM transactions on graphics (SIGGRAPH 2003), ACM, pp 809–818" href="/article/10.1007/s10055-012-0210-9#ref-CR17" id="ref-link-section-d69270e363">2003</a>), or a special tracking system not based on visible light, such as Dynamic Shader Lamps (Bandyopadhyay et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: 2001 IEEE and ACM international symposium on augmented reality (ISAR 2001), IEEE computer society, p 207" href="/article/10.1007/s10055-012-0210-9#ref-CR5" id="ref-link-section-d69270e366">2001</a>), but users need to paste specially prepared markers to objects they might want to interact with. Other researchers have designed imperceptible structured light, as used in the Office of the Future (Raskar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: 25th international conference on computer graphics and interactive techniques (SIGGRAPH 98). ACM Press, New York, pp 179–188" href="/article/10.1007/s10055-012-0210-9#ref-CR16" id="ref-link-section-d69270e369">1998</a>). However, this not only reduces the dynamic range of the projector, but requires a synchronized projector-camera system running at frequencies higher than 60 Hz to avoid flicker. More simply the Perceptive Workbench (Leibe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: towards spontaneous and natural interaction in semi-immersive virtual environments. In: 2000 IEEE virtual reality conference (VR 2000), IEEE computer society, pp 13–20" href="/article/10.1007/s10055-012-0210-9#ref-CR14" id="ref-link-section-d69270e372">2000</a>) has adopted near-infrared cameras and uses computer vision to track without interference from the projector, but calibrating projectors and cameras whose light spectra do not overlap can be problematic. Although all the above options work, they cannot directly align the displayed content with real-world texture. They solely depend on accurate geometric calibration between the sensors, the projectors, and the real-world objects. Also, the system does not see the same thing as the user, possibly causing confusion when the algorithm fails. In contrast, Tele-Graffiti (Takao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Takao N, Shi J, Baker S (2003) Tele-graffiti: a camera-projector based remote sketching system with hand-based user interface and automatic session summarization. Int J Comput Vis 53(2):115–133" href="/article/10.1007/s10055-012-0210-9#ref-CR21" id="ref-link-section-d69270e375">2003</a>) features a paper tracking algorithm that detects the orientation of a piece of paper or a clipboard inside images captured from a normal camera. The authors designed the algorithm robustly enough to work in the one limited case where the edges of the rectangular object differ markedly from the background, but they still consider the light emitted from the projector as unwanted interference.</p><p>In a nutshell, the performance of markerless tracking leaves to be desired. If augmented reality based on video projection, also known as spatial augmented reality (Bimber and Raskar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., Natick" href="/article/10.1007/s10055-012-0210-9#ref-CR7" id="ref-link-section-d69270e381">2005</a>), is to become the basis for a new paradigm of user interaction, we need more general, robust, and easy-to-use methods.</p><h3 class="c-article__sub-heading" id="Sec3">Proposed method</h3><p>Contrarily to existing approaches that treat the displayed content as interference, we propose to harness its knowledge as additional information that can help the system align it with real-world objects, a method we initially presented at a conference (2010), but have since enhanced both its speed and robustness. More specifically, we derived a direct image alignment algorithm that takes projector illumination into account using a generative model. It models how the light coming out of the projector reflects onto a real-world object and comes back in the camera. As a first step, we decided to make a few simplifying assumptions. Most importantly, the object must be planar and feature diffuse reflectance properties with no specular reflections. Still, thanks to the inherent robustness of direct alignment methods, we found that the algorithm can cope with large amounts of noise. To provide some results, we implemented in software our method for planar surfaces and ran experiments on both simulated and real images. At this time, the program runs in real time at about eight frames per second on commodity hardware, and this restricts the speed at which a user can move the object, but it achieves subpixel accuracy. Future research will revolve around optimization and generalization.</p><p>In the following sections, we first explain in more details the system model and its simplifying assumptions and then the direct image alignment algorithm itself, which minimizes an objective function that requires initialization when presented with a new planar surface. To simplify the explanations, we describe a system with only one camera and one projector, yet it can easily be extended to include any number of them.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">System model</h2><div class="c-article-section__content" id="Sec4-content"><p>A projector-camera system can be treated as a stereo pair, and multiple view geometry (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR11" id="ref-link-section-d69270e402">2004</a>) still applies as is. However, unlike traditional stereo systems made from two or more similar camera devices, color information does not transfer as easily from a projector to a camera. We need a more sophisticated color model. In this section, before explaining the geometric and color models, to simplify them, we start by enumerating assumptions we found useful.</p><h3 class="c-article__sub-heading" id="Sec5">Simplifying assumptions</h3><p>We first assume that the projector and camera are fixed together, like a typical stereo camera, implying that the strong epipolar constraint pertains. The system may nonetheless move with respect to the scene or vice versa, the problem remains unchanged. Secondly, the color responses of both camera and projector must be linear with respect to the light intensity. We consider this to be reasonable for two reasons. On one hand, typical CCD and CMOS sensors respond linearly. On the other hand, most devices today support the sRGB color space standard (IEC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="International Electrotechnical Commission (1999) IEC 61966-2-1 (1999-10-18): multimedia systems and equipment—colour measurement and management—part 2-1: colour management—default RGB colour space—sRGB" href="/article/10.1007/s10055-012-0210-9#ref-CR12" id="ref-link-section-d69270e412">1999</a>) that features a well-defined color response curve approximating a transfer function with a gamma of 2.2, from which the intensities can be linearized. We expect any deviations to be engulfed in the inaccuracies introduced by the unknown light sources and reflectance properties of the surface material, which brings us to the third assumption. The scene consists of a planar object with a matte surface exhibiting uniform diffuse reflectance, where the radiance changes smoothly as the angle and distance vary. In particular, specularities are not modeled. Lastly, all light shining on the plane comes from point sources at infinity, such that no local shadows or highlights appear. Only global changes in illumination are observed, which we refer to as <i>ambient light</i>.</p><h3 class="c-article__sub-heading" id="Sec6">Geometric model</h3><p>To model geometrically both camera and projector devices, we use the familiar pinhole camera model (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR11" id="ref-link-section-d69270e426">2004</a>). Although originally designed for cameras, it also applies to projectors. It maps a (Euclidean) 3D point <b>x</b>
                  <sub>3</sub> of the surface to
</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x}_2 = \hbox{K}_{3 \times 3} (\hbox{R}_{3 \times 3} {\bf x}_3 + {\bf t}_3) , $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>a (homogeneous) 2D point on the image plane of the device, where K, the camera matrix, contains the internal projective parameters or intrinsics, and where R and <b>t</b>, the external parameters or extrinsics, model the orientation and position in 3D space of the device. It follows that the projection onto the image planes of a camera placed at the origin and of a calibrated projector can be respectively written
</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x}_{\rm c}=\hbox{K}_{\rm c} (\hbox{I}\,{\bf x}_3 + {\bf 0}), $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                  <div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x}_{\rm p}=\hbox{K}_{\rm p}(\hbox{R}_{\rm p} {\bf x}_3 + {\bf t}_{\rm p}) . $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                <p>To avoid confusion, we refer to the matrix K as the <i>camera matrix</i> even in the case of a projector. Further, even though equations dealing with homogeneous coordinates are only equal up to an arbitrary scaling factor, we use the equal sign for convenience and clarity.</p><h3 class="c-article__sub-heading" id="Sec7">Color model</h3><p>While the geometric model of a device stands independently on its own, for the color model, we decided instead for simplicity to model only the relationship between devices. Specifically, if a projector pixel shines the color <b>p</b>
                  <sub>p</sub> on a surface point of <i>reflectance</i>
                  <b>p</b>
                  <sub>s</sub>, we expect the corresponding camera pixel to observe the color
</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \hat{{\bf p}}_{\rm c} = {\bf p}_{\rm s} \times [g \hbox{X}_{3 \times 3} {\bf p}_{\rm p} + {\bf a} ] + {\bf b} , $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>g</i> is the <i>gain</i> of the projector light, which we assume varies smoothly and uniformly with the distance and angle to the surface; X is the <i>color mixing matrix</i> as defined by Chen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen X, Yang X, Xiao S, Li M (2008) Color mixing property of a projector-camera system. In: Fifth international workshop on projector-camera systems (Procams 2008), ACM, pp 1–6" href="/article/10.1007/s10055-012-0210-9#ref-CR10" id="ref-link-section-d69270e527">2008</a>), also called the “projector-camera coupling matrix” by Caspi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Caspi D, Kiryati N, Shamir J (1998) Range imaging with adaptive color structured light. IEEE Trans Pattern Anal Mach Intell 20(5):470–480" href="/article/10.1007/s10055-012-0210-9#ref-CR9" id="ref-link-section-d69270e530">1998</a>); <b>a</b>, the ambient light; and <b>b</b>, the noise bias of the camera. All vectors are three-vectors in the RGB color space, and their multiplication is done elementwise. We derived the model directly from the bidirectional reflectance distribution function (BRDF) of a flat matte surface, which Johnson and Fuchs (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Johnson T, Fuchs H (2007) Real-time projector tracking on complex geometry using ordinary imagery. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007)—workshops (Procams 2007), IEEE computer society, pp 1–8" href="/article/10.1007/s10055-012-0210-9#ref-CR13" id="ref-link-section-d69270e540">2007</a>) have shown to be valid in the case of projector-camera systems. Although the equation explicitly models the relation between only one camera and one projector, as long as one identifies the reference device, the model can adapt to any number of them.</p><h3 class="c-article__sub-heading" id="Sec8">Image formation model</h3><p>Using the geometric and color models, we can simulate how an image forms on the sensor of the camera. The explanation that follows is also summarized in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig2">2</a>. Assuming the geometric parameters <b>n</b> of the plane are known and given a 3D point <b>x</b>
                  <sub>3</sub> on the surface plane such that <b>n</b>
                  <sup>T</sup>
                  <b>x</b>
                  <sub>3</sub> + 1 = 0, it follows that the 2D point <b>x</b>
                  <sub>p</sub> from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ3">3</a> can also be expressed as
</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {\bf x}_{\rm p} &amp; = \hbox{K}_{\rm p}\, (\hbox{R}_{\rm p} {\bf x}_3 - {\bf t}_{\rm p} {\bf n}^{\rm T} {\bf x}_3 )\,\hbox { since } {\bf n}^{\rm T} {\bf x}_3 = -1 \\ &amp; = \hbox{K}_{\rm p}\,(\hbox{R}_{\rm p} - {\bf t}_{\rm p} {\bf n}^{\rm T})\,{\bf x}_3 \\ &amp; =\hbox{K}_{\rm p}\,(\hbox{R}_{\rm p} - {\bf t}_{\rm p} {\bf n}^{\rm T})\,\hbox{K}_{\rm c}^{-1}\,{\bf x}_{\rm c} \\ &amp; = \qquad\quad\hbox{H}_{\rm pc}\qquad\qquad{\bf x}_{\rm c} ,\\ \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where the before last substitution comes from the fact that homogeneous vector <b>x</b>
                  <sub>c</sub> from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ2">2</a> can be arbitrarily scaled to fit in. The matrix H<sub>pc</sub> embodies a homography, from which we define the <i>camera-to-projector warping function</i>, corresponding to step 1 in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig2">2</a>:
</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf w}_{\rm p}({\bf x}_{\rm c}) \equiv \hbox{H}_{\rm pc}\,{\bf x}_{\rm c} . $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Sketch of the image formation model (Audet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Audet S, Okutomi M, Tanaka M (2010) Direct image alignment of projector-camera systems with planar surfaces. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR 2010), IEEE computer society, © 2010 IEEE" href="/article/10.1007/s10055-012-0210-9#ref-CR2" id="ref-link-section-d69270e636">2010</a>) © 2010 IEEE</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Similarly, one can map a camera image point <b>x</b>
                  <sub>c</sub> onto a point <b>x</b>
                  <sub>s</sub> of the image of the surface plane, assumed to have been initialized at a prior moment using the same camera. The camera motion R<sub>s</sub>, <b>t</b>
                  <sub>s</sub> required to return to the prior orientation and position can be seen as a second camera (one can think of it as the “surface camera”) with the same set of internal parameters, but with different external parameters, as follows:
</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {\bf x}_{\rm s} &amp; = \hbox{K}_{\rm c}\,(\hbox{R}_{\rm s} {\bf x}_3 - {\bf t}_{\rm s} {\bf n}^{\rm T} {\bf x}_3 )\hbox { since }{\bf n}^{\rm T} {\bf x}_3 = \hbox {-}1\\ &amp; = \hbox{K}_{\rm c}\,(\hbox{R}_{\rm s} -{\bf t}_{\rm s} {\bf n}^{\rm T}){\bf x}_3\\ &amp; =\hbox{K}_{\rm c}\,(\hbox{R}_{\rm s} - {\bf t}_{\rm s} {\bf n}^{\rm T})\hbox{K}_{\rm c}^{-1}{\bf x}_{\rm c} \\ &amp; =\qquad\quad\hbox{H}_{\rm sc}\qquad{\bf x}_{\rm c} , \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>which again becomes a homography H<sub>sc</sub> from which we define the <i>camera-to-surface warping function</i>, corresponding to step 2 in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig2">2</a>:
</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf w}_{\rm s}({\bf x}_{\rm c}) \equiv \hbox{H}_{\rm sc}{\bf x}_{\rm c} . $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                <p>Finally, plugging these coordinates into the color model of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ4">4</a> by considering the pixel colors <b>p</b>
                  <sub>c</sub>, <b>p</b>
                  <sub>s</sub>, and <b>p</b>
                  <sub>p</sub> as functions over the images, we expect the pixel color at point <b>x</b>
                  <sub>c</sub> of the camera to be
</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \hat{{\bf p}}_{\rm c}({\bf x}_{\rm c}) = {\bf p}_{\rm s}({\bf w}_{\rm s}({\bf x}_{\rm c})) \times [g \hbox{X} {\bf p}_{\rm p}({\bf w}_{\rm p}({\bf x}_{\rm c})) + {\bf a}] + {\bf b} , $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>reflecting the final third and fourth steps in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig2">2</a>.</p><h3 class="c-article__sub-heading" id="Sec9">Calibration</h3><p>To estimate the parameters that these models require to function, we proceed with calibration in a manner similar to current methods for both geometric parameters K<sub>c</sub>, K<sub>p</sub>, R<sub>p</sub>, and <b>t</b>
                  <sub>p</sub> (Audet and Okutomi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Audet S, Okutomi M (2009) A user-friendly method to geometrically calibrate projector-camera systems. In: 2009 IEEE conference on computer vision and pattern recognition (CVPR 2009)—workshops (Procams 2009), IEEE computer society, © 2009 IEEE, pp 47–54" href="/article/10.1007/s10055-012-0210-9#ref-CR1" id="ref-link-section-d69270e767">2009</a>; Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334" href="/article/10.1007/s10055-012-0210-9#ref-CR25" id="ref-link-section-d69270e770">2000</a>) and color parameters X and <b>b</b> (Caspi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Caspi D, Kiryati N, Shamir J (1998) Range imaging with adaptive color structured light. IEEE Trans Pattern Anal Mach Intell 20(5):470–480" href="/article/10.1007/s10055-012-0210-9#ref-CR9" id="ref-link-section-d69270e776">1998</a>; Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Chen X, Yang X, Xiao S, Li M (2008) Color mixing property of a projector-camera system. In: Fifth international workshop on projector-camera systems (Procams 2008), ACM, pp 1–6" href="/article/10.1007/s10055-012-0210-9#ref-CR10" id="ref-link-section-d69270e779">2008</a>), considering the black offset of the projector as part of the ambient light <b>a</b> and linearizing the color responses assuming they follow the sRGB standard (IEC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="International Electrotechnical Commission (1999) IEC 61966-2-1 (1999-10-18): multimedia systems and equipment—colour measurement and management—part 2-1: colour management—default RGB colour space—sRGB" href="/article/10.1007/s10055-012-0210-9#ref-CR12" id="ref-link-section-d69270e786">1999</a>), which produces in our experience sufficiently linear responses, allowing us to calibrate fully a system from scratch in less than 1 min.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Direct image alignment</h2><div class="c-article-section__content" id="Sec10-content"><p>Based on the simplifying assumptions listed previously and the models for projector-camera systems illustrated above, we designed an iterative method to align directly their images. In this section, we explain the objective function and the algorithm to minimize it, which can hopefully converge to the correct alignment under normal circumstances. Following this, we provide details as to how one may initialize the algorithm for a new surface plane. However, we assume at first that the reflectance <b>p</b>
                <sub>s</sub> and the initial parameters <b>n</b> of the surface plane found during initialization are known.</p><h3 class="c-article__sub-heading" id="Sec11">Objective function</h3><p>To align best the images, we seek an optimal set of parameters. Intuitively, these parameters should minimally model the relative motion between the surface plane and the projector-camera system, since all other parameters are determined either during calibration or initialization. Looking back at Eqs. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ6">6</a> and <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ8">8</a>, these unknown parameters relating to motion are R<sub>s</sub> and <b>t</b>
                  <sub>s</sub>, a typical egomotion problem, which represents six degrees of freedom. Four others come from the gain <i>g</i> and the ambient light <b>a</b>, which we assumed may change during motion, for a total of ten degrees of freedom and consequently ten parameters to optimize. We can thus formulate our goal into an objective function to minimize, which basically consists of the norm of the residual between the real observed camera image <b>p</b>
                  <sub>c</sub> and the expected one as defined by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ9">9</a>, that is:
</p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(\varvec{\uptheta}, g, {\bf a}) = \sum_{i=1}^l || {\bf r}_i(\varvec{\uptheta}, g, {\bf a}) ||^2, \hbox{ where} $$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div>
                  <div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} {\bf r}_i(\varvec{\uptheta}, g,{\bf a})&amp; = {\bf p}_{\rm c}({\bf x}_{{\rm c},i}) - {\bf p}_{\rm s}({\bf w}_{\rm s}({\bf x}_{{\rm c},i} ; \varvec{\uptheta})) \\  &amp;\qquad\qquad\times[g \hbox{X} {\bf p}_{\rm p}({\bf w}_{\rm p}({\bf  x}_{\rm{c},i} ; \varvec{\uptheta})) + {\bf a}] - {\bf b}, \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>for the <i>l</i> pixels inside the region of interest (ROI) of the camera image with coordinates <b>x</b>
                  <sub>c,</sub>
                  <sub>
                    <i>i</i>
                  </sub>, using bilinear interpolation to resample, and where <span class="mathjax-tex">\(\varvec{\uptheta}\)</span> denotes the geometric parameters R<sub>s</sub> and <b>t</b>
                  <sub>s</sub>. Its minimization amounts to the maximum likelihood estimate (MLE) under the assumption of Gaussian noise. We chose the two-norm to simplify the implementation of the traditional Gauss-Newton optimization algorithm favored for image alignment problems (Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255" href="/article/10.1007/s10055-012-0210-9#ref-CR3" id="ref-link-section-d69270e901">2004</a>), but more robust norms could also be used.</p><h3 class="c-article__sub-heading" id="Sec12">Analysis of local minima</h3><p>To minimize the objective function of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ11">11</a>, while one could optimize the 3D motion parameters directly, this parameterization lacks robustness. First, the inherent ambiguity between 3D rotation and translation may cause incorrect convergence (Baker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Baker S, Datta A, Kanade T (2006) Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA" href="/article/10.1007/s10055-012-0210-9#ref-CR4" id="ref-link-section-d69270e915">2006</a>). Second, while we may argue that to solve this issue we could place the origin somewhere in the middle of the planar surface, numerous local minima remain around the global minimum of the objective function. As an example of this, consider the sketches of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>. In this case, the halfway configuration associated with 3D parameters usually has an error larger than the initial one, rendering it a local minimum. This does not happen with the 2D parameterization, making it more robust under local minimization. As a consequence, we no longer recommend the hard constraint applied via a Lagrange multiplier proposed in our previous paper (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Audet S, Okutomi M, Tanaka M (2010) Direct image alignment of projector-camera systems with planar surfaces. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR 2010), IEEE computer society, © 2010 IEEE" href="/article/10.1007/s10055-012-0210-9#ref-CR2" id="ref-link-section-d69270e921">2010</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Sample configurations (<i>dashed blue line</i>) an optimizer may encounter, with a plane rotated to the right as initial alignment and a plane rotated to the left as target alignment (<i>continuous red line</i>), using as parameterizations for the objective function either 3D rotation and translation (<i>top row</i>) or a 2D homography (<i>bottom row</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For all these reasons, when minimizing the objective function, our new algorithm warps images using 2D objects exclusively. However, doing so, <b>w</b>
                  <sub>s</sub>(<b>x</b>
                  <sub>c</sub>) loses its 3D relation with <b>w</b>
                  <sub>p</sub>(<b>x</b>
                  <sub>c</sub>), and we need to model them separately. Naturally, we implemented the first warp as the homography H<sub>sc</sub> and the second as a traditional stereo warp as expressed by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ5">5</a>. Although the latter uses plane parameters, a 3D entity, the epipolar constraint limits motion to 1D lines, so it produces in reality well-behaved 2D warps, as confirmed by Baker et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Baker S, Datta A, Kanade T (2006) Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA" href="/article/10.1007/s10055-012-0210-9#ref-CR4" id="ref-link-section-d69270e984">2006</a>). Furthermore, to obtain even better convergence properties based on their other findings, we parameterize the homography using four points, and the plane parameters using three disparities, all of which measured in pixel units. For the homography parameterization, we exploited directly the direct linear transformation (DLT) algorithm (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR11" id="ref-link-section-d69270e988">2004</a>), which takes four pairs of points as input and outputs a homography matrix. To convert back and forth between three disparities and the plane parameters, we derived another algorithm that works in a similar fashion.</p><p>In particular, we are interested in recovering the plane parameters <b>n</b> from three pairs of corresponding points <b>x</b>
                  <sub>c</sub> and <b>x</b>
                  <sub>p</sub>. Rearranging Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ5">5</a>, and using the cross-product and skew-symmetric matrix notation to obtain as usual a true equality, we find that</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x'}_{\rm p} \times (\hbox{R}_{\rm p} - {\bf t}_{\rm p} {\bf n}^{\rm T}){\bf x'}_{\rm c} = {\bf 0} , $$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div>
                  <div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf x'}_{\rm p} \times {\bf t}_{\rm p} {\bf n}^{\rm T} {\bf x'}_{\rm c} = {\bf x'}_{\rm p} \times \hbox{R}_{\rm p} {\bf x'}_{\rm c} , $$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div>
                  <div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ [{\bf x'}_{\rm p}]_\times \, {\bf t}_{\rm p}{\bf x'}_{\rm c}^{\rm T}\, {\bf n} = [{\bf x'}_{\rm p}]_\times \, \hbox{R}_{\rm p}{\bf x'}_{\rm c} . $$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p>where <b>x′</b>
                  <sub>p</sub> = K<span class="c-stack">
                    <sup>−1</sup><sub>p</sub>
                    
                  </span> <b>x</b>
                  <sub>p</sub> and <b>x′</b>
                  <sub>c</sub> = K<span class="c-stack">
                    <sup>−1</sup><sub>c</sub>
                    
                  </span> <b>x</b>
                  <sub>c</sub>. We arbitrarily choose the second row from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ14">14</a>, which provides one constraint based on the <i>x</i>-coordinates. (Although this does not correspond to the exact definition of the stereo disparity, one may easily project onto the <i>x</i>-axis a given disparity value). Stacking three of these rows for three different pairs of points results in a linear system of the form A<b>x</b> = <b>b</b> with, in general, a single unique solution. Intuitively, this works because three points determine a unique plane. The three reference points in the source image <b>x</b>
                  <sub>c</sub> provide the (<i>x</i>, <i>y</i>) coordinates, while their <i>z</i>-coordinates come from their depths, which vary directly proportionally to their disparities with <b>x</b>
                  <sub>p</sub>.</p><p>Even though the stereo warp works well in general, it may be stumped in cases such as the one illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>, which cause large disparities, or when the projector image lacks in texture. To prevent those cases from causing trouble, after each iteration of the minimizer, our algorithm tries to reset the plane parameters to their initial values <b>n</b> transformed by the closest 3D rotation and translation to the homography H<sub>sc</sub> as documented by Sturm (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sturm P (2000) Algorithms for plane-based pose estimation. In: 2000 IEEE conference on computer vision and pattern recognition (CVPR 2000), IEEE computer society, Hilton Head Island, South Carolina, USA, pp 1706–1711" href="/article/10.1007/s10055-012-0210-9#ref-CR19" id="ref-link-section-d69270e1133">2000</a>).</p><h3 class="c-article__sub-heading" id="Sec13">The subspace error</h3><p>Although 2D warps have less local minima, they are bound to produce physically impossible configurations such as the one of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>e, for which there are in reality only six degrees of freedom as 3D rotation and translation, so we devised a distance function to measure this <i>subspace error</i> and let the minimizer use the available 3D information as well for additional robustness. As explained in the previous section, these physical impossibilities also mean that a given 2D homography (eight parameters) may have no corresponding stereo warp (three parameters), so we need to optimize in fact a total of eleven 2D parameters. In the following equations, to differentiate the 2D geometric parameters from the 3D ones, we will denote them by the vectors <span class="mathjax-tex">\(\varvec{\uptheta}_2\)</span> and <span class="mathjax-tex">\(\varvec{\uptheta}_3\)</span>, respectively, the latter being equivalent to the <span class="mathjax-tex">\(\varvec{\uptheta}\)</span> first defined under Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ11">11</a>. The former contains coordinates displaced from their reference points <b>x</b>
                  <span class="c-stack">
                    <sup>0</sup><sub>s,1…<i>m</i>
                    </sub>
                    
                  </span> for the surface (<i>m</i> = 4 in this case) and <b>x</b>
                  <span class="c-stack">
                    <sup>0</sup><sub>p,1…<i>n</i>
                    </sub>
                    
                  </span> for the projector image (<i>n</i> = 3 in this case). As reference points for the surface, the four corners of a quadrilateral ROI provide the perfect match, while for the projector ones, we decided to take the two top corners and the bottom middle point. For a 1024 × 768 projector image, those would be (0, 0), (1024, 0), and (512, 768).</p><p>Generally, we want both sets of parameters to produce transformations close to one another, so we define a distance function such that <span class="mathjax-tex">\(d(\varvec{\uptheta}_2, \varvec{\uptheta}_3) = 0\)</span> if and only if <span class="mathjax-tex">\(\varvec{\uptheta}_2\)</span> and <span class="mathjax-tex">\(\varvec{\uptheta}_3\)</span> produce the same transformation, while a nonzero value indicates some proportional amount of subspace error. Since we would like this error to be as close as possible to zero, while still allowing our algorithm to converge in situations such as the one of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>, we simply append it to our objective function</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f_d(\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a}) = f(\varvec{\uptheta}_2, g, {\bf a}) + k d(\varvec{\uptheta}_2, \varvec{\uptheta}_3), $$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>along with an adjustable coefficient <i>k</i> that we need to find experimentally. We chose to measure the error in pixel units by warping (inversely) the reference points, of the point-based parameterization described above, with parameters <span class="mathjax-tex">\(\varvec{\uptheta}_2, \)</span> then <span class="mathjax-tex">\(\varvec{\uptheta}_3, \)</span> and by comparing the resulting coordinates. Expanding this equation a bit more under this definition, we discover that it reduces to a sum of squared error terms that fits well within the least squares framework:</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} f_d(\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a}) &amp;= \sum_{i=1}^l || {\bf r}_i(\varvec{\uptheta}_2, g, {\bf a}) ||^2 \, \\ &amp;\quad + k \left(\sum_{i=1}^m || {\bf d}_{\rm{s},\it{i}} (\varvec{\uptheta}_2, \varvec{\uptheta}_3) ||^2 + \sum_{i=1}^n || d_{\rm{p},\it{i}}(\varvec{\uptheta}_2, \varvec{\uptheta}_3) ||^2\right), \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (16)
                </div></div>
                  <div id="Equ17" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf d}_{\rm{s},\it{i}}(\varvec{\uptheta}_2, \varvec{\uptheta}_3) ={\bf w}_{\rm s}^{-1}({\bf x}^0_{\rm{s},\it{i}} ; \varvec{\uptheta}_2) -{\bf w}_{\rm s}^{-1}({\bf x}_{\rm{s},\it{i}}^0 ; \varvec{\uptheta}_3), $$</span></div><div class="c-article-equation__number">
                    (17)
                </div></div>
                  <div id="Equ18" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ d_{\rm{p},\it{i}}(\varvec{\uptheta}_2, \varvec{\uptheta}_3) = w_{\rm{p},\it{x}}^{-1}({\bf x}_{\rm{p},\it{i}}^0 ; \varvec{\uptheta}_2) - w_{\rm{p},\it{x}}^{-1}({\bf x}_{\rm{p},\it{i}}^0 ; \varvec{\uptheta}_3), $$</span></div><div class="c-article-equation__number">
                    (18)
                </div></div><p>for the <i>l</i> pixels inside the ROI of the camera image.</p><p>Xiao et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Xiao J, Baker S, Matthews I, Kanade T (2004) Real-time combined 2D+3D active appearance models. In: 2004 IEEE conference on computer vision and pattern recognition (CVPR 2004), IEEE computer society, Washinton, DC, USA, vol 2, pp 535–542" href="/article/10.1007/s10055-012-0210-9#ref-CR24" id="ref-link-section-d69270e1346">2004</a>) proposed a similar method attempting to impose a hard constraint using a “suitably large value for <i>k</i>,” but we realized that this may not be actually desirable. In fact, compared to a constant coefficient, we found that an adaptive one could work more efficiently. To balance the weight of the subspace error, it makes sense to base the coefficient on the residual error of the pixel values, normalized by the dimensionality of the subspace term (2<i>m</i> + <i>n</i>), that is:</p><div id="Equ19" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ k = \alpha^2 \, \frac{f(\varvec{\uptheta}_2, g, {\bf a})}{2 m + n} . $$</span></div><div class="c-article-equation__number">
                    (19)
                </div></div><p>The rationale behind this equation derives from our previously explained observations of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>. For configurations where the residual error is low, but where the subspace error may be high, such as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig3">3</a>e, this coefficient effectively lessens its importance, as desired. On the other hand, in the case of high residual error, we would like our algorithm to try to make use of the additional 3D information found in the subspace term. To accommodate large displacements, such as the ones tested during our simulation experiments, values of α ≈ 0.1 work well, while values as high as 100 may be used to encourage faster convergence in cases when one does not anticipate large displacements of otherwise noisy data, such as with real images streaming from a video camera.</p><h3 class="c-article__sub-heading" id="Sec14">Minimization</h3><p>Taking all the above into consideration, to minimize iteratively the augmented objective function of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ16">16</a>, our Gauss-Newton update equation looks like this:</p><div id="Equ20" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ (\hbox{J}_{\bf r}^{\rm T}\hbox{J}_{\bf r} + k \hbox{J}_{\bf d}^{\rm T}\hbox{J}_{\bf d}) \varvec{\Updelta}_{\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a}} = - (\hbox{J}_{\bf r}^{\rm T}{\bf r} + k \hbox{J}_{\bf d}^{\rm T}{\bf d}) . $$</span></div><div class="c-article-equation__number">
                    (20)
                </div></div><p>where <b>r</b> is the vector of residual functions <b>r</b>
                  <sub>
                    <i>i</i>
                  </sub>, and <b>d</b>, the one for the subspace error functions <b>d</b>
                  <sub>s,</sub>
                  <sub>
                    <i>i</i>
                  </sub> and <i>d</i>
                  <sub>p,</sub>
                  <sub>
                    <i>i</i>
                  </sub>, while J<sub>
                    <b>r</b>
                  </sub> and J<sub>
                    <b>d</b>
                  </sub> are their Jacobian matrices.</p><p>For simplicity and contrarily to Lucas and Kanade (Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255" href="/article/10.1007/s10055-012-0210-9#ref-CR3" id="ref-link-section-d69270e1457">2004</a>), we decided to estimate the Jacobian J<sub>
                    <b>r</b>
                  </sub> numerically using finite difference with a step δ = 0.1. In other words, its <i>i</i>th column</p><div id="Equ21" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf j}_{{\bf r},i}(\varvec{\Uptheta}) = \frac{{\bf r}(\varvec{\Uptheta} + \delta {\bf e}_i) - {\bf r}(\varvec{\Uptheta})}{\delta} , $$</span></div><div class="c-article-equation__number">
                    (21)
                </div></div><p>and similarly for J<sub>
                    <b>d</b>
                  </sub> (Moritani et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Moritani T, Hiura S, Sato K (2006) Real-time object tracking without feature extraction. In: 2006 IEEE conference on pattern recognition (ICPR 2006), IEEE computer society, Los Alamitos, CA, USA, vol 1, pp 747–750" href="/article/10.1007/s10055-012-0210-9#ref-CR15" id="ref-link-section-d69270e1489">2006</a>), where <span class="mathjax-tex">\(\varvec{\Uptheta} = (\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a})\)</span> and <b>e</b>
                  <sub>
                    <i>i</i>
                  </sub> is a unit vector whose <i>i</i>th element equals one. This avoids the need to differentiate analytically the objective function or to precompute image gradients with respect to the axes, which has to be done numerically in any case using an operator such as the Sobel filter. Rather, since we parameterize all our geometric warps in pixel units, we may skip these operations, and evaluating the Jacobian amounts to computing image gradients directly with respect to the parameters, in exactly the right way. Computationally, our approach requires warping eleven images per iteration versus only four gradient images for the more traditional approach, but ours does not need to precompute those and does not require the (cache) memory to hold them either. Additionally, we need to consider other important characteristics such as accuracy and numerical stability. Although we feel confident our approach provides good results, we plan to investigate the matter in the future to better understand the pros and cons.</p><p>For additional robustness and performance, we implemented the traditional coarse-to-fine multiresolution estimation, where each level higher in the resolution pyramid is first smoothed with a 5 × 5 Gaussian filter and then subsampled by a factor of two. We achieved best results using from five to seven levels, depending on size of the ROI, the largest anticipated displacement, and the quality of the textures. For an initial image size of 1024 × 768 pixels, this means resolutions down to as low as 16 × 12 pixels.</p><p>To accelerate the evaluation of the Jacobian J<sub>
                    <b>r</b>
                  </sub> itself, we introduced <i>zero thresholds</i> into the differentiation. Since we assume relatively small displacements, the residuals <b>r</b>
                  <sub>
                    <i>i</i>
                  </sub> are often close to zero. The Jacobian intrinsically models small displacements, so the intuition is that, for a given pixel number <i>i</i>, if <b>r</b>
                  <sub>
                    <i>i</i>
                  </sub> ≈ <b>0</b> ⇒J<sub>
                    <b>r</b>,</sub>
                  <sub>
                    <i>i</i>
                  </sub> ≈ <b>0</b> for all parameters with high probability. Therefore, when the algorithm finds the magnitude of <b>r</b>
                  <sub>
                    <i>i</i>
                  </sub> to be below a certain <span class="mathjax-tex">\(\epsilon, \)</span> we let it skip the computation of the derivatives for that pixel and simply set them to zeros. However, the value of this threshold is influenced by the amount of noise, which in turn reduces at each level higher in the resolution pyramid. Once again, the value of the objective function gives us for free a good indication of the noise level as the root mean square error (RMSE):</p><div id="Equ22" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \epsilon = t \sqrt{\frac{f(\varvec{\uptheta}_2, g, {\bf a})}{l}} , $$</span></div><div class="c-article-equation__number">
                    (22)
                </div></div><p>where values for <i>t</i> in the range [0, 1] give usable results, with the largest one frequently skipping more than half of the pixels for an acceleration factor of about two. Moreover, since heavy processing occurs mostly in the lower pyramid levels, we found that setting bigger thresholds for those gives more optimal results.</p><p>Even with zero thresholds, computing the Jacobian takes at least an order of magnitude more time than the residual image. To help our algorithm make progress faster, we included on top of everything a simple backtracking line search strategy, which splits iteratively the update length in half until the error of the objective function <span class="mathjax-tex">\(f_d(\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a})\)</span> diminishes or until it meets a stopping criterion. To not stop, the update has to be large enough, but not too large to start with either, that is:</p><div id="Equ23" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \Updelta_{\rm min} &lt; || \varvec{\Updelta}_{\varvec{\uptheta}_2, \varvec{\uptheta}_3, g, {\bf a}} || &lt; \Updelta_{\rm max}, $$</span></div><div class="c-article-equation__number">
                    (23)
                </div></div><p>where <span class="mathjax-tex">\(\Updelta_{\rm min} \approx 0.1\)</span> for good accuracy such as used in the simulation experiments or, for faster convergence in the case of, for example, real-time applications, <span class="mathjax-tex">\(\Updelta_{\rm min} \approx 10. \)</span> We also define an upper bound <span class="mathjax-tex">\(\Updelta_{\rm max} \approx 300, \)</span> determined according to the image sizes mainly, to detect obvious instances of divergence.</p><h3 class="c-article__sub-heading" id="Sec15">Initialization</h3><p>The above minimization algorithm works only if the reflectance map <b>p</b>
                  <sub>s</sub> is known. The initial surface plane parameters <b>n</b> should also be acquired somehow for better robustness. Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ4">4</a> shows that solving for the two unknowns <b>p</b>
                  <sub>s</sub> and <b>a</b> requires capturing at least two images from the camera. Although there are undoubtedly many possible ways to perform initialization, we designed an approach that can cope with small movements, allowing users to hold the surface plane in their hands. This approach works in three phases, by first estimating the ambient light <b>a</b>, then the reflectance map <b>p</b>
                  <sub>s</sub> and finally the initial geometric plane parameters <b>n</b>.</p><p>Concretely, the procedure goes as follows. It starts by the projection and the capture as fast as possible of three consecutive shots, to obtain images with the smallest interframe motion possible, from which the user may select a suitable ROI. For the first image, the system sets the projector color <b>p</b>
                  <sub>p</sub> to zero, and for the second, to maximum intensity <b>p</b>
                  <span class="c-stack">
                    <sup>max</sup><sub>p</sub>
                    
                  </span>. (More details about the third image below). Taking the first two images, it can then estimate the ambient light by defining the reference gain <i>g</i> = 1 and isolating <b>a</b> from Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ4">4</a>:</p><div id="Equ24" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf a} = \hbox{X}{\bf p}_{\rm p}^{\rm max} \frac{{\bf p}_{\rm c}^1 - {\bf b}}{ {\bf p}_{\rm c}^2 - {\bf p}_{\rm c}^1 } , $$</span></div><div class="c-article-equation__number">
                    (24)
                </div></div><p>where <b>p</b>
                  <span class="c-stack">
                    <sup>1</sup><sub>c</sub>
                    
                  </span> is the color from the first camera image, and <b>p</b>
                  <span class="c-stack">
                    <sup>2</sup><sub>c</sub>
                    
                  </span>, from the second image. To reduce the undesirable effect of motion, we use at this phase severely smoothed versions of the first two images, by convolving with a 51 × 51 Gaussian kernel for example, which is reasonable since we assumed that ambient light varies only globally. This assumption also allows the system to ignore points where <b>p</b>
                  <span class="c-stack">
                    <sup>2</sup><sub>c</sub>
                    
                  </span> − <b>p</b>
                  <span class="c-stack">
                    <sup>1</sup><sub>c</sub>
                    
                  </span> is too close to zero and to evaluate only the average.</p><p>The second phase consists of using the second image, this time the original crisp version, along with the estimated ambient light to recover a crisp reflectance map</p><div id="Equ25" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf p}_{\rm s} = \frac{{\bf p}_{\rm c}^2 - {\bf b}}{\hbox{X}{\bf p}_{\rm p}^{\rm max} + {\bf a} } . $$</span></div><div class="c-article-equation__number">
                    (25)
                </div></div>
                <p>For the third phase, the idea is to actually run the minimization algorithm described in the previous subsection, optimizing for <span class="mathjax-tex">\(\varvec{\uptheta}_2, \)</span>
                  <i>g</i>, and <b>a</b>, with α = 0, and extracting the initial <b>n</b> from <span class="mathjax-tex">\(\varvec{\uptheta}_2.\)</span> As initial values, one can reasonably assume <i>g</i> = 1, reuse the ambient light computed in the first phase, and compose <span class="mathjax-tex">\(\varvec{\uptheta}_2\)</span> with the coordinates of the ROI plus the extreme left, right, and middle <i>x</i>-coordinates, in other words matching the projector reference points <b>x</b>
                  <span class="c-stack">
                    <sup>0</sup><sub>p,1…<i>n</i>
                    </sub>
                    
                  </span>. For a 1280 × 960 camera image, those would be 0, 1280, and 640. For the algorithm to converge properly though, some sort of texture needs to be displayed on the surface plane. We chose a fractal image, the exact one shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig4">4</a>d, but orthogonally aligned to the epipolar lines. Because this texture contains the same pattern at all scales, running the minimization algorithm at multiple resolutions should allow for easy and accurate convergence over a wide range of displacements. More formally, assuming epipolar lines aligned with the <i>x</i>-axis, the intensities assigned to the projector image of coordinates <i>x</i>  ∈ [<i>x</i>
                  <sub>1</sub>, <i>x</i>
                  <sub>2</sub>] equal</p><div id="Equ26" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\bf p}_{\rm p}(x, y) = \left\{\begin{array}{ll} {\bf 1} p(x, x_1, x_m, 0, 0, 1) &amp; \hbox{if } x_1 \le x &lt; x_m \\ {\bf 1} p(x, x_m, x_2, 0, 0, -1) &amp; \hbox{if } x_m \le x \le x_2, \end{array}\right. $$</span></div><div class="c-article-equation__number">
                    (26)
                </div></div><p>whose amplitude is scaled appropriately, and where</p><div id="Equ27" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \begin{aligned} &amp; p(x,x_1,x_2, y_1, y_2, n)\\ &amp; {\quad = \left\{\begin{array}{ll} p(x, x_1, x_m, y_1, y_m, n') &amp; \hbox{if } x_1 \le x &lt; x_m \\ y_m &amp; \hbox{if } x = x_m \\ p(x, x_m, x_2, y_m, y_2, -n') &amp; \hbox{if } x_m &lt; x \le x_2 \end{array}\right.}, \end{aligned} $$</span></div><div class="c-article-equation__number">
                    (27)
                </div></div>
                  <div id="Equ28" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ x_m = \frac{x_1+x_2}{2}, \quad y_m = \frac{y_1+y_2}{2} + n, \quad n' = \frac{n}{\sqrt{2}} . $$</span></div><div class="c-article-equation__number">
                    (28)
                </div></div><p>Although the barlike effect may give the impression of an image used for structured light, the approach is totally different as our pattern contains no code and tolerates well any surface texture.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The source images used to generate our simulated images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Results</h2><div class="c-article-section__content" id="Sec16-content"><p>The description of our method is now complete, and we present here some results. We programmed an application in Java, integrating OpenCV as appropriate, to implement the procedures and algorithms introduced in this paper, then performed various tests running the software on a Dell Vostro 400 computer with an Intel Core 2 Quad Q6600 2.4 GHz CPU for both simulated images and real images, and obtained the results described in the following sections.</p><h3 class="c-article__sub-heading" id="Sec17">Simulated images</h3><p>To analyze the convergence behavior of the algorithm, we ran simulation experiments, using as parameters for the virtual camera and projector the values listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0210-9#Tab1">1</a> and as input images the ones shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig4">4</a>. The orientation of the images in the first column matches with the initial images of the camera, while the corresponding ones from the second column indicate also at initialization the unmodulated projector images, which overlap the board almost exactly when <span class="mathjax-tex">\({\bf n} = [0 \, 0 \, \frac{1}{2 \, \hbox{m}}].\)</span> We selected for processing the middle 640 × 480 rectangular ROI of the camera image. Each simulation trial consisted of the following procedure that our test program followed to produce and align the transformed images: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>Add noise taken from a normal distribution <span class="mathjax-tex">\({{\mathcal{N}}(0, \sigma^2)}\)</span> to the four coordinates of the camera ROI.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>Use this displacement to compute via DLT (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR11" id="ref-link-section-d69270e2012">2004</a>), a random homography H.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>Decompose H into rotation R, translation <b>t</b>, and plane parameters <b>n</b> (Triggs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Triggs B (1998) Autocalibration from planar scenes. In: 5th European conference on computer vision (ECCV ’98), vol I, Springer, Berlin, pp 89–105" href="/article/10.1007/s10055-012-0210-9#ref-CR22" id="ref-link-section-d69270e2032">1998</a>), choosing the set associated with the <b>n</b> closest to the <i>z</i>-axis.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p>Scale <b>n</b> and <b>t</b> to place the board 2 m in front of the camera.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">5.</span>
                      
                        <p>Let the projector gain <span class="mathjax-tex">\({g \sim {\mathcal{N}}(0.5, 0.1^2)}\)</span> and ambient light <span class="mathjax-tex">\({\bf a} \sim (\mathcal{A}, \mathcal{A}, \mathcal{A})\)</span> where <span class="mathjax-tex">\({\mathcal{A} = {\mathcal{N}}(0.1, (\frac{0.1}{3})^2). }\)</span>
                        </p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">6.</span>
                      
                        <p>Create a simulated camera image by first transforming the images using the above H, <span class="mathjax-tex">\({\bf n'} = \frac{\hbox{R}{\bf n}}{1 - {\bf t}^{\rm T} {\bf n}}\)</span>, <i>g</i> and <b>a</b> parameters, then adding to the pixel values an amount of Gaussian noise <span class="mathjax-tex">\({{\mathcal{N}}(0, 0.01^2),}\)</span> and finally saturating (clamping) the intensity sums within their valid range [0, 1].</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">7.</span>
                      
                        <p>Execute the alignment algorithm using as settings seven pyramid levels, no zero thresholds, <span class="mathjax-tex">\(\Updelta_{\rm min} = 0.1, \)</span> and as initial parameters the four reference points of the camera ROI, <b>n</b> from step 4, <i>g</i> = 0.5, and <b>a</b> = (0.1, 0.1, 0.1).</p>
                      
                    </li>
                  </ol>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 The camera and projector parameters used during our simulation experiments</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0210-9/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>We organized the results in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig5">5</a> for 10,000 trials per data point, where we judged that our algorithm had successfully converged when the final RMSE with the four points of the warped ROI was below 0.1 pixels. We note that the subspace error term helps most the less well-textured images (in order: demo, page, and fractal), as the additional information from the projector image helps better constrain the motion of the board. With the first set of images at σ = 100 pixels, the convergence frequency goes up by as much as 37 %. The convergence speed however, measured in number of iterations, does not change significantly.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Results from the simulation experiments, where for a given σ associated with the random homography, the curves in the <i>upper part</i> of the graphs indicate the convergence frequency, while the ones in the <i>lower parts</i> show how many iterations it took to reach convergence on average, and the error curves delineate the standard deviations (SD) below and above average</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec18">Real images</h3><p>We also challenged our algorithm with real images captured using libdc1394. Our test hardware consisted of a Casio XJ-S68 (1024 × 768 color DLP) projector, and a PGR Flea2 FL2G-13S2C-C (1280 × 960 Bayer color CCD) camera attached to a Pentax H1212B (12 mm) lens. For the surface planes, we inkjet printed the patterns described below on A4 size sheets of paper and pasted them on (mostly) flat foam boards. We conducted two sets of experiments: one to measure accuracy quantitatively by comparing alignment results with easy-to-detect markers, and the other to demonstrate real-time operation and support for arbitrary textures.</p><p>Given that we want an algorithm that runs as fast as possible and given that the displacements are usually less extreme than the ones tested with our simulations, we used only five pyramid levels with zero thresholds <i>t</i> = 1.0, 0.75, 0.5, 0.25, and 0 respectively, let the subspace α = 100 and <span class="mathjax-tex">\(\Updelta_{\rm min} = 10, \)</span> and reduced the line search to only two values, 1.0 and 0.25. Surprisingly perhaps, these diminished settings did not degrade accuracy at all.</p><p>As representative of the current methods, we chose ARToolKitPlus (Wagner and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Wagner D, Schmalstieg D (2007) ARToolKitPlus for pose tracking on mobile devices. In: 12th computer vision winter workshop (CVWW’07), Graz University of Technology, St. Lambrecht, Austria" href="/article/10.1007/s10055-012-0210-9#ref-CR23" id="ref-link-section-d69270e2418">2007</a>) along with OpenCV (Bradski and Kaehler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bradski G, Kaehler A (2008) Learning OpenCV: computer vision with the OpenCV library. O’Reilly, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR8" id="ref-link-section-d69270e2421">2008</a>) for subpixel detection and compared its results with ours. We put the texture of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig6">6</a> on the planar surface. This fractal pattern is the 2D equivalent of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-012-0210-9#Equ26">26</a>, but where triangles split the space at each recursion. Obviously, we also used Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig4">4</a>d for the projector pattern in an attempt to extract the best accuracy. On the initialized reflectance image, the markers delimited a ROI of 197255 pixels. For each subsequent frame, our algorithm took on average a total of 109 ± 16 (SD) ms to converge, but we noted that iterations at pyramid level 0 brought no additional accuracy, possibly due to the noise level, so the effective processing time equaled in fact 45 ± 6 (SD) ms. We placed shots of the frames as captured by the camera in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig7">7</a>. The full sequence can be found in the supplementary material (Online Resource 1). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-012-0210-9#Tab2">2</a> lists the average number of iterations and the time taken for each at different levels of the resolution pyramid, where the warping function alone took most of the time, as can be seen from the processing time roughly halving at each level higher in the pyramid. From this execution, the difference in pixels between our results and the centers of the four markers over the whole test video is plotted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig8">8</a>. Although markers do not provide the ground truth either, we consider the difference as errors of our algorithm since the error of corner detection should be less than 0.10 pixels (Bradski and Kaehler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bradski G, Kaehler A (2008) Learning OpenCV: computer vision with the OpenCV library. O’Reilly, Cambridge" href="/article/10.1007/s10055-012-0210-9#ref-CR8" id="ref-link-section-d69270e2443">2008</a>). The total RMSE sums up to 0.29 pixels, which even includes images violating the assumed single gain <i>g</i>, especially true of slanted planes, and portions of motion blur (e.g., Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig7">7</a>b).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The printed triangular fractal pattern along with four markers that we used to obtain accuracy data</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Frames from then markers test video (Online Resource 1), which features the patterns of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig4">4</a>d and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig6">6</a>, with offline drawn <i>red crosses</i> representing detected markers and <i>green rectangles</i> denoting the corresponding regions aligned by our program: The <i>rectangle</i> corners match the <i>crosses</i> with subpixel accuracy as shown in the blowups</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Average number of iterations and time per iteration of the markers test ± their standard deviations (SD)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-012-0210-9/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Difference in pixels between our results and the detected centers of the markers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To show that our program works in real time even with poor and arbitrary textures that easily overlap on the surface, we ran it using the images shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig9">9</a>, the first pasted on the surface plane and the second displayed by the projector, along with other graphical elements as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig1">1</a>. More precisely, after each frame, the system warped the projector image to achieve a geometric correction on the physical plane by applying the homography H<sub>ps</sub> = H<sub>pc</sub>H<span class="c-stack">
                    <sup>−1</sup><sub>sc</sub>
                    
                  </span>, which transforms points from the surface to the projector. (Although we could also correct the projector colors to match the printed ones, we intentionally left them as is for simplicity and ease of visual inspection). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig1">1</a> shows the system in action. We placed more shots of the demo video in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig10">10</a>. The full sequence can be found in the supplementary material (Online Resource 2). We limited to 40 ms the amount of time the program could spend iterating, which in reality ran on average for 54 ± 8 (SD) ms. Updating, (un)distorting, and creating resolution pyramids of the projector and camera images in parallel took approximately 69 ± 12 (SD) ms, covering entirely the delays of the camera’s software trigger and of the projector’s display. This gave a total of 123 ± 17 (SD) ms on average for each frame, or approximately eight frames per second.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The images used for our demo video</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0210-9/MediaObjects/10055_2012_210_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Frames from our demo video (Online Resource 2), which features the patterns of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0210-9#Fig9">9</a> plus a chronometer, a video animation, and a red virtual ball, being aligned in real time, here grouped in pairs of a misaligned image caused by user motion followed by its correction, except for the last two, which reveal that our algorithm even features some amount of robustness to occlusion, albeit not modeled</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0210-9/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>In both cases, the algorithm successfully converged given any displacements reasonable for a direct alignment method (Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255" href="/article/10.1007/s10055-012-0210-9#ref-CR3" id="ref-link-section-d69270e2799">2004</a>). We also found that the single gain <i>g</i> used for the entire image was sufficient when all points of the surface plane are not far from each other compared to their average distance from the projector; otherwise, vignettinglike effects obviously occur, a possibly impractical limitation for some applications.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Discussion and conclusion</h2><div class="c-article-section__content" id="Sec19-content"><p>Using mathematical approximations and algorithmic tricks detailed in this paper, even though we managed to boost the performance of our algorithm to decent levels, while retaining robustness and accuracy, there surely exist other ways to accelerate it still further. One interesting idea we had was to formulate an approximate model that the inverse compositional (IC) image alignment algorithm (Baker and Matthews <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255" href="/article/10.1007/s10055-012-0210-9#ref-CR3" id="ref-link-section-d69270e2814">2004</a>; Bartoli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bartoli A (2008) Groupwise geometric and photometric direct image registration. IEEE Trans Pattern Anal Mach Intell 30(12):2098–2108" href="/article/10.1007/s10055-012-0210-9#ref-CR6" id="ref-link-section-d69270e2817">2008</a>) could work with. The intuition was that since we physically place the camera and projector close together, the displacement of the projector image on the physical surface could be assumed negligible. We could let the optimizer deal with any remaining misalignment as noise. Unfortunately, the generated noise proved to be too great, and both robustness and accuracy dropped unacceptably. Nevertheless, other avenues for optimization undoubtedly exist, and in the worst case, we should be able to enhance the execution performance without great difficulty by an order of magnitude with an implementation for graphics processing units (GPUs).</p><p>Once sufficiently accelerated, our method could also be generalized to more complex reflectance properties and 3D surfaces. We could, for example, use a piecewise planar model and align multiple planes simultaneously, a widely adopted approach, as implemented by Sugimoto and Okutomi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sugimoto S, Okutomi M (2007) A direct and efficient method for piecewise-planar surface reconstruction from stereo images. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society" href="/article/10.1007/s10055-012-0210-9#ref-CR20" id="ref-link-section-d69270e2823">2007</a>), among others. Such a model could also compensate for our limited reflectance model. By dividing the surface in pieces that way, each may support different gain and ambient light, as demonstrated by Silveira and Malis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Silveira G, Malis E (2007) Real-time visual tracking under arbitrary illumination changes. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society" href="/article/10.1007/s10055-012-0210-9#ref-CR18" id="ref-link-section-d69270e2826">2007</a>) for camera-only systems, an approach that even allows for specularities.</p><p>Other possibly interesting directions of future work could center on dealing with occlusions and changes in surface reflectance. For example, as user interface input for a desktop system at the office or to provide larger interactive screens to mobile devices, the system could detect and exploit occlusions occurring as the result of gestures by the users in front of the projector. Further, they may want to add physically new information on the surface. In that case, the system would need a way to gradually update the reflectance properties.</p><p>As secondary contributions, some of the insights conceived within the course of our research, most notably zero thresholds and the subspace error term, can be adapted to provide solutions to other related computer vision problems. In particular, piecewise planar models should benefit most effectively from the subspace error term.</p><p>Although much work remains, we have demonstrated, at the very least, the feasibility of the approach. With some optimizations, we consider that direct image alignment could become the basis of future applications using projector-based augmented reality. Additionally, to encourage research in this direction, we are making the whole software system available as open source under the name of ProCamTracker. Its package can be freely obtained from our Web site at <a href="http://www.ok.ctrl.titech.ac.jp/~saudet/procamtracker/">http://www.ok.ctrl.titech.ac.jp/~saudet/procamtracker/</a>.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Audet S, Okutomi M (2009) A user-friendly method to geometrically calibrate projector-camera systems. In: 2009" /><p class="c-article-references__text" id="ref-CR1">Audet S, Okutomi M (2009) A user-friendly method to geometrically calibrate projector-camera systems. In: 2009 IEEE conference on computer vision and pattern recognition (CVPR 2009)—workshops (Procams 2009), IEEE computer society, © 2009 IEEE, pp 47–54</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Audet S, Okutomi M, Tanaka M (2010) Direct image alignment of projector-camera systems with planar surfaces. I" /><p class="c-article-references__text" id="ref-CR2">Audet S, Okutomi M, Tanaka M (2010) Direct image alignment of projector-camera systems with planar surfaces. In: 2010 IEEE conference on computer vision and pattern recognition (CVPR 2010), IEEE computer society, © 2010 IEEE</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Baker, I. Matthews, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255" /><p class="c-article-references__text" id="ref-CR3">Baker S, Matthews I (2004) Lucas-Kanade 20 years on: a unifying framework. Int J Comput Vis 56(1):221–255</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FB%3AVISI.0000011205.11775.fd" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Lucas-Kanade%2020%C2%A0years%20on%3A%20a%20unifying%20framework&amp;journal=Int%20J%20Comput%20Vis&amp;volume=56&amp;issue=1&amp;pages=221-255&amp;publication_year=2004&amp;author=Baker%2CS&amp;author=Matthews%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baker S, Datta A, Kanade T (2006) Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute," /><p class="c-article-references__text" id="ref-CR4">Baker S, Datta A, Kanade T (2006) Parameterizing homographies. Tech. Rep. CMU-RI-TR-06-11, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: 2001 IEEE and" /><p class="c-article-references__text" id="ref-CR5">Bandyopadhyay D, Raskar R, Fuchs H (2001) Dynamic shader lamps: painting on movable objects. In: 2001 IEEE and ACM international symposium on augmented reality (ISAR 2001), IEEE computer society, p 207</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Bartoli, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bartoli A (2008) Groupwise geometric and photometric direct image registration. IEEE Trans Pattern Anal Mach I" /><p class="c-article-references__text" id="ref-CR6">Bartoli A (2008) Groupwise geometric and photometric direct image registration. IEEE Trans Pattern Anal Mach Intell 30(12):2098–2108</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2008.22" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Groupwise%20geometric%20and%20photometric%20direct%20image%20registration&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=30&amp;issue=12&amp;pages=2098-2108&amp;publication_year=2008&amp;author=Bartoli%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="O. Bimber, R. Raskar, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., Nati" /><p class="c-article-references__text" id="ref-CR7">Bimber O, Raskar R (2005) Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., Natick</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20augmented%20reality%3A%20merging%20real%20and%20virtual%20worlds&amp;publication_year=2005&amp;author=Bimber%2CO&amp;author=Raskar%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="G. Bradski, A. Kaehler, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bradski G, Kaehler A (2008) Learning OpenCV: computer vision with the OpenCV library. O’Reilly, Cambridge" /><p class="c-article-references__text" id="ref-CR8">Bradski G, Kaehler A (2008) Learning OpenCV: computer vision with the OpenCV library. O’Reilly, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20OpenCV%3A%20computer%20vision%20with%20the%20OpenCV%20library&amp;publication_year=2008&amp;author=Bradski%2CG&amp;author=Kaehler%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Caspi, N. Kiryati, J. Shamir, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Caspi D, Kiryati N, Shamir J (1998) Range imaging with adaptive color structured light. IEEE Trans Pattern Ana" /><p class="c-article-references__text" id="ref-CR9">Caspi D, Kiryati N, Shamir J (1998) Range imaging with adaptive color structured light. IEEE Trans Pattern Anal Mach Intell 20(5):470–480</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.682177" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Range%20imaging%20with%20adaptive%20color%20structured%20light&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=20&amp;issue=5&amp;pages=470-480&amp;publication_year=1998&amp;author=Caspi%2CD&amp;author=Kiryati%2CN&amp;author=Shamir%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen X, Yang X, Xiao S, Li M (2008) Color mixing property of a projector-camera system. In: Fifth internationa" /><p class="c-article-references__text" id="ref-CR10">Chen X, Yang X, Xiao S, Li M (2008) Color mixing property of a projector-camera system. In: Fifth international workshop on projector-camera systems (Procams 2008), ACM, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Hartley, A. Zisserman, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University " /><p class="c-article-references__text" id="ref-CR11">Hartley R, Zisserman A (2004) Multiple view geometry in computer vision, second edition. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multiple%20view%20geometry%20in%20computer%20vision%2C%20second%20edition&amp;publication_year=2004&amp;author=Hartley%2CR&amp;author=Zisserman%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="International Electrotechnical Commission (1999) IEC 61966-2-1 (1999-10-18): multimedia systems and equipment—" /><p class="c-article-references__text" id="ref-CR12">International Electrotechnical Commission (1999) IEC 61966-2-1 (1999-10-18): multimedia systems and equipment—colour measurement and management—part 2-1: colour management—default RGB colour space—sRGB</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Johnson T, Fuchs H (2007) Real-time projector tracking on complex geometry using ordinary imagery. In: 2007 IE" /><p class="c-article-references__text" id="ref-CR13">Johnson T, Fuchs H (2007) Real-time projector tracking on complex geometry using ordinary imagery. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007)—workshops (Procams 2007), IEEE computer society, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: tow" /><p class="c-article-references__text" id="ref-CR14">Leibe B, Starner T, Ribarsky W, Wartell Z, Krum D, Singletary B, Hodges L (2000) The perceptive workbench: towards spontaneous and natural interaction in semi-immersive virtual environments. In: 2000 IEEE virtual reality conference (VR 2000), IEEE computer society, pp 13–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moritani T, Hiura S, Sato K (2006) Real-time object tracking without feature extraction. In: 2006 IEEE confere" /><p class="c-article-references__text" id="ref-CR15">Moritani T, Hiura S, Sato K (2006) Real-time object tracking without feature extraction. In: 2006 IEEE conference on pattern recognition (ICPR 2006), IEEE computer society, Los Alamitos, CA, USA, vol 1, pp 747–750</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to i" /><p class="c-article-references__text" id="ref-CR16">Raskar R, Welch G, Cutts M, Lake A, Stesin L, Fuchs H (1998) The office of the future: a unified approach to image-based modeling and spatially immersive displays. In: 25th international conference on computer graphics and interactive techniques (SIGGRAPH 98). ACM Press, New York, pp 179–188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2003) iLamps: geometrically aware and self" /><p class="c-article-references__text" id="ref-CR17">Raskar R, van Baar J, Beardsley P, Willwacher T, Rao S, Forlines C (2003) iLamps: geometrically aware and self-configuring projectors. In: ACM transactions on graphics (SIGGRAPH 2003), ACM, pp 809–818</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Silveira G, Malis E (2007) Real-time visual tracking under arbitrary illumination changes. In: 2007 IEEE confe" /><p class="c-article-references__text" id="ref-CR18">Silveira G, Malis E (2007) Real-time visual tracking under arbitrary illumination changes. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sturm P (2000) Algorithms for plane-based pose estimation. In: 2000 IEEE conference on computer vision and pat" /><p class="c-article-references__text" id="ref-CR19">Sturm P (2000) Algorithms for plane-based pose estimation. In: 2000 IEEE conference on computer vision and pattern recognition (CVPR 2000), IEEE computer society, Hilton Head Island, South Carolina, USA, pp 1706–1711</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sugimoto S, Okutomi M (2007) A direct and efficient method for piecewise-planar surface reconstruction from st" /><p class="c-article-references__text" id="ref-CR20">Sugimoto S, Okutomi M (2007) A direct and efficient method for piecewise-planar surface reconstruction from stereo images. In: 2007 IEEE conference on computer vision and pattern recognition (CVPR 2007), IEEE computer society</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Takao, J. Shi, S. Baker, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Takao N, Shi J, Baker S (2003) Tele-graffiti: a camera-projector based remote sketching system with hand-based" /><p class="c-article-references__text" id="ref-CR21">Takao N, Shi J, Baker S (2003) Tele-graffiti: a camera-projector based remote sketching system with hand-based user interface and automatic session summarization. Int J Comput Vis 53(2):115–133</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1023084706295" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tele-graffiti%3A%20a%20camera-projector%20based%20remote%20sketching%20system%20with%20hand-based%20user%20interface%20and%20automatic%20session%20summarization&amp;journal=Int%20J%20Comput%20Vis&amp;volume=53&amp;issue=2&amp;pages=115-133&amp;publication_year=2003&amp;author=Takao%2CN&amp;author=Shi%2CJ&amp;author=Baker%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Triggs B (1998) Autocalibration from planar scenes. In: 5th European conference on computer vision (ECCV ’98)," /><p class="c-article-references__text" id="ref-CR22">Triggs B (1998) Autocalibration from planar scenes. In: 5th European conference on computer vision (ECCV ’98), vol I, Springer, Berlin, pp 89–105</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Schmalstieg D (2007) ARToolKitPlus for pose tracking on mobile devices. In: 12th computer vision win" /><p class="c-article-references__text" id="ref-CR23">Wagner D, Schmalstieg D (2007) ARToolKitPlus for pose tracking on mobile devices. In: 12th computer vision winter workshop (CVWW’07), Graz University of Technology, St. Lambrecht, Austria</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Xiao J, Baker S, Matthews I, Kanade T (2004) Real-time combined 2D+3D active appearance models. In: 2004 IEEE " /><p class="c-article-references__text" id="ref-CR24">Xiao J, Baker S, Matthews I, Kanade T (2004) Real-time combined 2D+3D active appearance models. In: 2004 IEEE conference on computer vision and pattern recognition (CVPR 2004), IEEE computer society, Washinton, DC, USA, vol 2, pp 535–542</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhang, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):133" /><p class="c-article-references__text" id="ref-CR25">Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 22(11):1330–1334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.888718" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20flexible%20new%20technique%20for%20camera%20calibration&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=22&amp;issue=11&amp;pages=1330-1334&amp;publication_year=2000&amp;author=Zhang%2CZ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-012-0210-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by a scholarship from the Ministry of Education, Culture, Sports, Science and Technology (MEXT) of the Japanese Government.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan</p><p class="c-article-author-affiliation__authors-list">Samuel Audet, Masatoshi Okutomi &amp; Masayuki Tanaka</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Samuel-Audet"><span class="c-article-authors-search__title u-h3 js-search-name">Samuel Audet</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Samuel+Audet&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samuel+Audet" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samuel+Audet%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Masatoshi-Okutomi"><span class="c-article-authors-search__title u-h3 js-search-name">Masatoshi Okutomi</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Masatoshi+Okutomi&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Masatoshi+Okutomi" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Masatoshi+Okutomi%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Masayuki-Tanaka"><span class="c-article-authors-search__title u-h3 js-search-name">Masayuki Tanaka</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Masayuki+Tanaka&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Masayuki+Tanaka" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Masayuki+Tanaka%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-012-0210-9/email/correspondent/c1/new">Samuel Audet</a>.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec20-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv908612"><script src="https://www.edge-cdn.net/videojs_908612?jsdiv=mijsvdiv908612&amp;playerskin=37016" defer="defer"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>MPG (12816 KB)</p></div></div>
                
                  <div id="MOESM2"><div class="video" id="mijsvdiv908620"><script src="https://www.edge-cdn.net/videojs_908620?jsdiv=mijsvdiv908620&amp;playerskin=37016" defer="defer"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>MPG (36770 KB)MPG (36770 KB)</p></div></div>
                </div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Augmenting%20moving%20planar%20surfaces%20robustly%20with%20video%20projection%20and%20direct%20image%20alignment&amp;author=Samuel%20Audet%20et%20al&amp;contentID=10.1007%2Fs10055-012-0210-9&amp;publication=1359-4338&amp;publicationDate=2012-04-11&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Audet, S., Okutomi, M. &amp; Tanaka, M. Augmenting moving planar surfaces robustly with video projection and direct image alignment.
                    <i>Virtual Reality</i> <b>17, </b>157–168 (2013). https://doi.org/10.1007/s10055-012-0210-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-012-0210-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-02-23">23 February 2011</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-03-21">21 March 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-04-11">11 April 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06">June 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-012-0210-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-012-0210-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Vision-based tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Projector-camera systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Video projection</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Image alignment</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0210-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=210;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

