<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Exploring individual user differences in the 2D/3D interaction with me"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="User-centered design is often performed without regard to individual user differences. In this paper, we report results of an empirical study aimed to evaluate whether computer experience and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/14/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Exploring individual user differences in the 2D/3D interaction with medical image data"/>

    <meta name="dc.source" content="Virtual Reality 2009 14:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2009-09-16"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2009 The Author(s)"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="User-centered design is often performed without regard to individual user differences. In this paper, we report results of an empirical study aimed to evaluate whether computer experience and demographic user characteristics would have an effect on the way people interact with the visualized medical data in a 3D virtual environment using 2D and 3D input devices. We analyzed the interaction through performance data, questionnaires and observations. The results suggest that differences in gender, age and game experience have an effect on people&#8217;s behavior and task performance, as well as on subjective user preferences."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2009-09-16"/>

    <meta name="prism.volume" content="14"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="105"/>

    <meta name="prism.endingPage" content="118"/>

    <meta name="prism.copyright" content="2009 The Author(s)"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-009-0131-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-009-0131-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-009-0131-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-009-0131-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Exploring individual user differences in the 2D/3D interaction with medical image data"/>

    <meta name="citation_volume" content="14"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2010/06"/>

    <meta name="citation_online_date" content="2009/09/16"/>

    <meta name="citation_firstpage" content="105"/>

    <meta name="citation_lastpage" content="118"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_fulltext_world_readable" content=""/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-009-0131-4"/>

    <meta name="DOI" content="10.1007/s10055-009-0131-4"/>

    <meta name="citation_doi" content="10.1007/s10055-009-0131-4"/>

    <meta name="description" content="User-centered design is often performed without regard to individual user differences. In this paper, we report results of an empirical study aimed to eval"/>

    <meta name="dc.creator" content="Elena Zudilova-Seinstra"/>

    <meta name="dc.creator" content="Boris van Schooten"/>

    <meta name="dc.creator" content="Avan Suinesiaputra"/>

    <meta name="dc.creator" content="Rob van der Geest"/>

    <meta name="dc.creator" content="Betsy van Dijk"/>

    <meta name="dc.creator" content="Johan Reiber"/>

    <meta name="dc.creator" content="Peter Sloot"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=MAGMA (Magn Reson Mater Phys Biol Med); citation_title=Automatic segmentation and plaque characterization in atherosclerotic carotid artery MR images; citation_author=IM Adame, RJ Geest, BA Wasserman, M Mohamed, JHC Reiber, BPF Lelieveldt; citation_volume=16; citation_issue=5; citation_publication_date=2004; citation_pages=227-234; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Dev Psychol; citation_title=Age differences in the speed of mental rotation; citation_author=C Berg, C Hertzog, E Hunt; citation_volume=18; citation_issue=1; citation_publication_date=1982; citation_pages=95-107; citation_doi=10.1037/0012-1649.18.1.95; citation_id=CR2"/>

    <meta name="citation_reference" content="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: Proceedings of the IEEE symposium on 3D user interfaces, pp 29&#8211;36"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Res Ther; citation_title=Virtual reality treatment of claustrophobia: a case report; citation_author=C Botella, RM Ba&#241;os, C Perpi&#241;&#225;, H Villa, M Alca&#241;iz, A Reym; citation_volume=36; citation_issue=2; citation_publication_date=1998; citation_pages=239-246; citation_doi=10.1016/S0005-7967(97)10006-7; citation_id=CR4"/>

    <meta name="citation_reference" content="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, London UK, pp 26&#8211;33"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces: theory and practice; citation_publication_date=2005; citation_id=CR6; citation_author=DA Bowman; citation_author=E Kruijff; citation_author=JJ LaViola; citation_author=I Poupyrev; citation_publisher=Pearson Education"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=What&#8217;s real about virtual reality?; citation_author=FP Brooks; citation_volume=19; citation_issue=6; citation_publication_date=1999; citation_pages=16-27; citation_doi=10.1109/38.799723; citation_id=CR7"/>

    <meta name="citation_reference" content="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: Proceedings of the 1992 symposium on interactive 3D graphics. ACM Press, New York, NY, USA, pp 183&#8211;188"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Context analysis to support development of virtual reality applications; citation_author=HSM Cramer, V Evers, EV Zudilova, PMA Sloot; citation_volume=7; citation_issue=3; citation_publication_date=2004; citation_pages=177-186; citation_doi=10.1007/s10055-004-0130-4; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_title=Human computer interaction; citation_publication_date=1993; citation_id=CR10; citation_author=A Dix; citation_author=J Finlay; citation_author=G Abowd; citation_author=R Beale; citation_publisher=Prentice-Hall"/>

    <meta name="citation_reference" content="citation_title=User interface design; citation_publication_date=1994; citation_id=CR11; citation_author=RE Eberts; citation_publisher=Prentice-Hall"/>

    <meta name="citation_reference" content="citation_title=Individual differences in human&#8211;computer interaction; citation_inbook_title=Handbook of human&#8211;computer interaction; citation_publication_date=1988; citation_pages=543-568; citation_id=CR12; citation_author=DE Egan; citation_publisher=Elsevier"/>

    <meta name="citation_reference" content="citation_title=Assaying, isolating, and accommodating individual differences in learning a complex skill; citation_inbook_title=Individual differences in cognition; citation_publication_date=1985; citation_pages=173-217; citation_id=CR13; citation_author=DE Egan; citation_author=M Gomez; citation_publisher=Academic Press"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Sci; citation_title=Playing an action video game reduces gender differences in spatial cognition; citation_author=J Feng, I Spence, J Pratt; citation_volume=18; citation_issue=10; citation_publication_date=2007; citation_pages=850-855; citation_doi=10.1111/j.1467-9280.2007.01990.x; citation_id=CR14"/>

    <meta name="citation_reference" content="Gabbard JL, Swartz K, Richey K, Hix D (1999) Usability evaluation techniques: a novel method for assessing the usability of an immersive medical visualization VE. In: Proceedings VWSIM&#8217;99, pp 165&#8211;170"/>

    <meta name="citation_reference" content="citation_journal_title=Educ Commun Technol; citation_title=Videogames and spatial skills: an exploratory study; citation_author=D Gagnon; citation_volume=33; citation_issue=4; citation_publication_date=1985; citation_pages=263-275; citation_id=CR16"/>

    <meta name="citation_reference" content="Hartman NW, Connolly PE, Gilger JW, Bertoline GR, Heisler J (2006) Virtual reality-based spatial skills assessment and its role in computer graphics education. In: Proceedings of the international conference on computer graphics and interactive techniques (ACM SIGGRAPH 2006), Educators program, article No. 46"/>

    <meta name="citation_reference" content="He T, Kaufman AE (1993) Virtual input devices for 3D systems. In: Proceedings of IEEE visualization 1993. IEEE Computer Society, San Jose, CA, pp 142&#8211;148"/>

    <meta name="citation_reference" content="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassel DN (1997) Usability analysis of 3D rotation techniques. In: UIST &#8216;97: proceedings of the 10th annual ACM symposium on user interface software and technology, New York, NY, pp 1&#8211;10"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Treating psychological and physical disorders with VR; citation_author=L Hodges, R Anderson, G Burdea, H Hoffman, B Rothbaum; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=25-33; citation_doi=10.1109/38.963458; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Clin J Pain; citation_title=The effectiveness of virtual reality based pain control with multiple treatments; citation_author=HG Hoffman, DR Patterson, GJ Carrougher; citation_volume=17; citation_publication_date=2001; citation_pages=229-235; citation_doi=10.1097/00002508-200109000-00007; citation_id=CR21"/>

    <meta name="citation_reference" content="Jacobson J, Redern MS, Furnan JM, Whiney LW, Sparto PJ, Wilson JB, Hodges LF (2001) Balance NAVE: a virtual reality facility for research and rehabilitation of balance disorders. In: Proceedings of virtual reality software technology (VR&#8217;01), pp 103&#8211;109"/>

    <meta name="citation_reference" content="Jin W, Lim Y-L, Xu XG, Singh TP, Suvranu DE (2005) Improving the visual realism of virtual surgery. In: Proceedings medicine meets virtual reality, vol 13, pp 227&#8211;233"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Top scientific visualization research problems; citation_author=C Johnson; citation_volume=4; citation_publication_date=2004; citation_pages=13-17; citation_doi=10.1109/MCG.2004.20; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_title=International encyclopedia of ergonomics and human factors; citation_publication_date=2006; citation_id=CR25; citation_author=W Karwowski; citation_publisher=CRC Press"/>

    <meta name="citation_reference" content="Kr&#252;ger A, Irrgang S, Hertel I, Strauss G, Preim P (2007) Comparison of 2D and 3D input devices for virtual endoscopy. In: CURAC 2007, pp 215&#8211;218"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Gender issues in the use of virtual environments; citation_author=P Larson, AA Rizzo, JG Buckwalter, A Rooyen, K Kratz, U Neumann, C Kesselman, M Thiebaux, C Zang; citation_volume=2; citation_issue=2; citation_publication_date=1999; citation_pages=113-124; citation_doi=10.1089/cpb.1999.2.113; citation_id=CR27"/>

    <meta name="citation_reference" content="Leitheiser B, Munro D (1995) An experimental study of the relationship between spatial ability and the learning of a graphical user interface. In: Proceedings of the inaugural America&#8217;s conference on information systems, Pittsburgh, PA, pp 122&#8211;124"/>

    <meta name="citation_reference" content="citation_title=Spatial ability and G; citation_inbook_title=Human abilities: their nature and assessment; citation_publication_date=1996; citation_pages=97-116; citation_id=CR29; citation_author=DF Lohman; citation_publisher=Erlbaum"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=The role of stereopsis in virtual anatomical learning; citation_author=J-M Luursema, WB Verwey, PAM Kommers, J-H Annema; citation_volume=20; citation_issue=4; citation_publication_date=2008; citation_pages=455-460; citation_doi=10.1016/j.intcom.2008.04.003; citation_id=CR30"/>

    <meta name="citation_reference" content="Martens J-B, van Liere R, Kok A (2007) Widget manipulation revisited: a case study in modeling interactions between experimental conditions. In: Proceedings of the IPT-EGVE symposium, The Eurographics Association, paper 1021"/>

    <meta name="citation_reference" content="citation_journal_title=J Digit Imaging; citation_title=Evaluating different radiology workstation interaction techniques with radiologists and laypersons; citation_author=A Moise, MS Atkins, R Rohling; citation_volume=18; citation_issue=2; citation_publication_date=2005; citation_pages=116-130; citation_doi=10.1007/s10278-004-1906-5; citation_id=CR32"/>

    <meta name="citation_reference" content="Myers B, Bhatnagar R, Nichols J, Peck C, Kong D, Miller R, Long AC (2002) Interaction at a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the conference on human factors and computing systems (CHI&#8217;2002), Minneapolis, Minnesota, USA, pp 33&#8211;40"/>

    <meta name="citation_reference" content="citation_journal_title=Cyberpsychol Behav; citation_title=Virtual reality as a tool for improving spatial rotation among deaf and hard-of-hearing children; citation_author=D Passig, S Eden; citation_volume=4; citation_issue=6; citation_publication_date=2001; citation_pages=681-686; citation_doi=10.1089/109493101753376623; citation_id=CR34"/>

    <meta name="citation_reference" content="citation_title=Statistics for nursing and allied health; citation_publication_date=2008; citation_id=CR35; citation_author=SB Plichta; citation_author=LS Garzon; citation_publisher=Williams &amp; Wilkins"/>

    <meta name="citation_reference" content="citation_title=Virtual environment applications in clinical neuropsychology. The handbook of virtual environments; citation_publication_date=2000; citation_id=CR36; citation_author=A Rizzo; citation_author=JG Buckwalter; citation_author=C Zaag; citation_publisher=L.A. Erlbaum"/>

    <meta name="citation_reference" content="citation_title=Cognition and second language instruction; citation_publication_date=2001; citation_id=CR37; citation_author=P Robinson; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="Roessler A, Grantz V (1998) Performance evaluation of input devices in virtual environments. In: Proceedings of the IFIP working group 13.2 conference on designing effective and usable multimedia systems, vol 133, pp 68&#8211;73"/>

    <meta name="citation_reference" content="citation_journal_title=Dev Psychol; citation_title=Age and experience effects in spatial visualization; citation_author=T Salthouse, R Babcock, E Skovronek, D Mitchell, R Palmon; citation_volume=26; citation_issue=1; citation_publication_date=1990; citation_pages=128-136; citation_doi=10.1037/0012-1649.26.1.128; citation_id=CR39"/>

    <meta name="citation_reference" content="citation_title=The visualization toolkit: an object-oriented approach to 3D graphics; citation_publication_date=2002; citation_id=CR40; citation_author=WJ Schroeder; citation_author=K Martin; citation_author=B Lorensen; citation_publisher=Prentice-Hall"/>

    <meta name="citation_reference" content="citation_title=Simulation and visualization in medical diagnosis: perspectives and computational requirements; citation_inbook_title=Advanced infrastructures for future healthcare; citation_publication_date=2000; citation_pages=275-282; citation_id=CR41; citation_author=PMA Sloot; citation_publisher=IOS Press"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=Human factors issues in virtual environments: a review of the literature; citation_author=KM Stanney, RR Mourant, RS Kennedy; citation_volume=7; citation_issue=4; citation_publication_date=1998; citation_pages=327-351; citation_doi=10.1162/105474698565767; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_journal_title=J Ind Technol; citation_title=Spatial visualization: fundamentals and trends in engineering graphics; citation_author=S Strong, R Smith; citation_volume=18; citation_issue=1; citation_publication_date=2001; citation_pages=1-13; citation_id=CR43"/>

    <meta name="citation_reference" content="Velez MC, Silver D, Tremaine M (2005) Understanding visualization through spatial ability differences. In: Proceedings of IEEE visualization 2005, Minneapolis, MN, USA, pp 511&#8211;518"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Assaying and isolating individual differences in searching a hierarchical file system; citation_author=K Vicente, B Hayes, R Williges; citation_volume=29; citation_issue=3; citation_publication_date=1987; citation_pages=349-359; citation_id=CR45"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Appl; citation_title=Individual differences in spatial learning from computer-simulated environments; citation_author=D Waller; citation_volume=8; citation_publication_date=2000; citation_pages=307-321; citation_doi=10.1037/1076-898X.6.4.307; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Factors; citation_title=Spatial representations of virtual mazes: the role of visual fidelity and individual differences; citation_author=D Waller, D Knapp, E Hunt; citation_volume=43; citation_issue=1; citation_publication_date=2001; citation_pages=147-158; citation_doi=10.1518/001872001775992561; citation_id=CR47"/>

    <meta name="citation_reference" content="Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased selection: strategies and traits. In: Proceedings of IEEE virtual reality 05, pp 163&#8211;170"/>

    <meta name="citation_reference" content="citation_journal_title=Future Gener Comput Syst; citation_title=Bringing combined interaction to a problem solving environment for vascular reconstruction; citation_author=EV Zudilova, PMA Sloot; citation_volume=21; citation_issue=7; citation_publication_date=2005; citation_pages=1167-1176; citation_doi=10.1016/j.future.2004.04.004; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Interfaces; citation_title=Combining desktop and virtual realities: addressing demands of real life clinical environments; citation_author=EV Zudilova-Seinstra; citation_volume=67; citation_publication_date=2006; citation_pages=11-13; citation_id=CR50"/>

    <meta name="citation_author" content="Elena Zudilova-Seinstra"/>

    <meta name="citation_author_email" content="E.V.Zudilova-Seinstra@uva.nl"/>

    <meta name="citation_author_institution" content="Computational Science, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands"/>

    <meta name="citation_author" content="Boris van Schooten"/>

    <meta name="citation_author_institution" content="Human-Media Interaction, Department of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands"/>

    <meta name="citation_author" content="Avan Suinesiaputra"/>

    <meta name="citation_author_institution" content="Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands"/>

    <meta name="citation_author" content="Rob van der Geest"/>

    <meta name="citation_author_institution" content="Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands"/>

    <meta name="citation_author" content="Betsy van Dijk"/>

    <meta name="citation_author_institution" content="Human-Media Interaction, Department of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands"/>

    <meta name="citation_author" content="Johan Reiber"/>

    <meta name="citation_author_institution" content="Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands"/>

    <meta name="citation_author" content="Peter Sloot"/>

    <meta name="citation_author_institution" content="Computational Science, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-009-0131-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2010/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-009-0131-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Exploring individual user differences in the 2D/3D interaction with medical image data"/>
        <meta property="og:description" content="User-centered design is often performed without regard to individual user differences. In this paper, we report results of an empirical study aimed to evaluate whether computer experience and demographic user characteristics would have an effect on the way people interact with the visualized medical data in a 3D virtual environment using 2D and 3D input devices. We analyzed the interaction through performance data, questionnaires and observations. The results suggest that differences in gender, age and game experience have an effect on people’s behavior and task performance, as well as on subjective user preferences."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Exploring individual user differences in the 2D/3D interaction with medical image data | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-009-0131-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"2D/3D interaction, Medical segmentation, Virtual environments, Multimodal, User study","kwrd":["2D/3D_interaction","Medical_segmentation","Virtual_environments","Multimodal","User_study"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"Y","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"vgzm.415900-10.1007-s10055-009-0131-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-009-0131-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=131;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-009-0131-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Exploring individual user differences in the 2D/3D interaction with medical image data
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0131-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0131-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
        <li class="c-article-identifiers__item">
            <span class="c-article-identifiers__open" data-test="open-access">Open Access</span>
        </li>
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2009-09-16" itemprop="datePublished">16 September 2009</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Exploring individual user differences in the 2D/3D interaction with medical image data</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Elena-Zudilova_Seinstra" data-author-popup="auth-Elena-Zudilova_Seinstra" data-corresp-id="c1">Elena Zudilova-Seinstra<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Amsterdam" /><meta itemprop="address" content="grid.7177.6, 0000000084992262, Computational Science, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Boris-Schooten" data-author-popup="auth-Boris-Schooten">Boris van Schooten</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Twente" /><meta itemprop="address" content="grid.6214.1, 0000000403998953, Human-Media Interaction, Department of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Avan-Suinesiaputra" data-author-popup="auth-Avan-Suinesiaputra">Avan Suinesiaputra</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Leiden University Medical Center" /><meta itemprop="address" content="grid.10419.3d, 0000000089452978, Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rob-Geest" data-author-popup="auth-Rob-Geest">Rob van der Geest</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Leiden University Medical Center" /><meta itemprop="address" content="grid.10419.3d, 0000000089452978, Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Betsy-Dijk" data-author-popup="auth-Betsy-Dijk">Betsy van Dijk</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Twente" /><meta itemprop="address" content="grid.6214.1, 0000000403998953, Human-Media Interaction, Department of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Johan-Reiber" data-author-popup="auth-Johan-Reiber">Johan Reiber</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Leiden University Medical Center" /><meta itemprop="address" content="grid.10419.3d, 0000000089452978, Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Peter-Sloot" data-author-popup="auth-Peter-Sloot">Peter Sloot</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Amsterdam" /><meta itemprop="address" content="grid.7177.6, 0000000084992262, Computational Science, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 14</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">105</span>–<span itemprop="pageEnd">118</span>(<span data-test="article-publication-year">2010</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1574 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">6 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-009-0131-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>User-centered design is often performed without regard to individual user differences. In this paper, we report results of an empirical study aimed to evaluate whether computer experience and demographic user characteristics would have an effect on the way people interact with the visualized medical data in a 3D virtual environment using 2D and 3D input devices. We analyzed the interaction through performance data, questionnaires and observations. The results suggest that differences in gender, age and game experience have an effect on people’s behavior and task performance, as well as on subjective user preferences.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>High-end clinical workstations may vary from non-immersive desktop systems to semi- and fully immersive virtual reality (VR) environments (Brooks <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Brooks F P Jr (1999) What’s real about virtual reality? IEEE Comput Graph Appl 19(6):16–27" href="/article/10.1007/s10055-009-0131-4#ref-CR7" id="ref-link-section-d30139e401">1999</a>). However, broad exploration of VR-based medical applications is hampered today by various usability and user-acceptance problems. These arise not only from uncomfortable user interfaces, but also from input/output devices chosen incorrectly for the deployment of an interactive medical environment.</p><p>Although medical systems have been among the targeted application areas of VR for years (Jin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Jin W, Lim Y-L, Xu XG, Singh TP, Suvranu DE (2005) Improving the visual realism of virtual surgery. In: Proceedings medicine meets virtual reality, vol 13, pp 227–233" href="/article/10.1007/s10055-009-0131-4#ref-CR23" id="ref-link-section-d30139e407">2005</a>; Hoffman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hoffman HG, Patterson DR, Carrougher GJ et al (2001) The effectiveness of virtual reality based pain control with multiple treatments. Clin J Pain 17:229–235" href="/article/10.1007/s10055-009-0131-4#ref-CR21" id="ref-link-section-d30139e410">2001</a>; Gabbard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Gabbard JL, Swartz K, Richey K, Hix D (1999) Usability evaluation techniques: a novel method for assessing the usability of an immersive medical visualization VE. In: Proceedings VWSIM’99, pp 165–170" href="/article/10.1007/s10055-009-0131-4#ref-CR15" id="ref-link-section-d30139e413">1999</a>), VR is hardly used today in the real-life clinical environment. Also, to our knowledge, available literature about the usage of virtual environments within the medical context does not provide much information on the problems and choices encountered when developing VR systems intended for such a specific context, especially with regard to optimal input/output devices.</p><p>Very often we do not take into account the fact that clinicians are mostly inexperienced computer users, and therefore they need intuitive interaction support and relevant feedback adapted to their knowledge and everyday skills (Sloot <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Sloot PMA (2000) Simulation and visualization in medical diagnosis: perspectives and computational requirements. In: Marsh A, Grandinetti L, Kauranne T (eds) Advanced infrastructures for future healthcare. IOS Press, Amsterdam, pp 275–282" href="/article/10.1007/s10055-009-0131-4#ref-CR41" id="ref-link-section-d30139e419">2000</a>). To provide clinicians with an intuitive environment to solve a target class of problems, a medical application has to be built in such a way that the user can exploit modern technologies without specialized knowledge of underlying hardware and software. Unfortunately, in reality the situation is far from ideal.</p><p>Not only 3D user interfaces are generally unfamiliar to medical specialists but also using them brings along new issues that do not come into play when dealing with traditional 2D desktop applications. A complete analysis of a VR-based medical application needs to take into account how the interaction techniques and devices being offered allow the clinician to map his/her high-level objectives and tasks into specific actions that can be interpreted and executed by the system.</p><p>To address this research concern, we developed an experimental multimodal visualization framework that supports input and display devices of both VR and desktop systems. It includes the 2D/3D switchable Sharp LL-151-3D auto-stereoscopic monitor and the 2D/3D Essential Reality P5 glove. These devices allow semi-immersive virtual and non-immersive desktop realities to be alternated in a sequential manner.</p><p>The paper reports the current implementation status and presents an experimental study conducted to evaluate whether individual user differences (i.e., gender, age, computer experience) have an effect on the way people interact with 3D medical image data while performing interactive steering tasks. Semi-automated medical segmentation served as the context for this research. We compared the virtual P5 glove in a 2D/3D mode and the 2D Logitech PC mouse. Our design was repeated measures within-subjects for input method/device and task complexity. We report our main findings suggesting criteria for applying 2D/3D interaction to a medical exploration environment.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Evaluation has often been the missing component in the field of 3D interaction and visualization (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2005) 3D user interfaces: theory and practice. Pearson Education, Boston" href="/article/10.1007/s10055-009-0131-4#ref-CR6" id="ref-link-section-d30139e437">2005</a>). For years, researchers focused on the development of new interaction devices, techniques and metaphors for exploring 3D spaces without taking time to assess how good their designs are in comparison to alternative solutions (Johnson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Johnson C (2004) Top scientific visualization research problems. IEEE Comput Graph Appl 4:13–17" href="/article/10.1007/s10055-009-0131-4#ref-CR24" id="ref-link-section-d30139e440">2004</a>).</p><p>Prior research has shown that the efficient use of 2D graphical user interfaces strongly depends on human abilities. One of the primary user characteristics that interface designers adapt to is the level of experience or the expert-versus-novice paradigm. Eberts (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Eberts RE (1994) User interface design. Prentice-Hall, Englewood Cliffs" href="/article/10.1007/s10055-009-0131-4#ref-CR11" id="ref-link-section-d30139e446">1994</a>) reports that experts and novices have diverse capabilities and requirements that may not be compatible. Experience level influences the skills of the user, the abilities that predict performance and the manner in which users understand and organize task information (Dix et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Dix A, Finlay J, Abowd G, Beale R (1993) Human computer interaction. Prentice-Hall, New York" href="/article/10.1007/s10055-009-0131-4#ref-CR10" id="ref-link-section-d30139e449">1993</a>; Egan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Egan DE (1988) Individual differences in human–computer interaction. In: Helander M (ed) Handbook of human–computer interaction, Elsevier, North Holland, pp 543–568" href="/article/10.1007/s10055-009-0131-4#ref-CR12" id="ref-link-section-d30139e452">1988</a>).</p><p>Another adaptive approach addresses the plasticity of human cognitive abilities (Stanney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Stanney KM, Mourant RR, Kennedy RS (1998) Human factors issues in virtual environments: a review of the literature. Presence 7(4):327–351" href="/article/10.1007/s10055-009-0131-4#ref-CR42" id="ref-link-section-d30139e458">1998</a>). Several studies suggest that technical aptitudes (e.g., spatial visualization, orientation, memory, etc.) are significant in predicting HCI performance. Leitheiser and Munro (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Leitheiser B, Munro D (1995) An experimental study of the relationship between spatial ability and the learning of a graphical user interface. In: Proceedings of the inaugural America’s conference on information systems, Pittsburgh, PA, pp 122–124" href="/article/10.1007/s10055-009-0131-4#ref-CR28" id="ref-link-section-d30139e461">1995</a>) and Vicente et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Vicente K, Hayes B, Williges R (1987) Assaying and isolating individual differences in searching a hierarchical file system. Hum Factors 29(3):349–359" href="/article/10.1007/s10055-009-0131-4#ref-CR45" id="ref-link-section-d30139e464">1987</a>) concur that measures of spatial abilities predict performance in a variety of file management tasks, while experience alone does not influence task performance. Gagnon (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Gagnon D (1985) Videogames and spatial skills: an exploratory study. Educ Commun Technol 33(4):263–275" href="/article/10.1007/s10055-009-0131-4#ref-CR16" id="ref-link-section-d30139e467">1985</a>) reported the surprising result that computer game scores were not correlated with hand–eye coordination but were correlated with scores on a spatial memory test. Egan and Gomez (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1985" title="Egan D, Gomez M (1985) Assaying, isolating, and accommodating individual differences in learning a complex skill. In: Dillon R (ed) Individual differences in cognition, Academic Press, London, pp 173–217" href="/article/10.1007/s10055-009-0131-4#ref-CR13" id="ref-link-section-d30139e470">1985</a>) found that measures of spatial memory and age provided the best predictors of how well participants learned to use a text editor.</p><p>Spatial abilities as a component of human intelligence have been considered by cognitive psychologists for many years. Consequently, plenty of related studies have been performed. Some well agreed upon findings are that there are considerable differences in spatial abilities among the general population. Velez et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Velez MC, Silver D, Tremaine M (2005) Understanding visualization through spatial ability differences. In: Proceedings of IEEE visualization 2005, Minneapolis, MN, USA, pp 511–518" href="/article/10.1007/s10055-009-0131-4#ref-CR44" id="ref-link-section-d30139e476">2005</a>) report that males on average score better on standard paper tests of spatial abilities, while Salthouse et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Salthouse T, Babcock R, Skovronek E, Mitchell D, Palmon R (1990) Age and experience effects in spatial visualization. Dev Psychol 26(1):128–136" href="/article/10.1007/s10055-009-0131-4#ref-CR39" id="ref-link-section-d30139e479">1990</a>) argue that increased age is associated with lower levels of performance on spatial visualization tests for both unselected adults and adults with extensive spatial visualization experience. According to Lohman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Lohman DF (1996) Spatial ability and G. In: Dennis I, Tapsfield P (eds) Human abilities: their nature and assessment. Erlbaum, Hillsdale, pp 97–116" href="/article/10.1007/s10055-009-0131-4#ref-CR29" id="ref-link-section-d30139e482">1996</a>), spatial abilities can be improved via training and experience, e.g., playing action video games helps to reduce gender differences in spatial cognition (Feng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Feng J, Spence I, Pratt J (2007) Playing an action video game reduces gender differences in spatial cognition. Psychol Sci 18(10):850–855" href="/article/10.1007/s10055-009-0131-4#ref-CR14" id="ref-link-section-d30139e485">2007</a>).</p><p>Virtual environments have often been used as a means to study human spatial behavior. Related literature reports on notable individual differences in spatial behavior attributable to computer experience (Wingrave et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased selection: strategies and traits. In: Proceedings of IEEE virtual reality 05, pp 163–170" href="/article/10.1007/s10055-009-0131-4#ref-CR48" id="ref-link-section-d30139e492">2005</a>), gender (Larson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Larson P, Rizzo AA, Buckwalter JG, van Rooyen A, Kratz K, Neumann U, Kesselman C, Thiebaux M, van der Zang C (1999) Gender issues in the use of virtual environments. Cyberpsychol Behav 2(2):113–124" href="/article/10.1007/s10055-009-0131-4#ref-CR27" id="ref-link-section-d30139e495">1999</a>; Waller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Waller D (2000) Individual differences in spatial learning from computer-simulated environments. J Exp Psychol Appl 8:307–321" href="/article/10.1007/s10055-009-0131-4#ref-CR46" id="ref-link-section-d30139e498">2000</a>) and spatial abilities (Luursema et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Luursema J-M, Verwey WB, Kommers PAM, Annema J-H (2008) The role of stereopsis in virtual anatomical learning. Interact Comput 20(4):455–460" href="/article/10.1007/s10055-009-0131-4#ref-CR30" id="ref-link-section-d30139e501">2008</a>; Rizzo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Rizzo A, Buckwalter JG, van der Zaag C (2000) Virtual environment applications in clinical neuropsychology. The handbook of virtual environments. L.A. Erlbaum, New York" href="/article/10.1007/s10055-009-0131-4#ref-CR36" id="ref-link-section-d30139e504">2000</a>). Relevant research includes comparing spatial information transfer of virtual environments to the real world (Waller et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Waller D, Knapp D, Hunt E (2001) Spatial representations of virtual mazes: the role of visual fidelity and individual differences. Hum Factors 43(1):147–158" href="/article/10.1007/s10055-009-0131-4#ref-CR47" id="ref-link-section-d30139e508">2001</a>) and real world studies such as selecting objects with a laser pointer (Myers et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Myers B, Bhatnagar R, Nichols J, Peck C, Kong D, Miller R, Long AC (2002) Interaction at a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the conference on human factors and computing systems (CHI’2002), Minneapolis, Minnesota, USA, pp 33–40" href="/article/10.1007/s10055-009-0131-4#ref-CR33" id="ref-link-section-d30139e511">2002</a>). Results show that users are able to exploit spatial abilities and to transfer organizational knowledge to a 3D virtual environment. Furthermore, virtual environments have been used to assess and to treat balance (Jacobson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Jacobson J, Redern MS, Furnan JM, Whiney LW, Sparto PJ, Wilson JB, Hodges LF (2001) Balance NAVE: a virtual reality facility for research and rehabilitation of balance disorders. In: Proceedings of virtual reality software technology (VR’01), pp 103–109" href="/article/10.1007/s10055-009-0131-4#ref-CR22" id="ref-link-section-d30139e514">2001</a>) and psychological disorders (Hodges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Hodges L, Anderson R, Burdea G, Hoffman H, Rothbaum B (2001) Treating psychological and physical disorders with VR. IEEE Comput Graph Appl 21(6):25–33" href="/article/10.1007/s10055-009-0131-4#ref-CR20" id="ref-link-section-d30139e517">2001</a>; Botella et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Botella C, Baños RM, Perpiñá C, Villa H, Alcañiz M, Reym A (1998) Virtual reality treatment of claustrophobia: a case report. Behav Res Ther 36(2):239–246" href="/article/10.1007/s10055-009-0131-4#ref-CR4" id="ref-link-section-d30139e520">1998</a>), as well as to improve spatial rotation among deaf and hard-of-hearing children (Passig and Eden <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Passig D, Eden S (2001) Virtual reality as a tool for improving spatial rotation among deaf and hard-of-hearing children. Cyberpsychol Behav 4(6):681–686" href="/article/10.1007/s10055-009-0131-4#ref-CR34" id="ref-link-section-d30139e523">2001</a>).</p><p>A variety of input devices like data gloves, joysticks and hand-held wands allow the user to navigate through a virtual environment and to interact with virtual objects. Input devices can be characterized by their degrees of freedom (DOF), which describe the possible interaction space (He and Kaufman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="He T, Kaufman AE (1993) Virtual input devices for 3D systems. In: Proceedings of IEEE visualization 1993. IEEE Computer Society, San Jose, CA, pp 142–148" href="/article/10.1007/s10055-009-0131-4#ref-CR18" id="ref-link-section-d30139e529">1993</a>). 2D input devices (e.g., mouse, joystick, etc.) are bound to the (<i>x</i>, <i>y</i>) plane and have only 2DOF available for interaction. 3D input devices (e.g., space mouse, data glove, phantom, wand, etc.) have 6DOF describing translation of the device along any of three perpendicular axes (<i>x</i>, <i>y</i>, <i>z</i>) and rotation of the device around any of these axes.</p><p>There have been several experiments performed to compare 2D and 3D input, which showed that 2D and 3D input devices have their advantages and disadvantages in the sense that some are better suited for certain tasks than others. Most of the studies, however, have been performed across basic manipulation, docking or navigation tasks (Martens et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Martens J-B, van Liere R, Kok A (2007) Widget manipulation revisited: a case study in modeling interactions between experimental conditions. In: Proceedings of the IPT-EGVE symposium, The Eurographics Association, paper 1021" href="/article/10.1007/s10055-009-0131-4#ref-CR31" id="ref-link-section-d30139e551">2007</a>; Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, London UK, pp 26–33" href="/article/10.1007/s10055-009-0131-4#ref-CR5" id="ref-link-section-d30139e554">1999</a>; Roessler and Grantz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Roessler A, Grantz V (1998) Performance evaluation of input devices in virtual environments. In: Proceedings of the IFIP working group 13.2 conference on designing effective and usable multimedia systems, vol 133, pp 68–73" href="/article/10.1007/s10055-009-0131-4#ref-CR38" id="ref-link-section-d30139e557">1998</a>; Hinckley et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassel DN (1997) Usability analysis of 3D rotation techniques. In: UIST ‘97: proceedings of the 10th annual ACM symposium on user interface software and technology, New York, NY, pp 1–10" href="/article/10.1007/s10055-009-0131-4#ref-CR19" id="ref-link-section-d30139e560">1997</a>), while realistic medical tasks have been very rarely considered (Krüger et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Krüger A, Irrgang S, Hertel I, Strauss G, Preim P (2007) Comparison of 2D and 3D input devices for virtual endoscopy. In: CURAC 2007, pp 215–218" href="/article/10.1007/s10055-009-0131-4#ref-CR26" id="ref-link-section-d30139e563">2007</a>; Bornik et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: Proceedings of the IEEE symposium on 3D user interfaces, pp 29–36" href="/article/10.1007/s10055-009-0131-4#ref-CR3" id="ref-link-section-d30139e567">2006</a>).</p><p>In the medical context, neither usability, nor human-related factors have been sufficiently addressed yet with regard to the choice of input devices. Information on which interaction technique and device are most suitable for a specific medical exploration task is yet too scarce to make an informed choice. Also, it remains unclear whether individual user differences have an effect on the 2D/3D interaction with the visualized medical data, as well as on subjective user preferences for available input methods.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">A multimodal visualization framework for medical image analysis</h2><div class="c-article-section__content" id="Sec3-content"><p>Virtual and desktop realities are alternative solutions that allow users to manipulate and navigate through visualized datasets. Even though both virtual and desktop systems are viable alternatives for the image-based exploration, none of them is able to provide optimal means for analyzing medical data. In our prior research (Zudilova and Sloot <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Zudilova EV, Sloot PMA (2005) Bringing combined interaction to a problem solving environment for vascular reconstruction. Future Gener Comput Syst 21(7):1167–1176" href="/article/10.1007/s10055-009-0131-4#ref-CR49" id="ref-link-section-d30139e580">2005</a>), we discovered that for the medical exploration tasks, where the insight view or collaboration between clinicians is important, VR would be the best choice. But when performance and accuracy are vital, the medical application running on a desktop system is usually preferable.</p><p>Having this in mind, we developed a multimodal visualization framework that supports features of both desktop and VR systems. As can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig1">1</a>, right, the framework does not require much space. It is portable and relatively cheap, which makes it a valuable option for hospitals, as they usually do not have sufficient space and budget available for more complex VR configurations (Cramer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Cramer HSM, Evers V, Zudilova EV, Sloot PMA (2004) Context analysis to support development of virtual reality applications. Virtual Real 7(3):177–186" href="/article/10.1007/s10055-009-0131-4#ref-CR9" id="ref-link-section-d30139e589">2004</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A multimodal visualization framework for medical image analysis (<i>right</i>) and the virtual P5 glove (<i>left</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The framework includes a 15 in. auto-stereoscopic monitor Sharp LL-151-3D providing a view on a virtual environment (<a href="http://www.vrealities.com/sharpll1513d.html">http://www.vrealities.com/sharpll1513d.html</a>). The Sharp’s TFT 3D LCD Technology makes the image on the screen appear in 3D without the need for the user to wear special glasses. The display can be set to monoscopic or stereoscopic viewing modes electronically, offering a single display for both VR-based visualization and normal 2D work.</p><p>The handling of objects in a 3D virtual environment typically involves manipulation and system control, which often supports manipulation itself. The multimodal visualization framework uses keyboard input for the system control and allows mixing of the glove and mouse input for direct manipulations. We chose a P5 Glove Controller from Essential Reality (<a href="http://www.vrealities.com/P5.html">http://www.vrealities.com/P5.html</a>) because it is a switchable device that can be used to control both 2D and 3D input.</p><p>The virtual P5 glove features five bend sensors to track bending of the user’s fingers and an infrared-based optical tracking system allowing computation of the glove position and orientation with the frequency of 60 times/s. The glove consists of a base station housing infrared receptors enabling spatial tracking. The glove itself is connected to the base station with a cable and consists of a plastic housing that is strapped to the back of the user’s hand, with five bendable strips connected to the fingers to determine the bend of each individual finger. The glove has 2.4 mm resolution and 9.7 mm accuracy for position and 1° resolution and accuracy for orientation measurements. Also, it provides five single joint independent finger measurements with 0.5° resolution. The measurement of a finger bend returns an integer value in the range [0, 63]. These values can be personalized in a quick calibration phase, such that they are converted to the actual finger bending of each user. Furthermore, on top of the housing there are four buttons that can be used to provide additional functionality (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig1">1</a>, left).</p><p>The main disadvantage of the virtual P5 glove is a relatively small range from the receptor (1.5 m) that allows accurate tracking of position and orientation and the tracking of a single joint finger bending. However, since the user usually sits about 40 cm away from the computer screen, this disadvantage is easily dissolved by putting the infrared receptor next to the screen. Also, during our work with the glove, we learned that the spatial tracking data were not always reliable. To ascertain sufficiently reliable values, additional filtering mechanisms were developed, including the dynamic averaging procedure based on the rate of changes in motion and rotation data.</p><p>The semi-automated segmentation of the medical images of the patients suffering from atherosclerosis serves as the context for our research. Image segmentation techniques are essential to provide objective quantitative data characterizing a vascular abnormality. In many pathologies, a limited number of quantitative parameters describe the relevant clinical findings from the imaging study. Like any image processing system, automated segmentation algorithms can produce mistakes, e.g., when the contrast level or the amount of noise differs from those specified or when the bifurcations or closely located vessels are misinterpreted by the algorithm and as such affect the segmentation result (Adame et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Adame IM, van der Geest RJ, Wasserman BA, Mohamed M, Reiber JHC, Lelieveldt BPF (2004) Automatic segmentation and plaque characterization in atherosclerotic carotid artery MR images. MAGMA (Magn Reson Mater Phys Biol Med) 16(5):227–234" href="/article/10.1007/s10055-009-0131-4#ref-CR1" id="ref-link-section-d30139e649">2004</a>). To obtain correct measurements, manual adjustments or overwrites to automated segmentation results are frequently needed in routine clinical practice. Often manual editing is a time-consuming and tedious procedure and affects the otherwise objective measurements. To overcome these problems, semi-automated image processing techniques need to be integrated in the data exploration process such that image segmentation, visualization and user steering become a unified process.</p><p>The multimodal visualization framework developed in this project is built on the principle that the user (clinician) will be able to alternate desktop and virtual realities while performing interactive steering tasks related to medical segmentation, e.g., selection of the region of interest, interactive placement of seed points, labeling, centerline correction, contour editing, etc.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Method</h2><div class="c-article-section__content" id="Sec4-content"><p>All users of an interface bring their preferences, aptitudes (physical, perceptual and cognitive) and prior experiences in the world. These attributes can be considered as distinct from, but interacting with, the user’s knowledge and skills that result from direct experience, practice, feedback and training on an interface (Wingrave et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased selection: strategies and traits. In: Proceedings of IEEE virtual reality 05, pp 163–170" href="/article/10.1007/s10055-009-0131-4#ref-CR48" id="ref-link-section-d30139e662">2005</a>).</p><p>The present study examines whether individual user differences influence task performance and subjective preferences for 2D/3D input methods, applied to manipulate the visualized medical data. By having participants surveyed for demographic information (age, gender, education, etc.), as well as for computer experience (computer use, game experience, experience with 3D graphics, etc.) and physical characteristics (hand dominance, acuity of vision, etc.) that have potential relations to skills needed to perform specific interactive steering tasks, we can begin to uncover predictors of performance and highlight user attributes that may influence the choice of input methods/devices and design of a virtual medical environment in general.</p><h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec5">Tasks</h3><p>To perform the study, we chose two interactive steering tasks related to medical segmentation: selection of the region of interest (selection task) and correction of the automatically generated centerline (positioning task). These tasks were selected for two reasons. Both tasks are frequently performed and are crucial for the successful completion of the segmentation process. Also, for these tasks, objectives can be precisely defined and potentially confounding factors can be controlled.</p><p>We compared the virtual P5 glove in a 2D/3D mode and the 2D Logitech PC mouse. In the tasks, participants had to manipulate so-called 3D widgets. Simply speaking, a widget is an object in a scene that responds to user events (e.g., mouse clicks) and data changes by corresponding changes in its appearance or behavior (Conner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: Proceedings of the 1992 symposium on interactive 3D graphics. ACM Press, New York, NY, USA, pp 183–188" href="/article/10.1007/s10055-009-0131-4#ref-CR8" id="ref-link-section-d30139e676">1992</a>). 3D widgets make the user interaction with 3D objects more intuitive by providing fast semantic feedback.</p><p>In the selection task, participants had to select the region of interest by manipulating a 3D box widget. In the positioning task, they had to adjust the position of a centerline by manipulating a 3D spline widget. 3D box and spline widgets are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig2">2</a> and described in more detail in the next section.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A 3D box widget applied to the selection task (<i>left</i>), a 3D spline widget applied to the positioning task (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading c-article__sub-heading--divider" id="Sec6">The widget interface</h3><p>The custom experimental environment was developed using the kitware visualization toolkit (VTK), where different types of widgets require their own way of interaction (Schroeder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Schroeder WJ, Martin K, Lorensen B (2002) The visualization toolkit: an object-oriented approach to 3D graphics, 3rd edn. Prentice-Hall, Englewood Cliffs" href="/article/10.1007/s10055-009-0131-4#ref-CR40" id="ref-link-section-d30139e718">2002</a>).</p><p>Represented by an arbitrarily oriented hexahedron with orthogonal faces, a 3D box widget defines a region of interest (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig2">2</a>, left). It has seven handles that can be manipulated. The first six correspond to the six faces and can be used for the face-based scaling of the widget. By grabbing these six face handles, faces can be moved in the direction of one of three axes (<i>x</i>, <i>y</i> or <i>z</i>) depending on the handle position.</p><p>The seventh handle is in the center of the hexahedron. By grabbing the central handle, the entire hexahedron can be translated in 3D space. With 2DOF input, the positioning of the hexahedron in 3D space requires two sequential actions: translation of the central handle in the (<i>x</i>, <i>y</i>) plane combined with scene rotation, performed in the direction defined from the center of the viewport toward the cursor position. With 6DOF input, the positioning of the hexahedron in 3D space can be achieved via one atomic action with a 3D input device.</p><p>In addition, all faces of the hexahedron can be manipulated. These allow the face-based rotation of the hexahedron. With 2DOF input, face-based rotation is determined by <i>x</i> and <i>y</i> coordinates of the input device, which implies that orientation of the hexahedron can be adjusted, while the position of the central handle remains the same. For instance, starting from the initial condition (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig3">3</a>a), the user selects the upper face (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig3">3</a>b) and drags the cursor down with a 2DOF input device causing the hexahedron to be rotated around the <i>x</i>-axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig3">3</a>c). With 6DOF input, face-based rotation is determined by <i>x</i>, <i>y</i> and <i>z</i> coordinates and orientation of the input device and performed in such a way that orientation of a selected widget face is always identical to that of the input device (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig3">3</a>d).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Illustrations of the face-based rotation technique</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>A 3D spline widget has spherical handles that can be translated to change the shape of the spline (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig2">2</a>, right). With 2DOF input, each handle can be translated only within the (<i>x</i>, <i>y</i>) plane. With 6DOF input, handles can be freely translated and oriented in 3D space. By picking on a line segment, forming the spline, the complete spline can be translated. The translation of the spline in 3D space is performed in a similar way as the 3D box widget translation explained earlier.</p><p>Visualization toolkit allows both widgets to be controlled via a standard PC mouse. By moving the mouse while keeping the left button pressed, widget elements (e.g., handle, line segment, face, etc.) can be manipulated. Scaling is achieved by using the right mouse button “up” the render window (makes the widget bigger) or “down” the render window (makes the widget smaller).</p><p>The VTK C++ hierarchy has been extended with new classes to support the P5 glove 2DOF/6DOF interaction with 3D box and spline widgets and to record the user interaction data. To optimize time required for training subjects, we decided to mostly use buttons (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig1">1</a>, left) for the widget control. In particular, to select the widget element, the user has to press the button “A” when the cursor reaches the element that has to be selected. In a 3D mode, the widget element can be manipulated by changing position and orientation of the glove. In a 2D mode, the virtual P5 glove functions as the mouse. To deactivate selection, the button “B” should be pressed. Scaling can be achieved by keeping the button “C” pressed and changing the position of the glove, while moving the arm up and down, vertically.</p><p>With the mouse, scene rotation occurs continuously as long as the mouse left button is pressed. With a glove, rotation starts when the wearer’s index finger is bended to a certain degree. Rotation stops, when the index finger is no longer bended.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Experimental setup</h2><div class="c-article-section__content" id="Sec7-content"><p>Our experiment aimed at quantifying hypotheses formulated based on the argumentation from previous literature. We decided to focus on gender, age and computer experience-related differences for two reasons. These individual user characteristics are easily observed and they are often considered as categorical distinctions for noting differences in people’s spatial abilities (Strong and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Strong S, Smith R (2001) Spatial visualization: fundamentals and trends in engineering graphics. J Ind Technol 18(1):1–13" href="/article/10.1007/s10055-009-0131-4#ref-CR43" id="ref-link-section-d30139e829">2001</a>; Hartman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hartman NW, Connolly PE, Gilger JW, Bertoline GR, Heisler J (2006) Virtual reality-based spatial skills assessment and its role in computer graphics education. In: Proceedings of the international conference on computer graphics and interactive techniques (ACM SIGGRAPH 2006), Educators program, article No. 46" href="/article/10.1007/s10055-009-0131-4#ref-CR17" id="ref-link-section-d30139e832">2006</a>).</p><p>Before running the experiment, we defined the following four hypotheses:</p><p><i>H1</i>: Due to different spatial abilities of men and women (Luursema et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Luursema J-M, Verwey WB, Kommers PAM, Annema J-H (2008) The role of stereopsis in virtual anatomical learning. Interact Comput 20(4):455–460" href="/article/10.1007/s10055-009-0131-4#ref-CR30" id="ref-link-section-d30139e842">2008</a>; Waller <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Waller D (2000) Individual differences in spatial learning from computer-simulated environments. J Exp Psychol Appl 8:307–321" href="/article/10.1007/s10055-009-0131-4#ref-CR46" id="ref-link-section-d30139e845">2000</a>), it is expected that gender will have an influence on the task completion time for both selection and positioning tasks, as well as on the people’s choice for available interaction strategies, where strategies are observed behaviors that certain participants performed to increase their overall performance.</p><p><i>H2</i>: Age is expected to play a role in both time and accuracy. Due to the age-related differences in mental rotation (Berg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Berg C, Hertzog C, Hunt E (1982) Age differences in the speed of mental rotation. Dev Psychol 18(1):95–107" href="/article/10.1007/s10055-009-0131-4#ref-CR2" id="ref-link-section-d30139e853">1982</a>), we expect that younger subjects will be able to benefit more from the usage of the virtual P5 glove for the interaction with a 3D virtual environment than older subjects. This may also influence user preferences for available input methods/devices.</p><p><i>H3</i>: Computer experience (i.e., game experience, computer and graphics usage) is expected to have an effect on task performance, as well as on subjective user preferences (Wingrave et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased selection: strategies and traits. In: Proceedings of IEEE virtual reality 05, pp 163–170" href="/article/10.1007/s10055-009-0131-4#ref-CR48" id="ref-link-section-d30139e862">2005</a>).</p><p><i>H4</i>: Due to different demands (i.e., visual display, complexity, etc.) imposed upon the users by selection and positioning tasks, the influence of individual user differences will vary depending on the task being performed (Karwowski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Karwowski W (2006) International encyclopedia of ergonomics and human factors. CRC Press, Boca Raton" href="/article/10.1007/s10055-009-0131-4#ref-CR25" id="ref-link-section-d30139e870">2006</a>).</p><h3 class="c-article__sub-heading" id="Sec8">Experimental conditions</h3><p>Our design was 3 × 3 repeated measures within-subjects for input method/device and task complexity. The virtual P5 glove in a 3D mode (3D glove) was tested against the Logitech PC mouse and the virtual P5 glove in a 2D mode (2D glove). We experimented with one 6DOF (3D glove) and two 2DOF (mouse and 2D glove) input methods. The 2D glove condition was included to ensure that our results would not be biased due to the prior intensive mouse experience of participants and resolution differences of devices. The order of input methods was counterbalanced to prevent carry-over effects (e.g., learning or fatigue).</p><p>In our study, we applied the evaluation methodology introduced by Moise et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Moise A, Atkins MS, Rohling R (2005) Evaluating different radiology workstation interaction techniques with radiologists and laypersons. J Digit Imaging 18(2):116–130" href="/article/10.1007/s10055-009-0131-4#ref-CR32" id="ref-link-section-d30139e882">2005</a>). According to Moise et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Moise A, Atkins MS, Rohling R (2005) Evaluating different radiology workstation interaction techniques with radiologists and laypersons. J Digit Imaging 18(2):116–130" href="/article/10.1007/s10055-009-0131-4#ref-CR32" id="ref-link-section-d30139e885">2005</a>), it is possible to test the radiology workstation interaction features using look-alike radiological tasks and inexperienced laypersons, and that the results transfer to radiologists performing the same tasks. We adjusted the custom experimental environment in such a way, that selection and positioning tasks could be easily interpreted and performed by people without medical background and ran a small pilot study to make sure that our experiment was indeed suitable for laypersons.</p><p>In the selection task, participants were asked to select the specified region of interest. To achieve this, they had to manipulate a 3D box widget, initially positioned in such a way that all vessel structures displayed on the screen were covered by the widget. We introduced three complexity levels (low, medium, high) for each task. The complexity of the selection task was defined by the number and density of vessels, from which participants had to choose the correct vessel segment (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig4">4</a>):</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Illustrations of the selection task in the 3D widget experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <ul class="u-list-style-bullet">
                    <li>
                      <p>Level 1 (low)—one vessel;</p>
                    </li>
                    <li>
                      <p>Level 2 (medium)—two closely located vessels;</p>
                    </li>
                    <li>
                      <p>Level 3 (high)—three vessels, where two vessels are closely located.</p>
                    </li>
                  </ul>
                        <p>In the positioning task, participants were asked to adjust the position of a centerline represented by a 3D spline widget in such a way that all spline handles would be located inside the vessel segment. Initially, the 3D spline widget was located such that only the first and last handles were positioned correctly. To allow participants to easily notice positioning problems, we used occlusion cues. The complexity of the task was defined by length and curvature of the vessel segment and the number of handles, which positions had to be adjusted (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig5">5</a>):</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Illustrations of the positioning task in the 3D widget experiment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <ul class="u-list-style-bullet">
                    <li>
                      <p>Level 1 (low)—a five-handle spline widget has to be positioned inside a small vessel branch;</p>
                    </li>
                    <li>
                      <p>Level 2 (medium)—a nine-handle spline widget has to be positioned inside a mid-size highly curved vessel branch;</p>
                    </li>
                    <li>
                      <p>Level 3 (high)—a 12-handle spline widget has to be positioned inside a large-size vessel branch curved in the middle.</p>
                    </li>
                  </ul>
                        <h3 class="c-article__sub-heading" id="Sec9">Procedure</h3><p>A total of 30 volunteers (13 female, 17 male) were recruited from different departments of the Informatics Institute. All participants performed both selection and positioning tasks, which were assigned in a random order. Participants had varied levels of computer and graphics usage, as well as of game experience. None reported previous experience with 3D data glove devices. 29 participants were right-handed. One was ambidextrous.</p><p>The experimental sessions consisted of four trial series and lasted approximately 45 min. Participants first reviewed instructions and completed a prior-trial on-line questionnaire. The following information has been collected for each participant: name, age, gender, background, hand dominance, acuity of vision, computer use, gaming experience, experience with graphics and interaction devices.</p><p>Then participants received a short demonstration regarding tasks and the interface. Before each trial series, participants completed a training session to get familiar with the task and the input method. For each condition, trials were assigned in ascending order of task complexity to provide optimum conditions for the task-related skill development and efficient scheduling of task performance components (Robinson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Robinson P (2001) Cognition and second language instruction. Cambridge University Press, London" href="/article/10.1007/s10055-009-0131-4#ref-CR37" id="ref-link-section-d30139e984">2001</a>).</p><p>Participants were instructed that time and accuracy were of equal importance and provided with the indication of accuracy for both tasks. However, no indication was given what would be fast enough. The precise definition of performance was left to their own judgment. When satisfied with the result, participants selected the next trial (complexity level) from the system menu.</p><p>Dependent variables were the task completion time, accuracy and subjective ratings of the ease-of-use and preference. We defined the task completion time as the duration between the moment when the current trial was loaded and the moment when the next trial was selected from the menu. Accuracy of the selection task was measured via a surface-based comparison of the selected region of interest and the ideal result with allowed precision of 5%. To measure accuracy of the positioning task, the number of line segments were counted that either had one intersection with or were positioned completely outside the vessel segment.</p><p>In a post-trial questionnaire, participants rated 2D/3D input methods available for selection and positioning tasks and indicated their preferences. Subjective ratings were administered using a four point scale and open-ended questions. We used the SurveyMonkey.com (<a href="http://www.surveymonkey.com">http://www.surveymonkey.com</a>) online tool to develop questionnaires and to collect responses.</p><p>In addition, a few specific measurements were taken per each condition, including the scene rotation time and the total interaction time. The interaction time is the actual time spent on direct manipulation. Furthermore, we measured the face-based scaling time, the face-based rotation time and the hexahedron translation time for the selection task. For the positioning task, we measured the handle translation time and the time spent on spline translation.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Results</h2><div class="c-article-section__content" id="Sec10-content"><p>In this section, we discuss our results suggesting that differences in gender, age and game experience have an effect on user behavior and performance, as well as on subjective user preferences. In the sample of 30 study participants, there were no correlations among these three independent variables found.</p><p>For testing significance, we used analysis of variance (ANOVA) and post hoc Tukey HSD (honestly significant difference) tests. We applied Tukey HSD tests for pairwise comparisons because the Tukey HSD test is more sensitive when making large number of comparisons than other commonly used post hoc tests, i.e., Bonferroni <i>t</i> tests (Plichta and Garzon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Plichta SB, Garzon LS (2008) Statistics for nursing and allied health. Williams &amp; Wilkins, Lippincott" href="/article/10.1007/s10055-009-0131-4#ref-CR35" id="ref-link-section-d30139e1019">2008</a>). Timing data were transformed using a natural logarithm to improve the fit to a normal curve and then analyzed using repeated measures factorial 3 × 3 ANOVA. Repeated measures factorial ANOVA was applied because each subject was tested in all conditions. When Mauchly’s test of sphericity indicated it was necessary, we used the Huynh–Feldt correction. For rating scale data, the non-parametric Kruskal–Wallis ANOVA by Ranks was applied.</p><h3 class="c-article__sub-heading" id="Sec11">Gender issues</h3><p>The effect of gender turned out to be slightly less than we expected. In particular, we do not have enough statistical evidence to support the first part of hypothesis H1, stating that the task completion time was affected by gender differences. However, our results suggest that gender had a significant effect on the total interaction time measured for both selection and positioning tasks (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Average interaction time for the selection (<i>left</i>) and positioning (<i>right</i>) tasks. Timing data are normalized by log transformation; <i>vertical bars</i> denote 0.95 confidence interval</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>ANOVA revealed that female subjects spent significantly more time on the actual interaction with a virtual environment while performing selection tasks than male subjects, <i>F</i>(1, 29) = 7.4, <i>p</i> = 0.011. The average interaction time was 24.8 s for women and 21.32 s for men. There was also the main effect of task complexity on the interaction time, <i>F</i>(1.85, 51.81) = 91.11, <i>p</i> &lt; 0.001, indicating that more complex selection tasks were generally much longer and as such required more time to be spent on direct manipulation by both men and women. Post hoc Tukey HSD tests (at <i>p</i> ≤ 0.05) were conducted to examine further the effect of task complexity on the gender-related differences in the interaction time. A significant difference between men and women in the average time spent on interaction was found only for the most complex trial of Level 3.</p><p>In the positioning task, the average interaction time was 65.07 s for women and 46.74 s for men. ANOVA found the significant interaction effect between gender and input method [<i>F</i>(2, 56) = 3.3, <i>p</i> = 0.044], suggesting that the difference in the total interaction time between female and male subjects was greater with the 2D glove than with the 3D glove and with the 3D glove than with the mouse. Also, we found the main effect of task complexity on the interaction time, <i>F</i>(1.85, 51.8) = 40.6, <i>p</i> &lt; 0.001, indicating that both men and women spent significantly more time on direct manipulation when performing more complex trials of the positioning task. Tukey HSD tests (at <i>p</i> ≤ 0.05) revealed that the average time spent by male subjects on direct manipulation while performing more complex trials of Level 2 and Level 3 was significantly lower compared to female subjects.</p><p>Hence, women spent more time on the actual interaction with a virtual environment than men while performing both selection and positioning tasks. These can be due to different interaction strategies chosen by male and female subjects.</p><p>As mentioned in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0131-4#Sec6">4.2</a>, participants had a choice of three manipulation techniques that could be used (separately or in combination) to perform the selection task: face-based scaling, face-based rotation and hexahedron translation. Although any of these techniques in principle allows the correct result to be achieved, they require from users different skills. For instance, face-based rotation and hexahedron translation require more motor skills than face-based scaling. On the other hand, face-based scaling usually generates more errors and as such requires more precision and decision-making.</p><p>To check whether men and women made different choices for manipulation techniques, we combined timing data obtained for face-based scaling, face-based rotation and hexahedron translation and then analyzed these merged data via 3 × 3 × 3 repeated measures factorial ANOVA (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Average time spent on face-based scaling, face-based rotation and box (<i>hexahedron</i>) translation. Timing data are normalized by log transformation; <i>vertical bars</i> denote 0.95 confidence interval</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Overall, the face-based scaling technique was frequently applied by men and women to perform selection tasks. However, while male subjects mostly used face-based scaling, female subjects actively combined all three manipulation techniques, especially when performing more complex trials. ANOVA found the significant interaction effect between gender and the type of manipulation [<i>F</i>(2, 38) = 7.5, <i>p</i> = 0.02], suggesting that face-based scaling was indeed used significantly more by men than by women, while with two other manipulation techniques it was other way around. These eventually resulted in a significant difference in the average interaction time.</p><p>Tukey HSD tests (at <i>p</i> ≤ 0.05) revealed that participants mostly used face-based rotation to perform complex selection tasks with the 2D/3D glove, while with the mouse this technique was applied significantly less. This can be partially explained by resolution/accuracy differences of input devices. The Logitech PC mouse maps the position of the hand, while the virtual P5 glove maps its movement. Consequently, the precise cursor positioning required by the face-based scaling and hexahedron translation techniques becomes more difficult to achieve using the 2D/3D glove, while the least sensitive to the cursor position technique, face-based rotation, can be always easily performed.</p><p>In the positioning task, the difference in the average interaction time was mostly due to the different amount of time spent on scene rotation by male and female subjects (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig8">8</a>). We observed during testing sessions that many male subjects spent quite some time on reasoning what the best viewpoint would be to perform the task, while female subjects mostly preferred “multiple probes and trials” approach. Consequently, men (mean 9.75 s) spent less time on scene rotation than women (mean 17.48 s). ANOVA found the main effect of gender on the average scene rotation time, <i>F</i>(1, 28) = 4.23, <i>p</i> = 0.049. Both men and women spent significantly more time on scene rotation when performing more complex trials, <i>F</i>(1.92, 53.89) = 87.02, <i>p</i> &lt; 0.001.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Average scene rotation time for the positioning task. Timing data are normalized by log transformation; <i>vertical bars</i> denote 0.95 confidence interval</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>There was also the main effect of input method on the scene rotation time, <i>F</i>(1.95, 54.6) = 8.09, <i>p</i> = 0.001, indicating that significantly less time was spent on rotation with the 3D glove than with the mouse and with the 2D glove. Significant differences in the scene rotation time for women at <i>p</i> ≤ 0.05 were 6.54 s between the 3D glove and the mouse and 16.27 s between the 2D and the 3D glove. For men, the scene rotation time for all three input methods was not significantly different from each other. These results suggest that female subjects benefited from 3D input more than male subjects, as women spent significantly less time on scene rotation using the 3D glove compared to other input methods.</p><h3 class="c-article__sub-heading" id="Sec12">Age issues</h3><p>The age of participants varied from 19 to 45 years old. Mean age was 28. To perform statistical analysis, we split participants in two age groups: “≤28 years old” (63%) and “&gt;28 years old” (37%). We expected that the age difference would have an effect on task performance (i.e., time and accuracy), as well as on subjective user preferences for available input methods/devices (hypothesis H2).</p><p>In general, the experimental data does not contradict our hypothesis H2. However, there was not enough statistical evidence revealed to reason about the effect of age on error data. The rest of hypothesis H2 is well supported by our results.</p><p>In the selection task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig9">9</a>, left), the average task completion time for younger subjects (mean 50.19 s) was significantly shorter than for older subjects (mean 84.24 s). ANOVA found a significant main effect for age, <i>F</i>(1, 28) = 6.65, <i>p</i> = 0.015, and the significant interaction effect between input method and task complexity [<i>F</i>(3.69, 103.29) = 7.19, <i>p</i> &lt; 0.001], indicating that more complex selection tasks were performed significantly slower with the 2D glove than with the mouse or with the 3D glove by both age groups. The fact that the task completion time with the 2D glove was higher than with any other input method can be explained by the difference in DOF between the 2D and 3D glove, as well as by different prior experiences of subjects with the virtual P5 glove and with the mouse.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Average task completion time for the selection (<i>left</i>) and positioning (<i>right</i>) tasks. Timing data are normalized by log transformation; <i>vertical bars</i> denote 0.95 confidence interval</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>A significant difference at <i>p</i> ≤ 0.05 between younger and older participants in the task completion time was found for the most complex trial of Level 3. Tukey HSD tests also revealed that the trial of Level 3 of the selection task was performed significantly faster by younger subjects with the glove in a 3D mode than with the 2D glove.</p><p>In the positioning task (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig9">9</a>, right), the average task completion time was much longer for older subjects (mean 173.7 s) than for younger subjects (mean 122.44 s). ANOVA found a significant main effect for age, <i>F</i>(1, 28) = 5.8, <i>p</i> = 0.023, and the main effect of the trial type, <i>F</i>(1.93, 54.08) = 52.14, <i>p</i> &lt; 0.001, indicating that more complex positioning tasks generally required more time. Tukey HSD tests (at <i>p</i> ≤ 0.05) revealed that, for the trials of Levels 2 and 3 of the positioning task, the average completion time was significantly lower for younger people than for older people.</p><p>Positioning tasks were performed significantly faster with the mouse than with the 3D glove and significantly faster with the 3D glove than with the 2D glove by both age groups, <i>F</i>(2, 56) = 53.4, <i>p</i> &lt; 0.001. Post hoc tests (at <i>p</i> ≤ 0.05) revealed that older subjects were significantly slower with the glove in a 2D mode compared to the glove in a 3D mode and to the mouse. For younger subjects, the only discovered statistically significant difference was between the 2D glove and the mouse.</p><p>Due to extensive prior experience of participants with the mouse and lack of experience with the virtual P5 glove, the mouse turned out to be the best device for performing both selection and positioning tasks. However, under the condition of similar user experiences with 2D and 3D devices (2D/3D glove), our results suggest that 3D input is more beneficial for the positioning task than for the selection one. In the positioning task, both age groups performed trials significantly faster with the 3D glove compared to the 2D glove, irrespective of the trial type. In the selection task, 3D input was only beneficial for younger people while they were performing the trial of the highest complexity.</p><p>We then analyzed scale data of the ease-of-use ratings of input methods/devices and user preferences for 2D/3D input in general. Although younger and older participants rated input methods differently, there was not enough statistical evidence found to reason about the influence of the user’s age on the ease-of-use ratings. With regard to subjective user preferences for 2D/3D input, the Kruskal–Wallis ANOVA by Ranks revealed some significant age-related differences (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Percent of 2D/3D input preferences for the selection and positioning tasks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In the selection task, many young subjects did not have any preference, while all older subjects indicated their preferences for 2D/3D input. These resulted in a main effect for the user’s age, <i>H</i> = 7.19, <i>p</i> = 0.007, suggesting that older people more clearly expressed their preferences than younger ones.</p><p>In the positioning task, younger subjects preferred 3D input significantly more than older subjects, and vice versa, older people preferred 2D input significantly more than younger ones. The Kruskal–Wallis ANOVA by Ranks showed a significant age difference in user preferences for 2D (<i>H</i> = 7.71, <i>p</i> = 0.006) and 3D (<i>H</i> = 6.8, <i>p</i> = 0.009) input.</p><p>These results also support our hypothesis H4 about task-related differences in the influence of individual user characteristics. In the positioning task, younger people were significantly more positive about their experience with the virtual P5 glove in a 3D mode than older people. While in the selection task, age-related differences in user preferences for 2D/3D input were not significantly different. Also, in the selection task, 3D input was more beneficial for younger subjects than for older ones (with respect to the task completion time), which was not the case in the positioning task, where both age groups benefited from the 3D glove in a similar way.</p><h3 class="c-article__sub-heading" id="Sec13">Computer experience</h3><p>The hypothesis about the influence of computer experience (H3) is only partially supported by the experimental data. According to our results, main effects for computer and graphics usage were non-significant. However, there appeared to be some transfer from game experience to subjective ratings of the ease-of-use of input methods/devices as can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig11">11</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0131-4/MediaObjects/10055_2009_131_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Average ratings of the ease-of-use of the input devices for the selection (<i>left</i>) and positioning (<i>right</i>) tasks</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0131-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Game experience was administered using a three point scale, as well as open-ended questions. Most subjects claimed that they had prior game experience: 46% played games occasionally, 30% played games intensively. Only 23% had no prior game experience at all.</p><p>The Kruskal–Wallis ANOVA by Ranks showed a significant difference between subjects with different game experiences for the selection task (<i>H</i> = 6.04, <i>p</i> = 0.049), indicating that the average score of the ease-of-use received from the subjects with no game experience was significantly lower than the average scores from the subjects with occasional game experience and from experienced gamers. On average, the mouse was rated the highest by all three groups, i.e., 1.4–0.9 points higher than the glove in a 3D mode and 1.71–1.34 points higher than the glove in a 2D mode (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig11">11</a>, left).</p><p>In the positioning task, the Kruskal–Wallis ANOVA by Ranks showed a significant difference in the average ease-of-use ratings of the 3D glove between subjects with different game experiences (<i>H</i> = 9.77, <i>p</i> = 0.008). Subjects with intensive game experience rated the glove in a 3D mode on average higher than any other input method, while less experienced subjects preferred the 2D mouse-based interaction the most (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0131-4#Fig11">11</a>, right).</p><p>As can be seen from above, game experience had different effect on subjective user ratings of input methods/devices depending on the task. In the positioning task, experienced gamers rated the glove in a 3D mode significantly higher than less experienced participants. While in the selection task, the 2D mouse-based interaction was preferred the most, irrespective of differences in game experience between the groups.</p><p>Pairwise comparisons of the device condition for the task completion time at <i>p</i> ≤ 0.05 revealed that people with some occasional game experience performed the positioning task significantly faster using the 3D glove than people without any game experience. While for the selection task, the average task completion time for the glove in a 3D mode was not significantly different from the mouse and from the 2D glove. These imply that subjects with more intensive game experience were able to benefit from the 3D glove much more while performing the positioning task.</p><p>Hence, our results suggest that hypothesis H4 is well supported by the experimental data not only with respect to age (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0131-4#Sec12">6.2</a>) but also with respect to game experience.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Discussion and conclusions</h2><div class="c-article-section__content" id="Sec14-content"><p>In this paper, we investigated the influence of individual user differences (i.e., gender, age, computer experience) on the way people interact with a 3D medical virtual environment while performing interactive steering tasks. The semi-automated segmentation of the patient vascular condition served as the context for this research.</p><p>We conducted an empirical study, where participants were asked to perform two tasks: selection of the region of interest (selection task) and correction of the automatically generated centerline (positioning task). Both tasks are part of the semi-automated medical segmentation process and important for its successful completion. We tested the virtual P5 glove in a 2D/3D mode against the Logitech PC mouse.</p><p>Our results suggest that gender plays an important role in the user interaction with the visualized medical data. We found the main effect of gender on the average interaction time for both selection and positioning tasks. In particular, female subjects spent significantly more time on the interaction with a virtual environment compared to male subjects. The results indicate that the difference in the average interaction time between men and women was greater as task complexity increased. In the positioning task, the gender-related difference in the average interaction time was greater with the glove in a 2D mode than with the 3D glove. The latter suggests that providing additional DOF for performing positioning tasks may potentially help to reduce differences in the spatial behavior between men and women.</p><p>The experimental data showed that differences in the total interaction time were mostly due to different interaction strategies preferred by men and women. In the selection task, women were more inclined to experiment with alternative manipulation techniques than men. In the positioning task, male subjects spent significantly less time on scene rotation than female subjects, which can be explained by the fact that men were more focused on finding the best viewpoint than women (according to our observations).</p><p>We also found that the task completion time was significantly affected by age. In particular, younger people (≤28 years old) were able to perform selection and positioning tasks significantly faster than slightly older people (&gt;28 years old). Our results suggest that 3D input was more beneficial for the positioning task than for the selection task. In the positioning task, both age groups performed trials much faster with the 3D glove than with the 2D glove, irrespective of task complexity. In the selection task, 3D input was mainly beneficial for younger people, when they were performing the most complex trial. Furthermore, in the positioning task, younger people were significantly more positive about their experience with the virtual P5 glove in a 3D mode than older people.</p><p>Statistical analysis revealed that game experience had an influence on subjective user ratings of the ease-of-use of input methods/devices. On average, subjects with more intensive game experience rated input devices significantly higher. Game experience had much stronger effect on subjective ratings for the positioning task than for the selection task. In the positioning task, people that played games intensively gave the virtual P5 glove in a 3D mode the highest rates. While in the selection task, the mouse got the highest average rates from all three groups.</p><p>Overall, the experimental data suggest that young people and people with prior game experience were able to benefit from the virtual P5 glove in a 3D mode the most and that in general 3D input was more beneficial for the positioning task than for the selection task. Moreover, it appeared that the 3D glove was especially advantageous for female subjects for performing positioning tasks, as they had to spend significantly less time on scene rotation compared to other input methods. As such, we argue that these specific user groups should be provided with a possibility to perform positioning tasks using a 6DOF input device (e.g., P5 glove in a 3D mode).</p><p>As for the selection task, it is less clear from the data obtained whether the choice of a certain input method/device can be controlled by the individual user differences explored in this paper. Hence, we consider that it would be sufficient to provide a 2DOF input device (e.g., mouse or P5 glove in a 2D mode) to perform relatively simple selection tasks.</p><p>This research is part of a larger project aimed to develop a multimodal visualization environment allowing clinicians to alternate desktop and virtual realities in an adaptive manner while performing medical exploration tasks (Zudilova-Seinstra <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zudilova-Seinstra EV (2006) Combining desktop and virtual realities: addressing demands of real life clinical environments. Interfaces 67:11–13" href="/article/10.1007/s10055-009-0131-4#ref-CR50" id="ref-link-section-d30139e1458">2006</a>). Future studies will consider more complex display and device configurations, as well as the importance of stereopsis in noticing selection and positioning challenges.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IM. Adame, RJ. Geest, BA. Wasserman, M. Mohamed, JHC. Reiber, BPF. Lelieveldt, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Adame IM, van der Geest RJ, Wasserman BA, Mohamed M, Reiber JHC, Lelieveldt BPF (2004) Automatic segmentation " /><p class="c-article-references__text" id="ref-CR1">Adame IM, van der Geest RJ, Wasserman BA, Mohamed M, Reiber JHC, Lelieveldt BPF (2004) Automatic segmentation and plaque characterization in atherosclerotic carotid artery MR images. MAGMA (Magn Reson Mater Phys Biol Med) 16(5):227–234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20segmentation%20and%20plaque%20characterization%20in%20atherosclerotic%20carotid%20artery%20MR%20images&amp;journal=MAGMA%20%28Magn%20Reson%20Mater%20Phys%20Biol%20Med%29&amp;volume=16&amp;issue=5&amp;pages=227-234&amp;publication_year=2004&amp;author=Adame%2CIM&amp;author=Geest%2CRJ&amp;author=Wasserman%2CBA&amp;author=Mohamed%2CM&amp;author=Reiber%2CJHC&amp;author=Lelieveldt%2CBPF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Berg, C. Hertzog, E. Hunt, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Berg C, Hertzog C, Hunt E (1982) Age differences in the speed of mental rotation. Dev Psychol 18(1):95–107" /><p class="c-article-references__text" id="ref-CR2">Berg C, Hertzog C, Hunt E (1982) Age differences in the speed of mental rotation. Dev Psychol 18(1):95–107</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0012-1649.18.1.95" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Age%20differences%20in%20the%20speed%20of%20mental%20rotation&amp;journal=Dev%20Psychol&amp;volume=18&amp;issue=1&amp;pages=95-107&amp;publication_year=1982&amp;author=Berg%2CC&amp;author=Hertzog%2CC&amp;author=Hunt%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of " /><p class="c-article-references__text" id="ref-CR3">Bornik A, Beichel R, Kruijff E, Reitinger B, Schmalstieg D (2006) A hybrid user interface for manipulation of volumetric medical data. In: Proceedings of the IEEE symposium on 3D user interfaces, pp 29–36</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Botella, RM. Baños, C. Perpiñá, H. Villa, M. Alcañiz, A. Reym, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Botella C, Baños RM, Perpiñá C, Villa H, Alcañiz M, Reym A (1998) Virtual reality treatment of claustrophobia:" /><p class="c-article-references__text" id="ref-CR4">Botella C, Baños RM, Perpiñá C, Villa H, Alcañiz M, Reym A (1998) Virtual reality treatment of claustrophobia: a case report. Behav Res Ther 36(2):239–246</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0005-7967%2897%2910006-7" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20treatment%20of%20claustrophobia%3A%20a%20case%20report&amp;journal=Behav%20Res%20Ther&amp;volume=36&amp;issue=2&amp;pages=239-246&amp;publication_year=1998&amp;author=Botella%2CC&amp;author=Ba%C3%B1os%2CRM&amp;author=Perpi%C3%B1%C3%A1%2CC&amp;author=Villa%2CH&amp;author=Alca%C3%B1iz%2CM&amp;author=Reym%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: " /><p class="c-article-references__text" id="ref-CR5">Bowman DA, Johnson DB, Hodges LF (1999) Testbed evaluation of virtual environment interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology, London UK, pp 26–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DA. Bowman, E. Kruijff, JJ. LaViola, I. Poupyrev, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2005) 3D user interfaces: theory and practice. Pearson Educat" /><p class="c-article-references__text" id="ref-CR6">Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (2005) 3D user interfaces: theory and practice. Pearson Education, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20theory%20and%20practice&amp;publication_year=2005&amp;author=Bowman%2CDA&amp;author=Kruijff%2CE&amp;author=LaViola%2CJJ&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="FP. Brooks, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Brooks F P Jr (1999) What’s real about virtual reality? IEEE Comput Graph Appl 19(6):16–27" /><p class="c-article-references__text" id="ref-CR7">Brooks F P Jr (1999) What’s real about virtual reality? IEEE Comput Graph Appl 19(6):16–27</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.799723" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2221507" aria-label="View reference 7 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=What%E2%80%99s%20real%20about%20virtual%20reality%3F&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=19&amp;issue=6&amp;pages=16-27&amp;publication_year=1999&amp;author=Brooks%2CFP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: Pro" /><p class="c-article-references__text" id="ref-CR8">Conner BD, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, van Dam A (1992) Three-dimensional widgets. In: Proceedings of the 1992 symposium on interactive 3D graphics. ACM Press, New York, NY, USA, pp 183–188</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HSM. Cramer, V. Evers, EV. Zudilova, PMA. Sloot, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Cramer HSM, Evers V, Zudilova EV, Sloot PMA (2004) Context analysis to support development of virtual reality " /><p class="c-article-references__text" id="ref-CR9">Cramer HSM, Evers V, Zudilova EV, Sloot PMA (2004) Context analysis to support development of virtual reality applications. Virtual Real 7(3):177–186</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-004-0130-4" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Context%20analysis%20to%20support%20development%20of%20virtual%20reality%20applications&amp;journal=Virtual%20Real&amp;volume=7&amp;issue=3&amp;pages=177-186&amp;publication_year=2004&amp;author=Cramer%2CHSM&amp;author=Evers%2CV&amp;author=Zudilova%2CEV&amp;author=Sloot%2CPMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Dix, J. Finlay, G. Abowd, R. Beale, " /><meta itemprop="datePublished" content="1993" /><meta itemprop="headline" content="Dix A, Finlay J, Abowd G, Beale R (1993) Human computer interaction. Prentice-Hall, New York" /><p class="c-article-references__text" id="ref-CR10">Dix A, Finlay J, Abowd G, Beale R (1993) Human computer interaction. Prentice-Hall, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20computer%20interaction&amp;publication_year=1993&amp;author=Dix%2CA&amp;author=Finlay%2CJ&amp;author=Abowd%2CG&amp;author=Beale%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="RE. Eberts, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Eberts RE (1994) User interface design. Prentice-Hall, Englewood Cliffs" /><p class="c-article-references__text" id="ref-CR11">Eberts RE (1994) User interface design. Prentice-Hall, Englewood Cliffs</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=User%20interface%20design&amp;publication_year=1994&amp;author=Eberts%2CRE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DE. Egan, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Egan DE (1988) Individual differences in human–computer interaction. In: Helander M (ed) Handbook of human–com" /><p class="c-article-references__text" id="ref-CR12">Egan DE (1988) Individual differences in human–computer interaction. In: Helander M (ed) Handbook of human–computer interaction, Elsevier, North Holland, pp 543–568</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Handbook%20of%20human%E2%80%93computer%20interaction&amp;pages=543-568&amp;publication_year=1988&amp;author=Egan%2CDE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DE. Egan, M. Gomez, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Egan D, Gomez M (1985) Assaying, isolating, and accommodating individual differences in learning a complex ski" /><p class="c-article-references__text" id="ref-CR13">Egan D, Gomez M (1985) Assaying, isolating, and accommodating individual differences in learning a complex skill. In: Dillon R (ed) Individual differences in cognition, Academic Press, London, pp 173–217</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20differences%20in%20cognition&amp;pages=173-217&amp;publication_year=1985&amp;author=Egan%2CDE&amp;author=Gomez%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Feng, I. Spence, J. Pratt, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Feng J, Spence I, Pratt J (2007) Playing an action video game reduces gender differences in spatial cognition." /><p class="c-article-references__text" id="ref-CR14">Feng J, Spence I, Pratt J (2007) Playing an action video game reduces gender differences in spatial cognition. Psychol Sci 18(10):850–855</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-9280.2007.01990.x" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Playing%20an%20action%20video%20game%20reduces%20gender%20differences%20in%20spatial%20cognition&amp;journal=Psychol%20Sci&amp;volume=18&amp;issue=10&amp;pages=850-855&amp;publication_year=2007&amp;author=Feng%2CJ&amp;author=Spence%2CI&amp;author=Pratt%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gabbard JL, Swartz K, Richey K, Hix D (1999) Usability evaluation techniques: a novel method for assessing the" /><p class="c-article-references__text" id="ref-CR15">Gabbard JL, Swartz K, Richey K, Hix D (1999) Usability evaluation techniques: a novel method for assessing the usability of an immersive medical visualization VE. In: Proceedings VWSIM’99, pp 165–170</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Gagnon, " /><meta itemprop="datePublished" content="1985" /><meta itemprop="headline" content="Gagnon D (1985) Videogames and spatial skills: an exploratory study. Educ Commun Technol 33(4):263–275" /><p class="c-article-references__text" id="ref-CR16">Gagnon D (1985) Videogames and spatial skills: an exploratory study. Educ Commun Technol 33(4):263–275</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=876380" aria-label="View reference 16 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Videogames%20and%20spatial%20skills%3A%20an%20exploratory%20study&amp;journal=Educ%20Commun%20Technol&amp;volume=33&amp;issue=4&amp;pages=263-275&amp;publication_year=1985&amp;author=Gagnon%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hartman NW, Connolly PE, Gilger JW, Bertoline GR, Heisler J (2006) Virtual reality-based spatial skills assess" /><p class="c-article-references__text" id="ref-CR17">Hartman NW, Connolly PE, Gilger JW, Bertoline GR, Heisler J (2006) Virtual reality-based spatial skills assessment and its role in computer graphics education. In: Proceedings of the international conference on computer graphics and interactive techniques (ACM SIGGRAPH 2006), Educators program, article No. 46</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="He T, Kaufman AE (1993) Virtual input devices for 3D systems. In: Proceedings of IEEE visualization 1993. IEEE" /><p class="c-article-references__text" id="ref-CR18">He T, Kaufman AE (1993) Virtual input devices for 3D systems. In: Proceedings of IEEE visualization 1993. IEEE Computer Society, San Jose, CA, pp 142–148</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hinckley K, Tullio J, Pausch R, Proffitt D, Kassel DN (1997) Usability analysis of 3D rotation techniques. In:" /><p class="c-article-references__text" id="ref-CR19">Hinckley K, Tullio J, Pausch R, Proffitt D, Kassel DN (1997) Usability analysis of 3D rotation techniques. In: UIST ‘97: proceedings of the 10th annual ACM symposium on user interface software and technology, New York, NY, pp 1–10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Hodges, R. Anderson, G. Burdea, H. Hoffman, B. Rothbaum, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hodges L, Anderson R, Burdea G, Hoffman H, Rothbaum B (2001) Treating psychological and physical disorders wit" /><p class="c-article-references__text" id="ref-CR20">Hodges L, Anderson R, Burdea G, Hoffman H, Rothbaum B (2001) Treating psychological and physical disorders with VR. IEEE Comput Graph Appl 21(6):25–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963458" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Treating%20psychological%20and%20physical%20disorders%20with%20VR&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=6&amp;pages=25-33&amp;publication_year=2001&amp;author=Hodges%2CL&amp;author=Anderson%2CR&amp;author=Burdea%2CG&amp;author=Hoffman%2CH&amp;author=Rothbaum%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HG. Hoffman, DR. Patterson, GJ. Carrougher, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Hoffman HG, Patterson DR, Carrougher GJ et al (2001) The effectiveness of virtual reality based pain control w" /><p class="c-article-references__text" id="ref-CR21">Hoffman HG, Patterson DR, Carrougher GJ et al (2001) The effectiveness of virtual reality based pain control with multiple treatments. Clin J Pain 17:229–235</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2F00002508-200109000-00007" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effectiveness%20of%20virtual%20reality%20based%20pain%20control%20with%20multiple%20treatments&amp;journal=Clin%20J%20Pain&amp;volume=17&amp;pages=229-235&amp;publication_year=2001&amp;author=Hoffman%2CHG&amp;author=Patterson%2CDR&amp;author=Carrougher%2CGJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jacobson J, Redern MS, Furnan JM, Whiney LW, Sparto PJ, Wilson JB, Hodges LF (2001) Balance NAVE: a virtual re" /><p class="c-article-references__text" id="ref-CR22">Jacobson J, Redern MS, Furnan JM, Whiney LW, Sparto PJ, Wilson JB, Hodges LF (2001) Balance NAVE: a virtual reality facility for research and rehabilitation of balance disorders. In: Proceedings of virtual reality software technology (VR’01), pp 103–109</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jin W, Lim Y-L, Xu XG, Singh TP, Suvranu DE (2005) Improving the visual realism of virtual surgery. In: Procee" /><p class="c-article-references__text" id="ref-CR23">Jin W, Lim Y-L, Xu XG, Singh TP, Suvranu DE (2005) Improving the visual realism of virtual surgery. In: Proceedings medicine meets virtual reality, vol 13, pp 227–233</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Johnson, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Johnson C (2004) Top scientific visualization research problems. IEEE Comput Graph Appl 4:13–17" /><p class="c-article-references__text" id="ref-CR24">Johnson C (2004) Top scientific visualization research problems. IEEE Comput Graph Appl 4:13–17</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2004.20" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Top%20scientific%20visualization%20research%20problems&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=4&amp;pages=13-17&amp;publication_year=2004&amp;author=Johnson%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="W. Karwowski, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Karwowski W (2006) International encyclopedia of ergonomics and human factors. CRC Press, Boca Raton" /><p class="c-article-references__text" id="ref-CR25">Karwowski W (2006) International encyclopedia of ergonomics and human factors. CRC Press, Boca Raton</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=International%20encyclopedia%20of%20ergonomics%20and%20human%20factors&amp;publication_year=2006&amp;author=Karwowski%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krüger A, Irrgang S, Hertel I, Strauss G, Preim P (2007) Comparison of 2D and 3D input devices for virtual end" /><p class="c-article-references__text" id="ref-CR26">Krüger A, Irrgang S, Hertel I, Strauss G, Preim P (2007) Comparison of 2D and 3D input devices for virtual endoscopy. In: CURAC 2007, pp 215–218</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Larson, AA. Rizzo, JG. Buckwalter, A. Rooyen, K. Kratz, U. Neumann, C. Kesselman, M. Thiebaux, C. Zang, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Larson P, Rizzo AA, Buckwalter JG, van Rooyen A, Kratz K, Neumann U, Kesselman C, Thiebaux M, van der Zang C (" /><p class="c-article-references__text" id="ref-CR27">Larson P, Rizzo AA, Buckwalter JG, van Rooyen A, Kratz K, Neumann U, Kesselman C, Thiebaux M, van der Zang C (1999) Gender issues in the use of virtual environments. Cyberpsychol Behav 2(2):113–124</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fcpb.1999.2.113" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gender%20issues%20in%20the%20use%20of%20virtual%20environments&amp;journal=Cyberpsychol%20Behav&amp;volume=2&amp;issue=2&amp;pages=113-124&amp;publication_year=1999&amp;author=Larson%2CP&amp;author=Rizzo%2CAA&amp;author=Buckwalter%2CJG&amp;author=Rooyen%2CA&amp;author=Kratz%2CK&amp;author=Neumann%2CU&amp;author=Kesselman%2CC&amp;author=Thiebaux%2CM&amp;author=Zang%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leitheiser B, Munro D (1995) An experimental study of the relationship between spatial ability and the learnin" /><p class="c-article-references__text" id="ref-CR28">Leitheiser B, Munro D (1995) An experimental study of the relationship between spatial ability and the learning of a graphical user interface. In: Proceedings of the inaugural America’s conference on information systems, Pittsburgh, PA, pp 122–124</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="DF. Lohman, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Lohman DF (1996) Spatial ability and G. In: Dennis I, Tapsfield P (eds) Human abilities: their nature and asse" /><p class="c-article-references__text" id="ref-CR29">Lohman DF (1996) Spatial ability and G. In: Dennis I, Tapsfield P (eds) Human abilities: their nature and assessment. Erlbaum, Hillsdale, pp 97–116</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20abilities%3A%20their%20nature%20and%20assessment&amp;pages=97-116&amp;publication_year=1996&amp;author=Lohman%2CDF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J-M. Luursema, WB. Verwey, PAM. Kommers, J-H. Annema, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Luursema J-M, Verwey WB, Kommers PAM, Annema J-H (2008) The role of stereopsis in virtual anatomical learning." /><p class="c-article-references__text" id="ref-CR30">Luursema J-M, Verwey WB, Kommers PAM, Annema J-H (2008) The role of stereopsis in virtual anatomical learning. Interact Comput 20(4):455–460</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.intcom.2008.04.003" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20stereopsis%20in%20virtual%20anatomical%20learning&amp;journal=Interact%20Comput&amp;volume=20&amp;issue=4&amp;pages=455-460&amp;publication_year=2008&amp;author=Luursema%2CJ-M&amp;author=Verwey%2CWB&amp;author=Kommers%2CPAM&amp;author=Annema%2CJ-H">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martens J-B, van Liere R, Kok A (2007) Widget manipulation revisited: a case study in modeling interactions be" /><p class="c-article-references__text" id="ref-CR31">Martens J-B, van Liere R, Kok A (2007) Widget manipulation revisited: a case study in modeling interactions between experimental conditions. In: Proceedings of the IPT-EGVE symposium, The Eurographics Association, paper 1021</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Moise, MS. Atkins, R. Rohling, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Moise A, Atkins MS, Rohling R (2005) Evaluating different radiology workstation interaction techniques with ra" /><p class="c-article-references__text" id="ref-CR32">Moise A, Atkins MS, Rohling R (2005) Evaluating different radiology workstation interaction techniques with radiologists and laypersons. J Digit Imaging 18(2):116–130</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10278-004-1906-5" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluating%20different%20radiology%20workstation%20interaction%20techniques%20with%20radiologists%20and%20laypersons&amp;journal=J%20Digit%20Imaging&amp;volume=18&amp;issue=2&amp;pages=116-130&amp;publication_year=2005&amp;author=Moise%2CA&amp;author=Atkins%2CMS&amp;author=Rohling%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Myers B, Bhatnagar R, Nichols J, Peck C, Kong D, Miller R, Long AC (2002) Interaction at a distance: measuring" /><p class="c-article-references__text" id="ref-CR33">Myers B, Bhatnagar R, Nichols J, Peck C, Kong D, Miller R, Long AC (2002) Interaction at a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the conference on human factors and computing systems (CHI’2002), Minneapolis, Minnesota, USA, pp 33–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Passig, S. Eden, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Passig D, Eden S (2001) Virtual reality as a tool for improving spatial rotation among deaf and hard-of-hearin" /><p class="c-article-references__text" id="ref-CR34">Passig D, Eden S (2001) Virtual reality as a tool for improving spatial rotation among deaf and hard-of-hearing children. Cyberpsychol Behav 4(6):681–686</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2F109493101753376623" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20as%20a%20tool%20for%20improving%20spatial%20rotation%20among%20deaf%20and%20hard-of-hearing%20children&amp;journal=Cyberpsychol%20Behav&amp;volume=4&amp;issue=6&amp;pages=681-686&amp;publication_year=2001&amp;author=Passig%2CD&amp;author=Eden%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SB. Plichta, LS. Garzon, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Plichta SB, Garzon LS (2008) Statistics for nursing and allied health. Williams &amp; Wilkins, Lippincott" /><p class="c-article-references__text" id="ref-CR35">Plichta SB, Garzon LS (2008) Statistics for nursing and allied health. Williams &amp; Wilkins, Lippincott</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Statistics%20for%20nursing%20and%20allied%20health&amp;publication_year=2008&amp;author=Plichta%2CSB&amp;author=Garzon%2CLS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Rizzo, JG. Buckwalter, C. Zaag, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Rizzo A, Buckwalter JG, van der Zaag C (2000) Virtual environment applications in clinical neuropsychology. Th" /><p class="c-article-references__text" id="ref-CR36">Rizzo A, Buckwalter JG, van der Zaag C (2000) Virtual environment applications in clinical neuropsychology. The handbook of virtual environments. L.A. Erlbaum, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20environment%20applications%20in%20clinical%20neuropsychology.%20The%20handbook%20of%20virtual%20environments&amp;publication_year=2000&amp;author=Rizzo%2CA&amp;author=Buckwalter%2CJG&amp;author=Zaag%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Robinson, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Robinson P (2001) Cognition and second language instruction. Cambridge University Press, London" /><p class="c-article-references__text" id="ref-CR37">Robinson P (2001) Cognition and second language instruction. Cambridge University Press, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cognition%20and%20second%20language%20instruction&amp;publication_year=2001&amp;author=Robinson%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roessler A, Grantz V (1998) Performance evaluation of input devices in virtual environments. In: Proceedings o" /><p class="c-article-references__text" id="ref-CR38">Roessler A, Grantz V (1998) Performance evaluation of input devices in virtual environments. In: Proceedings of the IFIP working group 13.2 conference on designing effective and usable multimedia systems, vol 133, pp 68–73</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Salthouse, R. Babcock, E. Skovronek, D. Mitchell, R. Palmon, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Salthouse T, Babcock R, Skovronek E, Mitchell D, Palmon R (1990) Age and experience effects in spatial visuali" /><p class="c-article-references__text" id="ref-CR39">Salthouse T, Babcock R, Skovronek E, Mitchell D, Palmon R (1990) Age and experience effects in spatial visualization. Dev Psychol 26(1):128–136</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F0012-1649.26.1.128" aria-label="View reference 39">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Age%20and%20experience%20effects%20in%20spatial%20visualization&amp;journal=Dev%20Psychol&amp;volume=26&amp;issue=1&amp;pages=128-136&amp;publication_year=1990&amp;author=Salthouse%2CT&amp;author=Babcock%2CR&amp;author=Skovronek%2CE&amp;author=Mitchell%2CD&amp;author=Palmon%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="WJ. Schroeder, K. Martin, B. Lorensen, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Schroeder WJ, Martin K, Lorensen B (2002) The visualization toolkit: an object-oriented approach to 3D graphic" /><p class="c-article-references__text" id="ref-CR40">Schroeder WJ, Martin K, Lorensen B (2002) The visualization toolkit: an object-oriented approach to 3D graphics, 3rd edn. Prentice-Hall, Englewood Cliffs</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visualization%20toolkit%3A%20an%20object-oriented%20approach%20to%203D%20graphics&amp;publication_year=2002&amp;author=Schroeder%2CWJ&amp;author=Martin%2CK&amp;author=Lorensen%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PMA. Sloot, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Sloot PMA (2000) Simulation and visualization in medical diagnosis: perspectives and computational requirement" /><p class="c-article-references__text" id="ref-CR41">Sloot PMA (2000) Simulation and visualization in medical diagnosis: perspectives and computational requirements. In: Marsh A, Grandinetti L, Kauranne T (eds) Advanced infrastructures for future healthcare. IOS Press, Amsterdam, pp 275–282</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advanced%20infrastructures%20for%20future%20healthcare&amp;pages=275-282&amp;publication_year=2000&amp;author=Sloot%2CPMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Stanney, RR. Mourant, RS. Kennedy, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Stanney KM, Mourant RR, Kennedy RS (1998) Human factors issues in virtual environments: a review of the litera" /><p class="c-article-references__text" id="ref-CR42">Stanney KM, Mourant RR, Kennedy RS (1998) Human factors issues in virtual environments: a review of the literature. Presence 7(4):327–351</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565767" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factors%20issues%20in%20virtual%20environments%3A%20a%20review%20of%20the%20literature&amp;journal=Presence&amp;volume=7&amp;issue=4&amp;pages=327-351&amp;publication_year=1998&amp;author=Stanney%2CKM&amp;author=Mourant%2CRR&amp;author=Kennedy%2CRS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Strong, R. Smith, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Strong S, Smith R (2001) Spatial visualization: fundamentals and trends in engineering graphics. J Ind Technol" /><p class="c-article-references__text" id="ref-CR43">Strong S, Smith R (2001) Spatial visualization: fundamentals and trends in engineering graphics. J Ind Technol 18(1):1–13</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20visualization%3A%20fundamentals%20and%20trends%20in%20engineering%20graphics&amp;journal=J%20Ind%20Technol&amp;volume=18&amp;issue=1&amp;pages=1-13&amp;publication_year=2001&amp;author=Strong%2CS&amp;author=Smith%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Velez MC, Silver D, Tremaine M (2005) Understanding visualization through spatial ability differences. In: Pro" /><p class="c-article-references__text" id="ref-CR44">Velez MC, Silver D, Tremaine M (2005) Understanding visualization through spatial ability differences. In: Proceedings of IEEE visualization 2005, Minneapolis, MN, USA, pp 511–518</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Vicente, B. Hayes, R. Williges, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Vicente K, Hayes B, Williges R (1987) Assaying and isolating individual differences in searching a hierarchica" /><p class="c-article-references__text" id="ref-CR45">Vicente K, Hayes B, Williges R (1987) Assaying and isolating individual differences in searching a hierarchical file system. Hum Factors 29(3):349–359</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assaying%20and%20isolating%20individual%20differences%20in%20searching%20a%20hierarchical%20file%20system&amp;journal=Hum%20Factors&amp;volume=29&amp;issue=3&amp;pages=349-359&amp;publication_year=1987&amp;author=Vicente%2CK&amp;author=Hayes%2CB&amp;author=Williges%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Waller, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Waller D (2000) Individual differences in spatial learning from computer-simulated environments. J Exp Psychol" /><p class="c-article-references__text" id="ref-CR46">Waller D (2000) Individual differences in spatial learning from computer-simulated environments. J Exp Psychol Appl 8:307–321</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F1076-898X.6.4.307" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20differences%20in%20spatial%20learning%20from%20computer-simulated%20environments&amp;journal=J%20Exp%20Psychol%20Appl&amp;volume=8&amp;pages=307-321&amp;publication_year=2000&amp;author=Waller%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Waller, D. Knapp, E. Hunt, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Waller D, Knapp D, Hunt E (2001) Spatial representations of virtual mazes: the role of visual fidelity and ind" /><p class="c-article-references__text" id="ref-CR47">Waller D, Knapp D, Hunt E (2001) Spatial representations of virtual mazes: the role of visual fidelity and individual differences. Hum Factors 43(1):147–158</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1518%2F001872001775992561" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20representations%20of%20virtual%20mazes%3A%20the%20role%20of%20visual%20fidelity%20and%20individual%20differences&amp;journal=Hum%20Factors&amp;volume=43&amp;issue=1&amp;pages=147-158&amp;publication_year=2001&amp;author=Waller%2CD&amp;author=Knapp%2CD&amp;author=Hunt%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased select" /><p class="c-article-references__text" id="ref-CR48">Wingrave C, Tintner R, Walker B, Bowman D, Hodges L (2005) Exploring individual differences in raybased selection: strategies and traits. In: Proceedings of IEEE virtual reality 05, pp 163–170</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EV. Zudilova, PMA. Sloot, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Zudilova EV, Sloot PMA (2005) Bringing combined interaction to a problem solving environment for vascular reco" /><p class="c-article-references__text" id="ref-CR49">Zudilova EV, Sloot PMA (2005) Bringing combined interaction to a problem solving environment for vascular reconstruction. Future Gener Comput Syst 21(7):1167–1176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.future.2004.04.004" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Bringing%20combined%20interaction%20to%20a%20problem%20solving%20environment%20for%20vascular%20reconstruction&amp;journal=Future%20Gener%20Comput%20Syst&amp;volume=21&amp;issue=7&amp;pages=1167-1176&amp;publication_year=2005&amp;author=Zudilova%2CEV&amp;author=Sloot%2CPMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EV. Zudilova-Seinstra, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Zudilova-Seinstra EV (2006) Combining desktop and virtual realities: addressing demands of real life clinical " /><p class="c-article-references__text" id="ref-CR50">Zudilova-Seinstra EV (2006) Combining desktop and virtual realities: addressing demands of real life clinical environments. Interfaces 67:11–13</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Combining%20desktop%20and%20virtual%20realities%3A%20addressing%20demands%20of%20real%20life%20clinical%20environments&amp;journal=Interfaces&amp;volume=67&amp;pages=11-13&amp;publication_year=2006&amp;author=Zudilova-Seinstra%2CEV">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-009-0131-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research is funded by the NWO/VIEW project “A Multimodal Visualization Environment for Interactive Analysis of Medical Data” (<a href="http://www.science.uva.nl/research/scs/projects/MultiVis/">http://www.science.uva.nl/research/scs/projects/MultiVis/</a>). We would like to acknowledge people, who volunteered to participate in our study, and to thank all partners for their contribution to this project and Dr. Vanessa Evers for her comments related to this paper.</p>
                <h3 class="c-article__sub-heading">Open Access</h3>
                <p>This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
              </div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Computational Science, Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Elena Zudilova-Seinstra &amp; Peter Sloot</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Human-Media Interaction, Department of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Boris van Schooten &amp; Betsy van Dijk</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Division of Image Processing, Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands</p><p class="c-article-author-affiliation__authors-list">Avan Suinesiaputra, Rob van der Geest &amp; Johan Reiber</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Elena-Zudilova_Seinstra"><span class="c-article-authors-search__title u-h3 js-search-name">Elena Zudilova-Seinstra</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Elena+Zudilova-Seinstra&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Elena+Zudilova-Seinstra" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Elena+Zudilova-Seinstra%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Boris-Schooten"><span class="c-article-authors-search__title u-h3 js-search-name">Boris van Schooten</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Boris+van+Schooten&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Boris+van+Schooten" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Boris+van+Schooten%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Avan-Suinesiaputra"><span class="c-article-authors-search__title u-h3 js-search-name">Avan Suinesiaputra</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Avan+Suinesiaputra&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Avan+Suinesiaputra" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Avan+Suinesiaputra%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Rob-Geest"><span class="c-article-authors-search__title u-h3 js-search-name">Rob van der Geest</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Rob+van der+Geest&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rob+van der+Geest" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rob+van der+Geest%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Betsy-Dijk"><span class="c-article-authors-search__title u-h3 js-search-name">Betsy van Dijk</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Betsy+van+Dijk&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Betsy+van+Dijk" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Betsy+van+Dijk%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Johan-Reiber"><span class="c-article-authors-search__title u-h3 js-search-name">Johan Reiber</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Johan+Reiber&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Johan+Reiber" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Johan+Reiber%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Peter-Sloot"><span class="c-article-authors-search__title u-h3 js-search-name">Peter Sloot</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Peter+Sloot&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Peter+Sloot" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Peter+Sloot%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-009-0131-4/email/correspondent/c1/new">Elena Zudilova-Seinstra</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b> This is an open access article distributed under the terms of the Creative Commons Attribution Noncommercial License (<a href="https://creativecommons.org/licenses/by-nc/2.0" rel="license" itemprop="license">https://creativecommons.org/licenses/by-nc/2.0</a>), which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Exploring%20individual%20user%20differences%20in%20the%202D%2F3D%20interaction%20with%20medical%20image%20data&amp;author=Elena%20Zudilova-Seinstra%20et%20al&amp;contentID=10.1007%2Fs10055-009-0131-4&amp;publication=1359-4338&amp;publicationDate=2009-09-16&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY-NC">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Zudilova-Seinstra, E., van Schooten, B., Suinesiaputra, A. <i>et al.</i> Exploring individual user differences in the 2D/3D interaction with medical image data.
                    <i>Virtual Reality</i> <b>14, </b>105–118 (2010). https://doi.org/10.1007/s10055-009-0131-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-009-0131-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-02-27">27 February 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-08-19">19 August 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-09-16">16 September 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-06">June 2010</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-009-0131-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-009-0131-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">2D/3D interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Medical segmentation</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User study</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0131-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=131;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

