<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="On study of design and implementation of virtual fixtures"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Virtual fixtures (VFs) can be defined as guiding constraints designed to enhance or assist human performance in a computer-controlled system by providing cues of haptic or audiovisual nature. In..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/13/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="On study of design and implementation of virtual fixtures"/>

    <meta name="dc.source" content="Virtual Reality 2009 13:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2009-03-26"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Virtual fixtures (VFs) can be defined as guiding constraints designed to enhance or assist human performance in a computer-controlled system by providing cues of haptic or audiovisual nature. In this paper we present a new characterization of VFs based on mechanics, and provide a set practical guidelines for the designers of such fixtures from a software architecture point of view. We propose an event-driven approach that facilitates the integration of these guiding constraints in a scene graphed-based environment. In this context some novel implementation of VFs are presented, where users may interact with a single or an assembled set of fixtures. We present two types of force attributes for VF and present their implications in a trajectory-following problem."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2009-03-26"/>

    <meta name="prism.volume" content="13"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="117"/>

    <meta name="prism.endingPage" content="129"/>

    <meta name="prism.copyright" content="2009 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-009-0115-4"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-009-0115-4"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-009-0115-4.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-009-0115-4"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="On study of design and implementation of virtual fixtures"/>

    <meta name="citation_volume" content="13"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2009/06"/>

    <meta name="citation_online_date" content="2009/03/26"/>

    <meta name="citation_firstpage" content="117"/>

    <meta name="citation_lastpage" content="129"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-009-0115-4"/>

    <meta name="DOI" content="10.1007/s10055-009-0115-4"/>

    <meta name="citation_doi" content="10.1007/s10055-009-0115-4"/>

    <meta name="description" content="Virtual fixtures (VFs) can be defined as guiding constraints designed to enhance or assist human performance in a computer-controlled system by providing c"/>

    <meta name="dc.creator" content="Rodolfo Prada"/>

    <meta name="dc.creator" content="Shahram Payandeh"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Abbot JJ, Okamura AM (2003) Analysis of virtual fixture contact stability for telemanipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 2699&#8211;2706"/>

    <meta name="citation_reference" content="Bettini A, Lang S, Okamura AW, Hager G (2001) Vision assisted control for manipulation using virtual fixtures. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1171&#8211;1176"/>

    <meta name="citation_reference" content="Chan M, Dill J, Payandeh S (2004) Observations of visual and audio coaching methods in a virtual laparoscopic training environment. In: Proceedings of the 1st symposium on applied perception in graphics and visualization, p 173"/>

    <meta name="citation_reference" content="Galeano D, Payandeh S (2005) Artificial and natural force constraints in haptic-aided path planning. In: IEEE international workshop on haptic audio visual environments and their applications, pp 45&#8211;50"/>

    <meta name="citation_reference" content="Gillespie B, O&#8217;Modhrain S, Tang P, Pham C, Zaretsky D (1998) The virtual teacher. In: Proceedings of the ASME IMECE, DSC, vol 64, pp 171&#8211;178"/>

    <meta name="citation_reference" content="Kuang AB, Payandeh S, Zheng B, Henigman F, Mackenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: Proceedings of the 12th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367&#8211;374"/>

    <meta name="citation_reference" content="Marayong P, Bettini A, Okamura AW (2002) Effect of virtual fixture compliance on human-machine cooperative manipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1089&#8211;1095"/>

    <meta name="citation_reference" content="Nolin JT, Stemniski PM, Okamura AW (2003) Activation cues and force scaling methods for virtual fixtures. In: Proceedings of the 11th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 404&#8211;409"/>

    <meta name="citation_reference" content="Park S, Howe R, Torchiana D (2001) Virtual fixtures for robotic cardiac surgery. In: Proceedings of the 4th international conference on medical image computing and computer-assisted intervention, pp 1419&#8211;1420"/>

    <meta name="citation_reference" content="Payandeh S (2001) Application of shared control strategy in the design of a robotic device. In: Proceedings of american control conference, pp 4532&#8211;4536"/>

    <meta name="citation_reference" content="Payandeh S, Stanisic Z (2002) On application of virtual fixtures as an aid for telemanipulation and training. In: Proceedings of the 10th symposium on haptic interfaces for virtual environment and teleoperator systems. Orlando, FL, pp 18&#8211;23"/>

    <meta name="citation_reference" content="Payandeh S, Dill J, Wilson G, Zhang H, Shi L, Lomax A, Mackenzie CL (2003) Demo: a multi-modal training environment for surgeons. In: Proceedings of the 5th international conference on multimodal interfaces, pp 301&#8211;302. 
                    http://web.ensc.sfu.ca/research/erl/ViTEn/
                    
                  
                        "/>

    <meta name="citation_reference" content="Prada R, Payandeh S (2005) A study on design and analysis of virtual fixtures for cutting in training environment. In: Proceedings of the 1st joint eurohaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems, pp 375&#8211;380"/>

    <meta name="citation_reference" content="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In&quot; Proceedings of IEEE virtual reality international symposium, pp 76&#8211;82"/>

    <meta name="citation_reference" content="Sayers CP, Paul RP (1994) An operator interface for teleprogramming employing synthetic fixtures presence, vol 3, pp 4&#8211;309"/>

    <meta name="citation_reference" content="Schroeder WJ, Martin KM, Lorensen WE (1996) The design and implementation of an object-oriented toolkit for 3d graphics and visualization. In: Proceedings of the 7th conference on visualization. San Francisco, USA, 93-ff"/>

    <meta name="citation_reference" content="Strauss PS, Carey R (1992) An object-oriented 3d graphics toolkit. In: Proceedings of the 19th annual conference on computer graphics and interactive techniques, pp 341&#8211;349"/>

    <meta name="citation_reference" content="VRML Consortium (1997) The virtual reality modeling language specification ISO/IEC DIS 14772-1"/>

    <meta name="citation_reference" content="Zhang H, Payandeh S, Dill J (2004) On cutting and dissection of virtual deformable objects. In: Proceedings of the IEEE international conference on robotics and automation, pp 3908&#8211;3913"/>

    <meta name="citation_author" content="Rodolfo Prada"/>

    <meta name="citation_author_institution" content="Experimental Robotics and Graphics Laboratory, Simon Fraser University, Burnaby, Canada"/>

    <meta name="citation_author" content="Shahram Payandeh"/>

    <meta name="citation_author_email" content="shahram@cs.sfu.ca"/>

    <meta name="citation_author_institution" content="Experimental Robotics and Graphics Laboratory, Simon Fraser University, Burnaby, Canada"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-009-0115-4&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2009/06/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-009-0115-4"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="On study of design and implementation of virtual fixtures"/>
        <meta property="og:description" content="Virtual fixtures (VFs) can be defined as guiding constraints designed to enhance or assist human performance in a computer-controlled system by providing cues of haptic or audiovisual nature. In this paper we present a new characterization of VFs based on mechanics, and provide a set practical guidelines for the designers of such fixtures from a software architecture point of view. We propose an event-driven approach that facilitates the integration of these guiding constraints in a scene graphed-based environment. In this context some novel implementation of VFs are presented, where users may interact with a single or an assembled set of fixtures. We present two types of force attributes for VF and present their implications in a trajectory-following problem."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>On study of design and implementation of virtual fixtures | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-009-0115-4","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Multimodal cues, Haptic interaction, Automatic constraints, Human–computer interaction, Scene graph environments, User performance","kwrd":["Multimodal_cues","Haptic_interaction","Automatic_constraints","Human–computer_interaction","Scene_graph_environments","User_performance"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-009-0115-4","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-009-0115-4","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=115;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-009-0115-4">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            On study of design and implementation of virtual fixtures
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0115-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0115-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2009-03-26" itemprop="datePublished">26 March 2009</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">On study of design and implementation of virtual fixtures</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Rodolfo-Prada" data-author-popup="auth-Rodolfo-Prada">Rodolfo Prada</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Simon Fraser University" /><meta itemprop="address" content="grid.61971.38, 0000000419367494, Experimental Robotics and Graphics Laboratory, Simon Fraser University, Burnaby, BC, Canada" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shahram-Payandeh" data-author-popup="auth-Shahram-Payandeh" data-corresp-id="c1">Shahram Payandeh<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Simon Fraser University" /><meta itemprop="address" content="grid.61971.38, 0000000419367494, Experimental Robotics and Graphics Laboratory, Simon Fraser University, Burnaby, BC, Canada" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 13</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">117</span>–<span itemprop="pageEnd">129</span>(<span data-test="article-publication-year">2009</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">527 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">25 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-009-0115-4/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Virtual fixtures (VFs) can be defined as guiding constraints designed to enhance or assist human performance in a computer-controlled system by providing cues of haptic or audiovisual nature. In this paper we present a new characterization of VFs based on mechanics, and provide a set practical guidelines for the designers of such fixtures from a software architecture point of view. We propose an event-driven approach that facilitates the integration of these guiding constraints in a scene graphed-based environment. In this context some novel implementation of VFs are presented, where users may interact with a single or an assembled set of fixtures. We present two types of force attributes for VF and present their implications in a trajectory-following problem.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>A virtual fixture (VF) is a guiding constraint designed to correct or assist human performance in any computer-controlled system that requires user input. A VF provides guidance for users by generating haptic and/or audio-visual cues and delivering those by force-feedback devices and other computer peripherals.</p><p>VFs have been developed to provide assistance in a variety of applications. Early implementations demonstrated a significant increase on user performance in tele-manipulation tasks (Rosenberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In&#34; Proceedings of IEEE virtual reality international symposium, pp 76–82" href="/article/10.1007/s10055-009-0115-4#ref-CR14" id="ref-link-section-d88198e296">1993</a>). More recently, there have been further studies assessing performance in other robot tele-manipulation and shared-control environment scenarios (Payandeh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Payandeh S (2001) Application of shared control strategy in the design of a robotic device. In: Proceedings of american control conference, pp 4532–4536" href="/article/10.1007/s10055-009-0115-4#ref-CR11" id="ref-link-section-d88198e299">2001</a>; Abbott et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Abbot JJ, Okamura AM (2003) Analysis of virtual fixture contact stability for telemanipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 2699–2706" href="/article/10.1007/s10055-009-0115-4#ref-CR9" id="ref-link-section-d88198e302">2003</a>) as well as other areas including tracking tasks, training environments, path-following, dead-reckoning, targeting, and object avoidance exercises in 2D environments (Bettini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bettini A, Lang S, Okamura AW, Hager G (2001) Vision assisted control for manipulation using virtual fixtures. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1171–1176" href="/article/10.1007/s10055-009-0115-4#ref-CR2" id="ref-link-section-d88198e305">2001</a>; Marayong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Marayong P, Bettini A, Okamura AW (2002) Effect of virtual fixture compliance on human-machine cooperative manipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1089–1095" href="/article/10.1007/s10055-009-0115-4#ref-CR7" id="ref-link-section-d88198e308">2002</a>; Nolin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Nolin JT, Stemniski PM, Okamura AW (2003) Activation cues and force scaling methods for virtual fixtures. In: Proceedings of the 11th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 404–409" href="/article/10.1007/s10055-009-0115-4#ref-CR8" id="ref-link-section-d88198e312">2003</a>). Other experiments addressed the design and performance analysis of VFs in complex 3D environments (Kuang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kuang AB, Payandeh S, Zheng B, Henigman F, Mackenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: Proceedings of the 12th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367–374" href="/article/10.1007/s10055-009-0115-4#ref-CR6" id="ref-link-section-d88198e315">2004</a>; <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference Sayers and Paul 1994" title="Sayers CP, Paul RP (1994) An operator interface for teleprogramming employing synthetic fixtures presence, vol 3, pp 4–309" href="/article/10.1007/s10055-009-0115-4#ref-CR16" id="ref-link-section-d88198e318">Sayers and Paul 1994</a>). Most studies on these applications have found VFs to be effective in improving operators’ performance through reduction of mental workload, increased precision and/or enhanced speed (Payandeh and Stanisic <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Payandeh S, Stanisic Z (2002) On application of virtual fixtures as an aid for telemanipulation and training. In: Proceedings of the 10th symposium on haptic interfaces for virtual environment and teleoperator systems. Orlando, FL, pp 18–23" href="/article/10.1007/s10055-009-0115-4#ref-CR12" id="ref-link-section-d88198e321">2002</a>; Bettini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Bettini A, Lang S, Okamura AW, Hager G (2001) Vision assisted control for manipulation using virtual fixtures. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1171–1176" href="/article/10.1007/s10055-009-0115-4#ref-CR2" id="ref-link-section-d88198e324">2001</a>; Marayong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Marayong P, Bettini A, Okamura AW (2002) Effect of virtual fixture compliance on human-machine cooperative manipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1089–1095" href="/article/10.1007/s10055-009-0115-4#ref-CR7" id="ref-link-section-d88198e327">2002</a>; Park et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Park S, Howe R, Torchiana D (2001) Virtual fixtures for robotic cardiac surgery. In: Proceedings of the 4th international conference on medical image computing and computer-assisted intervention, pp 1419–1420" href="/article/10.1007/s10055-009-0115-4#ref-CR10" id="ref-link-section-d88198e331">2001</a>).</p><p>This paper presents an approach for designing and implementing VFs in a virtual environment. Unlike other published methods which focused on the implications of using VF, this paper presents practical consideration for <i>creating</i> VF and assembling (force field overlapping) a stable interactive environment consisting of two classes of force fields.</p><p>The paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec2">2</a> presents the characterization of the notion of VF which is used in the paper from both mechanics-based modeling and software architecture point of view; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec14">3</a> presents guidelines for practical implementation of VFs detailing issues related to force field stability and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec20">4</a> presents a case study of using VF in trajectory-following problem and discusses issues related to assembly of the VF along a predefined path. Finally, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec24">5</a> presents some concluding remarks.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Characterization and design of VFs</h2><div class="c-article-section__content" id="Sec2-content"><p>People have been inventing a number of specific utility tools to assist performing various physical tasks such as drawing, cutting, and painting. There are rulers for drawing a straight line, a table saw with various guiding edges and laser lights to help with the alignments and so on. Similarly, VFs can also be designed to assist in performing virtual or remote manipulation tasks. The primary objective and contribution of the paper is to describe the main aspects for designing and assembling a set of VFs to assist tasks in 3D applications and in virtual environments (e.g. exploration, path-following, object-interaction) by providing cues of haptic and audiovisual nature. Specifically, we developed the concept of mechanics-based VFs, which are force-field software agents in the 3D space having a simple shape that follows that of the volume or the surface of basic geometric structures. As it will be described later, the shapes (surfaces and volumes) of these fixtures can be used as activation (switching) functions in order to trigger certain functionality assigned to these fixtures. The paper formulates the design and implementation of VF which can be used as a set of programing standard. This is the first attempt for defining a formal software framework for such implementation. In this paper, we focus primarily on haptic and graphic feedback which are some of the components of the multi-modal cues.</p><h3 class="c-article__sub-heading" id="Sec3">General design considerations</h3><p>In this paper, we present the design of a set of VFs that are highly compatible and adaptable to real-time applications that provide user interaction through a 3D graphics engine. Generally, these applications provide a graphic environment to the user that is described using a scene graph, which is an object-oriented data structure that organizes the logical and spatial representations of the graphical scene hierarchically. To accomplish this, the scene is described in a directed acyclic graph structure where each node represents an element or object in the scene. The parent–child relationships arising from the tree organization define spatial dependencies between scene objects in a similar fashion as the VRML file format specifications (VRML Consortium <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="VRML Consortium (1997) The virtual reality modeling language specification ISO/IEC DIS 14772-1" href="/article/10.1007/s10055-009-0115-4#ref-CR1" id="ref-link-section-d88198e369">1997</a>).</p><p>Because of the real-time graphic and haptic rendering rates of these interactive environments, we tried to minimize the time complexity of our VF algorithms. These methods generally include collision detection, force-feedback calculation, and possibly update of the VF’s position and orientation. As a general practice in software development, we analyzed different possibilities of programing paradigms for the design stage that were suitable for a wide range of applications.</p><p>In general, object-oriented model can facilitate further development of any implementation of our proposed VFs through object inheritance and code reusability, and will ease the integration of these implementations in software systems that are built based on this methodology. The object-oriented paradigm has been the standard programing methodology for implementing many general-purpose graphics libraries, which have found object-oriented systems to be highly maintainable, and flexible to define generic or specific process objects. Some of these implementations include the Visualization Toolkit (Schroeder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Schroeder WJ, Martin KM, Lorensen WE (1996) The design and implementation of an object-oriented toolkit for 3d graphics and visualization. In: Proceedings of the 7th conference on visualization. San Francisco, USA, 93-ff" href="/article/10.1007/s10055-009-0115-4#ref-CR17" id="ref-link-section-d88198e377">1996</a>), Open Inventor (Strauss and Carey <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Strauss PS, Carey R (1992) An object-oriented 3d graphics toolkit. In: Proceedings of the 19th annual conference on computer graphics and interactive techniques, pp 341–349" href="/article/10.1007/s10055-009-0115-4#ref-CR18" id="ref-link-section-d88198e380">1992</a>), and other specific-purpose 3D applications (Payandeh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Payandeh S, Dill J, Wilson G, Zhang H, Shi L, Lomax A, Mackenzie CL (2003) Demo: a multi-modal training environment for surgeons. In: Proceedings of the 5th international conference on multimodal interfaces, pp 301–302. &#xA;                    http://web.ensc.sfu.ca/research/erl/ViTEn/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-009-0115-4#ref-CR15" id="ref-link-section-d88198e383">2003</a>).</p><h3 class="c-article__sub-heading" id="Sec4">Overall characterization of VFs</h3><p>The characterization of a software system and its entities forms the basis of the architectural design and implementation stages of development. In our case, it involves the specific definition of a VF, the environment where it interacts, its guiding capabilities, and the particular objects and events that trigger their actions in the system.</p><p>In the context of this paper, a VF is characterized as an intelligent automated constraint agent assisting operators in tasks performed in 3D space, which is generally defined with a hierarchical scene graph. VF guidance may be accomplished by providing multimodal feedback to users in the form of haptic, graphic, and audio cues that are triggered by particular events. To characterize VFs that can serve environments of varied task nature, we propose integrating the fixture as a 3D entity defined in the software environment, and use their collisions/contacts with other particular objects in the surroundings as the events that trigger the multimodal capabilities of the fixture itself.</p><p>Although our particular design of VFs considers audiovisual multimodal cues, we focus primarily on the haptic guiding capabilities of the fixtures. In terms of haptic feedback, we identify two elementary types of guiding forces that a VF may provide: attractive force and reactive force. The first type of force is used to guide users toward a desired location, whereas a reactive force is only exerted when users diverge from their goal or enter forbidden regions.</p><p>Generally, the objects in the environment that directly interact with the fixtures are usually the virtual representation of the tools that users use to interact with the simulated world. The tip of these tools, in the case of a haptic interface, is usually referred to as the end-effector and may be internally represented by a point. We defined the VF objects as basic 3D primitives such as spheres, cylinders, cones, and rectangular prisms.</p><h3 class="c-article__sub-heading" id="Sec5">Architecture design</h3><p>Our VF architecture comprises different structures and levels of abstraction to separate the implementation of some of the varied characteristics of the VF. For example, we utilized different levels of object inheritance to separate the general aspects from the specialized features and behavior of the fixtures. We also encapsulated related properties and behavior in separate object entities and incorporated them as composition aggregations in our VF objects.</p><p>The hierarchy of our VF set of classes consists of two levels of inheritance as Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig1">1</a> illustrates. At the top level lies the VF base class, which holds the data and describes the behavior common to any type of VF. At the second level are the specialized classes that define the properties that support a particular fixture type.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Virtual fixture inheritance hierarchy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec6">Base class design</h3><p>In this section, we present some guidelines about what type of aspects should be defined and implemented in the most basic abstraction of the VF.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Geometric properties</h4><p>These set of properties define the type and dimensions of the primitive that represents the VF in the scene. The primitive type determines whether the fixture representation in space is a sphere, cone, cylinder, or rectangular prism. The dimension parameters comprise radius, height, width, and length, which are sufficient to describe the basic primitives listed above and which we encapsulated in a separate class structure for ease of retrieval and manipulation.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Graphic and spatial properties</h4><p>These properties are directly related and dependent on the geometric properties of the VF. Their main function is to define visual and positional information of the fixture and to provide rendering support in graphic applications. Some of these properties include color, transparency, and a triangular mesh representing the VF primitive. Color and transparency can be used to indicate users the location of a VF in a 3D environment without totally occluding other objects behind it. However, they can also be utilized to display reactions such as collisions between a VF and other objects in the scene. A triangular mesh is a description of a 3D object using triangles. These objects require data such as a vertex array, index array, and vertex normal array to describe topology and shading. Spatial properties are mainly position and orientation, which can be represented in a single homogenous transformation matrix.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Environment-related properties</h4><p>Environment-related properties store some details of the surrounding environment to make the fixture aware of different objects in the scene. This includes information about the objects in the scene that interact with the fixtures (e.g. end-effectors), such as their position in the current and previous time frames. These properties also contain information about the ancestors of the fixture in the scene graph describing the scene, which are necessary to calculate other properties, such as the position and orientation of end-effectors in the local coordinate system of the VF primitive.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Force-related properties</h4><p>These properties comprise some of the data and methods required to calculate and deliver force feedback. Data include output force direction and magnitude, maximum magnitude, and information about the force-feedback device.</p><p>Usually, architectures of scene-graph based environments already define data structures that contain some of the data and methods described above. These structures generally appear as classes that define scene nodes or models in the environment. For this reason, we suggest using these existing structures as composition aggregations in the VF classes or as super classes of these in order to facilitate integration. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig2">2</a> illustrates one possible VF base class architecture design in UML notation. In this example, the system at hand already defines some of the graphic properties in built-in class structures (SceneGraphNode and TriangularMesh) that are integrated in the VF class as composition aggregations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Example of a virtual fixture class architecture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec11">Specialized classes</h3><p>In the previous section, we provided some guidelines for designing the set of super classes that support the properties common to all types of VFs. In this section, we describe the characteristics of the specialized classes of VFs, which define and resolve in detail how and when guidance or force cues are provided to the users.</p><p>As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig1">1</a>, specialized classes of VFs are defined based on the type of guiding force exerted. On this basis, each specialized class can be thought to define a particular form of force field as follows:</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Point-attraction VF</h4><p>This type of fixture has an attracting behavior that pulls the end-effector toward a predefined location. The VF primitive is defined as a 3D volume, so guidance is activated every time the end-effector collides/contacts with the primitive and maintained while the end-effector is anywhere inside the defined volume. By default, the point of attraction for haptic feedback is the apex in the case of a cone, the center at one of the ends in the case of a cylinder, and the center of mass in the case of a sphere and rectangular prism. However, this point could be defined in any other location within the fixture to satisfy the tasks at hand.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Reactive VF</h4><p>These fixtures simulate barriers that react on contact. They basically simulate virtual walls that push back the end-effector when their surfaces are penetrated. For this reason, the VF primitive is defined as a 3D surface that has the form of any of the basic shapes described earlier. These primitives have some of their faces missing to create traps or duct-like structures. For example, if the primitive is a cone or a cylinder, then the barrier is defined as the surface of the conical object, except for the planar circle(s) at the end(s). In the case of a rectangular prism, the barrier is defined by all, but two opposing faces describing the prism. These cases are illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig3">3</a>. Reactive fixtures also have the property of being two-sided or one-sided barriers. A two-sided barrier reacts on both sides of the defined surface, constraining the end-effector to enter/leave the primitive via one of their missing faces only. An one-sided barrier produces a reactive force only when the end-effector attempts to cross the surface from the inside. This property allows easy entrance of the end-effector and constraints its exit from the fixture’s primitive. This is particularly useful for constraining an instrument within a specific area in space.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Some of the primitives used to represent reactive fixtures</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig4">4</a> shows the force exerted by the force fields defined by the different types of VFs. The arrows indicate the force direction.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Force direction exerted by virtual fixtures. <b>a</b> Point-attraction; <b>b</b> two-sided barriers; <b>c</b> one-sided barriers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Besides haptic guidance, specialized classes can define data and methods that support audiovisual cues that are particular of their own. For example, besides using two colors to indicate users the presence or absence of force-feedback, point-attraction fixtures can use different degrees of color saturation to inform users about their proximity to their goal in the virtual environment. An alteration of a sound’s pitch can be used similarly as an audio cue for the same purpose. Another visual cue may be the utilization of 3D arrows going from the end-effector to the point of attraction to show users the correct direction the end-effector should be heading.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">VF implementation</h2><div class="c-article-section__content" id="Sec14-content"><p>The implementation of VF follows the standard finite-state machine approach where a sub-set of inputs can be used as a triggering mechanism. As such, one must have a mechanism in place that constantly monitors the state of the fixture for the particular events that trigger its guiding capabilities. One way to implement this functionality is to encapsulate and initialize within each fixture object an independent thread of execution that is responsible for inspecting the conditions that trigger these events. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig5">5</a> presents the pseudo-code of the tasks that should be carried out at every time frame of execution for VFs that use collisions with the end-effector as triggering events of haptic cues.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Pseudocode of the main loop executed by a virtual fixture to keep track of event and provide cues</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>It should be noted that the rate of execution of this section of code should be directly dependent on the type of cues provided. Visual cues, as any other displayed graphics, should be rendered at a 30-Hz rate or higher, whereas haptic cues require significantly higher rendering rates (usually in the order of 1 kHz).</p><p>The following sections present some details about the collision detection and force vector calculation methods in the context of the proposed specialized fixtures.</p><h3 class="c-article__sub-heading" id="Sec15">Collision detection</h3><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig5">5</a>, VF cues are activated, updated, and deactivated based on collisions of the primitive with the end-effector. To determine when collision events occur, we defined a series of collision states based on the relative position of the end-effector with respect to the fixture primitive in the current and previous time frames of execution. For this, we divided the VF primitive’s space and its surroundings into regions as the ones labeled by letters in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig6">6</a>. In this figure, region A represents the space enclosed by the fixture’s primitive and regions B and C are areas outside of it.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Spatial regions around the primitive used to determine the collision state of a virtual fixture. <b>a</b> Sphere; <b>b</b> cone; <b>c</b> cylinder; <b>d</b> rectangular prism</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The determination of the collision is resolved differently for each type of VF. In the case of point-attraction fixtures, the system is in collision state when the end-effector is inside the fixture’s volume (region A). For two-sided reactive fixtures, the end-effector is considered to be in contact with the fixture from the moment the end-effector penetrates its surface (from A to B or vice versa) until it moves back to the previous region or one of the regions labeled as C. Collisions for one-sided reactive fixtures are defined similar to two-sided reactive fixtures, except for transitions of the end-effector from region B to A, which is not considered a collision.</p><h3 class="c-article__sub-heading" id="Sec16">Force-feedback calculation</h3><p>Our proposed VFs provide haptic cues in the form of a 3D force vector that affects the position of the tool tip of the force-feedback device that is used to interact with the environment. This vector can be described as</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} = m\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {d} , $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} \)</span> is the resulting force vector, <i>m</i> is the magnitude scalar component and <span class="mathjax-tex">\( \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {d} \)</span> is a 3D unit vector representing the direction of the force.</p><p>The value of the magnitude and the direction of the output force depend directly on both, the position of the end-effector with respect to the fixture’s primitive and the type of fixture that is providing the guidance. Therefore, each specialized VF defines its own implementation for calculating the force-feedback output.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Force of point-attraction VFs</h4><p>Point-attraction fixtures generate forces that push the end-effector toward a particular area in the fixture’s primitive. As a result, the direction and magnitude of the output force vector are calculated on the points that represent these locations in space.</p><p>Force direction, for example, is defined as the normalized vector formed between the point <i>P</i>
                              <sub>
                      <i>a</i>
                    </sub> that represents the position of the point of attraction and the point <i>P</i>
                              <sub>
                      <i>e</i>
                    </sub> that denotes the position of the end-effector inside the fixture’s representative volume as follows</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {d} = \frac{{P_{a} - P_{e} }}{{\left\| {P_{a} - P_{e} } \right\|}} $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig7">7</a>a shows a geometric interpretation of the two points that define the direction of point-attraction fixtures in the context of a cylinder-shaped fixture.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Points used to calculate the direction and magnitude of the force-feedback vector (the <i>arrows</i> represent the direction of the force). <b>a</b> Point-attraction fixture; <b>b</b> reactive fixture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The value of the magnitude of the force-feedback is the product of two scalars given by distance and time functions and a constant value that sets an upper boundary on the magnitude.</p><p>The distance-dependent function uses the distance between the end-effector and the fixture’s point of attraction as the dependent variable. The value of the function increases as the distance between the two points does. Therefore, stronger haptic cues are delivered as the end-effector moves away from the point of attraction, which is usually associated with the target location.</p><p>The purpose of the time-dependent function is to gradually increment the amount of force-feedback during the first few seconds after this one has been initiated. Without this time component, users of the haptic device would experience an abrupt increase in force magnitude every time the end-effector triggers haptic cues by entering the primitive’s volume.</p><p>Each of the functions that regulate the magnitude of the force feedback should be monotonically increasing and in the range [0, 1] controlled by predefined thresholds on the dependent variables. For example, if the increment of each scalar component were controlled linearly, then the functions would be</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ f(x) = \left\{ \begin{array}{ll} \frac{x}{\lambda }\quad &amp; {\text{ if }}x &lt; \lambda \hfill \\ 1\quad &amp; {\text{ otherwise}} \hfill \\ \end{array} \right., $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                              <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ h(t) = \left\{ \begin{array}{ll} \frac{t}{\tau }\quad &amp; {\text{ if t}} &lt; \tau \hfill \\ 1\quad &amp; {\text{ otherwise}} \hfill \\ \end{array} \right., $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>where <i>f</i> is the distance-dependent function, <i>h</i> is the time-dependent function and <i>λ</i> and <i>τ</i> are their corresponding predefined thresholds. It should be noted that other non-decreasing functions other than linear can be used in the non-constant part of these piece-wise functions as long as they produce smooth transitions of force-magnitude, particularly during force-feedback activation.</p><p>Having defined the function components, the calculation of the magnitude for point-attraction fixtures can be described as</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ m = \mu \cdot f(x) \cdot h(t), $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>where <i>μ</i> is a scalar that determines the maximum value of the force-feedback magnitude.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Force of reactive fixtures</h4><p>Reactive fixtures produce forces that try to constraint end-effectors from crossing the surface of the fixture’s geometric and topological primitive. Like point-attraction fixtures, reactive fixtures make use of two points to calculate the direction and magnitude of the force generated by the collision of the end-effector with the surface. These two points are <i>P</i>
                              <sub>
                      <i>e</i>
                    </sub> which represents the position of the end-effector and <i>P</i>
                              <sub>
                      <i>p</i>
                    </sub> which is the closest point to <i>P</i>
                              <sub>
                      <i>e</i>
                    </sub> on the contacted surface of the fixture primitive. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig7">7</a>b illustrates the direction of the force when the end-effector has crossed the surface of a reactive fixture from the interior of its cylinder. The direction of the force is calculated as in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-009-0115-4#Equ2">2</a>) using <i>P</i>
                              <sub>
                      <i>p</i>
                    </sub> instead of <i>P</i>
                              <sub>
                      <i>a</i>
                    </sub>.</p><p>Similarly, the force magnitude can be calculated using the threshold <i>μ</i> that also limits magnitude of point-attraction fixtures and a non-decreasing function <i>g</i> that increases with the distance <i>s</i> between <i>P</i>
                              <sub>
                      <i>e</i>
                    </sub> and <i>P</i>
                              <sub>
                      <i>p</i>
                    </sub>. The resulting formula can be stated as</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ m = \mu \cdot g(s) $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div>
                           <h3 class="c-article__sub-heading" id="Sec19">Force stability around the point of attraction</h3><p>As seen in (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-009-0115-4#Equ5">5</a>), point-attraction fixtures generate a force-feedback vector of low magnitude when the end-effector is close to the point of attraction. However, due to user interactions, force-feedback instability may arise in the form of vibrations in the haptic device due to sudden changes of force direction. This is particularly the case when the point of attraction does not lie on the surface of the fixture’s primitive, providing sufficient force-field area for the instrument to prolong the force-feedback instability.</p><p>The force vector is generated based on the position of the end-effector and the point of attraction sampled at discrete time frames. Despite the haptic feedback of the fixture to guide the end-effector toward the point of attraction, the position of these two will rarely coincide at given sampled events. This causes the end-effector to constantly miss the point of attraction redirecting it toward the missed target, eventually causing oscillations of the haptic tool in the form of rapid vibrations. This behavior is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig8">8</a> in the context of a spherical VF and where <i>P</i>
                           <sub>
                    <i>a</i>
                  </sub> describes the position of the point of attraction whereas <i>P</i>
                           <sub>
                    <i>e,n</i>
                  </sub>, <i>P</i>
                           <sub>
                    <i>e,n+1</i>
                  </sub> denote the positions of the end-effector at time frames <i>n</i>, <i>n</i> + 1, and so on.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Behavior of the end-effector around the point of attraction during four consecutive time frames</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>There are many approaches which can be followed to compensate and prevent this type of instabilities. One approach is to decrease the force magnitude based on the instantaneous velocity of the end-effector. The control law of equation (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-009-0115-4#Equ1">1</a>) can then be rewritten as,</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {F} = m\overset{\lower0.5em\hbox{$\smash{\scriptscriptstyle\rightharpoonup}$}} {d} - b\left( {\frac{{P_{e,n} - P_{e,n - 1} }}{\Updelta t}} \right), $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where <i>b</i> is a damping constant, Δ<i>t</i> is the duration of the time frame and <i>P</i>
                           <sub>
                    <i>e,n</i>
                  </sub> and <i>P</i>
                           <sub>
                    <i>e,n-1</i>
                  </sub> are the position of the end-effector at the current and previous time frames, respectively. Another preventive measure is to cease the force-feedback when the end-effector is sufficiently close to the point of attraction. This can be accomplished by defining a zero-force spherical region centered at the point of attraction that is used to drop the magnitude of the force-feedback to zero when the end-effector enters it.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">A design example</h2><div class="c-article-section__content" id="Sec20-content"><p>We applied our VF design and implementation methodologies to provide guiding cues during a cutting task in a virtual environment. This section describes the nature of the task, the 3D environment, and the different issues arising from the design, implementation, and integration of the VF objects in the application.</p><h3 class="c-article__sub-heading" id="Sec21">Definition of the virtual environment and the task</h3><p>In this example, we used the frame work of virtual training environment (ViTEn) as a basis to extend and implement the notions of VF. The ViTEn (Payandeh et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Payandeh S, Dill J, Wilson G, Zhang H, Shi L, Lomax A, Mackenzie CL (2003) Demo: a multi-modal training environment for surgeons. In: Proceedings of the 5th international conference on multimodal interfaces, pp 301–302. &#xA;                    http://web.ensc.sfu.ca/research/erl/ViTEn/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-009-0115-4#ref-CR15" id="ref-link-section-d88198e1081">2003</a>) is a multi-modal virtual environment that provides a set of interactive training tasks for surgeons, including cutting, suturing, and other maneuvers performed in laparoscopic operations. The application is made of three major components, one to support object geometry and graphics, another to take care of collision detection and deformation modeling and the third, a force-feedback module for haptic support.</p><p>The training module that was selected for experimenting with the design and implementation of our VFs is a cutting task in which the user is presented with a deformable object. The desired cutting path which consists of straight-line segments between seed nodes are drawn on its surface. The user interaction with the scene is accomplished through a special-purpose haptic device. The deformable object is modeled as a triangle-mesh surface, which also defines a mass-spring-damper structure that is used to compute the deflections and the force-feedback which arises when the virtual tool interacts and deforms the object. In the case when the object is intersected with a virtual scalpel, the surface mesh is deformed based on the differential equations that describe the mechanics of the surface mesh. After sufficient penetration of the scalpel, the surface of the object is also altered in order to simulate an incision on the object. These incisions are created based on a progressive cutting algorithm (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Zhang H, Payandeh S, Dill J (2004) On cutting and dissection of virtual deformable objects. In: Proceedings of the IEEE international conference on robotics and automation, pp 3908–3913" href="/article/10.1007/s10055-009-0115-4#ref-CR19" id="ref-link-section-d88198e1087">2004</a>) that dynamically generates and appends new triangles to the surface mesh to model the internal structure of the cut object based on the position and degree of penetration of the scalpel. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig9">9</a> depicts an instance of the cutting task as seen from the user’s point of view.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A scene from the virtual environment displaying a deformable object, a cutting path, and a virtual scalpel</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The objective of the task was to cut the deformable model with the virtual scalpel along the path drawn on the surface of the object. Our goal consisted in designing and deploying on the scene a number of VFs to help users in the execution of the task. Since other studies (Chan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Chan M, Dill J, Payandeh S (2004) Observations of visual and audio coaching methods in a virtual laparoscopic training environment. In: Proceedings of the 1st symposium on applied perception in graphics and visualization, p 173" href="/article/10.1007/s10055-009-0115-4#ref-CR3" id="ref-link-section-d88198e1114">2004</a>) have explored audiovisual aids during similar tasks, we focused on designing VFs that convey guiding cues in the form of force-feedback.</p><h3 class="c-article__sub-heading" id="Sec22">Designing, assembling, and implementing VFs</h3><p>The ViTEn provides a scene engine as one of its main modules. This engine is based on set of classes and procedures that define and load scenes and models from files, manage collision detection between input devices and objects, and render the environment using the OpenGL libraries.</p><p>In a scene graph environment, a scene graph provides the representation of the graphical environment. These nodes considered to be the basic elements defined by the scene node class. Our generic VF class is derived from a class in the virtual environment that encapsulates the properties of 3D model, which is itself derived from the scene node class. The available scene node class defines data structures and methods to support homogenous transformation lists (which are used to denote spatial properties, such as position and orientation) and pointers to other nodes in the scene, such as the parent and children nodes (which are some of the environment-related properties of our VF design specification). The model class defines some of the graphic properties listed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec2">2</a> to define triangular meshes, as VRML does, using arrays of vertices, indices, vertex normals, and vertex colors.</p><p>The rest of the properties needed to support the functionality of the VFs are defined in the VF base class and the more specialized derivations of it. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig10">10</a> illustrates a class diagram with all the major class descriptors that define the various properties described earlier.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Set of classes that contain the properties and methods that support the functionality of the virtual fixtures</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Our main objective in this example is to test the guidelines given in this paper while comparing the effect that each type of VF has on user performance when providing haptic cues. As a result, we designed a version of the cutting task that uses point-attraction fixtures and a second version that provides guidance with reactive fixtures.</p><p>In each task, we utilized multiple assembly of VFs, some of which were located too close or even overlapped partially each other. To avoid ambiguities as to which fixture provided guidance in these overlapping areas, we devised a system of rules based of the position of the end-effector which maintained only one VF enabled at a time.</p><p>In this example, we also purpose two particular metrics which can be used as a performance measure of the user. The first is task time completion which is measured from task initialization until the user reaches the end of the cutting path with the scalpel. Since it is unlikely for users to exactly cut through the last node in the path, we deem the task to be completed when the projection of the end-effector is sufficiently close to the end of the cutting path. The second metric is the total deviation of the actual cut path from the defined path drawn on the surface of the tissue model. The objective was to minimize the values of each of these metrics using of VFs.</p><p>Based on these metrics, we identified two major steps in the cutting task where the guiding capabilities of VFs could be used to increase user performance. One is the initial approach of the instrument toward the beginning of the cutting path on the object’s surface. The second is the cutting procedure itself along the predefined path.</p><p>We determined that guidance in the first step was necessary to compensate for the limited depth information that can be conveyed through 2D displays. This is a common problem of 3D virtual environment exploration, which negatively affects, in our case, the time users spend approaching cautiously the model with the virtual scalpel. Also, it was desirable to have users initiate the cutting of the model as close as possible to the beginning of the cutting path to minimize path deviation. To achieve these objectives we used a single fixture for this step in each task. In the first task, a relatively large spherical point-attraction fixture was centered at the start of the cutting path. To avoid instability around the point of attraction we utilized the zero-force region method, as it was the least computationally demanding. For the second task, we positioned the apex of a conic reactive fixture at the start of the path, with the base of the cone oriented toward the point at which the virtual scalpel pivots. The placement of these fixtures is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig11">11</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Wireframe screen captures of the round model and the virtual fixtures used to help users approach the start of the cutting path. <b>a</b> Task with point-attraction fixture; <b>b</b> task with reactive fixture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To guide the scalpel along the cutting path, we utilized a cylindrical fixture for each segment of the defined path (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig12">12</a>). The main axis of each cylinder was aligned to the cutting path segment that it belonged to. This arrangement allowed us to enable fixtures sequentially, one at a time, to guide the scalpel from segment to segment of the cutting path in order. The first fixture of these series was automatically enabled after the end-effector was sufficiently close to the start of the cutting path. Consecutive fixtures were activated in order of path location.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Wireframe screen captures of the round model and the virtual fixtures used to help users cut along the predefined path. <b>a</b> Task with point-attraction fixtures; <b>b</b> task with reactive fixtures</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>To ensure continuity and stability of haptic guidance, we created the fixture primitives slightly larger than their corresponding path segments, making their force fields overlap moderately. To decide if the next fixture in the path sequence had to be enabled we projected the end-effector on the current segment of the path being cut. If this projection lied beyond the end of the current segment, then the next fixture in the sequence was enabled (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig13">13</a>). Task termination was determined based on the projection of the end-effector onto the last segment of the cutting path. These heuristics resolved which particular fixture was providing the guiding cues in regions where fixture volumes overlapped. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig14">14</a> demonstrates the conceptual implementation of a sequence of force-feedback. The arrows in the diagram indicate the current enabled fixture and the approximate direction of its forces.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Virtual fixture activation rule based on projection of the end-effector. The <i>solid</i> and <i>striped circles</i> are the end-effector and its projection on the path segment, respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Sequence demonstrating the activation of virtual fixtures based on the end-effector projection onto the path segment. The <i>arrows</i> indicate the current active fixture and the approximate direction of its force field</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We set up the radius of the fixtures along the path sufficiently large in the case of point-attraction fixtures to create sufficiently large force fields that ensured continuous guidance. On the contrary, the radius of the fixture’s cylinder was set sufficiently small for reactive fixtures to constraint the end-effector to a very small area around the cutting path.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-009-0115-4#Fig15">15</a> shows another example of implementation of VF where the objective is to pluck deformable strings attached to the surface of a sphere. In this figure the presence of the VFs are shown as semi-transparent cylinders. When the user enters the prescribed volume of the cylinder, an attractive force field (shown by an arrow) will pull the user in the specific directions. When the user is not within the prescribed force fields, the shading of the VF will change to another color to notify the user of the event of de-activation of the force field.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-009-0115-4/MediaObjects/10055_2009_115_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Application of VF to teasing away learning task. <b>a</b> The user enters a VF and the presence of the user in the force field is represented by a <i>shaded volume</i> of the attractive force field. The <i>arrow</i> represents the force field vector initiating from the user to the point of attraction. <b>b</b> The user has grasped the deformable string and the point-attraction force is guiding the user away from the big sphere in order for the string to get detached. <b>c</b> Similar force field to <b>a</b>. <b>d</b> An example, when the user is outside of a given force field when the surface representation of VF changes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-009-0115-4/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec23"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23">Discussion</h2><div class="c-article-section__content" id="Sec23-content"><p>The design and implementation stages of the VFs and their integration in the given environment requires pre-processing stage since one needs to integrate various simple algorithms, such as collision detection and force calculation of the proposed 3D representations of the VFs. This paper presents a set of standards which can be followed by the designer of the VF in developing a stable interaction environment.</p><p>For example, the arrangement of VFs around the predefined cutting path combined with the series of rules used for VF activation enabled us to provide guidance during the execution of the task. This approach can be generalized to other tasks that require users to follow non-smooth paths such as the ones presented in the cutting tasks. However, users may experience a discontinuous change in force direction at the times when the force-feedback is switched from one fixture to the next one.</p><p>An important issue that still needs to be addressed in depth is the role that these mechanics-based types of VFs have on user performance. A preliminary study (Prada and Payandeh <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Prada R, Payandeh S (2005) A study on design and analysis of virtual fixtures for cutting in training environment. In: Proceedings of the 1st joint eurohaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems, pp 375–380" href="/article/10.1007/s10055-009-0115-4#ref-CR13" id="ref-link-section-d88198e1344">2005</a>) comparing three versions of the cutting task (i.e. the two described in this section and one without VFs) revealed a reduction of task time completion and cut path deviation in the cases where users received guidance through VFs.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusions and future work</h2><div class="c-article-section__content" id="Sec24-content"><p>We introduced a characterization of mechanics-based VFs and analyzed various aspects of their design and implementation from a software architecture point of view. We presented an object-oriented approach for implementing these guiding entities, the advantages and the issues at the time of integrating them in scene graph-based 3D environments. In addition, we described how these fixtures can be assembled to provide guidance during a path-following task in the context of a cutting task. More importantly, we demonstrated the feasibility of utilizing these guidelines to integrate VFs in existing 3D environments.</p><p>The case study presented here served as the basis for a preliminary user study that measured the impact that our fixtures have on user performance. Although initial conclusions suggest an increase on user performance in tasks utilizing our approach to VFs, further studies are require to find out what particular types of fixtures and cues are optimal in terms of user performance.</p><p>The VFs presented here follow a set of basic primitives that are used to represent them and characterize the events that drive the haptic and audiovisual cues. Further studies can explore more complex types of primitives or design fixtures that follow the shapes of the models in the scene. In this latter case, a hybrid of artificial and natural force constraints like the one proposed by Galeano and Payandeh (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Galeano D, Payandeh S (2005) Artificial and natural force constraints in haptic-aided path planning. In: IEEE international workshop on haptic audio visual environments and their applications, pp 45–50" href="/article/10.1007/s10055-009-0115-4#ref-CR4" id="ref-link-section-d88198e1360">2005</a>) may be used to derive the different forces that guide the user.</p><p>Similar case studies as the ones introduced in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-009-0115-4#Sec20">4</a> may study and emphasize on the audiovisual cues that VFs can provide, or expand on the presented suggestions as to include complex cues, such as animated characters that can provide assistance when required.</p><p>Although the use of VFs for training has already been addressed (Gillespie et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Gillespie B, O’Modhrain S, Tang P, Pham C, Zaretsky D (1998) The virtual teacher. In: Proceedings of the ASME IMECE, DSC, vol 64, pp 171–178" href="/article/10.1007/s10055-009-0115-4#ref-CR5" id="ref-link-section-d88198e1373">1998</a>), further exploration can be made using our proposed implementation in different scenarios that may involve common 3D operations other than path-following.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Abbot JJ, Okamura AM (2003) Analysis of virtual fixture contact stability for telemanipulation. In: Proceeding" /><p class="c-article-references__text" id="ref-CR9">Abbot JJ, Okamura AM (2003) Analysis of virtual fixture contact stability for telemanipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 2699–2706</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bettini A, Lang S, Okamura AW, Hager G (2001) Vision assisted control for manipulation using virtual fixtures." /><p class="c-article-references__text" id="ref-CR2">Bettini A, Lang S, Okamura AW, Hager G (2001) Vision assisted control for manipulation using virtual fixtures. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1171–1176</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chan M, Dill J, Payandeh S (2004) Observations of visual and audio coaching methods in a virtual laparoscopic " /><p class="c-article-references__text" id="ref-CR3">Chan M, Dill J, Payandeh S (2004) Observations of visual and audio coaching methods in a virtual laparoscopic training environment. In: Proceedings of the 1st symposium on applied perception in graphics and visualization, p 173</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Galeano D, Payandeh S (2005) Artificial and natural force constraints in haptic-aided path planning. In: IEEE " /><p class="c-article-references__text" id="ref-CR4">Galeano D, Payandeh S (2005) Artificial and natural force constraints in haptic-aided path planning. In: IEEE international workshop on haptic audio visual environments and their applications, pp 45–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gillespie B, O’Modhrain S, Tang P, Pham C, Zaretsky D (1998) The virtual teacher. In: Proceedings of the ASME " /><p class="c-article-references__text" id="ref-CR5">Gillespie B, O’Modhrain S, Tang P, Pham C, Zaretsky D (1998) The virtual teacher. In: Proceedings of the ASME IMECE, DSC, vol 64, pp 171–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kuang AB, Payandeh S, Zheng B, Henigman F, Mackenzie CL (2004) Assembling virtual fixtures for guidance in tra" /><p class="c-article-references__text" id="ref-CR6">Kuang AB, Payandeh S, Zheng B, Henigman F, Mackenzie CL (2004) Assembling virtual fixtures for guidance in training environments. In: Proceedings of the 12th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 367–374</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marayong P, Bettini A, Okamura AW (2002) Effect of virtual fixture compliance on human-machine cooperative man" /><p class="c-article-references__text" id="ref-CR7">Marayong P, Bettini A, Okamura AW (2002) Effect of virtual fixture compliance on human-machine cooperative manipulation. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 1089–1095</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nolin JT, Stemniski PM, Okamura AW (2003) Activation cues and force scaling methods for virtual fixtures. In: " /><p class="c-article-references__text" id="ref-CR8">Nolin JT, Stemniski PM, Okamura AW (2003) Activation cues and force scaling methods for virtual fixtures. In: Proceedings of the 11th international symposium on haptic interfaces for virtual environment and teleoperator systems, pp 404–409</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Park S, Howe R, Torchiana D (2001) Virtual fixtures for robotic cardiac surgery. In: Proceedings of the 4th in" /><p class="c-article-references__text" id="ref-CR10">Park S, Howe R, Torchiana D (2001) Virtual fixtures for robotic cardiac surgery. In: Proceedings of the 4th international conference on medical image computing and computer-assisted intervention, pp 1419–1420</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Payandeh S (2001) Application of shared control strategy in the design of a robotic device. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR11">Payandeh S (2001) Application of shared control strategy in the design of a robotic device. In: Proceedings of american control conference, pp 4532–4536</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Payandeh S, Stanisic Z (2002) On application of virtual fixtures as an aid for telemanipulation and training. " /><p class="c-article-references__text" id="ref-CR12">Payandeh S, Stanisic Z (2002) On application of virtual fixtures as an aid for telemanipulation and training. In: Proceedings of the 10th symposium on haptic interfaces for virtual environment and teleoperator systems. Orlando, FL, pp 18–23</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Payandeh S, Dill J, Wilson G, Zhang H, Shi L, Lomax A, Mackenzie CL (2003) Demo: a multi-modal training enviro" /><p class="c-article-references__text" id="ref-CR15">Payandeh S, Dill J, Wilson G, Zhang H, Shi L, Lomax A, Mackenzie CL (2003) Demo: a multi-modal training environment for surgeons. In: Proceedings of the 5th international conference on multimodal interfaces, pp 301–302. <a href="http://web.ensc.sfu.ca/research/erl/ViTEn/">http://web.ensc.sfu.ca/research/erl/ViTEn/</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Prada R, Payandeh S (2005) A study on design and analysis of virtual fixtures for cutting in training environm" /><p class="c-article-references__text" id="ref-CR13">Prada R, Payandeh S (2005) A study on design and analysis of virtual fixtures for cutting in training environment. In: Proceedings of the 1st joint eurohaptics conference and symposium on haptic interfaces for virtual environment and teleoperator systems, pp 375–380</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In&#34; Proceedings of IEEE vi" /><p class="c-article-references__text" id="ref-CR14">Rosenberg L (1993) Virtual fixtures: perceptual tools for telerobotic manipulation. In" Proceedings of IEEE virtual reality international symposium, pp 76–82</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sayers CP, Paul RP (1994) An operator interface for teleprogramming employing synthetic fixtures presence, vol" /><p class="c-article-references__text" id="ref-CR16">Sayers CP, Paul RP (1994) An operator interface for teleprogramming employing synthetic fixtures presence, vol 3, pp 4–309</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schroeder WJ, Martin KM, Lorensen WE (1996) The design and implementation of an object-oriented toolkit for 3d" /><p class="c-article-references__text" id="ref-CR17">Schroeder WJ, Martin KM, Lorensen WE (1996) The design and implementation of an object-oriented toolkit for 3d graphics and visualization. In: Proceedings of the 7th conference on visualization. San Francisco, USA, 93-ff</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Strauss PS, Carey R (1992) An object-oriented 3d graphics toolkit. In: Proceedings of the 19th annual conferen" /><p class="c-article-references__text" id="ref-CR18">Strauss PS, Carey R (1992) An object-oriented 3d graphics toolkit. In: Proceedings of the 19th annual conference on computer graphics and interactive techniques, pp 341–349</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="VRML Consortium (1997) The virtual reality modeling language specification ISO/IEC DIS 14772-1" /><p class="c-article-references__text" id="ref-CR1">VRML Consortium (1997) The virtual reality modeling language specification ISO/IEC DIS 14772-1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang H, Payandeh S, Dill J (2004) On cutting and dissection of virtual deformable objects. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR19">Zhang H, Payandeh S, Dill J (2004) On cutting and dissection of virtual deformable objects. In: Proceedings of the IEEE international conference on robotics and automation, pp 3908–3913</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-009-0115-4-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Experimental Robotics and Graphics Laboratory, Simon Fraser University, Burnaby, BC, Canada</p><p class="c-article-author-affiliation__authors-list">Rodolfo Prada &amp; Shahram Payandeh</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Rodolfo-Prada"><span class="c-article-authors-search__title u-h3 js-search-name">Rodolfo Prada</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Rodolfo+Prada&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Rodolfo+Prada" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Rodolfo+Prada%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Shahram-Payandeh"><span class="c-article-authors-search__title u-h3 js-search-name">Shahram Payandeh</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Shahram+Payandeh&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shahram+Payandeh" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shahram+Payandeh%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-009-0115-4/email/correspondent/c1/new">Shahram Payandeh</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=On%20study%20of%20design%20and%20implementation%20of%20virtual%20fixtures&amp;author=Rodolfo%20Prada%20et%20al&amp;contentID=10.1007%2Fs10055-009-0115-4&amp;publication=1359-4338&amp;publicationDate=2009-03-26&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Prada, R., Payandeh, S. On study of design and implementation of virtual fixtures.
                    <i>Virtual Reality</i> <b>13, </b>117–129 (2009). https://doi.org/10.1007/s10055-009-0115-4</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-009-0115-4.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2008-05-20">20 May 2008</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-03-12">12 March 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-03-26">26 March 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-06">June 2009</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-009-0115-4" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-009-0115-4</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal cues</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Haptic interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Automatic constraints</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Scene graph environments</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User performance</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-009-0115-4.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=115;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

