<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="3D teleimmersion for collaboration and interaction of geographically d"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Teleimmersion is an emerging technology that enables users to collaborate remotely by generating realistic 3D avatars in real time and rendering them inside a shared virtual space. The..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="3D teleimmersion for collaboration and interaction of geographically distributed users"/>

    <meta name="dc.source" content="Virtual Reality 2012 17:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2012-11-25"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2012 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Teleimmersion is an emerging technology that enables users to collaborate remotely by generating realistic 3D avatars in real time and rendering them inside a shared virtual space. The teleimmersive environment thus provides a venue for collaborative work on 3D data such as medical imaging, scientific data and models, archaeological datasets, architectural or mechanical designs, remote training (e.g., oil rigs, military applications), and remote teaching of physical activities (e.g., rehabilitation, dance). In this paper, we present our research work performed over the course of several years in developing the teleimmersive technology using image-based stereo and more recently Kinect. We outline the issues pertaining to the capture, transmission, rendering, and interaction. We describe several applications where we have explored the use of the 3D teleimmersion for remote interaction and collaboration among professional and scientific users. We believe the presented findings are relevant for future developers in teleimmersion and apply across various 3D video capturing technologies."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2012-11-25"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="29"/>

    <meta name="prism.endingPage" content="43"/>

    <meta name="prism.copyright" content="2012 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-012-0217-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-012-0217-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-012-0217-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-012-0217-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="3D teleimmersion for collaboration and interaction of geographically distributed users"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2013/03"/>

    <meta name="citation_online_date" content="2012/11/25"/>

    <meta name="citation_firstpage" content="29"/>

    <meta name="citation_lastpage" content="43"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-012-0217-2"/>

    <meta name="DOI" content="10.1007/s10055-012-0217-2"/>

    <meta name="citation_doi" content="10.1007/s10055-012-0217-2"/>

    <meta name="description" content="Teleimmersion is an emerging technology that enables users to collaborate remotely by generating realistic 3D avatars in real time and rendering them insid"/>

    <meta name="dc.creator" content="Gregorij Kurillo"/>

    <meta name="dc.creator" content="Ruzena Bajcsy"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Media Psychol; citation_title=The effect of interactivity on learning physical actions in virtual reality; citation_author=JN Bailenson, K Patel, A Nielsen, R Bajcsy, S Jung, G Kurillo; citation_volume=11; citation_publication_date=2008; citation_pages=354-376; citation_doi=10.1080/15213260802285214; citation_id=CR1"/>

    <meta name="citation_reference" content="Bajcsy P, McHenry K, Na HJ, Malik R, Spencer A, Lee SK, Kooper R, Frogley M (2009) Immersive environments for rehabilitation activities. In: Proceedings of the 17th ACM international conference on multimedia, ACM, New York, MM &#8217;09, pp 829&#8211;832"/>

    <meta name="citation_reference" content="Baker H, Tanguay D, Sobel I, Gelb D, Gross M, Culbertson W, Malzenbender T (2002) The coliseum immersive teleconferencing system. In: Proceedings of international workshop on immersive telepresence, Juan-les-Pins, France"/>

    <meta name="citation_reference" content="Benford S, Greenhalgh C, Bowers J, Snowdon D, Fahlen LE (1995) User embodiment in collaborative virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI &#8217;95), ACM Press/Addison-Wesley Publishing Co. New York, pp 242&#8211;249"/>

    <meta name="citation_reference" content="Benko H, Jota R, Wilson A (2012) Miragetable: freehand interaction on a projected augmented reality tabletop. In: CHI, pp 199&#8211;208"/>

    <meta name="citation_reference" content="CadFaster (2012) Cadfaster. 
                    http://www.cadfaster.com/index.php/Products
                    
                  
                "/>

    <meta name="citation_reference" content="Cheng X, Davis J, Slusallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR 2000)"/>

    <meta name="citation_reference" content="DeFanti T, Sandin D, Brown M, Pape D, Anstey J, Bogucki M, Dawe G, Johnson A, Huang TS (1999) Technologies for virtual reality/tele-immersion applications: issues of research in image display and global networking. In: EC/NSF workshop on research frontiers in virtual environments and human-centered computing"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=On consistency and network latency in distributed interactive applications: a survey&#8211;part I; citation_author=D Delaney, T Ward, S McLoone; citation_volume=15; citation_publication_date=2006; citation_pages=218-234; citation_doi=10.1162/pres.2006.15.2.218; citation_id=CR9"/>

    <meta name="citation_reference" content="De Silva DVSX, Fernando WAC, Kodikaraarachchi H, Worrall ST, Kondoz AM (2010) A depth map post-processing technique for 3D-TV systems based on compression artifact analysis. IEEE J Sel Top Signal Process November 2011"/>

    <meta name="citation_reference" content="citation_journal_title=J Exp Psychol Appl; citation_title=Face-to-face and video-mediated communication: a comparison of dialogue structure and task performance; citation_author=G Doherty-Sneddon, A Anderson, C O&#8217;Malley, S Langton, S Garrod, V Bruce; citation_volume=3; citation_issue=2; citation_publication_date=1997; citation_pages=105-125; citation_doi=10.1037/1076-898X.3.2.105; citation_id=CR11"/>

    <meta name="citation_reference" content="Eon (2009) Eon Reality: EON Coliseum. 
                    http://www.eonreality.com/product_coliseum.html
                    
                  
                "/>

    <meta name="citation_reference" content="Forte M, Kurillo G (2010) Cyberarchaeology&#8212;experimenting with teleimmersive archaeology. In: Proceedings of 16th international conference on virtual systems and multimedia (VSMM 2010), Seoul"/>

    <meta name="citation_reference" content="Forte M, Kurillo G, Matlock T (2010) Teleimmersive archaeology: simulation and cognitive impact. In: Ioannides M, Fellner AG D, Hadjimitsis D (eds) EuroMed 2010&#8212;digital heritage, Lemesos"/>

    <meta name="citation_reference" content="citation_journal_title=J Soc Psychol; citation_title=The effects of feedback and eye contact on performance of a digit-coding task; citation_author=R Fry, G Smith; citation_volume=96; citation_publication_date=1975; citation_pages=145-146; citation_doi=10.1080/00224545.1975.9923275; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph; citation_title=blue-c: a spatially immersive display and 3D video portal for telepresence; citation_author=M Gross, S W&#252;rmlin, M Naef, E Lamboray, C Spagno, A Kunz, E Koller-Meier, T Svoboda, LV Gool, S Lang, K Strehlke, AV Moere, O Staadt; citation_volume=22; citation_issue=3; citation_publication_date=2003; citation_pages=819-827; citation_doi=10.1145/882262.882350; citation_id=CR16"/>

    <meta name="citation_reference" content="Gutwin C (2001) The effects of network delays on group work in real-time groupware. In: Proceedings of the seventh conference on European conference on computer supported cooperative work, Kluwer, Norwell, pp 299&#8211;318"/>

    <meta name="citation_reference" content="Hasenfratz J, Lapierre M, Sillion F (2004) A real-time system for full-body interaction with virtual worlds. In: Proceedings of Eurographics symposium on virtual environments, The Eurographics Association, pp 147&#8211;156"/>

    <meta name="citation_reference" content="Hypercosm (2009) Hypercosm. 
                    http://hypercosm.com
                    
                  
                "/>

    <meta name="citation_reference" content="Ihrke I, Ahrenberg L, M Magnor M (2004) External camera calibration for synchronized multi-video systems. In: Proceedings of 12th international conference on computer graphics, visualization and computer vision 2004, vol 12, Plzen, pp 537&#8211;544"/>

    <meta name="citation_reference" content="citation_journal_title=J Multime&#39;d; citation_title=A framework for constructing real-time immersive environments for training physical activities; citation_author=S Jung, R Bajcsy; citation_volume=1; citation_issue=7; citation_publication_date=2006; citation_pages=9-17; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graphics Appl; citation_title=Real-time animation of realistic virtual humans; citation_author=P Kalra, N Magnenat-Thalman, L Moccozet, G Sannier, A Aubel, D Thalman; citation_volume=18; citation_issue=25; citation_publication_date=1998; citation_pages=42-56; citation_doi=10.1109/38.708560; citation_id=CR22"/>

    <meta name="citation_reference" content="Kauff P, Schreer O (2002) An immersive 3D video-conferencing system using shared virtual team user environments. In: Proceedings of the 4th international conference on collaborative virtual environments, pp 105&#8211;112"/>

    <meta name="citation_reference" content="Khoshelham K (2011) Accuracy analysis of Kinect depth data. In: Proceedings of ISPRS workshop laser scanning, Calgary"/>

    <meta name="citation_reference" content="Knoblauch D, Font P, Kuester F (2010) VirtualizeMe: real-time avatar creation for tele-Immersion environments. In: Virtual reality conference (VR), 2010 IEEE, pp 279&#8211;280"/>

    <meta name="citation_reference" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of Eurographics 2009&#8212;state of the art reports, pp 119&#8211;134"/>

    <meta name="citation_reference" content="Kreylos O (2008) Environment-independent VR development. In: Bebis G, et&#160;al. (eds) Advances in visual computing, lecture notes in computer science. Springer, Berlin, pp 901&#8211;912"/>

    <meta name="citation_reference" content="Kurillo G, Li Z, Bajcsy R (2008a) Wide-area external multi-camera calibration using vision graphs and virtual calibration object. In: Proceedings of 2nd ACM/IEEE international conference on distributed smart cameras (ICDSC 08), IEEE, Stanford"/>

    <meta name="citation_reference" content="Kurillo G, Vasudevan R, Lobaton E, Bajcsy R (2008b) A framework for collaborative real-time 3D teleimmersion in a geographically distributed environment. In: Proceedings of 10th IEEE international symposium on multimedia (ISM 2008), Berkeley, pp 111&#8211;118"/>

    <meta name="citation_reference" content="Kurillo G, Bajcsy R, Kreylos O, Rodriguez R (2009a) Teleimmersive environment for remote medical collaboration. In: Westwood J, et&#160;al. (eds) Medicine meets virtual reality 17, IOS press, Ohmsha, pp 148&#8211;150"/>

    <meta name="citation_reference" content="Kurillo G, Li Z, Bajcsy R (2009b) Framework for hierarchical calibration of multi-camera systems for teleimmersion. In: Proceedings of IMMERSCOM 2009, Berkeley, CA, pp 1:1&#8211;1:6"/>

    <meta name="citation_reference" content="Kurillo G, Forte M, Bajcsy R (2010a) Teleimmersive 3D collaborative environment for cyberarchaeology. In: IEEE/CVPR workshop, applications of computer vision in archaeology (ACVA 2010), San Francisco"/>

    <meta name="citation_reference" content="Kurillo G, Koritnik T, Bajd T, Bajcsy R (2010b) Real-time 3D avatars for tele-rehabilitation in virtual reality. In: Westwood J, Westwood S, et&#160;al. (eds) Proceedings of 18th medicine meets virtual reality (MMVR) conference., IOS press, pp 290&#8211;296"/>

    <meta name="citation_reference" content="Kuster C, Popa T, Zach C, Gotsman C, Gross M (2011) Freecam: A hybrid camera system for interactive free-viewpoint video. In: Proceedings of vision, modeling, and visualization (VMV)"/>

    <meta name="citation_reference" content="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: IEEE computer society conference on computer vision and pattern recognition workshops, 2008. CVPR Workshops, pp 1&#8211;8"/>

    <meta name="citation_reference" content="libjpeg (2012) libjpeg-turbo. 
                    http://libjpeg-turbo.virtualgl.org
                    
                  
                "/>

    <meta name="citation_reference" content="Litos G, Zabulis X, Triantafyllidis G (2006) Synchronous image acquisition based on network synchronization. In: Computer vision and pattern recognition workshop 2006. CVPRW &#8217;06. Conference on, pp 167&#8211;173"/>

    <meta name="citation_reference" content="Maimone A, Fuchs H (2011) Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras. In: Proceedings of the 2011 10th IEEE international symposium on mixed and augmented reality, IEEE Computer Society, Washington, ISMAR &#8217;11, pp 137&#8211;146"/>

    <meta name="citation_reference" content="Maimone A, Fuchs H (2012) Reducing interference between multiple structured light depth sensors using motion. In: Virtual reality workshops (VR), IEEE, pp 51&#8211;54"/>

    <meta name="citation_reference" content="Microsoft (2010) Microsoft Kinect. 
                    http://www.xbox.com/en-US/kinect
                    
                  
                "/>

    <meta name="citation_reference" content="Mulligan J, Daniilidis K (2001) Real time trinocular stereo for tele-immersion. In: Proceedings of 2001 international conference on image processing, Thessaloniki, pp 959&#8211;962"/>

    <meta name="citation_reference" content="Nahrstedt K, Bajcsy R, Wymore L, Kurillo G, Mezur K, Sheppard R, Yang Z, Wu W (2007) Symbiosis of tele-immersive environments with creative choreography. In: ACM workshop on supporting creative acts beyond dissemination, associated with 6th ACM creativity and cognition conference, Washington"/>

    <meta name="citation_reference" content="Nintendo (2006) Nintendo Wii. 
                    http://www.nintendo.com/wii
                    
                  
                "/>

    <meta name="citation_reference" content="Obdrzalek S, Kurilo G, Han J, Abresch T, Bajcsy R (2012) Real-time human pose detection and tracking for tele-rehabilitation in virtual reality. In: Proceedings of the 19th medicine meets virtual reality conference (MMVR), Newport Beach"/>

    <meta name="citation_reference" content="OpenWonderland (2010) Sun Systems, openWonderLand. 
                    http://openwonderland.org
                    
                  
                "/>

    <meta name="citation_reference" content="Petit B, Lesage JD, Menier C, Allard J, Franco JS, Raffin B, Boyer E, Faure F (2010) Multicamera real-time 3D modeling for telepresence and remote collaboration. Int J Digit Multime&#39;d Broadcast 12"/>

    <meta name="citation_reference" content="Schr&#246;der Y, Scholz A, Berger K, Ruhl K, Guthe S, Magnor M (2011) Multiple kinect studies. Tech Rep 09&#8211;15, ICG"/>

    <meta name="citation_reference" content="SecondLife (2003) SecondLife. 
                    http://secondlife.com
                    
                  
                "/>

    <meta name="citation_reference" content="Sheppard R, Wu W, Yang Z, Nahrstedt K, Wymore L, Kurillo G, Bajcsy R, Mezur K (2007) New digital options in geographically distributed dance collaborations with TEEVE: tele-immersive environments for everybody. In: Proceedings of the 15th international conference on multimedia, ACM, New York, MULTIMEDIA &#8217;07, pp 1085&#8211;1086"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A convenient multicamera self-calibration for virtual environments; citation_author=T Svoboda, D Martinec, T Pajdla; citation_volume=14; citation_issue=4; citation_publication_date=2005; citation_pages=407-422; citation_doi=10.1162/105474605774785325; citation_id=CR50"/>

    <meta name="citation_reference" content="Vasudevan R, Lobaton E, Kurillo G, Bajcsy R, Bernardin T, Hamann B, Nahrstedt K (2010a) A methodology for remote virtual interaction in teleimmersive environments. In: Proceedings of first ACM multimedia systems conference, Scottsdale, pp 281&#8211;292"/>

    <meta name="citation_reference" content="Vasudevan R, Zhou Z, Kurillo G, Lobaton E, Bajcsy R, Nahrstedt K (2010b) Real-time stereo-vision system for 3D teleimmersive collaboration. In: Proceedings of IEEE international conference on multimedia &amp; expo (ICME 2010), Singapore, pp 1208&#8211;1213"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Multime&#39;d; citation_title=High quality visualization for geographically distributed 3D teleimmersive applications; citation_author=R Vasudevan, G Kurillo, E Lobaton, T Bernardin, O Kreylos, R Bajcsy, K Nahrstedt; citation_volume=13; citation_issue=3; citation_publication_date=2011; citation_pages=573-584; citation_doi=10.1109/TMM.2011.2123871; citation_id=CR53"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=Scalable 3D video of dynamic scenes; citation_author=M Waschb&#252;sch, S W&#252;rmlin, D Cotting, F Sadlo, MH Gross; citation_volume=21; citation_issue=8-10; citation_publication_date=2005; citation_pages=629-638; citation_doi=10.1007/s00371-005-0346-7; citation_id=CR54"/>

    <meta name="citation_reference" content="Wu W, Arefin MA, Huang Z, Agarwal P, Shi S, Rivas R, Nahrstedt K (2010) &#8220;I&#8217;m the Jedi!&#8221;&#8212;a case study of user experience in 3D tele-immersive gaming. In: ISM, pp 220&#8211;227"/>

    <meta name="citation_reference" content="Wu W, Arefin A, Kurillo G, Agarwal P, Nahrstedt K, Bajcsy R (2011) Color-plus-depth level-of-details in 3D teleimmersive video&#8212;a psychophysical approach. In: Proceedings of ACM multimedia"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Sci Eng; citation_title=Rendering avatars in virtual reality: integrating a 3D model with 2D images; citation_author=Y Yang, X Wang, JX Chen; citation_volume=4; citation_issue=1; citation_publication_date=2002; citation_pages=86-91; citation_doi=10.1109/5992.976440; citation_id=CR57"/>

    <meta name="citation_reference" content="Wu W, Arefin A, Rivas R, Nahrstedt K, Sheppard R, Yang Z (2009) Quality of experience in distributed interactive multimedia environments: toward a theoretical framework. In: Proceedings of ACM multimedia, Beijing, pp 481-490"/>

    <meta name="citation_reference" content="Yang Z, Nahrstedt K, Cui Y, Yu B, Liang J, Jung SH, Bajcsy R (2005) TEEVE: The next generation architecture for tele-immersive environment. In: Seventh IEEE international symposium on multimedia (ISM 2005). IEEE Computer Society, Irvine, pp 112&#8211;119"/>

    <meta name="citation_reference" content="citation_journal_title=ACM transactions on multimedia computing, communications, and applications (TOMCCAP); citation_title=Enabling multi-party 3D tele-immersive environments with viewcast; citation_author=Z Yang, W Wu, K Nahrstedt, G Kurillo, R Bajcsy; citation_volume=6; citation_publication_date=2010; citation_pages=1-30; citation_id=CR60"/>

    <meta name="citation_reference" content="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajscy R (2007) ViewCast: view dissemination and management for multi-party 3D tele-immersive environments. In: Proceedings of ACM multimedia, Augsburg, pp 882&#8211;891"/>

    <meta name="citation_reference" content="Zhang D, Nomura Y, Fujii S (1991) Error analysis and optimization of camera calibration. In: Proceedings of IEEE/RSJ international workshop on intelligent robots and systems (IROS 91), Osaka, pp 292&#8211;296"/>

    <meta name="citation_author" content="Gregorij Kurillo"/>

    <meta name="citation_author_email" content="gregorij@eecs.berkeley.edu"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA"/>

    <meta name="citation_author" content="Ruzena Bajcsy"/>

    <meta name="citation_author_email" content="bajcsy@eecs.berkeley.edu"/>

    <meta name="citation_author_institution" content="Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-012-0217-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-012-0217-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="3D teleimmersion for collaboration and interaction of geographically distributed users"/>
        <meta property="og:description" content="Teleimmersion is an emerging technology that enables users to collaborate remotely by generating realistic 3D avatars in real time and rendering them inside a shared virtual space. The teleimmersive environment thus provides a venue for collaborative work on 3D data such as medical imaging, scientific data and models, archaeological datasets, architectural or mechanical designs, remote training (e.g., oil rigs, military applications), and remote teaching of physical activities (e.g., rehabilitation, dance). In this paper, we present our research work performed over the course of several years in developing the teleimmersive technology using image-based stereo and more recently Kinect. We outline the issues pertaining to the capture, transmission, rendering, and interaction. We describe several applications where we have explored the use of the 3D teleimmersion for remote interaction and collaboration among professional and scientific users. We believe the presented findings are relevant for future developers in teleimmersion and apply across various 3D video capturing technologies."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>3D teleimmersion for collaboration and interaction of geographically distributed users | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-012-0217-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"3D video, 3D teleimmersion, Human–computer interaction, Remote collaboration, Telepresence","kwrd":["3D_video","3D_teleimmersion","Human–computer_interaction","Remote_collaboration","Telepresence"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-012-0217-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-012-0217-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=217;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-012-0217-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            3D teleimmersion for collaboration and interaction of geographically distributed users
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0217-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0217-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2012-11-25" itemprop="datePublished">25 November 2012</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">3D teleimmersion for collaboration and interaction of geographically distributed users</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gregorij-Kurillo" data-author-popup="auth-Gregorij-Kurillo" data-corresp-id="c1">Gregorij Kurillo<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California, Berkeley" /><meta itemprop="address" content="grid.47840.3f, 0000000121817878, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, CA, 94720-1764, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ruzena-Bajcsy" data-author-popup="auth-Ruzena-Bajcsy">Ruzena Bajcsy</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of California, Berkeley" /><meta itemprop="address" content="grid.47840.3f, 0000000121817878, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, CA, 94720-1764, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">29</span>–<span itemprop="pageEnd">43</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1004 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">21 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-012-0217-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Teleimmersion is an emerging technology that enables users to collaborate remotely by generating realistic 3D avatars in real time and rendering them inside a shared virtual space. The teleimmersive environment thus provides a venue for collaborative work on 3D data such as medical imaging, scientific data and models, archaeological datasets, architectural or mechanical designs, remote training (e.g., oil rigs, military applications), and remote teaching of physical activities (e.g., rehabilitation, dance). In this paper, we present our research work performed over the course of several years in developing the teleimmersive technology using image-based stereo and more recently Kinect. We outline the issues pertaining to the capture, transmission, rendering, and interaction. We describe several applications where we have explored the use of the 3D teleimmersion for remote interaction and collaboration among professional and scientific users. We believe the presented findings are relevant for future developers in teleimmersion and apply across various 3D video capturing technologies.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>During the past two decades, the three-dimensional (3D) virtual environments have been rather limited to research institutions, military, and various corporate establishments due to high costs of virtual reality (VR) technology. In recent years, the technological advances have brought the technology closer to consumer market. The growing popularity of 3D cinema and television has led to the rapid development of numerous affordable 3D display devices. Release of various interface devices for gaming and entertainment, such as Nintendo Wii (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Nintendo (2006) Nintendo Wii. &#xA;                    http://www.nintendo.com/wii&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR43" id="ref-link-section-d120266e301">2006</a>) and Microsoft Kinect (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Microsoft (2010) Microsoft Kinect. &#xA;                    http://www.xbox.com/en-US/kinect&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR40" id="ref-link-section-d120266e304">2010</a>), has facilitated a cost-effective means for natural interaction in 3D environments. Furthermore, the high penetration of the Internet and growth of the social networks created new opportunities for collaboration and interaction of geographically distributed users beyond simple videoconferencing systems.</p><p>In many disciplines of science and engineering, 3D visualization software is used to visualize high-dimensional data on 2D and 3D displays (e.g., in geophysics, astronomy, medicine, archaeology, design, manufacturing, and many others). Real-time collaboration with these data, however, has been highly limited in these fields and majority of the interaction is still performed by uploading or downloading large datasets for off-line review or by sharing screenshots (or desktop) to discuss various points pertaining to the data through traditional videoconferencing tools.</p><p>Several attempts can be found in the development of open source and commercial software or web-based collaborative tools intended for sharing, interaction, and visualization of 3D (mesh) data. Two examples of collaborative tools for computer-aided design (CAD) include CadFaster (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="CadFaster (2012) Cadfaster. &#xA;                    http://www.cadfaster.com/index.php/Products&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR6" id="ref-link-section-d120266e312">2012</a>) and Hypercosm (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Hypercosm (2009) Hypercosm. &#xA;                    http://hypercosm.com&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR19" id="ref-link-section-d120266e315">2009</a>) where more complex 3D models can be shared and interacted with remotely. Most popular interaction in 3D environment via avatars is implemented in Second Life (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="SecondLife (2003) SecondLife. &#xA;                    http://secondlife.com&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR48" id="ref-link-section-d120266e318">2003</a>) which features a large virtual world where 3D models can be created and shared; Similar environment is Open Wonderland by Sun Microsystems (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="OpenWonderland (2010) Sun Systems, openWonderLand. &#xA;                    http://openwonderland.org&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR45" id="ref-link-section-d120266e321">2010</a>) where users can interact through their avatars and share spreadsheets and other types of data from desktop applications; Eon’s Coliseum (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Eon (2009) Eon Reality: EON Coliseum. &#xA;                    http://www.eonreality.com/product_coliseum.html&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR12" id="ref-link-section-d120266e324">2009</a>) includes presentation style interaction with (simplified) 3D models and other data (e.g., images, videos). Since the avatars are static, that is, pre-generated, their control and animation can be overly simplistic and fail to capture all the gestures and natural interaction that is common in real-life interaction.</p><p>Collaborative applications should provide a communication channel that allows users to verbally and nonverbally communicate and interact with the data. In case of videoconferencing systems, the visual and audio communication are established; however, majority of the systems cannot adequately capture gestures, eye contact, and other forms of nonverbal communication, which have been shown to increase trust, collaboration, and productivity (Fry and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1975" title="Fry R, Smith G (1975) The effects of feedback and eye contact on performance of a digit-coding task. J Soc Psychol 96:145–146" href="/article/10.1007/s10055-012-0217-2#ref-CR15" id="ref-link-section-d120266e330">1975</a>; Doherty-Sneddon et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Doherty-Sneddon G, Anderson A, O’Malley C, Langton S, Garrod S, Bruce V (1997) Face-to-face and video-mediated communication: a comparison of dialogue structure and task performance. J Exp Psychol Appl 3(2):105–125" href="/article/10.1007/s10055-012-0217-2#ref-CR11" id="ref-link-section-d120266e333">1997</a>). When using traditional videoconferencing techniques, users are disconnected from the data, as the latter is usually presented in a separate window on the screen, resulting in a very low level of immersion or presence. The experience can be enhanced by the teleimmersion (TI) technology (DeFanti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="DeFanti T, Sandin D, Brown M, Pape D, Anstey J, Bogucki M, Dawe G, Johnson A, Huang TS (1999) Technologies for virtual reality/tele-immersion applications: issues of research in image display and global networking. In: EC/NSF workshop on research frontiers in virtual environments and human-centered computing" href="/article/10.1007/s10055-012-0217-2#ref-CR8" id="ref-link-section-d120266e336">1999</a>) which connects geographically distributed users captured by multiple cameras. For the teleimmersion users, the benefit is clearly in the ability to interact with their remote collaborators inside the same environment, while experiencing an increased spatial awareness with respect to the data and other users (e.g., Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig1">1</a>, geographically distributed engineers can explore design of an aircraft interior through a remote virtual embodiment).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Using stereo cameras, a dynamic 3D avatar is created in real time and projected at the remote location into a shared virtual environment to facilitate visual experience similar to face-to-face interaction. The users can take advantage of 3D interaction and display technologies to collaborate with their remote partners</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In this paper, we report our experience of developing, deploying, and evaluating the 3D teleimmersion technology based primarily on stereo imaging. In the next chapter, we review the previous work in 3D capturing technologies for collaborative or local interaction in virtual environments. Next, we summarize the requirements of the teleimmersion technology and propose various interaction modes for distributed collaboration in a shared virtual environment. We continue with description of our TI framework and outline some of the issues pertaining to the development of such systems. Next, we report on experiments in several application areas where we have explored the use of this technology for geographically distributed human-to-human interaction and exploration of complex spatial data. In the concluding section, we propose several future research directions in this exciting new area.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>As identified in the early beginnings of collaborative virtual reality, the virtual presence, also referred to as <i>user embodiment</i>, represents a key component for successful collaboration of geographically distributed users (Benford et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Benford S, Greenhalgh C, Bowers J, Snowdon D, Fahlen LE (1995) User embodiment in collaborative virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’95), ACM Press/Addison-Wesley Publishing Co. New York, pp 242–249" href="/article/10.1007/s10055-012-0217-2#ref-CR4" id="ref-link-section-d120266e374">1995</a>). Traditional immersive virtual reality systems employ pre-generated avatars, to represent the human user inside the computer-generated environment (Kalra et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Kalra P, Magnenat-Thalman N, Moccozet L, Sannier G, Aubel A, Thalman D (1998) Real-time animation of realistic virtual humans. IEEE Comput Graphics Appl 18(25):42–56" href="/article/10.1007/s10055-012-0217-2#ref-CR22" id="ref-link-section-d120266e377">1998</a>; SecondLife <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="SecondLife (2003) SecondLife. &#xA;                    http://secondlife.com&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR48" id="ref-link-section-d120266e380">2003</a>; Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Yang Y, Wang X, Chen JX (2002) Rendering avatars in virtual reality: integrating a 3D model with 2D images. Comput Sci Eng 4(1):86–91" href="/article/10.1007/s10055-012-0217-2#ref-CR57" id="ref-link-section-d120266e383">2002</a>). The state-of-the-art in animation and computer graphics, however, cannot yet in real time convey the subtle communication cues that are used in human-to-human interaction. On the other hand, real-time 3D digitization of people through various capture technologies can create dynamic 3D avatars (in reminder of the text referred to as <i>3D avatars</i>) where the facial and hand gestures are captured, eye contact can be established, and spatial geometry of the body movements can be transformed into the virtual space.</p><p>In the past decade, several researchers have attempted to develop real-time 3D acquisition systems for telepresence. The first teleimmersion system based on stereo was presented by researchers at the University of Pennsylvania who used several stereo clusters to reconstruct the upper body (Mulligan and Daniilidis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Mulligan J, Daniilidis K (2001) Real time trinocular stereo for tele-immersion. In: Proceedings of 2001 international conference on image processing, Thessaloniki, pp 959–962" href="/article/10.1007/s10055-012-0217-2#ref-CR41" id="ref-link-section-d120266e393">2001</a>). A local user was able to communicate to a remote user while preserving gaze. A similar system was proposed by Kauff and Schreer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kauff P, Schreer O (2002) An immersive 3D video-conferencing system using shared virtual team user environments. In: Proceedings of the 4th international conference on collaborative virtual environments, pp 105–112" href="/article/10.1007/s10055-012-0217-2#ref-CR23" id="ref-link-section-d120266e396">2002</a>) who combined the 3D video data generated by multi-baseline algorithm from four views. The system featured a custom-built multi-processor board to achieve interactive frame rate. A desktop teleimmersive system based on reconstruction from silhouettes was initially proposed by Baker et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Baker H, Tanguay D, Sobel I, Gelb D, Gross M, Culbertson W, Malzenbender T (2002) The coliseum immersive teleconferencing system. In: Proceedings of international workshop on immersive telepresence, Juan-les-Pins, France" href="/article/10.1007/s10055-012-0217-2#ref-CR3" id="ref-link-section-d120266e399">2002</a>) who used five different views to obtain a 3D model of the user via a visual-hull approach. The system was based on a single PC which performed 3D reconstruction and rendering of the users in a simple virtual meeting room. Though compact, the system showed limited accuracy and speed. A full-body teleimmersive system was introduced by researchers at ETH Zurich who applied silhouette-based 3D reconstruction from several cameras to capture the user inside a CAVE-like environment (Gross et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Gross M, Würmlin S, Naef M, Lamboray E, Spagno C, Kunz A, Koller-Meier E, Svoboda T, Gool LV, Lang S, Strehlke K, Moere AV, Staadt O (2003) blue-c: a spatially immersive display and 3D video portal for telepresence. ACM Trans Graph 22(3):819–827" href="/article/10.1007/s10055-012-0217-2#ref-CR16" id="ref-link-section-d120266e402">2003</a>). The presented applications were focused on information visualization and simple interactions. More recently, Ladikos et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: IEEE computer society conference on computer vision and pattern recognition workshops, 2008. CVPR Workshops, pp 1–8" href="/article/10.1007/s10055-012-0217-2#ref-CR35" id="ref-link-section-d120266e405">2008</a>) proposed a 3D reconstruction approach using visual hulls built in CUDA working at 30 frames per second (FPS). Knoblauch et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knoblauch D, Font P, Kuester F (2010) VirtualizeMe: real-time avatar creation for tele-Immersion environments. In: Virtual reality conference (VR), 2010 IEEE, pp 279–280" href="/article/10.1007/s10055-012-0217-2#ref-CR25" id="ref-link-section-d120266e409">2010</a>) presented another visual-hull-based system which captures human user in real time and generates the volumetric model of the avatar for interaction in CAVE-like environments. Petit et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Petit B, Lesage JD, Menier C, Allard J, Franco JS, Raffin B, Boyer E, Faure F (2010) Multicamera real-time 3D modeling for telepresence and remote collaboration. Int J Digit Multime'd Broadcast 12" href="/article/10.1007/s10055-012-0217-2#ref-CR46" id="ref-link-section-d120266e412">2010</a>) presented a teleimmersive framework with the visual-hull-based multi-camera system which reconstructs 3D mesh of the user in collaborative environment with terrain rendering. The researchers explored the data transmission and simple interactions with the environment (e.g., moving across terrain and adding simple objects). The accuracy shortcoming of the visual-hull approach was overcome by Hasenfratz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hasenfratz J, Lapierre M, Sillion F (2004) A real-time system for full-body interaction with virtual worlds. In: Proceedings of Eurographics symposium on virtual environments, The Eurographics Association, pp 147–156" href="/article/10.1007/s10055-012-0217-2#ref-CR18" id="ref-link-section-d120266e415">2004</a>) who presented a voxel-based virtual reality system that worked at interactive frame rate; however, the accuracy of the reconstruction was low with inability to reconstruct clothing and facial features.</p><p>In addition to the image-based methods, several time-of-flight (TOF) cameras have been developed recently and applied for the real-scene reconstruction for the purpose of interaction and rendering (Kolb et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of Eurographics 2009—state of the art reports, pp 119–134" href="/article/10.1007/s10055-012-0217-2#ref-CR26" id="ref-link-section-d120266e421">2009</a>). TOF cameras obtain the distance of points in the scene by measuring the time the light travels from the infrared (IR) emitter and back to the camera sensor after reflecting from a 3D surface.</p><p>The 3D reconstruction with cameras can also be aided by projecting visible or invisible (e.g., infrared) structured light patterns. The captured image of a known pattern can be used to extract unknown surface geometry or to assist with the pixel correspondence matching for stereo. Waschbüsch et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Waschbüsch M, Würmlin S, Cotting D, Sadlo F, Gross MH (2005) Scalable 3D video of dynamic scenes. Vis Comput 21(8-10):629–638" href="/article/10.1007/s10055-012-0217-2#ref-CR54" id="ref-link-section-d120266e427">2005</a>) developed a 3D video acquisition framework where depth information was generated based on stereo reconstruction assisted by sequentially projected black and white patterns. The system consisted of three synchronized camera clusters capturing the scene from various points of view. Novel viewpoints of the dynamic scene were rendered off-line using volume splatting.</p><p>In the past year, we have witnessed an explosion of interactive applications using Microsoft Kinect camera (2010). Although the camera was initially designed and intended for use with the Xbox gaming system, it had found its way into the mainstream development for human–computer interaction. The 3D reconstruction with the Kinect camera is based on the projection of a known pattern generated by a laser projector whose deformation on 3D surface is captured by the infrared camera (Khoshelham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Khoshelham K (2011) Accuracy analysis of Kinect depth data. In: Proceedings of ISPRS workshop laser scanning, Calgary" href="/article/10.1007/s10055-012-0217-2#ref-CR24" id="ref-link-section-d120266e434">2011</a>). Similarly, as in the image-based stereo, the disparity calculated from point correspondence between the projector and the camera relates to the unknown depth value of each point. As opposed to an image-based stereo reconstruction, the active vision approach is more robust to illumination and other environmental changes (with exception to sunlight). The real-time (30 FPS) 3D reconstruction of the Kinect camera with its relatively high accuracy (in the order of 1–4 cm, depending on the range) offers an attractive acquisition system for immersive 3D interaction. Combining the Kinect camera with passive vision-based 3D reconstruction methods, such as visual hull and stereo, can further improve the visual quality of the output. Kuster et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kuster C, Popa T, Zach C, Gotsman C, Gross M (2011) Freecam: A hybrid camera system for interactive free-viewpoint video. In: Proceedings of vision, modeling, and visualization (VMV)" href="/article/10.1007/s10055-012-0217-2#ref-CR34" id="ref-link-section-d120266e437">2011</a>) recently presented FreeCam system which combines two Kinects with three vision cameras to synthesize a novel view of a general scene in relatively high quality with only small artifacts. The work is aimed to generate content for visualization on autostereoscopic 3D displays. Freehand interaction system using the Kinect camera was presented by Benko et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Benko H, Jota R, Wilson A (2012) Miragetable: freehand interaction on a projected augmented reality tabletop. In: CHI, pp 199–208" href="/article/10.1007/s10055-012-0217-2#ref-CR5" id="ref-link-section-d120266e440">2012</a>) where two users were able through gestures and hand-tracking interact with simple virtual objects projected to a curved surface.</p><p>Since the field of view and the range of the Kinect camera are rather limited, several attempts have been made to combine multiple devices in the same scene. Pointing the active light pattern emitters on to the same surface will cause interference and create artifacts in the depth map, such as missing or incorrect depth data. The interference of active sensors could be avoided by precise triggering of the active illumination. Since no hardware synchronization exists for the Kinect camera, several workarounds have been proposed to prevent the interferences between the projected patterns for a small number of cameras, either through video post-processing (Maimone and Fuchs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Maimone A, Fuchs H (2011) Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras. In: Proceedings of the 2011 10th IEEE international symposium on mixed and augmented reality, IEEE Computer Society, Washington, ISMAR ’11, pp 137–146" href="/article/10.1007/s10055-012-0217-2#ref-CR38" id="ref-link-section-d120266e446">2011</a>) or electro-mechanical solutions (Schröder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Schröder Y, Scholz A, Berger K, Ruhl K, Guthe S, Magnor M (2011) Multiple kinect studies. Tech Rep 09–15, ICG" href="/article/10.1007/s10055-012-0217-2#ref-CR47" id="ref-link-section-d120266e449">2011</a>; Maimone and Fuchs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Maimone A, Fuchs H (2012) Reducing interference between multiple structured light depth sensors using motion. In: Virtual reality workshops (VR), IEEE, pp 51–54" href="/article/10.1007/s10055-012-0217-2#ref-CR39" id="ref-link-section-d120266e452">2012</a>). In combining the output of multiple Kinect cameras or combining the Kinect with vision cameras, the temporal synchronization between the sensors is still an outstanding issue.</p><p>The majority of the above-mentioned real-time 3D capturing systems have only been demonstrated in relatively simple virtual environments with the focus on information visualization and evaluation of the overall telepresence experience. In this paper, we present our 3D teleimmersion framework and its application in more complex scenarios, such as remote interaction and teaching of dance and physical activity, geoscientific and medical 3D data interaction, archaeological data exploration, and various applications of telemedicine. The presented experimental applications involved users from the represented professional and scientific fields to investigate the research questions asked in context of TI technology.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Interaction in teleimmersive environments</h2><div class="c-article-section__content" id="Sec3-content"><p>The aim of the 3D teleimmersion is to enhance the experience of geographically distributed interaction in a virtual environment by facilitating digital embodiment of the users through 3D capturing technology. The overall idea is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig2">2</a>, where cameras at each location are used to digitize the local user in real time. The 3D data, either in a form of 3D video stream, point cloud, or mesh, are transmitted to the remote locations and combined with application data for rendering and interaction. The remote presence in these environments can be established through various modes depending on the application requirements as described in the following paragraphs.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Diagram of the 3D teleimmersion framework. User is captured by a set of 3D cameras. Data are then processed using a computing cluster. The model is transmitted over the network for display at another location. 3D video streams and user data are combined for rendering and interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In the <i>First-Person Mode</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig3">3</a>a), typically found in videoconferencing scenario, users are positioned facing each other, allowing them to establish the eye contact and observe each other’s gestures. The users are unable to see their own avatar as they interact directly with the screen. By adding data exploration capabilities, this mode allows each user to freely navigate through the space while the location of the remote users is indicated by their 3D avatar rendered at the location of their viewpoint. Since the 3D cameras (calibrated with the display) are capturing the real geometry of the user’s movement, each user can point at 3D objects as seen on the 3D display, while the remote users would see their avatar pointing at the same position in space. The experience of interaction is thus similar to real-life interaction of people who are colocated and interacting with real objects or devices.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Various interaction modes for geographically distributed 3D teleimmersion as observed by user 1 (<i>top row</i>) and user 2 (<i>bottom row</i>): <b>a</b>
                        <i>First-Person Mode</i>: user interacts with the environment in the first-person perspective, while the remote users see his/her 3D avatar at the corresponding position, <b>b</b>
                        <i>Third-Person Mode</i>: user observes the scene from a fixed viewpoint relative to his avatar to interact with the data and other users, and <b>c</b>
                        <i>Mirror Mode</i>: user observes a mirrored image of his and remote avatars which can be applied for instructing physical activities</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>In the <i>Third-Person Mode</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig3">3</a>b), the user observes the scene from a third-person view (usually fixed) while interacting with the environment. In this case, it is not possible to preserve the direct connection between the 3D geometry of the real space (i.e., user pointing at objects perceived on the display) and the virtual environment (i.e., avatar pointing at objects). This mode can be utilized when observing the virtual environment on a 2D display where there is a disconnect between the physical space and displayed 3D data. The rendering of the avatar thus provides spatial cues for pointing and interacting with objects in the scene.</p><p>The <i>Mirror Mode</i> is the third mode we propose for the remote interaction. In this mode, the screen represents a virtual mirror with the avatar mirroring user’s movements in the physical space. For remote interaction, the avatar of the remote user is projected in such a way as if both users were sharing the same physical space while their movements are also mirrored. This mode is applicable for instructing and teaching movement patterns, such as in rehabilitation, dance, or fitness training.</p><p>In addition to the aforementioned interaction modes where each user has their own avatars occupying the virtual space, users can adopt another person’s viewpoint. Such capability is useful in educational and training scenarios where multiple users may follow an instructor who wishes to point out various features in the observed data.</p><p>One of the advantages of the TI technology is in seamless integration of people and data in the same virtual space. Depending on the application, users may require various levels of immersion. Simple interactions, such as remote teaching of physical activities can be displayed on a 2D computer monitor or TV screen, usually rendered from a fixed point of view. When interaction includes 3D data exploration, users can benefit from a non-immersive or immersive 3D display with marker-based or markerless head tracking. Through the head tracking, the user can control the viewpoint by head movements while being able to perceive the proper geometric relationship between the geometry of the virtual space and rendered stereo images. For example, when a user points at a particular location on the surface of a 3D object, remote users will see the rendered hand of his/her avatar pointing at the same section of the object.</p><p>Another important issue to consider in the real-time interaction over the network is the latencies and jitter in the transmission of video and tracking data (Delaney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Delaney D, Ward T, McLoone S (2006) On consistency and network latency in distributed interactive applications: a survey–part I. Presence Teleoper Virtual Environ 15:218–234" href="/article/10.1007/s10055-012-0217-2#ref-CR9" id="ref-link-section-d120266e562">2006</a>). The latency, which is described as the lag between the time instances when data are sent and received on the other end, should in general be kept bellow 300–500 ms. Different strategies can be employed to compensate for longer latencies as long as the variability is small. One example includes coordinated interaction between the remote users where at each time instance one of the users is the leader while the others have a role of a follower. On the other hand, the network jitter, which refers to the variability of the latencies between the receiving packets, can cause significant disruption in the remote interaction. The network jitter can be influenced through various quality of service mechanisms that re-route the packets in complex networks. More comprehensive review of network delays for remote interaction was performed by Gutwin (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Gutwin C (2001) The effects of network delays on group work in real-time groupware. In: Proceedings of the seventh conference on European conference on computer supported cooperative work, Kluwer, Norwell, pp 299–318" href="/article/10.1007/s10055-012-0217-2#ref-CR17" id="ref-link-section-d120266e565">2001</a>).</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">3D teleimmersion system architecture</h2><div class="c-article-section__content" id="Sec4-content"><p>During the past several years, we have been incrementally developing the 3D teleimmersive infrastructure (Jung and Bajcsy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Jung S, Bajcsy R (2006) A framework for constructing real-time immersive environments for training physical activities. J Multime'd 1(7):9–17" href="/article/10.1007/s10055-012-0217-2#ref-CR21" id="ref-link-section-d120266e576">2006</a>; Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Kurillo G, Vasudevan R, Lobaton E, Bajcsy R (2008b) A framework for collaborative real-time 3D teleimmersion in a geographically distributed environment. In: Proceedings of 10th IEEE international symposium on multimedia (ISM 2008), Berkeley, pp 111–118" href="/article/10.1007/s10055-012-0217-2#ref-CR29" id="ref-link-section-d120266e579">2008b</a>; Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010a" title="Vasudevan R, Lobaton E, Kurillo G, Bajcsy R, Bernardin T, Hamann B, Nahrstedt K (2010a) A methodology for remote virtual interaction in teleimmersive environments. In: Proceedings of first ACM multimedia systems conference, Scottsdale, pp 281–292" href="/article/10.1007/s10055-012-0217-2#ref-CR51" id="ref-link-section-d120266e582">2010a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Vasudevan R, Zhou Z, Kurillo G, Lobaton E, Bajcsy R, Nahrstedt K (2010b) Real-time stereo-vision system for 3D teleimmersive collaboration. In: Proceedings of IEEE international conference on multimedia &amp; expo (ICME 2010), Singapore, pp 1208–1213" href="/article/10.1007/s10055-012-0217-2#ref-CR52" id="ref-link-section-d120266e585">b</a>; Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584" href="/article/10.1007/s10055-012-0217-2#ref-CR53" id="ref-link-section-d120266e588">2011</a>) which has been successfully demonstrated in several application areas of geographically distributed interaction and complex 3D data manipulation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig4">4</a>). In this section, we briefly describe the different components of our TI framework and issues pertaining to capture, transmission, and rendering. We also provide references to our previously published work where interested readers can find further details on various algorithms and components of our TI framework.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Two users are interacting using the 3D teleimmersion system. The local user is captured by a set of stereo cameras arranged around the screen which displays the remote user’s avatar integrated with a complex archaeological model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec5">Cameras and calibration</h3><p>Our TI framework supports multiple camera clusters to generate image-based stereo data from various viewpoints. The basic unit of the 3D reconstruction is a stereo pair which independently reconstructs the scene in real time. With additional views, one can compensate for occlusions or increase the operating workspace of the capturing system. To achieve the temporal synchronization of the multi-viewpoint image acquisition, we employ hardware triggering. In our work, we have used various models of Point Gray cameras which provide this type of precise synchronization. In case of cameras with no hardware triggering capabilities, the output from different camera clusters can be synchronized with lower precision using software triggering through various network protocols (e.g., for NTP synchronization, see Litos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Litos G, Zabulis X, Triantafyllidis G (2006) Synchronous image acquisition based on network synchronization. In: Computer vision and pattern recognition workshop 2006. CVPRW ’06. Conference on, pp 167–173" href="/article/10.1007/s10055-012-0217-2#ref-CR37" id="ref-link-section-d120266e620">2006</a>).</p><p>Another important issue when dealing with multiview 3D reconstruction is the geometric calibration of the cameras. The calibration involves determining the intrinsic parameters such as focal length, image center and lens distortion, and the extrinsic parameters which include the relative rotation and position of the cameras in space. Despite several methods for extrinsic camera calibration available (e.g., Svoboda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Svoboda T, Martinec D, Pajdla T (2005) A convenient multicamera self-calibration for virtual environments. Presence 14(4):407–422" href="/article/10.1007/s10055-012-0217-2#ref-CR50" id="ref-link-section-d120266e626">2005</a>; Cheng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Cheng X, Davis J, Slusallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR 2000)" href="/article/10.1007/s10055-012-0217-2#ref-CR7" id="ref-link-section-d120266e629">2000</a>; Ihrke et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ihrke I, Ahrenberg L, M Magnor M (2004) External camera calibration for synchronized multi-video systems. In: Proceedings of 12th international conference on computer graphics, visualization and computer vision 2004, vol 12, Plzen, pp 537–544" href="/article/10.1007/s10055-012-0217-2#ref-CR20" id="ref-link-section-d120266e632">2004</a>), many of the methods are slow when applied to large numbers of cameras or they require a high degree of overlap between the camera views or they provide only relative calibration (i.e., up to a scale).</p><p>For the 3D teleimmersion framework, we developed a calibration solution that incorporates intrinsic calibration of stereo (or multiview) camera arrays and their external registration (Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Kurillo G, Li Z, Bajcsy R (2008a) Wide-area external multi-camera calibration using vision graphs and virtual calibration object. In: Proceedings of 2nd ACM/IEEE international conference on distributed smart cameras (ICDSC 08), IEEE, Stanford" href="/article/10.1007/s10055-012-0217-2#ref-CR28" id="ref-link-section-d120266e638">2008a</a>; <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009b" title="Kurillo G, Li Z, Bajcsy R (2009b) Framework for hierarchical calibration of multi-camera systems for teleimmersion. In: Proceedings of IMMERSCOM 2009, Berkeley, CA, pp 1:1–1:6" href="/article/10.1007/s10055-012-0217-2#ref-CR31" id="ref-link-section-d120266e641">2009b</a>). Our method is scalable to multiple partially overlapping cameras and produces the Euclidean reconstruction of the camera pose (preserving metric information). In the first step, we calibrate intrinsic camera parameters using standard checkerboard homography using Zhang’s algorithm for single camera (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Zhang D, Nomura Y, Fujii S (1991) Error analysis and optimization of camera calibration. In: Proceedings of IEEE/RSJ international workshop on intelligent robots and systems (IROS 91), Osaka, pp 292–296" href="/article/10.1007/s10055-012-0217-2#ref-CR62" id="ref-link-section-d120266e644">1991</a>). Since the cameras in each cluster have large overlap, we can simultaneously obtain their geometric relationship. Once each cluster is internally calibrated, we can proceed with the external calibration of clusters to obtain their position and orientation with respect to a reference camera. The external calibration is performed using a calibration object fashioned out of two LEDs positioned at a fixed distance. Finally, we calibrate the display and the tracking system using a reference camera and a checkerboard to determine the spatial relationship of the data with respect to the world coordinate system in the virtual environment (Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009b" title="Kurillo G, Li Z, Bajcsy R (2009b) Framework for hierarchical calibration of multi-camera systems for teleimmersion. In: Proceedings of IMMERSCOM 2009, Berkeley, CA, pp 1:1–1:6" href="/article/10.1007/s10055-012-0217-2#ref-CR31" id="ref-link-section-d120266e647">2009b</a>). Although the external calibration algorithm was developed for multiple stereo pairs, the algorithm can be applied to any type of multi-viewpoint vision-based system.</p><h3 class="c-article__sub-heading" id="Sec6">3D stereo reconstruction</h3><p>In 2005, we have developed a working prototype of the teleimmersion system based on dense-stereo depth maps obtained via trinocular 3D reconstruction (Jung and Bajcsy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Jung S, Bajcsy R (2006) A framework for constructing real-time immersive environments for training physical activities. J Multime'd 1(7):9–17" href="/article/10.1007/s10055-012-0217-2#ref-CR21" id="ref-link-section-d120266e658">2006</a>). The presented approach, however, had two major shortcomings: its slow speed and its unreliability in texture-less regions of the scene. We have addressed these shortcomings by developing a new stereo algorithm which takes advantage of a region-based approach as briefly presented in this section.</p><p>The general idea of our stereo algorithm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig5">5</a>) is to perform accurate and efficient CPU-based stereo computation of the captured scene by employing fast stereo matching through an adaptive meshing scheme. In the first step, the images are aligned through the rectification process which also corrects for the lens distortion effects. In this process, the two images are aligned vertically to provide one dimensional search along the vertical epipolar lines for matching the pixels of the scene. For the reference camera, a conforming triangular mesh is generated based on the variance of the gray-scale regions of the captured image using a bisection algorithm (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig6">6</a>). The algorithm is initialized with a coarse triangulation of the image space, refining to smaller triangles around the regions with higher variance. The steps to obtain the final mesh for a given image can be efficiently stored and reconstructed. Once the mesh is generated on the base image, the nodes are used for the disparity computation where the region around each node is matched with the region in the other image. The corresponding triangle size is used to determine the size of the window for calculating the normalized cross-correlation across each epipolar line. The maximum value is used to determine the disparity. The disparity estimates at each node are further improved through the post-processing step to increase the overall accuracy of the reconstruction. The post-processing includes anisotropic diffusion which smooths disparity in the homogeneous regions of the image while preserving the values around the edges.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Diagram flow of the 3D reconstruction stereo algorithm with intermediate results from point of image capture to transmission of the compressed 3D data with texture over the TCP/IP protocol</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Triangular meshing algorithm in image space builds a hierarchical representation around any point × (<i>left</i>). A non-conforming mesh is generated by combining neighboring areas with similar level of intensity variance (<i>middle</i>). A conforming meshing is obtained after refining non-conforming triangles (<i>right</i>). Depth values determined from the stereo correspondence are assigned to each vertex to encode the information. © 2011 IEEE. Reprinted, with permission, from Vasudevan et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584" href="/article/10.1007/s10055-012-0217-2#ref-CR53" id="ref-link-section-d120266e707">2011</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>For the rendering, the depth data can be combined with the color data and passed to the rendering process over the network. We have achieved the processing rate of 40 FPS on images with the resolution of 320 × 240 pixels or about 20 FPS with the image resolution of 640 × 480 pixels. The algorithm scales evenly over multi-core CPUs since most of the processing on the mesh nodes can be done in parallel. For a typical image sequence in the resolution of 640 × 480 pixels, the total processing time per frame is about 47 ms (measured on Quad Core, i7-860, 2.8 GHz); with 16 ms of that time spent on image meshing, 23 ms on disparity computation, about 4 ms on post-processing, and 5 ms on the frame decompression (Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584" href="/article/10.1007/s10055-012-0217-2#ref-CR53" id="ref-link-section-d120266e722">2011</a>).</p><h3 class="c-article__sub-heading" id="Sec7">Kinect integration</h3><p>Although the triangular meshing algorithm described above was initially developed for fast and efficient stereo correspondence matching, we recently applied the algorithm for encoding the data from the Kinect camera to take advantage of its compression properties. As opposed to the stereo algorithm, where a finer mesh is generated in textured regions of the color image, the meshing here is performed directly on the depth map provided by the Kinect. In regions with small depth variance (e.g., flat regions), the resulting triangulation will be coarser while in the regions with high depth variance (e.g., curved surfaces), the triangulation will be finer (See Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig7">7</a> for comparison between the meshing on stereo and Kinect camera output). Converting depth map to a mesh representation has several advantages. The sparse representation of the depth map is automatically suppressing noise by representing flat surfaces with larger triangles in the final mesh. It can be efficiently post-processed on the acquisition side using various filters executed only on the nodes instead of the entire image. The mesh is also convenient for the rendering as it is readily available to be texture-mapped as compared to the point cloud data generated directly from the depth map. Such representation allows us to build hybrid camera systems and combine the properties of the active vision cameras, such as Kinect, and passive stereo cameras.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Output data of the stereo algorithm (<i>above</i>) and Kinect (<i>below</i>). On stereo cameras, the mesh refinement is based on the variance of <i>color</i> (intensity) resulting in a dense triangulation in textured regions. On the Kinect data, the meshing is performed on the depth image, resulting in a finer mesh only in the areas with high depth variance. The acquired 3D mesh (<i>center</i>) is overlaid with JPEG compressed texture <i>map</i> (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Finally, the mesh representation can be applied for efficient depth data encoding and compression. The output of the Kinect cameras is in general a depth map where each pixel value encodes the actual distance of the point from the camera (in millimeters). To encode the depth map, we first convert it into corresponding disparity. Disparity and depth map are in reciprocal relationship (<i>d</i> = <i>k</i>/<i>x</i>), where <i>d</i> represents the disparity value, <i>x</i> depth value, and <i>k</i> is a constant related to the focal length of the camera and baseline between the cameras (or camera and projector in the case of Kinect). The disparity data corresponding to each node is then transmitted along with the information on the hierarchical mesh construction. For a typical 320 × 240 image with 16-bit depth encoding, the typical package size for the depth map is about 15–20 KB with the encoding time of about 5 ms using the proposed algorithm.</p><h3 class="c-article__sub-heading" id="Sec8">Data transmission</h3><p>The network delays can introduce significant distortion in collaborative experience as discussed earlier. We address the delays twofold, by reducing the size of the 3D video packets and by implementing various network transmission schemes. In our framework, each view is generating an independent, but temporally synchronized, data stream. For compression, we take advantage of the mesh representation and regularity created by the stereo algorithm. Instead of transmitting coordinates of the triangle nodes, the packet includes information on how the mesh has been constructed for a particular frame, that is, which triangles are active and which are not. With this approach, we can achieve about 70% reduction in the frame size as compared to encoding the triangle coordinates. To encode the depth information, 16-bit subpixel disparity value is sent for each node, while the actual depth value of nodes is reconstructed at the receiving side. To encode the color information, we can either send color values for each node and interpolate the texture on the receiving end (Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584" href="/article/10.1007/s10055-012-0217-2#ref-CR53" id="ref-link-section-d120266e805">2011</a>) or send the texture in a compressed form. If the 3D frame is encoded without the texture, the number of triangles directly affects the visual quality of the data. More details on how this compression technique affects the subjective perception of the 3D video streams are described in (Wu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wu W, Arefin A, Kurillo G, Agarwal P, Nahrstedt K, Bajcsy R (2011) Color-plus-depth level-of-details in 3D teleimmersive video—a psychophysical approach. In: Proceedings of ACM multimedia" href="/article/10.1007/s10055-012-0217-2#ref-CR56" id="ref-link-section-d120266e808">2011</a>).</p><p>The quality of the rendering can be improved with the high-resolution texture mapping. For the compression, we have compared JPEG compression algorithm (libjpeg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="libjpeg (2012) libjpeg-turbo. &#xA;                    http://libjpeg-turbo.virtualgl.org&#xA;                    &#xA;                  &#xA;                " href="/article/10.1007/s10055-012-0217-2#ref-CR36" id="ref-link-section-d120266e814">2012</a>) where each frame is compressed independently and custom developed Border-Descriptor Inter-Frame Compression (BIFC) scheme which employs inter-frame motion estimation, which is in principal similar to MPEG encoding (Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Vasudevan R, Zhou Z, Kurillo G, Lobaton E, Bajcsy R, Nahrstedt K (2010b) Real-time stereo-vision system for 3D teleimmersive collaboration. In: Proceedings of IEEE international conference on multimedia &amp; expo (ICME 2010), Singapore, pp 1208–1213" href="/article/10.1007/s10055-012-0217-2#ref-CR52" id="ref-link-section-d120266e817">2010b</a>). Although the two encoding schemes are similar in quality and compression ratio, the BIFC has an advantage in faster decompression which is important for the rendering process where multiple streams received over the network have to be decoded. Typical frame for QVGA resolution mesh with VGA resolution texture is around 40–50 KB in size, while the VGA resolution mesh with texture is typically around 100 KB in size.</p><p>With our collaborators from University of Illinois, we have also examined various network protocols for the transmission of the TI data streams on long distances, such as between Berkeley, CA, and Urbana-Champaign, IL. In the two-way experiments, we observed the throughput of about 75.9 Mbps with average latencies of 0.2 s (and maximal delays of 0.5–1.0 s) (Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Yang Z, Nahrstedt K, Cui Y, Yu B, Liang J, Jung SH, Bajcsy R (2005) TEEVE: The next generation architecture for tele-immersive environment. In: Seventh IEEE international symposium on multimedia (ISM 2005). IEEE Computer Society, Irvine, pp 112–119" href="/article/10.1007/s10055-012-0217-2#ref-CR59" id="ref-link-section-d120266e823">2005</a>). For a three-way connection (Berkeley, CA—Urbana-Champaign, IL—Orlando, FL, USA), the observed throughput was about 40 Mbps with maximal latency of 0.6 s. Detailed analysis of the network delays and various network configurations can be found in (Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajscy R (2007) ViewCast: view dissemination and management for multi-party 3D tele-immersive environments. In: Proceedings of ACM multimedia, Augsburg, pp 882–891" href="/article/10.1007/s10055-012-0217-2#ref-CR61" id="ref-link-section-d120266e826">2007</a>) and (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajcsy R (2010) Enabling multi-party 3D tele-immersive environments with viewcast. ACM transactions on multimedia computing, communications, and applications (TOMCCAP) 6:1–30" href="/article/10.1007/s10055-012-0217-2#ref-CR60" id="ref-link-section-d120266e829">2010</a>).</p><h3 class="c-article__sub-heading" id="Sec9">Rendering and collaborative framework</h3><p>On the receiving end of the teleimmersion network, each 3D video stream is decompressed and rendered in the virtual environment as a textured 3D mesh. We have implemented OpenGL-based rendering using vertex shaders which can also combine several different views and perform blending of the pixel contributions to generate more visually pleasing images (Vasudevan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584" href="/article/10.1007/s10055-012-0217-2#ref-CR53" id="ref-link-section-d120266e840">2011</a>). We have implemented and tested our rendering with OpenGL-based open source Vrui VR Toolkit, developed by Kreylos (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kreylos O (2008) Environment-independent VR development. In: Bebis G, et al. (eds) Advances in visual computing, lecture notes in computer science. Springer, Berlin, pp 901–912" href="/article/10.1007/s10055-012-0217-2#ref-CR27" id="ref-link-section-d120266e843">2008</a>) at University of California, Davis. The Vrui Tookit provides abstraction of input devices and display technologies, allowing developed applications to scale from laptop computers to large-scale immersive 3D display systems, such as life-size display walls and CAVE systems, which can be found in many scientific and engineering communities. Various virtual tools can be implemented and used in Vrui-based applications, such as tools for navigation, measurements, annotation, object dragging, and interaction. The virtual tools can be dynamically linked to the various physical input devices supported by the Vrui environment.</p><p>The collaborative extension of Vrui allows linking two or more spatially distributed virtual environments. The clients in the network are connected via three different data streams. The collaboration data stream transmits location of input devices and virtual cameras to all the other clients. The conversation data stream provides communication via audio, video, or 3D videoconferencing. Finally, the application data stream can be customized to update application states between the server and the remote clients (e.g., transmitting object location). In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig8">8,</a> we present the implementation of a simple distributed scene graph to manage synchronization of the scene across distributed clients. The scene graph consists of hierarchically organized, inter-connected nodes with parametrized spatial representation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The collaborative framework for teleimmersive interaction. Connection between server and clients is established through three data streams: collaborative stream for navigation, application stream for scene updates, and conversation stream for audio and (3D) video communication</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The scene graph at this point supports various low-level nodes, such as (a) general node implementing the relationships within the scene graph, (b) geometric node representing the drawable geometries (e.g., triangle mesh, points, polygons, lines), (c) transformation node defining the geometric relationship between connected nodes, and (d) grid node used for the representation of environmental surfaces. On the higher level, we have implemented several data nodes with additional functionality available through a graphic user interface and interactive tools: (a) Wavefront 3D objects, (b) MeshLab layer files, and (c) shapefiles with database support.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Technology demonstrations</h2><div class="c-article-section__content" id="Sec10-content"><p>During the period of 2005–2011, we have demonstrated our teleimmersion technology in several application areas. Initially, we have experimented with distributed dancers in collaboration with University of Illinois at Urbana-Champaign (Nahrstedt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Nahrstedt K, Bajcsy R, Wymore L, Kurillo G, Mezur K, Sheppard R, Yang Z, Wu W (2007) Symbiosis of tele-immersive environments with creative choreography. In: ACM workshop on supporting creative acts beyond dissemination, associated with 6th ACM creativity and cognition conference, Washington" href="/article/10.1007/s10055-012-0217-2#ref-CR42" id="ref-link-section-d120266e881">2007</a>; Yang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajcsy R (2010) Enabling multi-party 3D tele-immersive environments with viewcast. ACM transactions on multimedia computing, communications, and applications (TOMCCAP) 6:1–30" href="/article/10.1007/s10055-012-0217-2#ref-CR60" id="ref-link-section-d120266e884">2010</a>). Our dance collaborators have challenged the integration of pre-recorded dancers and live performance of remote dancers. In the real-time dance interaction, one of the dancers took over the role of the leader, while other dancers were followers. With this approach, the dancers were able to adapt to various latencies in the network. With our TI system, we have successfully performed several experiments and live performances across the country (between Berkeley, CA, and Urbana-Champaign, IL, USA). Furthermore, we have explored how these tools can be applied to digital choreography across geographical distances (Sheppard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Sheppard R, Wu W, Yang Z, Nahrstedt K, Wymore L, Kurillo G, Bajcsy R, Mezur K (2007) New digital options in geographically distributed dance collaborations with TEEVE: tele-immersive environments for everybody. In: Proceedings of the 15th international conference on multimedia, ACM, New York, MULTIMEDIA ’07, pp 1085–1086" href="/article/10.1007/s10055-012-0217-2#ref-CR49" id="ref-link-section-d120266e887">2007</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig9">9</a> shows two dancers using the teleimmersion technology to interact in the virtual space. For interaction, the dancers use visual cues from their mirrored image to perform various moves in temporal and spatial synchrony.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Two distributed dancers are captured by a set of four stereo cameras arranged in two layers and projected into the virtual space. The dancers interact with each other in the mirror mode where they see their mirrored image integrated with their remote partner’s 3D avatar</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Next, we have investigated the value of teaching and learning of physical exercises with this technology as oppose to a simple 2D video (Bailenson et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bailenson JN, Patel K, Nielsen A, Bajcsy R, Jung S, Kurillo G (2008) The effect of interactivity on learning physical actions in virtual reality. Media Psychol 11:354–376" href="/article/10.1007/s10055-012-0217-2#ref-CR1" id="ref-link-section-d120266e914">2008</a>). We have compared learning of Tai Chi movements in two separate experiments in collaboration with Stanford University. The participants of the study were presented with either the 3D immersive feedback that included their 3D avatar and the avatar of a Tai Chi instructor, or a simple 2D video of the instructor. The results revealed that people were able to recognize more clearly and learn movements faster with the 3D immersive feedback of themselves and the teacher from various viewpoints as compared to the regular video of the instructor. With similar goal in mind (i.e., instructing the physical activity), the TI framework was also applied in evaluating remote coaching of the wheelchair basketball athletes for rehabilitation activities (Bajcsy et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Bajcsy P, McHenry K, Na HJ, Malik R, Spencer A, Lee SK, Kooper R, Frogley M (2009) Immersive environments for rehabilitation activities. In: Proceedings of the 17th ACM international conference on multimedia, ACM, New York, MM ’09, pp 829–832" href="/article/10.1007/s10055-012-0217-2#ref-CR2" id="ref-link-section-d120266e917">2009</a>) and teleimmersive gaming (Wu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Wu W, Arefin MA, Huang Z, Agarwal P, Shi S, Rivas R, Nahrstedt K (2010) “I’m the Jedi!”—a case study of user experience in 3D tele-immersive gaming. In: ISM, pp 220–227" href="/article/10.1007/s10055-012-0217-2#ref-CR55" id="ref-link-section-d120266e920">2010</a>).</p><p>In collaboration with Institute for Data Analysis and Visualization (IDAV) of University of California at Davis, we have developed software tools which incorporate our 3D teleimmersion technology and virtual reality environment for real-time interaction and collaboration. In 2007, we have performed a set of experiments with geographically distributed geoscientists between Berkeley, CA, and Davis, CA, USA (distance ∼70 miles) who were able to in real time interact with volumetric data while seeing each other in the 3D space using a CAVE-like environment (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig10">10</a>). The visualization was implemented using Vrui. From the geoscientific experiments, we have studied how the non-technical users utilize this new technology and take advantage of their remote presence in the virtual environment. We have also observed the importance of visualization on a 3D display as opposed to a 2D display. The geoscientists were able to visually explore the dataset (i.e., sample tomographic model of the earth crust) while interpreting different portions of the model. Using the annotation tools, they were able to in real time discuss possible dynamics (i.e., mantle flow) that is being produced by the geological structures. For example, one of the geoscientists reported: “It was also helpful to be able to ask (remote collaborator) about a feature that I couldn’t identify—much easier to point at it (and draw a little circle around it) than to say ’what is that blue thing in the northern end of the model, east of the slab?’” Within the same collaboration we have jointly explored also the interaction with medical data (e.g. MRI, 3D X-RAY) intended for remote collaboration, training and education of physicians (Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009a" title="Kurillo G, Bajcsy R, Kreylos O, Rodriguez R (2009a) Teleimmersive environment for remote medical collaboration. In: Westwood J, et al. (eds) Medicine meets virtual reality 17, IOS press, Ohmsha, pp 148–150" href="/article/10.1007/s10055-012-0217-2#ref-CR30" id="ref-link-section-d120266e929">2009a</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Two users are interacting with a tomographic model of the Earth interior in real time using the 3D teleimmersion technology and immersive visualization. The system is based on three key principles: interactive visualization, individual exploration, and collaborative analysis</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Since 2010, we have been in collaboration with University of California at Merced developing a collaborative virtual environment for real-time interaction with 3D objects in digital archaeology which also includes our TI technology (Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010a" title="Kurillo G, Forte M, Bajcsy R (2010a) Teleimmersive 3D collaborative environment for cyberarchaeology. In: IEEE/CVPR workshop, applications of computer vision in archaeology (ACVA 2010), San Francisco" href="/article/10.1007/s10055-012-0217-2#ref-CR32" id="ref-link-section-d120266e953">2010a</a>; Forte and Kurillo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Forte M, Kurillo G (2010) Cyberarchaeology—experimenting with teleimmersive archaeology. In: Proceedings of 16th international conference on virtual systems and multimedia (VSMM 2010), Seoul" href="/article/10.1007/s10055-012-0217-2#ref-CR13" id="ref-link-section-d120266e956">2010</a>; Forte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Forte M, Kurillo G, Matlock T (2010) Teleimmersive archaeology: simulation and cognitive impact. In: Ioannides M, Fellner AG D, Hadjimitsis D (eds) EuroMed 2010—digital heritage, Lemesos" href="/article/10.1007/s10055-012-0217-2#ref-CR14" id="ref-link-section-d120266e959">2010</a>). The software framework named TeleArch allows for virtual participation of geographically distributed archaeologists who can examine, measure, and interact with laser scanned 3D artifacts, dig virtual sites, visualize maps, and other data. The application features a distributed scene graph built on top of the collaborative Vrui framework as described in the previous section. We have performed a series of local and remote experiments between Berkeley and Merced, CA, USA (distance ∼125 miles), such as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-012-0217-2#Fig11">11</a>. Users at the two sites were, for example, able to collaboratively explore the reconstruction of a temple by manipulating and positioning in real time various scanned sections of ornaments to the temple model. In another experiment, we have explored interaction with geographical information system (GIS) data with 3D models through immersive visualization. Research questions that we are investigating with respect to the 3D data interaction for archaeology include the following: How do people interact with virtual characters and virtual humans, and how does this affect learning in a virtual environment? Is attention sustained and memory more robust for information about virtual historic objects (e.g., function of object and location of object) when virtual characters point at objects while they describe the objects? How will users as avatars grasp and manipulate virtual objects, and what are the cognitive benefits of this type of interaction? (Forte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Forte M, Kurillo G, Matlock T (2010) Teleimmersive archaeology: simulation and cognitive impact. In: Ioannides M, Fellner AG D, Hadjimitsis D (eds) EuroMed 2010—digital heritage, Lemesos" href="/article/10.1007/s10055-012-0217-2#ref-CR14" id="ref-link-section-d120266e965">2010</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-012-0217-2/MediaObjects/10055_2012_217_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Collaborative 3D puzzle featuring a reconstructed model of a temple from Mayan city of Copan. Two remote users captured by a set of cameras are interacting in the collaborative virtual space through 3D display</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-012-0217-2/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>During the last two years, we have been exploring how the teleimmersion framework can be applied in the remote health care delivery. Examples of envisioned telemedicine and telerehabilitation applications include remote patient diagnosis and rehabilitation therapy programs, where a care provider and the patient are dislocated. The remote delivery of health care is particularly applicable for rural population lacking access to specialist care, but also could be extended to home environments, in particular to the aging population. The most important factors in the teleimmersion for health care are the quality of the texture (e.g., in dermatological diagnosis), accuracy of geometry acquisition (e.g., in biomechanical measurements), and the visual fidelity (e.g., in patient-to-doctor and patient-to-therapist live interactions).</p><p>Using either stereo or Kinect cameras, the geometry and appearance of the body can be captured in real time. By applying the pose tracking algorithm (Obdrzalek et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Obdrzalek S, Kurilo G, Han J, Abresch T, Bajcsy R (2012) Real-time human pose detection and tracking for tele-rehabilitation in virtual reality. In: Proceedings of the 19th medicine meets virtual reality conference (MMVR), Newport Beach" href="/article/10.1007/s10055-012-0217-2#ref-CR44" id="ref-link-section-d120266e993">2012</a>), one can obtain the configuration of the joints at any time instance without using additional markers. The extracted skeleton can be used to augment the visual feedback provided to the patient (e.g., by showing color-coded target range of motion for a particular joint). The real-time 3D avatars of the patient and therapist can be positioned side by side in the virtual environment (see <i>mirror mode</i> in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-012-0217-2#Sec3">3</a>) to perform various training exercises. Although the fidelity of synthetic avatars has progressed significantly in the recent years, creating natural animation still requires use of a motion capture system which is usually prohibitive for live interaction with a human therapist, preferred by many patients.</p><p>In our pilot work (Kurillo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010b" title="Kurillo G, Koritnik T, Bajd T, Bajcsy R (2010b) Real-time 3D avatars for tele-rehabilitation in virtual reality. In: Westwood J, Westwood S, et al. (eds) Proceedings of 18th medicine meets virtual reality (MMVR) conference., IOS press, pp 290–296" href="/article/10.1007/s10055-012-0217-2#ref-CR33" id="ref-link-section-d120266e1005">2010b</a>), where a stepping-in-place task was used to evaluate movement patterns of healthy individuals from a stereo camera, we have shown the feasibility of the telepresence system for use in rehabilitation. We compared the tracking results of 3D video-only with 3D video enhanced by virtual targets and concluded that better spatial and temporal adaptation was achieved when additional tracking targets were displayed. Our current research efforts in this area are focusing on telerehabilitation of the upper extremity, as a part of our ongoing collaboration with University of California at Davis Medical Center (UCDMC). Our aim is to develop remotely administered accurate and reliable assessment of upper extremity function, which is critical in many activities of daily living. In this application, a remote (or pre-recorded therapist) can instruct the patient to perform a movement protocol through the teleimmersion technology driven by the Kinect camera. From acquired 3D data, the kinematics of the movement is extracted and analyzed to obtain the reachable workspace. The therapist can view the acquired data in real time. In our future work, we will superimpose the results from previous sessions as feedback to the patient to try to improve the performance or as feedback to the therapist to focus the therapy in specific part of the workspace.</p><p>In telehealth care applications, several research questions arise: How does the therapy with teleimmersion compare with traditional in-person therapy? Is the therapy more successful when patient is presented with the real-time avatar of the remote therapist as compared to a computer generated animation? How should the feedback on the task performance be presented to the patient to maximize performance and minimize cognitive load? What are the privacy considerations when a remote therapist coaches several patients?</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Discussion and conclusions</h2><div class="c-article-section__content" id="Sec11-content"><p>We believe that the teleimmersion technology has wide applicability as we have explored to some extent in our past research work. Through the application of our technology in various fields, we have realized that one form of the TI system cannot fit all the user needs. We have identified several commonalities and differences pertaining to particular scientific and engineering communities. Based on the initial evaluations, we can develop tools and adapt the framework to a particular discipline. For example, in virtual data exploration, the emphasis is on the 3D (natural) interaction and visualization. The teleimmersive system should therefore include a 3D display, head tracking, and 6-DOF input device, while using the <i>First-Person Mode</i> as the remote interaction model. On the other hand, the telehealth applications require simplified visualization with focus on the visual quality of the captured individuals and the privacy concerns related to the transmission of the data. The 3D visualization and unrestricted navigation are not as important and can actually be limiting with regard to the visual perception in some patients. In such applications, the accuracy of the captured raw data may also play an important role if the data are being used to perform measurements (e.g., extraction of skeleton from 3D video data for biomechanical analysis).</p><p>Although the work presented in this paper was primarily build upon stereo-based methods, we believe the presented findings are relevant for future developers of 3D teleimmersion and apply across various 3D video capturing technologies. In light of wide adaptation of Microsoft Kinect camera, we believe such systems have significant potential to be used for 3D teleimmersion in geographically distributed and colocated scenarios as already demonstrated by several other researchers (e.g., Maimone and Fuchs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Maimone A, Fuchs H (2011) Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras. In: Proceedings of the 2011 10th IEEE international symposium on mixed and augmented reality, IEEE Computer Society, Washington, ISMAR ’11, pp 137–146" href="/article/10.1007/s10055-012-0217-2#ref-CR38" id="ref-link-section-d120266e1025">2011</a>; Kuster et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Kuster C, Popa T, Zach C, Gotsman C, Gross M (2011) Freecam: A hybrid camera system for interactive free-viewpoint video. In: Proceedings of vision, modeling, and visualization (VMV)" href="/article/10.1007/s10055-012-0217-2#ref-CR34" id="ref-link-section-d120266e1028">2011</a>; Benko et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Benko H, Jota R, Wilson A (2012) Miragetable: freehand interaction on a projected augmented reality tabletop. In: CHI, pp 199–208" href="/article/10.1007/s10055-012-0217-2#ref-CR5" id="ref-link-section-d120266e1031">2012</a>). The RGB+D camera provides a robust and low-cost system with sufficient accuracy for indoor 3D acquisition. Some of the outstanding issues remain with respect to multi-sensor temporal synchronization, infrared light interference, and automatic calibration. To improve the visual fidelity of the data, the sensor could be paired with a high-resolution color camera. For the transmission of RGB+D data, more efficient compression techniques are needed to provide lossless compression of the depth data or lossy compression with reduced visual artifacts common in 3D video codecs (e.g., De Silva et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="De Silva DVSX, Fernando WAC, Kodikaraarachchi H, Worrall ST, Kondoz AM (2010) A depth map post-processing technique for 3D-TV systems based on compression artifact analysis. IEEE J Sel Top Signal Process November 2011" href="/article/10.1007/s10055-012-0217-2#ref-CR10" id="ref-link-section-d120266e1034">2010</a>).</p><p>In its essence, the teleimmersion can provide face-to-face experience in applications where people are geographically distributed and need to collaborate and interact with each other and/or multi-dimensional data. Such applications can be found in remote medical consultation, analysis of medical imaging, remote training of technicians, teaching of physical activities, collaboration in physical science, astronomy, architecture, education, digital humanities, and others. The broader goal of this technology is to transform the present-day 2D audio-visual telepresence model (e.g., videoconferencing) to a 3D teleimmersive multimodal collaborative model where users are virtually co-present allowing them to interact with each other and the data more intuitively.</p><p>In addition to the geographically distributed collaboration, 3D teleimmersive technology can be applied in local scenarios where one or more colocated users can interact with the virtual environment through their 3D embodiment provided by the cameras. The data from the cameras can be used to perform body tracking, extract head orientation, analyze gestures for human–computer interaction, or simply to visualize the users as a form of a virtual mirror. Such scenarios include visual feedback-based training of physical activities (e.g., rehabilitation), gaming and entertainment, validation of the design ergonomics through a third-person virtual walk-through (e.g., in architecture) and others. The primary focus in such applications should be in the accuracy of the geometry acquisition and the quality of the visual appearance (i.e., textures).</p><p>One of the outstanding challenges for this technology is to evaluate how it performs side by side with the existing (traditional) tools used in various fields of application. Such validation requires development of metric to quantify the user experience (QoE) with respect to provided quality of service (QoS). The QoE includes cognitive perceptions, such as easiness of use, perceived usefulness, sense of immersion, and enjoyment, while the QoS comprises of technical aspects of technology, such as interactivity, video quality, video frame rate, and end-to-end delays of the networking. Wu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wu W, Arefin A, Rivas R, Nahrstedt K, Sheppard R, Yang Z (2009) Quality of experience in distributed interactive multimedia environments: toward a theoretical framework. In: Proceedings of ACM multimedia, Beijing, pp 481-490" href="/article/10.1007/s10055-012-0217-2#ref-CR58" id="ref-link-section-d120266e1045">2009</a>) presented a theoretical framework to identify the mapping between the two concepts. Although a set of limited experiments can provide some insight, deployment of full-scale applications with 3D teleimmersion is needed to provide users with sufficiently advanced software tools to perform the same activities as they would with the traditional methods.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JN. Bailenson, K. Patel, A. Nielsen, R. Bajcsy, S. Jung, G. Kurillo, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bailenson JN, Patel K, Nielsen A, Bajcsy R, Jung S, Kurillo G (2008) The effect of interactivity on learning p" /><p class="c-article-references__text" id="ref-CR1">Bailenson JN, Patel K, Nielsen A, Bajcsy R, Jung S, Kurillo G (2008) The effect of interactivity on learning physical actions in virtual reality. Media Psychol 11:354–376</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F15213260802285214" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20interactivity%20on%20learning%20physical%20actions%20in%20virtual%20reality&amp;journal=Media%20Psychol&amp;volume=11&amp;pages=354-376&amp;publication_year=2008&amp;author=Bailenson%2CJN&amp;author=Patel%2CK&amp;author=Nielsen%2CA&amp;author=Bajcsy%2CR&amp;author=Jung%2CS&amp;author=Kurillo%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bajcsy P, McHenry K, Na HJ, Malik R, Spencer A, Lee SK, Kooper R, Frogley M (2009) Immersive environments for " /><p class="c-article-references__text" id="ref-CR2">Bajcsy P, McHenry K, Na HJ, Malik R, Spencer A, Lee SK, Kooper R, Frogley M (2009) Immersive environments for rehabilitation activities. In: Proceedings of the 17th ACM international conference on multimedia, ACM, New York, MM ’09, pp 829–832</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Baker H, Tanguay D, Sobel I, Gelb D, Gross M, Culbertson W, Malzenbender T (2002) The coliseum immersive telec" /><p class="c-article-references__text" id="ref-CR3">Baker H, Tanguay D, Sobel I, Gelb D, Gross M, Culbertson W, Malzenbender T (2002) The coliseum immersive teleconferencing system. In: Proceedings of international workshop on immersive telepresence, Juan-les-Pins, France</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benford S, Greenhalgh C, Bowers J, Snowdon D, Fahlen LE (1995) User embodiment in collaborative virtual enviro" /><p class="c-article-references__text" id="ref-CR4">Benford S, Greenhalgh C, Bowers J, Snowdon D, Fahlen LE (1995) User embodiment in collaborative virtual environments. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHI ’95), ACM Press/Addison-Wesley Publishing Co. New York, pp 242–249</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Benko H, Jota R, Wilson A (2012) Miragetable: freehand interaction on a projected augmented reality tabletop. " /><p class="c-article-references__text" id="ref-CR5">Benko H, Jota R, Wilson A (2012) Miragetable: freehand interaction on a projected augmented reality tabletop. In: CHI, pp 199–208</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="CadFaster (2012) Cadfaster. http://www.cadfaster.com/index.php/Products&#xA;                " /><p class="c-article-references__text" id="ref-CR6">CadFaster (2012) Cadfaster. <a href="http://www.cadfaster.com/index.php/Products">http://www.cadfaster.com/index.php/Products</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cheng X, Davis J, Slusallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proce" /><p class="c-article-references__text" id="ref-CR7">Cheng X, Davis J, Slusallek P (2000) Wide area camera calibration using virtual calibration objects. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR 2000)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="DeFanti T, Sandin D, Brown M, Pape D, Anstey J, Bogucki M, Dawe G, Johnson A, Huang TS (1999) Technologies for" /><p class="c-article-references__text" id="ref-CR8">DeFanti T, Sandin D, Brown M, Pape D, Anstey J, Bogucki M, Dawe G, Johnson A, Huang TS (1999) Technologies for virtual reality/tele-immersion applications: issues of research in image display and global networking. In: EC/NSF workshop on research frontiers in virtual environments and human-centered computing</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Delaney, T. Ward, S. McLoone, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Delaney D, Ward T, McLoone S (2006) On consistency and network latency in distributed interactive applications" /><p class="c-article-references__text" id="ref-CR9">Delaney D, Ward T, McLoone S (2006) On consistency and network latency in distributed interactive applications: a survey–part I. Presence Teleoper Virtual Environ 15:218–234</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.2006.15.2.218" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20consistency%20and%20network%20latency%20in%20distributed%20interactive%20applications%3A%20a%20survey%E2%80%93part%20I&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=15&amp;pages=218-234&amp;publication_year=2006&amp;author=Delaney%2CD&amp;author=Ward%2CT&amp;author=McLoone%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Silva DVSX, Fernando WAC, Kodikaraarachchi H, Worrall ST, Kondoz AM (2010) A depth map post-processing tech" /><p class="c-article-references__text" id="ref-CR10">De Silva DVSX, Fernando WAC, Kodikaraarachchi H, Worrall ST, Kondoz AM (2010) A depth map post-processing technique for 3D-TV systems based on compression artifact analysis. IEEE J Sel Top Signal Process November 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Doherty-Sneddon, A. Anderson, C. O’Malley, S. Langton, S. Garrod, V. Bruce, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Doherty-Sneddon G, Anderson A, O’Malley C, Langton S, Garrod S, Bruce V (1997) Face-to-face and video-mediated" /><p class="c-article-references__text" id="ref-CR11">Doherty-Sneddon G, Anderson A, O’Malley C, Langton S, Garrod S, Bruce V (1997) Face-to-face and video-mediated communication: a comparison of dialogue structure and task performance. J Exp Psychol Appl 3(2):105–125</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F1076-898X.3.2.105" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Face-to-face%20and%20video-mediated%20communication%3A%20a%20comparison%20of%20dialogue%20structure%20and%20task%20performance&amp;journal=J%20Exp%20Psychol%20Appl&amp;volume=3&amp;issue=2&amp;pages=105-125&amp;publication_year=1997&amp;author=Doherty-Sneddon%2CG&amp;author=Anderson%2CA&amp;author=O%E2%80%99Malley%2CC&amp;author=Langton%2CS&amp;author=Garrod%2CS&amp;author=Bruce%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Eon (2009) Eon Reality: EON Coliseum. http://www.eonreality.com/product_coliseum.html&#xA;                " /><p class="c-article-references__text" id="ref-CR12">Eon (2009) Eon Reality: EON Coliseum. <a href="http://www.eonreality.com/product_coliseum.html">http://www.eonreality.com/product_coliseum.html</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Forte M, Kurillo G (2010) Cyberarchaeology—experimenting with teleimmersive archaeology. In: Proceedings of 16" /><p class="c-article-references__text" id="ref-CR13">Forte M, Kurillo G (2010) Cyberarchaeology—experimenting with teleimmersive archaeology. In: Proceedings of 16th international conference on virtual systems and multimedia (VSMM 2010), Seoul</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Forte M, Kurillo G, Matlock T (2010) Teleimmersive archaeology: simulation and cognitive impact. In: Ioannides" /><p class="c-article-references__text" id="ref-CR14">Forte M, Kurillo G, Matlock T (2010) Teleimmersive archaeology: simulation and cognitive impact. In: Ioannides M, Fellner AG D, Hadjimitsis D (eds) EuroMed 2010—digital heritage, Lemesos</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Fry, G. Smith, " /><meta itemprop="datePublished" content="1975" /><meta itemprop="headline" content="Fry R, Smith G (1975) The effects of feedback and eye contact on performance of a digit-coding task. J Soc Psy" /><p class="c-article-references__text" id="ref-CR15">Fry R, Smith G (1975) The effects of feedback and eye contact on performance of a digit-coding task. J Soc Psychol 96:145–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F00224545.1975.9923275" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effects%20of%20feedback%20and%20eye%20contact%20on%20performance%20of%20a%20digit-coding%20task&amp;journal=J%20Soc%20Psychol&amp;volume=96&amp;pages=145-146&amp;publication_year=1975&amp;author=Fry%2CR&amp;author=Smith%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Gross, S. Würmlin, M. Naef, E. Lamboray, C. Spagno, A. Kunz, E. Koller-Meier, T. Svoboda, LV. Gool, S. Lang, K. Strehlke, AV. Moere, O. Staadt, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Gross M, Würmlin S, Naef M, Lamboray E, Spagno C, Kunz A, Koller-Meier E, Svoboda T, Gool LV, Lang S, Strehlke" /><p class="c-article-references__text" id="ref-CR16">Gross M, Würmlin S, Naef M, Lamboray E, Spagno C, Kunz A, Koller-Meier E, Svoboda T, Gool LV, Lang S, Strehlke K, Moere AV, Staadt O (2003) blue-c: a spatially immersive display and 3D video portal for telepresence. ACM Trans Graph 22(3):819–827</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F882262.882350" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=blue-c%3A%20a%20spatially%20immersive%20display%20and%203D%20video%20portal%20for%20telepresence&amp;journal=ACM%20Trans%20Graph&amp;volume=22&amp;issue=3&amp;pages=819-827&amp;publication_year=2003&amp;author=Gross%2CM&amp;author=W%C3%BCrmlin%2CS&amp;author=Naef%2CM&amp;author=Lamboray%2CE&amp;author=Spagno%2CC&amp;author=Kunz%2CA&amp;author=Koller-Meier%2CE&amp;author=Svoboda%2CT&amp;author=Gool%2CLV&amp;author=Lang%2CS&amp;author=Strehlke%2CK&amp;author=Moere%2CAV&amp;author=Staadt%2CO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gutwin C (2001) The effects of network delays on group work in real-time groupware. In: Proceedings of the sev" /><p class="c-article-references__text" id="ref-CR17">Gutwin C (2001) The effects of network delays on group work in real-time groupware. In: Proceedings of the seventh conference on European conference on computer supported cooperative work, Kluwer, Norwell, pp 299–318</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hasenfratz J, Lapierre M, Sillion F (2004) A real-time system for full-body interaction with virtual worlds. I" /><p class="c-article-references__text" id="ref-CR18">Hasenfratz J, Lapierre M, Sillion F (2004) A real-time system for full-body interaction with virtual worlds. In: Proceedings of Eurographics symposium on virtual environments, The Eurographics Association, pp 147–156</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hypercosm (2009) Hypercosm. http://hypercosm.com&#xA;                " /><p class="c-article-references__text" id="ref-CR19">Hypercosm (2009) Hypercosm. <a href="http://hypercosm.com">http://hypercosm.com</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ihrke I, Ahrenberg L, M Magnor M (2004) External camera calibration for synchronized multi-video systems. In: " /><p class="c-article-references__text" id="ref-CR20">Ihrke I, Ahrenberg L, M Magnor M (2004) External camera calibration for synchronized multi-video systems. In: Proceedings of 12th international conference on computer graphics, visualization and computer vision 2004, vol 12, Plzen, pp 537–544</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Jung, R. Bajcsy, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Jung S, Bajcsy R (2006) A framework for constructing real-time immersive environments for training physical ac" /><p class="c-article-references__text" id="ref-CR21">Jung S, Bajcsy R (2006) A framework for constructing real-time immersive environments for training physical activities. J Multime'd 1(7):9–17</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20framework%20for%20constructing%20real-time%20immersive%20environments%20for%20training%20physical%20activities&amp;journal=J%20Multime%27d&amp;volume=1&amp;issue=7&amp;pages=9-17&amp;publication_year=2006&amp;author=Jung%2CS&amp;author=Bajcsy%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Kalra, N. Magnenat-Thalman, L. Moccozet, G. Sannier, A. Aubel, D. Thalman, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Kalra P, Magnenat-Thalman N, Moccozet L, Sannier G, Aubel A, Thalman D (1998) Real-time animation of realistic" /><p class="c-article-references__text" id="ref-CR22">Kalra P, Magnenat-Thalman N, Moccozet L, Sannier G, Aubel A, Thalman D (1998) Real-time animation of realistic virtual humans. IEEE Comput Graphics Appl 18(25):42–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.708560" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20animation%20of%20realistic%20virtual%20humans&amp;journal=IEEE%20Comput%20Graphics%20Appl&amp;volume=18&amp;issue=25&amp;pages=42-56&amp;publication_year=1998&amp;author=Kalra%2CP&amp;author=Magnenat-Thalman%2CN&amp;author=Moccozet%2CL&amp;author=Sannier%2CG&amp;author=Aubel%2CA&amp;author=Thalman%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kauff P, Schreer O (2002) An immersive 3D video-conferencing system using shared virtual team user environment" /><p class="c-article-references__text" id="ref-CR23">Kauff P, Schreer O (2002) An immersive 3D video-conferencing system using shared virtual team user environments. In: Proceedings of the 4th international conference on collaborative virtual environments, pp 105–112</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Khoshelham K (2011) Accuracy analysis of Kinect depth data. In: Proceedings of ISPRS workshop laser scanning, " /><p class="c-article-references__text" id="ref-CR24">Khoshelham K (2011) Accuracy analysis of Kinect depth data. In: Proceedings of ISPRS workshop laser scanning, Calgary</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knoblauch D, Font P, Kuester F (2010) VirtualizeMe: real-time avatar creation for tele-Immersion environments." /><p class="c-article-references__text" id="ref-CR25">Knoblauch D, Font P, Kuester F (2010) VirtualizeMe: real-time avatar creation for tele-Immersion environments. In: Virtual reality conference (VR), 2010 IEEE, pp 279–280</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of Eurog" /><p class="c-article-references__text" id="ref-CR26">Kolb A, Barth E, Koch R, Larsen R (2009) Time-of-flight sensors in computer graphics. In: Proceedings of Eurographics 2009—state of the art reports, pp 119–134</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kreylos O (2008) Environment-independent VR development. In: Bebis G, et al. (eds) Advances in visual computin" /><p class="c-article-references__text" id="ref-CR27">Kreylos O (2008) Environment-independent VR development. In: Bebis G, et al. (eds) Advances in visual computing, lecture notes in computer science. Springer, Berlin, pp 901–912</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Li Z, Bajcsy R (2008a) Wide-area external multi-camera calibration using vision graphs and virtual " /><p class="c-article-references__text" id="ref-CR28">Kurillo G, Li Z, Bajcsy R (2008a) Wide-area external multi-camera calibration using vision graphs and virtual calibration object. In: Proceedings of 2nd ACM/IEEE international conference on distributed smart cameras (ICDSC 08), IEEE, Stanford</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Vasudevan R, Lobaton E, Bajcsy R (2008b) A framework for collaborative real-time 3D teleimmersion i" /><p class="c-article-references__text" id="ref-CR29">Kurillo G, Vasudevan R, Lobaton E, Bajcsy R (2008b) A framework for collaborative real-time 3D teleimmersion in a geographically distributed environment. In: Proceedings of 10th IEEE international symposium on multimedia (ISM 2008), Berkeley, pp 111–118</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Bajcsy R, Kreylos O, Rodriguez R (2009a) Teleimmersive environment for remote medical collaboration" /><p class="c-article-references__text" id="ref-CR30">Kurillo G, Bajcsy R, Kreylos O, Rodriguez R (2009a) Teleimmersive environment for remote medical collaboration. In: Westwood J, et al. (eds) Medicine meets virtual reality 17, IOS press, Ohmsha, pp 148–150</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Li Z, Bajcsy R (2009b) Framework for hierarchical calibration of multi-camera systems for teleimmer" /><p class="c-article-references__text" id="ref-CR31">Kurillo G, Li Z, Bajcsy R (2009b) Framework for hierarchical calibration of multi-camera systems for teleimmersion. In: Proceedings of IMMERSCOM 2009, Berkeley, CA, pp 1:1–1:6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Forte M, Bajcsy R (2010a) Teleimmersive 3D collaborative environment for cyberarchaeology. In: IEEE" /><p class="c-article-references__text" id="ref-CR32">Kurillo G, Forte M, Bajcsy R (2010a) Teleimmersive 3D collaborative environment for cyberarchaeology. In: IEEE/CVPR workshop, applications of computer vision in archaeology (ACVA 2010), San Francisco</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kurillo G, Koritnik T, Bajd T, Bajcsy R (2010b) Real-time 3D avatars for tele-rehabilitation in virtual realit" /><p class="c-article-references__text" id="ref-CR33">Kurillo G, Koritnik T, Bajd T, Bajcsy R (2010b) Real-time 3D avatars for tele-rehabilitation in virtual reality. In: Westwood J, Westwood S, et al. (eds) Proceedings of 18th medicine meets virtual reality (MMVR) conference., IOS press, pp 290–296</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kuster C, Popa T, Zach C, Gotsman C, Gross M (2011) Freecam: A hybrid camera system for interactive free-viewp" /><p class="c-article-references__text" id="ref-CR34">Kuster C, Popa T, Zach C, Gotsman C, Gross M (2011) Freecam: A hybrid camera system for interactive free-viewpoint video. In: Proceedings of vision, modeling, and visualization (VMV)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using" /><p class="c-article-references__text" id="ref-CR35">Ladikos A, Benhimane S, Navab N (2008) Efficient visual hull computation for real-time 3D reconstruction using CUDA. In: IEEE computer society conference on computer vision and pattern recognition workshops, 2008. CVPR Workshops, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="libjpeg (2012) libjpeg-turbo. http://libjpeg-turbo.virtualgl.org&#xA;                " /><p class="c-article-references__text" id="ref-CR36">libjpeg (2012) libjpeg-turbo. <a href="http://libjpeg-turbo.virtualgl.org">http://libjpeg-turbo.virtualgl.org</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Litos G, Zabulis X, Triantafyllidis G (2006) Synchronous image acquisition based on network synchronization. I" /><p class="c-article-references__text" id="ref-CR37">Litos G, Zabulis X, Triantafyllidis G (2006) Synchronous image acquisition based on network synchronization. In: Computer vision and pattern recognition workshop 2006. CVPRW ’06. Conference on, pp 167–173</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maimone A, Fuchs H (2011) Encumbrance-free telepresence system with real-time 3D capture and display using com" /><p class="c-article-references__text" id="ref-CR38">Maimone A, Fuchs H (2011) Encumbrance-free telepresence system with real-time 3D capture and display using commodity depth cameras. In: Proceedings of the 2011 10th IEEE international symposium on mixed and augmented reality, IEEE Computer Society, Washington, ISMAR ’11, pp 137–146</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maimone A, Fuchs H (2012) Reducing interference between multiple structured light depth sensors using motion. " /><p class="c-article-references__text" id="ref-CR39">Maimone A, Fuchs H (2012) Reducing interference between multiple structured light depth sensors using motion. In: Virtual reality workshops (VR), IEEE, pp 51–54</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Microsoft (2010) Microsoft Kinect. http://www.xbox.com/en-US/kinect&#xA;                " /><p class="c-article-references__text" id="ref-CR40">Microsoft (2010) Microsoft Kinect. <a href="http://www.xbox.com/en-US/kinect">http://www.xbox.com/en-US/kinect</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mulligan J, Daniilidis K (2001) Real time trinocular stereo for tele-immersion. In: Proceedings of 2001 intern" /><p class="c-article-references__text" id="ref-CR41">Mulligan J, Daniilidis K (2001) Real time trinocular stereo for tele-immersion. In: Proceedings of 2001 international conference on image processing, Thessaloniki, pp 959–962</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nahrstedt K, Bajcsy R, Wymore L, Kurillo G, Mezur K, Sheppard R, Yang Z, Wu W (2007) Symbiosis of tele-immersi" /><p class="c-article-references__text" id="ref-CR42">Nahrstedt K, Bajcsy R, Wymore L, Kurillo G, Mezur K, Sheppard R, Yang Z, Wu W (2007) Symbiosis of tele-immersive environments with creative choreography. In: ACM workshop on supporting creative acts beyond dissemination, associated with 6th ACM creativity and cognition conference, Washington</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nintendo (2006) Nintendo Wii. http://www.nintendo.com/wii&#xA;                " /><p class="c-article-references__text" id="ref-CR43">Nintendo (2006) Nintendo Wii. <a href="http://www.nintendo.com/wii">http://www.nintendo.com/wii</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Obdrzalek S, Kurilo G, Han J, Abresch T, Bajcsy R (2012) Real-time human pose detection and tracking for tele-" /><p class="c-article-references__text" id="ref-CR44">Obdrzalek S, Kurilo G, Han J, Abresch T, Bajcsy R (2012) Real-time human pose detection and tracking for tele-rehabilitation in virtual reality. In: Proceedings of the 19th medicine meets virtual reality conference (MMVR), Newport Beach</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="OpenWonderland (2010) Sun Systems, openWonderLand. http://openwonderland.org&#xA;                " /><p class="c-article-references__text" id="ref-CR45">OpenWonderland (2010) Sun Systems, openWonderLand. <a href="http://openwonderland.org">http://openwonderland.org</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Petit B, Lesage JD, Menier C, Allard J, Franco JS, Raffin B, Boyer E, Faure F (2010) Multicamera real-time 3D " /><p class="c-article-references__text" id="ref-CR46">Petit B, Lesage JD, Menier C, Allard J, Franco JS, Raffin B, Boyer E, Faure F (2010) Multicamera real-time 3D modeling for telepresence and remote collaboration. Int J Digit Multime'd Broadcast 12</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schröder Y, Scholz A, Berger K, Ruhl K, Guthe S, Magnor M (2011) Multiple kinect studies. Tech Rep 09–15, ICG" /><p class="c-article-references__text" id="ref-CR47">Schröder Y, Scholz A, Berger K, Ruhl K, Guthe S, Magnor M (2011) Multiple kinect studies. Tech Rep 09–15, ICG</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SecondLife (2003) SecondLife. http://secondlife.com&#xA;                " /><p class="c-article-references__text" id="ref-CR48">SecondLife (2003) SecondLife. <a href="http://secondlife.com">http://secondlife.com</a>
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sheppard R, Wu W, Yang Z, Nahrstedt K, Wymore L, Kurillo G, Bajcsy R, Mezur K (2007) New digital options in ge" /><p class="c-article-references__text" id="ref-CR49">Sheppard R, Wu W, Yang Z, Nahrstedt K, Wymore L, Kurillo G, Bajcsy R, Mezur K (2007) New digital options in geographically distributed dance collaborations with TEEVE: tele-immersive environments for everybody. In: Proceedings of the 15th international conference on multimedia, ACM, New York, MULTIMEDIA ’07, pp 1085–1086</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Svoboda, D. Martinec, T. Pajdla, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Svoboda T, Martinec D, Pajdla T (2005) A convenient multicamera self-calibration for virtual environments. Pre" /><p class="c-article-references__text" id="ref-CR50">Svoboda T, Martinec D, Pajdla T (2005) A convenient multicamera self-calibration for virtual environments. Presence 14(4):407–422</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474605774785325" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20convenient%20multicamera%20self-calibration%20for%20virtual%20environments&amp;journal=Presence&amp;volume=14&amp;issue=4&amp;pages=407-422&amp;publication_year=2005&amp;author=Svoboda%2CT&amp;author=Martinec%2CD&amp;author=Pajdla%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vasudevan R, Lobaton E, Kurillo G, Bajcsy R, Bernardin T, Hamann B, Nahrstedt K (2010a) A methodology for remo" /><p class="c-article-references__text" id="ref-CR51">Vasudevan R, Lobaton E, Kurillo G, Bajcsy R, Bernardin T, Hamann B, Nahrstedt K (2010a) A methodology for remote virtual interaction in teleimmersive environments. In: Proceedings of first ACM multimedia systems conference, Scottsdale, pp 281–292</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vasudevan R, Zhou Z, Kurillo G, Lobaton E, Bajcsy R, Nahrstedt K (2010b) Real-time stereo-vision system for 3D" /><p class="c-article-references__text" id="ref-CR52">Vasudevan R, Zhou Z, Kurillo G, Lobaton E, Bajcsy R, Nahrstedt K (2010b) Real-time stereo-vision system for 3D teleimmersive collaboration. In: Proceedings of IEEE international conference on multimedia &amp; expo (ICME 2010), Singapore, pp 1208–1213</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Vasudevan, G. Kurillo, E. Lobaton, T. Bernardin, O. Kreylos, R. Bajcsy, K. Nahrstedt, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualiza" /><p class="c-article-references__text" id="ref-CR53">Vasudevan R, Kurillo G, Lobaton E, Bernardin T, Kreylos O, Bajcsy R, Nahrstedt K (2011) High quality visualization for geographically distributed 3D teleimmersive applications. IEEE Trans Multime'd 13(3):573–584</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTMM.2011.2123871" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=High%20quality%20visualization%20for%20geographically%20distributed%203D%20teleimmersive%20applications&amp;journal=IEEE%20Trans%20Multime%27d&amp;volume=13&amp;issue=3&amp;pages=573-584&amp;publication_year=2011&amp;author=Vasudevan%2CR&amp;author=Kurillo%2CG&amp;author=Lobaton%2CE&amp;author=Bernardin%2CT&amp;author=Kreylos%2CO&amp;author=Bajcsy%2CR&amp;author=Nahrstedt%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Waschbüsch, S. Würmlin, D. Cotting, F. Sadlo, MH. Gross, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Waschbüsch M, Würmlin S, Cotting D, Sadlo F, Gross MH (2005) Scalable 3D video of dynamic scenes. Vis Comput 2" /><p class="c-article-references__text" id="ref-CR54">Waschbüsch M, Würmlin S, Cotting D, Sadlo F, Gross MH (2005) Scalable 3D video of dynamic scenes. Vis Comput 21(8-10):629–638</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-005-0346-7" aria-label="View reference 54">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%203D%20video%20of%20dynamic%20scenes&amp;journal=Vis%20Comput&amp;volume=21&amp;issue=8-10&amp;pages=629-638&amp;publication_year=2005&amp;author=Waschb%C3%BCsch%2CM&amp;author=W%C3%BCrmlin%2CS&amp;author=Cotting%2CD&amp;author=Sadlo%2CF&amp;author=Gross%2CMH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu W, Arefin MA, Huang Z, Agarwal P, Shi S, Rivas R, Nahrstedt K (2010) “I’m the Jedi!”—a case study of user e" /><p class="c-article-references__text" id="ref-CR55">Wu W, Arefin MA, Huang Z, Agarwal P, Shi S, Rivas R, Nahrstedt K (2010) “I’m the Jedi!”—a case study of user experience in 3D tele-immersive gaming. In: ISM, pp 220–227</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu W, Arefin A, Kurillo G, Agarwal P, Nahrstedt K, Bajcsy R (2011) Color-plus-depth level-of-details in 3D tel" /><p class="c-article-references__text" id="ref-CR56">Wu W, Arefin A, Kurillo G, Agarwal P, Nahrstedt K, Bajcsy R (2011) Color-plus-depth level-of-details in 3D teleimmersive video—a psychophysical approach. In: Proceedings of ACM multimedia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Yang, X. Wang, JX. Chen, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Yang Y, Wang X, Chen JX (2002) Rendering avatars in virtual reality: integrating a 3D model with 2D images. Co" /><p class="c-article-references__text" id="ref-CR57">Yang Y, Wang X, Chen JX (2002) Rendering avatars in virtual reality: integrating a 3D model with 2D images. Comput Sci Eng 4(1):86–91</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=812540" aria-label="View reference 57 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F5992.976440" aria-label="View reference 57">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Rendering%20avatars%20in%20virtual%20reality%3A%20integrating%20a%203D%20model%20with%202D%20images&amp;journal=Comput%20Sci%20Eng&amp;volume=4&amp;issue=1&amp;pages=86-91&amp;publication_year=2002&amp;author=Yang%2CY&amp;author=Wang%2CX&amp;author=Chen%2CJX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wu W, Arefin A, Rivas R, Nahrstedt K, Sheppard R, Yang Z (2009) Quality of experience in distributed interacti" /><p class="c-article-references__text" id="ref-CR58">Wu W, Arefin A, Rivas R, Nahrstedt K, Sheppard R, Yang Z (2009) Quality of experience in distributed interactive multimedia environments: toward a theoretical framework. In: Proceedings of ACM multimedia, Beijing, pp 481-490</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang Z, Nahrstedt K, Cui Y, Yu B, Liang J, Jung SH, Bajcsy R (2005) TEEVE: The next generation architecture fo" /><p class="c-article-references__text" id="ref-CR59">Yang Z, Nahrstedt K, Cui Y, Yu B, Liang J, Jung SH, Bajcsy R (2005) TEEVE: The next generation architecture for tele-immersive environment. In: Seventh IEEE international symposium on multimedia (ISM 2005). IEEE Computer Society, Irvine, pp 112–119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Yang, W. Wu, K. Nahrstedt, G. Kurillo, R. Bajcsy, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajcsy R (2010) Enabling multi-party 3D tele-immersive environments with" /><p class="c-article-references__text" id="ref-CR60">Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajcsy R (2010) Enabling multi-party 3D tele-immersive environments with viewcast. ACM transactions on multimedia computing, communications, and applications (TOMCCAP) 6:1–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 60 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Enabling%20multi-party%203D%20tele-immersive%20environments%20with%20viewcast&amp;journal=ACM%20transactions%20on%20multimedia%20computing%2C%20communications%2C%20and%20applications%20%28TOMCCAP%29&amp;volume=6&amp;pages=1-30&amp;publication_year=2010&amp;author=Yang%2CZ&amp;author=Wu%2CW&amp;author=Nahrstedt%2CK&amp;author=Kurillo%2CG&amp;author=Bajcsy%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajscy R (2007) ViewCast: view dissemination and management for multi-pa" /><p class="c-article-references__text" id="ref-CR61">Yang Z, Wu W, Nahrstedt K, Kurillo G, Bajscy R (2007) ViewCast: view dissemination and management for multi-party 3D tele-immersive environments. In: Proceedings of ACM multimedia, Augsburg, pp 882–891</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang D, Nomura Y, Fujii S (1991) Error analysis and optimization of camera calibration. In: Proceedings of IE" /><p class="c-article-references__text" id="ref-CR62">Zhang D, Nomura Y, Fujii S (1991) Error analysis and optimization of camera calibration. In: Proceedings of IEEE/RSJ international workshop on intelligent robots and systems (IROS 91), Osaka, pp 292–296</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-012-0217-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>We wish to thank Ram Vasudevan and Edgar Lobaton, University of California at Berkeley, for contribution on the stereo reconstruction and Zhong Zhou, University of Beijing, for texture compression. We also thank Tony Bernardin and Oliver Kreylos, University of California at Davis, for the implementation of the rendering and support in integration with the Vrui framework. Furthermore, we thank all of our past and current collaborators, including Jeremy Bailenson (Stanford University), Maurizio Forte (UCM), Jay Han (UCDMC), Louise Kellogg (UCD), Klara Nahrstedt (UIUC), and Lisa Wymore (UCB). This work was supported in part by the National Science Foundation (NSF grants: #0703787, #0724681, #0840399, #1111965), HP Labs, the European Aeronautic Defence and Space Company (EADS), and the Center for Information Technology in the Interest of Society (CITRIS) at University of California, Berkeley. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, Berkeley, CA, 94720-1764, USA</p><p class="c-article-author-affiliation__authors-list">Gregorij Kurillo &amp; Ruzena Bajcsy</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Gregorij-Kurillo"><span class="c-article-authors-search__title u-h3 js-search-name">Gregorij Kurillo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gregorij+Kurillo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gregorij+Kurillo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gregorij+Kurillo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ruzena-Bajcsy"><span class="c-article-authors-search__title u-h3 js-search-name">Ruzena Bajcsy</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ruzena+Bajcsy&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ruzena+Bajcsy" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ruzena+Bajcsy%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-012-0217-2/email/correspondent/c1/new">Gregorij Kurillo</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=3D%20teleimmersion%20for%20collaboration%20and%20interaction%20of%20geographically%20distributed%20users&amp;author=Gregorij%20Kurillo%20et%20al&amp;contentID=10.1007%2Fs10055-012-0217-2&amp;publication=1359-4338&amp;publicationDate=2012-11-25&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Kurillo, G., Bajcsy, R. 3D teleimmersion for collaboration and interaction of geographically distributed users.
                    <i>Virtual Reality</i> <b>17, </b>29–43 (2013). https://doi.org/10.1007/s10055-012-0217-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-012-0217-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-08-02">02 August 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-11-07">07 November 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-11-25">25 November 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-03">March 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-012-0217-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-012-0217-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">3D video</span></li><li class="c-article-subject-list__subject"><span itemprop="about">3D teleimmersion</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human–computer interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Remote collaboration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Telepresence</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-012-0217-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=217;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

