<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Analysis of expression in simple musical gestures to enhance audio in "/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Analysis of expression in simple musical gestures to enhance audio in interfaces"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-05-03"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers&#8217; strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human&#8211;computer interfaces."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-05-03"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="62"/>

    <meta name="prism.endingPage" content="70"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0029-3"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0029-3"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0029-3.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0029-3"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Analysis of expression in simple musical gestures to enhance audio in interfaces"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2006/05"/>

    <meta name="citation_online_date" content="2006/05/03"/>

    <meta name="citation_firstpage" content="62"/>

    <meta name="citation_lastpage" content="70"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0029-3"/>

    <meta name="DOI" content="10.1007/s10055-006-0029-3"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0029-3"/>

    <meta name="description" content="Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environm"/>

    <meta name="dc.creator" content="Luca Mion"/>

    <meta name="dc.creator" content="Gianluca D&#8217;Inc&#224;"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Behav Inf Technol; citation_title=Correcting menu usability problems with sound; citation_author=SA Brewster, MG Grease; citation_volume=18; citation_issue=3; citation_publication_date=1999; citation_pages=165-177; citation_doi=10.1080/014492999119066; citation_id=CR1"/>

    <meta name="citation_reference" content="Camurri A, De Poli G, Leman M, Volpe G (2001) A multi-layered conceptual framework for expressive gesture applications. In: Proceedings of the MOSART workshop on current research directions in computer music, Barcelona, pp 29&#8211;34"/>

    <meta name="citation_reference" content="citation_journal_title=J New Music Res; citation_title=An abstract control space for communication of sensory expressive intentions in music performance; citation_author=S Canazza, G Poli, A Rod&#224;, A Vidolin; citation_volume=32; citation_issue=3; citation_publication_date=2003; citation_pages=281-294; citation_doi=10.1076/jnmr.32.3.281.16862; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Modeling and control of expressiveness in music performance; citation_author=S Canazza, G Poli, C Drioli, A Rod&#224;, A Vidolin; citation_volume=92; citation_issue=4; citation_publication_date=2004; citation_pages=686-701; citation_doi=10.1109/JPROC.2004.825889; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Music and meaning: a theoretical introduction to musical aesthetics; citation_publication_date=1972; citation_id=CR5; citation_author=W Coker; citation_publisher=The Free Press"/>

    <meta name="citation_reference" content="Dannenberg R, Thom B, Watson D (1997) A machine learning approach to musical style recognition. In: Proceedings of the international computer music conference, San Francisco, USA, pp 344&#8211;347"/>

    <meta name="citation_reference" content="De Poli G (2003) Expressiveness in music performance: analysis and modeling. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 17&#8211;20"/>

    <meta name="citation_reference" content="citation_journal_title=J New Music Res; citation_title=Note-by-note analysis of the influence of expressive intentions and musical structure in violin performance; citation_author=G Poli, A Rod&#224;, A Vidolin; citation_volume=27; citation_issue=3; citation_publication_date=1998; citation_pages=293-321; citation_doi=10.1080/09298219808570750; citation_id=CR8"/>

    <meta name="citation_reference" content="De Poli G, D&#8217;inc&#224; G, Mion L (2005) Computational models for audio expressive communication. In: Proceedings of the Audio Engineering Society annual meeting, Como, Italy, 9&#8211;12 November 2005"/>

    <meta name="citation_reference" content="Duxbury C, Sandler M, Davies M (2002) A hybrid approach to musical note onset detection. In: Proceedings of the fifth international conference on digital audio effects (DAFX-02), Hamburg, Germany, 26&#8211;28 September 2002"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Does music performance allude to locomotion? A model of final ritardandi derived from measurements of stopping runners; citation_author=A Friberg, J Sundberg; citation_volume=105; citation_issue=3; citation_publication_date=1999; citation_pages=1469-1484; citation_doi=10.1121/1.426687; citation_id=CR11"/>

    <meta name="citation_reference" content="Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. In: Proceedings of the international computer music conference, G&#246;teborg, Sweden, pp 365&#8211;367"/>

    <meta name="citation_reference" content="citation_journal_title=Hum Comput Interact; citation_title=Auditory icons: using sound in computer interfaces; citation_author=W Gaver; citation_volume=2; citation_issue=2; citation_publication_date=1986; citation_pages=167-177; citation_doi=10.1207/s15327051hci0202_3; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE; citation_title=Sound and meaning in auditory data display; citation_author=T Hermann, H Ritter; citation_volume=92; citation_issue=4; citation_publication_date=2004; citation_pages=730-741; citation_doi=10.1109/JPROC.2004.825904; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=J New Music Res; citation_title=The importance of parameter mapping in electronic instrument design; citation_author=AD Hunt, M Paradis, M Wanderley; citation_volume=32; citation_issue=4; citation_publication_date=2003; citation_pages=429-440; citation_doi=10.1076/jnmr.32.4.429.18853; citation_id=CR15"/>

    <meta name="citation_reference" content="Leman M (2000) Visualization and calculation of roughness of acoustical musical signals using the synchronization index model (SIM). In: Proceedings of the COST G-6 conference on digital audio effects (DAFX-00), Verona, Italy, pp 125&#8211;130"/>

    <meta name="citation_reference" content="Leman M, Lesaffre M, Tanghe K (2001) An introduction to the IPEM toolbox for perception-based music analysis. Mikropolyphonie&#8212;The Online Contemporary Music Journal 7"/>

    <meta name="citation_reference" content="Mion L (2003) Application of Bayesian networks to automatic recognition of expressive content of piano improvisations. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 557&#8211;560"/>

    <meta name="citation_reference" content="Mion L, De Poli G (2004) Expressiveness detection of music performances in the kinematics energy space. In: Proceedings of sound and music computing conference (JIM/CIM 04), Paris, France, 20&#8211;22 October 2004, pp 257&#8211;261"/>

    <meta name="citation_reference" content="citation_journal_title=J Acoust Soc Am; citation_title=Patterns of expressive timing in performances of a Beethoven minuet by nineteen pianists; citation_author=B Repp; citation_volume=88; citation_issue=2; citation_publication_date=1990; citation_pages=622-641; citation_doi=10.1121/1.399766; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=J Pers Soc Psychol; citation_title=A circumplex model of affect; citation_author=JA Russell; citation_volume=39; citation_publication_date=1980; citation_pages=1161-1178; citation_doi=10.1037/h0077714; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_title=Human factors in alarm designs; citation_publication_date=1994; citation_id=CR22; citation_author=N Stanton; citation_publisher=Taylor &amp; Francis"/>

    <meta name="citation_reference" content="Wanderley M, Battier M (2000) Trends in gestural control of music. Edition lectronique. IRCAM, Paris"/>

    <meta name="citation_author" content="Luca Mion"/>

    <meta name="citation_author_email" content="luca.mion@dei.unipd.it"/>

    <meta name="citation_author_institution" content="Department of Information Engineering&#8212;CSC/DEI, University of Padova, Padova, Italy"/>

    <meta name="citation_author" content="Gianluca D&#8217;Inc&#224;"/>

    <meta name="citation_author_email" content="gianluca.dinca@dei.unipd.it"/>

    <meta name="citation_author_institution" content="Department of Information Engineering&#8212;CSC/DEI, University of Padova, Padova, Italy"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0029-3&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/05/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0029-3"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Analysis of expression in simple musical gestures to enhance audio in interfaces"/>
        <meta property="og:description" content="Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers’ strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human–computer interfaces."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Analysis of expression in simple musical gestures to enhance audio in interfaces | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0029-3","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Expression, Audio interfaces, Sonification","kwrd":["Expression","Audio_interfaces","Sonification"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0029-3","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0029-3","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=29;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0029-3">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Analysis of expression in simple musical gestures to enhance audio in interfaces
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0029-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0029-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-05-03" itemprop="datePublished">03 May 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Analysis of expression in simple musical gestures to enhance audio in interfaces</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Luca-Mion" data-author-popup="auth-Luca-Mion" data-corresp-id="c1">Luca Mion<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Padova" /><meta itemprop="address" content="grid.5608.b, 0000000417573470, Department of Information Engineering—CSC/DEI, University of Padova, Padova, Italy" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gianluca-D_Inc_" data-author-popup="auth-Gianluca-D_Inc_">Gianluca D’Incà</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Padova" /><meta itemprop="address" content="grid.5608.b, 0000000417573470, Department of Information Engineering—CSC/DEI, University of Padova, Padova, Italy" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, Article number: <span data-test="article-number">62</span> (<span data-test="article-publication-year">2006</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">131 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0029-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Expression could play a key role in the audio rendering of virtual reality applications. Its understanding is an ambitious issue in the scientific environment, and several studies have investigated the analysis techniques to detect expression in music performances. The knowledge coming from these analyses is widely applicable: embedding expression on audio interfaces can drive to attractive solutions to emphasize interfaces in mixed-reality environments. Synthesized expressive sounds can be combined with real stimuli to experience augmented reality, and they can be used in multi-sensory stimulations to provide the sensation of first-person experience in virtual expressive environments. In this work we focus on the expression of violin and flute performances, with reference to sensorial and affective domains. By means of selected audio features, we draw a set of parameters describing performers’ strategies which are suitable both for tuning expressive synthesis instruments and enhancing audio in human–computer interfaces.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The idea that an artist can control the multi-sensory stimulation of the audience while, at the same time, providing the users with the illusion and sensation of first-person experience of actually “being there” is quite old in the multi-sensory simulation environments design (e.g. Morton Heilig’s Sensorama machine). In this context, the analysis of music expression can lead to the design of synthesized expressive sounds which can be combined with real stimuli to experience augmented reality; it can also be used to provide the illusion in virtual expressive environments. Enhanced audio can provide navigational information supporting users’ exploration through virtual expressive environments; in addition, expressive information can be integrated into assisting tools (such as virtual agents in computer simulation systems) in order to improve the learning process of novel users while interacting with multi-modal interfaces in virtual environments. The usage of enhanced auditory displays in human–computer interaction (HCI) is assuming increasing importance, and auditory displays have to be designed in order to be intuitive, understandable and flexible. To perform this, both physical and psychoacoustic issues must be taken into account; furthermore, the choice of referring to sensorial and/or to affective domains depends on several considerations, according to the availability of metaphors and structural relations for connecting the input and output domains.</p><p>Traditionally, gestures have been intended as body movements that convey some kind of information (Wanderley and Battier <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Wanderley M, Battier M (2000) Trends in gestural control of music. Edition lectronique. IRCAM, Paris" href="/article/10.1007/s10055-006-0029-3#ref-CR23" id="ref-link-section-d2973e294">2000</a>); humanistic theories, which refer to the role that gestures have in communicating expressiveness (Coker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1972" title="Coker W (1972) Music and meaning: a theoretical introduction to musical aesthetics. The Free Press, New York" href="/article/10.1007/s10055-006-0029-3#ref-CR5" id="ref-link-section-d2973e297">1972</a>), tried to extend the definition of gesture to include “musical gestures” which are supposed to share the same kind of rules or spatial/temporal patterns to convey the expressive information (Camurri et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Camurri A, De Poli G, Leman M, Volpe G (2001) A multi-layered conceptual framework for expressive gesture applications. In: Proceedings of the MOSART workshop on current research directions in computer music, Barcelona, pp 29–34" href="/article/10.1007/s10055-006-0029-3#ref-CR2" id="ref-link-section-d2973e300">2001</a>). For example, in Friberg and Sundberg (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Friberg A, Sundberg J (1999) Does music performance allude to locomotion? A model of final ritardandi derived from measurements of stopping runners. J Acoust Soc Am 105(3):1469–1484" href="/article/10.1007/s10055-006-0029-3#ref-CR11" id="ref-link-section-d2973e303">1999</a>), investigations were conducted to find out if there is a common pattern among final musical ritardandi and stopping runner timing. In Repp (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Repp B (1990) Patterns of expressive timing in performances of a Beethoven minuet by nineteen pianists. J Acoust Soc Am 88(2):622–641" href="/article/10.1007/s10055-006-0029-3#ref-CR20" id="ref-link-section-d2973e306">1990</a>), there is no explicit relation with locomotion, but expressive timing patterns were analysed to investigate their relation with phrase boundaries. Thus, musical gestures are to be intended as another communication channel of affective or emotional content, which artists can use to communicate their intentions.</p><p>Musicians enrich their performances with expression acting on their available degrees of freedom (De Poli et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="De Poli G, Rodà A, Vidolin A (1998) Note-by-note analysis of the influence of expressive intentions and musical structure in violin performance. J New Music Res 27(3):293–321" href="/article/10.1007/s10055-006-0029-3#ref-CR8" id="ref-link-section-d2973e312">1998</a>) and introducing deviations from a mechanical playing of the score. Beyond interpretations of the score, expression is present in non-structured sounds: for example (De Poli et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="De Poli G, D’incà G, Mion L (2005) Computational models for audio expressive communication. In: Proceedings of the Audio Engineering Society annual meeting, Como, Italy, 9–12 November 2005" href="/article/10.1007/s10055-006-0029-3#ref-CR9" id="ref-link-section-d2973e315">2005</a>), in the 1960s’ science fiction movies synthetic sounds were used to communicate feeling of artificiality and sense of disquietude. Expressive synthesis and control of non-structured sounds have been less studied and explored; nevertheless, it is an interesting field for possible applications in the HCI. In perspective, the knowledge derived from these analyses can be used in an artistic or functional application which generates expressive sounds. In the case of <i>artistic</i> applications, the artist’s aesthetic sensibility can drive the synthesis towards scenarios like sounding physical objects, controlling in real time the expressive information by tactile interaction, gestural controllers or methods for mapping and transforming audio data to create sound material. By <i>functional</i>, we mean those applications that have the purpose to solve a well-defined problem, for instance the implementation of human–computer interfaces based on multi-modal communication (Hunt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hunt AD, Paradis M, Wanderley M (2003) The importance of parameter mapping in electronic instrument design. J New Music Res 32(4):429–440" href="/article/10.1007/s10055-006-0029-3#ref-CR15" id="ref-link-section-d2973e324">2003</a>) or the use in the medical-therapeutic field for rehabilitation of motor activities. In this case, adding expression to sounds becomes an enhancing reinforcement of the audio feedback, and the interaction mode can be learnt by means of the user exploration. Expression can also be added to systems for synthesizing and manipulating non-speech sounds like Auditory Icons, which refer to everyday listening sounds that can be either simple sounds or patterns (Gaver <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum Comput Interact 2(2):167–177" href="/article/10.1007/s10055-006-0029-3#ref-CR13" id="ref-link-section-d2973e328">1986</a>). Also, expressive content could be used in Earcons that are defined as abstract musical tones which can be used in structured combinations to create sound messages (Brewster and Grease <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Brewster SA, Grease MG (1999) Correcting menu usability problems with sound. Behav Inf Technol 18(3):165–177" href="/article/10.1007/s10055-006-0029-3#ref-CR1" id="ref-link-section-d2973e331">1999</a>), as well as in model-based sonifications based on metaphors (Hermann and Ritter <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hermann T, Ritter H (2004) Sound and meaning in auditory data display. Proc IEEE 92(4):730–741" href="/article/10.1007/s10055-006-0029-3#ref-CR14" id="ref-link-section-d2973e334">2004</a>). Other applications could be found in well-defined problems, like the design of alarm enunciators: in this field several studies have been conducted on the mapping between sonological parameters and urgency (Stanton <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Stanton N (1994) Human factors in alarm designs. Taylor &amp; Francis, London" href="/article/10.1007/s10055-006-0029-3#ref-CR22" id="ref-link-section-d2973e337">1994</a>), and expressiveness control could be added to give information about different levels of warning to the user. Furthermore, expression can also drive to attractive solutions for the audio enhancement of interfaces for artistic productions, mixed-reality environments for extreme gaming and entertainment industry in general.</p><p>The main goal of this paper is to define a set of parameters and their values to design a synthetic expressive instrument, which allows explicit modification of the spectral content and temporal evolution of a tone, and then to embed the results for enhancing audio in interfaces. When our research group studied how a performer plays a piece according to different expressive intentions, we discovered that a clearer interpretation and best results in simulation were obtained using neutral performance as reference (Canazza et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Canazza S, De Poli G, Drioli C, Rodà A, Vidolin A (2004) Modeling and control of expressiveness in music performance. Proc IEEE 92(4):686–701" href="/article/10.1007/s10055-006-0029-3#ref-CR4" id="ref-link-section-d2973e343">2004</a>). We call neutral performance a human performance played without any specific expressive intention, in a scholastic way and without any artistic aim. We refer to an experiment on violin and flute performances, which led to interesting observations on strategies that performers applied for their expressive rendering. We investigate the parameters found to be important for expression rendering, using neutral performance as reference: we describe the values of the parameters by relative deviations from the respective parameters in the neutral performance. The experiment presented here included different types of gestures: single notes, scales and excerpts.</p><p>To draw the set of parameters for the expressive interface, we start from the simplest non-structured gestures we recorded (with no score): single repeated notes, played with violin and flute. The expressive spaces we deal with are the <i>kinematics energy</i> space and the <i>valence arousal</i> space, both described in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec3">Sect. 2.1</a>. Previous results on the strategies employed by violin and flute performers are then described by particular features, which are selected from a wide set of recorded audio cues presented in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec4">Sect. 3</a>. The experiment and the features selection techniques are described in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec5">Sect. 4</a>, while in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec6">Sect. 4.1</a> we show how the employment of machine learning techniques and the dimensional reduction via principal component analyses (PCA) allowed us to confirm both the common strategies for the expression communication and some different approaches for the neutral performance rendering. Results of relative deviations’ analysis are presented in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec7">Sect. 4.2</a>, taking into account all the features for the intentions of the two spaces for violin and flute separately. In <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec8">Sect. 4.3</a> we discuss the application of analyses’ results to design a prototype of expressive tone generator; a preliminary evaluation is discussed in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec9">Sect. 5</a>. Finally, <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec11">Sect. 6</a> presents conclusions and remarks.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Analysis of expressive musical gestures</h2><div class="c-article-section__content" id="Sec2-content"><p>Some attempts have been made to identify an expressive model that could render different expressive intentions of a human performer (De Poli <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="De Poli G (2003) Expressiveness in music performance: analysis and modeling. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 17–20" href="/article/10.1007/s10055-006-0029-3#ref-CR7" id="ref-link-section-d2973e389">2003</a>; Canazza et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Canazza S, De Poli G, Drioli C, Rodà A, Vidolin A (2004) Modeling and control of expressiveness in music performance. Proc IEEE 92(4):686–701" href="/article/10.1007/s10055-006-0029-3#ref-CR4" id="ref-link-section-d2973e392">2004</a>). Moreover, the automatic detection of expressive performances is quite recent. An important work on machine learning musical style recognition has been carried out by Dannenberg et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dannenberg R, Thom B, Watson D (1997) A machine learning approach to musical style recognition. In: Proceedings of the international computer music conference, San Francisco, USA, pp 344–347" href="/article/10.1007/s10055-006-0029-3#ref-CR6" id="ref-link-section-d2973e395">1997</a>). They showed that high-level understanding of musical performance like style recognition is highly beneficial from a machine learning approach.</p><p>Another study on automatic analysis of expressiveness by Friberg et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. In: Proceedings of the international computer music conference, Göteborg, Sweden, pp 365–367" href="/article/10.1007/s10055-006-0029-3#ref-CR12" id="ref-link-section-d2973e401">2002</a>) shows a system able to predict what emotion the performer is trying to convey. One or several types of “listener panels” can be stored as models which are used to simulate judgments of new performances based on results from previous listening experiments. In Mion (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Mion L (2003) Application of Bayesian networks to automatic recognition of expressive content of piano improvisations. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 557–560" href="/article/10.1007/s10055-006-0029-3#ref-CR18" id="ref-link-section-d2973e404">2003</a>), Bayesian networks have been employed for the recognition of expressive content in piano improvisations, and the following expressive intentions were recognized: slanted, heavy, hopping, vacuous, bold, hollow, fluid, tender. The intentions are derived from the Laban’s basic effort theory of expressive movement. In Mion and De Poli (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mion L, De Poli G (2004) Expressiveness detection of music performances in the kinematics energy space. In: Proceedings of sound and music computing conference (JIM/CIM 04), Paris, France, 20–22 October 2004, pp 257–261" href="/article/10.1007/s10055-006-0029-3#ref-CR19" id="ref-link-section-d2973e407">2004</a>), performances on various instruments were investigated; the expressive intentions were recognized with reference to the Kinematics Energy expressive space and the expressive content was classified using machine learning techniques.</p><p>We can roughly identify studies on three levels of gestures, each one related to a structural complexity. In particular we have <i>basic gestures</i> (single tones for music) representing the smallest non-structured actions, which are combined together in <i>simple patterns</i> (scales or repetition of single tones), and highly structured gestures which are represented by <i>performances of scores</i> in music. The study of musical gestures rather than musical performances allows us to approach issues related to different levels of structural complexity, and with different functional and scientific aims. Musical gestures share low-level mechanisms and dynamics which can be studied, measured and used to train classifiers able to detect the expressive content of musical gestures. In this direction, research spans through various levels of structural complexity, from simple notes (assumed as simple sounding objects by means of physical metaphors) to scales, phrases, excerpts. Regarding different structures, machine learning techniques can be suitable for musical experience in an executive/artistic sense, and they can also be applied to medical/rehabilitation issues as well as for multimedia technologies, gaming, auditory warnings and auditory icons in general.</p><h3 class="c-article__sub-heading" id="Sec3">The affective and sensorial domains</h3><p>In this work we use a dimensional approach to conceptualize emotions. This approach consists in placing expressions on a space with a small number of dimensions. Dimensions are used to simplify the study of highly structured concepts and yield structures more suitable for a general comprehension of the phenomena. We refer to the Kinematics Energy space and the Valence Arousal space. The Kinematics Energy space has been conceptualized in Canazza et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Canazza S, De Poli G, Rodà A, Vidolin A (2003) An abstract control space for communication of sensory expressive intentions in music performance. J New Music Res 32(3):281–294" href="/article/10.1007/s10055-006-0029-3#ref-CR3" id="ref-link-section-d2973e429">2003</a>), where measurements of perceptive nature have been used. In this space we can distinguish four main categories situated at the opposite sides of the axis: high energy (HE), low energy (LE), high kinematics (HK), low kinematics (LK).</p><p>We selected the performances whose projection in this space was closer to the categories we want to classify. Thus we have these correspondences: <i>hard–soft</i> (HE–LE); <i>light–heavy</i> (HK–LK). The Valence Arousal space is derived from the “circumplex model of affect” designed by psychologist Russell (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Russell JA (1980) A circumplex model of affect. J Pers Soc Psychol 39:1161–1178" href="/article/10.1007/s10055-006-0029-3#ref-CR21" id="ref-link-section-d2973e441">1980</a>), who looks at emotions in terms of pleasure and displeasure (valence) and arousal. Russell found that people organize emotions in a system of coordinates where the <i>y</i>-axis is the degree of arousal and the <i>x</i>-axis the valence. Subjects, for example, placed the adjectives in order to induce the associations “happy versus sad” (valence) and “angry versus calm” (arousal). Similar to the case of the KE space, we selected the performances whose projection in this space was closer to the categories we want to classify, thus we consider the correspondences: <i>happy–sad</i> (high and low valence); <i>angry–calm</i> (high and low arousal).</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Audio cues description and collection</h2><div class="c-article-section__content" id="Sec4-content"><p>We extracted a set of 14 cues from audio data, selecting features that were found to be important for discriminating emotions in listening experiments and to classify the content in musical performances (Dannenberg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Dannenberg R, Thom B, Watson D (1997) A machine learning approach to musical style recognition. In: Proceedings of the international computer music conference, San Francisco, USA, pp 344–347" href="/article/10.1007/s10055-006-0029-3#ref-CR6" id="ref-link-section-d2973e466">1997</a>; Friberg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. In: Proceedings of the international computer music conference, Göteborg, Sweden, pp 365–367" href="/article/10.1007/s10055-006-0029-3#ref-CR12" id="ref-link-section-d2973e469">2002</a>). We extracted the same features with some additions, gathering the following set: note per second (NPS), note duration (<i>D</i>), inter-onset interval (IOI), attack (<i>A</i>), articulation or legato degree (<i>L</i>), roughness (<i>R</i>), centroid (<i>C</i>), peak sound level (PSL), sound level range (SLR) and five energy spectral ratios (SRa, SRb, SRl, SRm, SRh). Each audio feature is considered in terms of running average within overlapping windows with 4 s duration and 3.5 s overlap. For temporal features extraction, we implemented an algorithm for automatic detection of onset instants, using both derivative and pitch-tracking approaches (Duxbury et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Duxbury C, Sandler M, Davies M (2002) A hybrid approach to musical note onset detection. In: Proceedings of the fifth international conference on digital audio effects (DAFX-02), Hamburg, Germany, 26–28 September 2002" href="/article/10.1007/s10055-006-0029-3#ref-CR10" id="ref-link-section-d2973e488">2002</a>).</p><p>It was decided that the offset instant would coincide with the instant when the RMS profile falls down to 60% of its peak, reached after the onset instant. If this does not happen, offset time is taken as equal to the next onset. NPS is computed by dividing the number of onsets by the window length within the sliding window. For each tone we computed the duration, inter-onset interval and articulation (defined as <i>L</i> = <i>D</i>/IOI). The attack time is defined as the time required to reach the RMS peak, starting from the onset instant. Roughness is calculated using the Synchronization Index Model implemented by IPEM Matlab Toolbox (Leman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Leman M (2000) Visualization and calculation of roughness of acoustical musical signals using the synchronization index model (SIM). In: Proceedings of the COST G-6 conference on digital audio effects (DAFX-00), Verona, Italy, pp 125–130" href="/article/10.1007/s10055-006-0029-3#ref-CR16" id="ref-link-section-d2973e500">2000</a>; Leman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Leman M, Lesaffre M, Tanghe K (2001) An introduction to the IPEM toolbox for perception-based music analysis. Mikropolyphonie—The Online Contemporary Music Journal 7" href="/article/10.1007/s10055-006-0029-3#ref-CR17" id="ref-link-section-d2973e503">2001</a>). The spectrum centroid is calculated by the following formula: </p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ C = {\left( {{\sum\limits_{k = 1}^{N/2} {f_{k} {\left| {X(k)} \right|}} }} \right)}/{\sum\limits_{k = 1}^{N/2} {{\left| {X(k)} \right|}} }, $$</span></div></div><p> where <i>N</i> is the FFT size, <i>X</i> is the FFT of the input signal <i>x</i> and <i>f</i>
                        <sub>
                  <i>k</i>
                </sub>, <i>k</i> = 1...<i>N</i>, is the <i>k</i>th frequency bin. The computations of PSL<i> </i>= max(RMS(<i>t</i>)) and SLR<i> </i>= max(RMS(<i>t</i>)) − min(RMS(<i>t</i>)) derive directly from the RMS profile. Another set of cues describes the spectral energy as experimented in Mion and De Poli (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mion L, De Poli G (2004) Expressiveness detection of music performances in the kinematics energy space. In: Proceedings of sound and music computing conference (JIM/CIM 04), Paris, France, 20–22 October 2004, pp 257–261" href="/article/10.1007/s10055-006-0029-3#ref-CR19" id="ref-link-section-d2973e557">2004</a>), but in this experiment we do not subtract the deterministic component. We experimented with two kinds of spectral ratios derived from the original sound. The first is obtained by dividing the frequency range into two regions as in Friberg et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. In: Proceedings of the international computer music conference, Göteborg, Sweden, pp 365–367" href="/article/10.1007/s10055-006-0029-3#ref-CR12" id="ref-link-section-d2973e560">2002</a>), placed below and above a frequency of 1,000 Hz. Taking into account the frequencies above this, we obtain one feature called SRa. The second parameter is given by a separation into three regions: below 534 Hz, from 534 to 1,805 Hz and over 1,805 Hz (this division yields three features, called SRl, SRm and SRh). These bands are derived from the actual frequency separations of the cochlear filter-bank used for the roughness computation.</p><p>Two professional violin and flute performers were invited to play musical performances inspired by different expressive intentions, described by the adjectives that lie on the Kinematics Energy space and the Valence Arousal space: light (L), heavy (He), soft (S) and hard (Ha) for the KE space and happy (Hp), sad (Sa), angry (A) and calm (C) regarding the VA space. In this way each one of the adjectives had its opposite (soft vs. hard) in order to deliberately induce contrasting performances on the part of the musician. In addition, we asked them to play a neutral performance for each excerpt. Musical performances were recorded in monophonic digital form at 16 bits and 44,100 Hz at the CSC, Padua University.</p><p>We recorded three kinds of performances to cover three levels of structural complexity. The first set is formed by repeated notes (simple gestures); then we recorded two sets of G major scales (simple patterns). Finally, we recorded some excerpts from classical repertoire to cover the third type of gesture (performance of score): Twinkle Twinkle Little Star (traditional), Handel’s Flute Sonata in E minor (Adagio), Frère Jacques (traditional, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig1">1</a>). These excerpts were selected from simple traditional melodies in order to induce few artistic ambitions to the performer, but rather in this way we allowed the performer to feel free to play with extreme emphasis. Each recording was repeated twice, playing with both normal and extreme emphasis.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Score of traditional Frère Jacques melody</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">The experiment</h2><div class="c-article-section__content" id="Sec5-content"><p>We recorded about 130 min of music, gathering 6,500 different windows. After sliding over the audio file, mean and variance values for each feature are calculated. The relevance of the features has been investigated by applying ANOVA tests over performances, considering the two spaces separately. We considered a <i>p</i> value of 0.001 as the level for significance, obtaining the following set of significant features, ordered by decreasing value of <i>F</i>: violin <i>F</i>
                        <sub>3,312</sub>, <i>P</i> &lt; 0,001: <i>A</i>, NPS, <i>D</i>, IOI, PSL, SLR, <i>R</i>; flute <i>F</i>
                        <sub>3,166</sub>, <i>P</i> &lt; 0,001: NPS, <i>D</i>, IOI, <i>C</i>, PSL, SLR, <i>R</i>. In the valence arousal space we still considered <i>p</i> = 0.001 as the level for significance, obtaining the following set of significant features: violin <i>F</i>
                        <sub>3,312</sub>, <i>P</i> &lt; 0,001: NPS, <i>D</i>, <i>A</i>, <i>C</i>, IOI, SLR, <i>R</i>; flute <i>F</i>
                        <sub>3,166</sub>, <i>P</i> &lt; 0,001: <i>C</i>, PSL, SLR, NPS, <i>D</i>, IOI, <i>R</i>. We took into account the common set of features, given by the union of respective feature sets. Thus, we selected a set of eight features: attack (<i>A</i>), note duration (<i>D</i>), inter-onset interval (IOI), note per second (NPS), peak sound level (PSL), sound level range (SLR), roughness (<i>R</i>) and centroid (<i>C</i>).</p><h3 class="c-article__sub-heading" id="Sec6">Clustering expressions</h3><p>The PCA were applied on the selected features related to all the performances (notes, scales and excerpts). We first analysed single instruments separately (each one in two different spaces), resulting in well-separated clustering of intentions, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig2">2</a>. This suggests that the features we selected provide a good differentiation between performances played with different intentions.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Principal component analyses on violin (<i>left</i>) and flute (<i>right</i>) performances in the KE space (<i>top</i>) and the VA space (<i>bottom</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The PCA on features from both instruments in the Kinematics Energy space yield a good separation of adjectives along the dimensions (<i>hard</i> vs. <i>soft</i> and <i>heavy</i> vs. <i>light</i>). The first principal component (PC1), on the <i>x</i>-axis, explains 68.4% of the total variance, while the second principal component (PC2) accounts for 27.1%. PCA in the Valence Arousal space yield a good separation of dimensions, but there is still some confusion with adjectives <i>calm</i> and <i>sad</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig2">2</a>, bottom-left). PC1, on the <i>x</i>-axis, explains 55.3% of the total variance, while PC2 accounts for 42.7%.</p><p>Then we analysed violin and flute features together: in this case we obtained good results in clustering intentions, but some differences were noticed on neutral performances of the two instruments. In particular, in the Kinematics Energy space neutral performances of violin took position close to <i>soft</i> and <i>light</i>, whereas flute was placed near <i>hard</i> and <i>heavy</i>. Similar considerations arise from the analysis in Valence Arousal space.</p><p>We also experimented a principal component analysis over the whole set of eight categories taken from the Kinematics Energy space and the Valence Arousal space. Results are depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig3">3</a>, showing that the data set neatly splits into three clusters; also, sensorial performances tend to be located in positions reflecting the KE space characteristic.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The PCA of violin and flute features in the union of KE and VA spaces. The position of clusters allows us to investigate the association of Kinematics with Valence and Energy with Arousal, and positions reflect intuitive associations: <i>light</i> with <i>happy</i> and the cluster formed by <i>sad</i>, <i>calm</i> and <i>soft</i>. The other cluster places <i>hard</i> and <i>heavy</i> close to <i>angry</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The separation among the three clusters allows us to investigate the association of Kinematics with Valence and Energy with Arousal. In particular, we can notice that clustering positions reflect intuitive associations: <i>light</i> with <i>happy</i> and the cluster formed by <i>sad</i>, <i>calm</i> and <i>soft</i>. The other cluster places <i>hard</i> and <i>heavy</i> close to <i>angry</i>. We also noticed that the neutral performances were not intended in the same way by the performers; we can argue that a neutral performance may not exist in a common sense, i.e. in terms of absolute features values, and it may be strictly related to the different nature of the instruments. This fact can be easily understood upon hearing the recorded performances. Violin can be approached by the musician with his arm relaxed and an unstressed wrist movement; for flute, musician breathing and embouchure cannot easily change the level of sound. In some contexts the flute player is forced to use a different strategy and this affects the recognition in the common space. Nevertheless, it is useful to consider the features of each expressive intention in terms of relative deviation from the respective features of the neutral performance. Since the neutral performances are intended in different ways by performers, we decided to take their references (neutral performances) separately. For each feature and intention, we consider the relative deviation from the neutral performance value, given by the following formula: </p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">
$$ D_{{F,i}} = \frac{{F_{i} - F_{{\text{n}}} }} {{F_{{\text{n}}} }}, $$</span></div></div><p> where <i>F</i>
                           <sub>
                    <i>i</i>
                  </sub> indicates the mean value of feature <i>F</i> of intention <i>i</i> and <i>F</i>
                           <sub>
                    <i>n</i>
                  </sub> indicates the mean value of <i>F</i> in the neutral performance.</p><h3 class="c-article__sub-heading" id="Sec7">Analysis of parameters deviations</h3><p>Violin and flute data present similar deviations from their neutral values for most intentions, with some exceptions. For example (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig4">4</a>), observing the peak sound level and centroid values we can notice different values, which may not be the result of different approaches, but it could due to the physics of the instruments. Flute has less dynamic capabilities than violin, resulting in lower dynamic range from the neutral value. If we do not take into account the neutral performance, the trend of the deviations is in coherence with the values for violin (i.e. high/low tendency), and this reflects the adoption of similar strategies by performers.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Relative deviations of the parameters from the respective values in neutral performance. Violin data and flute data present similar deviations from their neutral values for most of the intentions, with some exceptions, for instance peak sound level and centroid. Diagrams refer to single repeated notes</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Moreover, different signs reflect different interpretations for neutral performances, as shown for <i>calm</i> and <i>sad</i> in the PSL diagram, and even more clearly in the centroid diagram (centre bottom, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig4">4</a>). More information is provided by the centroid diagram; flute can produce harmonics in different intensities, resulting in higher differences among the intentions. Parameter values change in the case of single notes only, and centroid might not be selected for classification tests in general but only for flute.</p><p>We also calculated the relative deviations after gathering recordings in the three clusters mentioned before: Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig5">5</a> shows deviations for all features, displaying the differences among the three groups.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0029-3/MediaObjects/10055_2006_29_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Diagrams of the relative deviations of features values from the neutral performance considering the three clusters</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0029-3/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec8">Expressive sound synthesis</h3><p>The considerations presented in the previous section refer to relative deviations of the parameters with respect to the neutral performance. We wanted to understand the behaviour of each performer (in terms of the selected features) in discriminating the expressions. To synthesize the expressive sounds we make the hypothesis that the expressive information can be added to the tones generated by a synthetic instrument, applying deviations to the parameters; the idea is to set a <i>neutral tone</i> and then to synthesize tones with different expressions applying the deviations discussed in the analysis. We implemented the expressive tone generator using the open source real-time synthesis environment <i>pd</i> (pure data). In the application, sound synthesis is very simple, and we consider a small set of parameters, in order to avoid side effects. We control the ADSR envelope of a five harmonics additive sinusoidal synthesis, varying three different kinds of parameters: tempo-related features (<i>attack, duration, note per second</i>), intensity-related features (<i>peak sound level</i> and <i>sound level range</i>) and perception-related features (<i>roughness</i> and <i>centroid</i>). Intensity and tempo parameters are controlled using the ADSR envelope values, while <i>roughness</i> and <i>centroid</i> are modified by changing the frequency and amplitude of the harmonics. The tone generator performs a single repeated note with the calculated values of these parameters.</p><p>We defined the <i>neutral note</i> using the violin neutral performances’ parameters. We have shown the different approaches the performer used to convey the neutral expression, while they affected in a very similar way the parameters that were taken into account. The matter of the different neutral rendering is still an open question and would need deeper investigations. We have gathered only violin and flute performances, thus the data set is not wide enough to make the results suitable for a generalization. Up to now, we must refer to different ideas of neutral performance. We considered violin performances because the parameters related to different adjectives were more clearly separated. Then, we applied the transformations (deviations) resulting from the analyses to synthesize the <i>expressive notes</i>, according to the eight categories. Synthesized sounds were then used for evaluation in a listening test that will be described in the next section.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Listening test and results</h2><div class="c-article-section__content" id="Sec9-content"><p>For the preliminary listening test we used the forced-choice identification method, asking 26 subjects (age 18–30 years) to evaluate the expression of a 16 stimuli series: 8 sounds were both violin and synthetic and referred to sensorial categories (<i>hard, soft, heavy, light</i>); the other 8 sounds referred to affective categories (<i>happy, sad, angry, calm</i>). Also in this case, stimuli were both violin and synthetic. The test has two objectives: the first one, regarding violin stimuli, is to test the actual possibility to differentiate and communicate intentions in non-structured sounds (in this case, a single repeated note). The second one is to comprehend the amount of expressive content that a simple synthesis model is able to convey, with respect to the real sounds. Results on violin and synthetic stimuli will be presented in the next two sections.</p><h3 class="c-article__sub-heading" id="Sec10">Violin stimuli</h3><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0029-3#Tab1">1</a> shows the confusion matrix of listeners’ responses. On the top left we have the violin stimuli Kinematics Energy space matrix, related to the sensorial categories, while on the bottom left we have the Valence Arousal space results (affective categories). We have good results in both the spaces (baseline accuracy is 25%). Affective categories lead to better accuracy of detection, but confirm some confusion between <i>calm</i> and <i>sad</i>. In general, opposite adjectives (e.g. <i>happy</i> and <i>sad</i>) are well discriminated, while confusion arises when trying to distinguish between other categories. This is confirmed by the remark of some listeners: they tended to make two mental categories (e.g. <i>hard</i> with <i>heavy</i> and <i>light</i> with <i>soft</i>) and then tried to discriminate inside the categories, having more difficulty.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Confusion matrices for violin and synthetic stimuli in the Kinematics Energy space and the Valence Arousal space</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0029-3/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec11">Synthetic stimuli</h3><p>Synthetic stimuli results are shown in the top and the bottom right of Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0029-3#Tab1">1</a>. We still exceed the baseline accuracy, but high confusion arises between similar categories, confirming listeners’ observations. On the Kinematics Energy space, <i>heavy</i> is often confused with <i>hard</i>; in the Valence Arousal space, <i>happy</i> is confused with <i>angry</i> and the difficulty to discriminate <i>sad</i> and <i>calm</i> is also confirmed. We gathered listeners’ answers with reference to the three clusters mentioned in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec6">Sect. 4.1</a>: results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-006-0029-3#Tab2">2</a>. As one may expect, violin stimuli results are good, while some difficulty arises in synthetic stimuli.
</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Confusion matrices for violin and synthetic stimuli considering the three clusters</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-006-0029-3/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In particular, <i>Cluster 2</i> is often (36% of cases) confused with <i>Cluster 1</i>: this fact derives not only from the confusion between <i>happy</i> and <i>angry</i>, but also from the non-effective rendering of intention <i>light</i>, which is often perceived as <i>hard</i>.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Conclusions</h2><div class="c-article-section__content" id="Sec12-content"><p>This work is yet another step in furthering the knowledge on designing synthetic expressive instruments. In perspective, this result will be used to enhance the audio rendering in virtual reality applications, and in interfaces with both artistic and functional aims.</p><p>Results of the analyses confirmed that a small set of parameters provides a good differentiation between performances played with different intentions, as seen in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec6">Sect. 4.1</a>. Furthermore, the extracted parameters are not related to the musical structure of performances. Different approaches on the neutral performances have been found; nevertheless, relative deviations’ analysis (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec7">Sect. 4.2</a>) confirmed common rendering strategies of the performers in terms of parameter deviations, as can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0029-3#Fig4">4</a>. A first prototype of an expressive tone generator has been implemented and preliminary listening experiments on original and synthetic data show encouraging results on recognition of the synthesized expressions, as shown in <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-006-0029-3#Sec8">Sect. 5</a>. Moreover, accurate evaluation tests will have to be conducted to gain understanding about the amount of expressive information that the model is able to convey.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SA. Brewster, MG. Grease, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Brewster SA, Grease MG (1999) Correcting menu usability problems with sound. Behav Inf Technol 18(3):165–177" /><p class="c-article-references__text" id="ref-CR1">Brewster SA, Grease MG (1999) Correcting menu usability problems with sound. Behav Inf Technol 18(3):165–177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F014492999119066" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Correcting%20menu%20usability%20problems%20with%20sound&amp;journal=Behav%20Inf%20Technol&amp;volume=18&amp;issue=3&amp;pages=165-177&amp;publication_year=1999&amp;author=Brewster%2CSA&amp;author=Grease%2CMG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Camurri A, De Poli G, Leman M, Volpe G (2001) A multi-layered conceptual framework for expressive gesture appl" /><p class="c-article-references__text" id="ref-CR2">Camurri A, De Poli G, Leman M, Volpe G (2001) A multi-layered conceptual framework for expressive gesture applications. In: Proceedings of the MOSART workshop on current research directions in computer music, Barcelona, pp 29–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Canazza, G. Poli, A. Rodà, A. Vidolin, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Canazza S, De Poli G, Rodà A, Vidolin A (2003) An abstract control space for communication of sensory expressi" /><p class="c-article-references__text" id="ref-CR3">Canazza S, De Poli G, Rodà A, Vidolin A (2003) An abstract control space for communication of sensory expressive intentions in music performance. J New Music Res 32(3):281–294</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1076%2Fjnmr.32.3.281.16862" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20abstract%20control%20space%20for%20communication%20of%20sensory%20expressive%20intentions%20in%20music%20performance&amp;journal=J%20New%20Music%20Res&amp;volume=32&amp;issue=3&amp;pages=281-294&amp;publication_year=2003&amp;author=Canazza%2CS&amp;author=Poli%2CG&amp;author=Rod%C3%A0%2CA&amp;author=Vidolin%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Canazza, G. Poli, C. Drioli, A. Rodà, A. Vidolin, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Canazza S, De Poli G, Drioli C, Rodà A, Vidolin A (2004) Modeling and control of expressiveness in music perfo" /><p class="c-article-references__text" id="ref-CR4">Canazza S, De Poli G, Drioli C, Rodà A, Vidolin A (2004) Modeling and control of expressiveness in music performance. Proc IEEE 92(4):686–701</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJPROC.2004.825889" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Modeling%20and%20control%20of%20expressiveness%20in%20music%20performance&amp;journal=Proc%20IEEE&amp;volume=92&amp;issue=4&amp;pages=686-701&amp;publication_year=2004&amp;author=Canazza%2CS&amp;author=Poli%2CG&amp;author=Drioli%2CC&amp;author=Rod%C3%A0%2CA&amp;author=Vidolin%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="W. Coker, " /><meta itemprop="datePublished" content="1972" /><meta itemprop="headline" content="Coker W (1972) Music and meaning: a theoretical introduction to musical aesthetics. The Free Press, New York" /><p class="c-article-references__text" id="ref-CR5">Coker W (1972) Music and meaning: a theoretical introduction to musical aesthetics. The Free Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Music%20and%20meaning%3A%20a%20theoretical%20introduction%20to%20musical%20aesthetics&amp;publication_year=1972&amp;author=Coker%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dannenberg R, Thom B, Watson D (1997) A machine learning approach to musical style recognition. In: Proceeding" /><p class="c-article-references__text" id="ref-CR6">Dannenberg R, Thom B, Watson D (1997) A machine learning approach to musical style recognition. In: Proceedings of the international computer music conference, San Francisco, USA, pp 344–347</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Poli G (2003) Expressiveness in music performance: analysis and modeling. In: Proceedings of the SMAC03 Sto" /><p class="c-article-references__text" id="ref-CR7">De Poli G (2003) Expressiveness in music performance: analysis and modeling. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 17–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Poli, A. Rodà, A. Vidolin, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="De Poli G, Rodà A, Vidolin A (1998) Note-by-note analysis of the influence of expressive intentions and musica" /><p class="c-article-references__text" id="ref-CR8">De Poli G, Rodà A, Vidolin A (1998) Note-by-note analysis of the influence of expressive intentions and musical structure in violin performance. J New Music Res 27(3):293–321</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F09298219808570750" aria-label="View reference 8">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Note-by-note%20analysis%20of%20the%20influence%20of%20expressive%20intentions%20and%20musical%20structure%20in%20violin%20performance&amp;journal=J%20New%20Music%20Res&amp;volume=27&amp;issue=3&amp;pages=293-321&amp;publication_year=1998&amp;author=Poli%2CG&amp;author=Rod%C3%A0%2CA&amp;author=Vidolin%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Poli G, D’incà G, Mion L (2005) Computational models for audio expressive communication. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR9">De Poli G, D’incà G, Mion L (2005) Computational models for audio expressive communication. In: Proceedings of the Audio Engineering Society annual meeting, Como, Italy, 9–12 November 2005</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Duxbury C, Sandler M, Davies M (2002) A hybrid approach to musical note onset detection. In: Proceedings of th" /><p class="c-article-references__text" id="ref-CR10">Duxbury C, Sandler M, Davies M (2002) A hybrid approach to musical note onset detection. In: Proceedings of the fifth international conference on digital audio effects (DAFX-02), Hamburg, Germany, 26–28 September 2002</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Friberg, J. Sundberg, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Friberg A, Sundberg J (1999) Does music performance allude to locomotion? A model of final ritardandi derived " /><p class="c-article-references__text" id="ref-CR11">Friberg A, Sundberg J (1999) Does music performance allude to locomotion? A model of final ritardandi derived from measurements of stopping runners. J Acoust Soc Am 105(3):1469–1484</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.426687" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Does%20music%20performance%20allude%20to%20locomotion%3F%20A%20model%20of%20final%20ritardandi%20derived%20from%20measurements%20of%20stopping%20runners&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=105&amp;issue=3&amp;pages=1469-1484&amp;publication_year=1999&amp;author=Friberg%2CA&amp;author=Sundberg%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. I" /><p class="c-article-references__text" id="ref-CR12">Friberg A, Schoonderwaldt E, Juslin P, Bresin R (2002) Automatic real-time extraction of musical expression. In: Proceedings of the international computer music conference, Göteborg, Sweden, pp 365–367</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Gaver, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum Comput Interact 2(2):167–177" /><p class="c-article-references__text" id="ref-CR13">Gaver W (1986) Auditory icons: using sound in computer interfaces. Hum Comput Interact 2(2):167–177</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2Fs15327051hci0202_3" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Auditory%20icons%3A%20using%20sound%20in%20computer%20interfaces&amp;journal=Hum%20Comput%20Interact&amp;volume=2&amp;issue=2&amp;pages=167-177&amp;publication_year=1986&amp;author=Gaver%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Hermann, H. Ritter, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Hermann T, Ritter H (2004) Sound and meaning in auditory data display. Proc IEEE 92(4):730–741" /><p class="c-article-references__text" id="ref-CR14">Hermann T, Ritter H (2004) Sound and meaning in auditory data display. Proc IEEE 92(4):730–741</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FJPROC.2004.825904" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sound%20and%20meaning%20in%20auditory%20data%20display&amp;journal=Proc%20IEEE&amp;volume=92&amp;issue=4&amp;pages=730-741&amp;publication_year=2004&amp;author=Hermann%2CT&amp;author=Ritter%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AD. Hunt, M. Paradis, M. Wanderley, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Hunt AD, Paradis M, Wanderley M (2003) The importance of parameter mapping in electronic instrument design. J " /><p class="c-article-references__text" id="ref-CR15">Hunt AD, Paradis M, Wanderley M (2003) The importance of parameter mapping in electronic instrument design. J New Music Res 32(4):429–440</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1076%2Fjnmr.32.4.429.18853" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20importance%20of%20parameter%20mapping%20in%20electronic%20instrument%20design&amp;journal=J%20New%20Music%20Res&amp;volume=32&amp;issue=4&amp;pages=429-440&amp;publication_year=2003&amp;author=Hunt%2CAD&amp;author=Paradis%2CM&amp;author=Wanderley%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leman M (2000) Visualization and calculation of roughness of acoustical musical signals using the synchronizat" /><p class="c-article-references__text" id="ref-CR16">Leman M (2000) Visualization and calculation of roughness of acoustical musical signals using the synchronization index model (SIM). In: Proceedings of the COST G-6 conference on digital audio effects (DAFX-00), Verona, Italy, pp 125–130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Leman M, Lesaffre M, Tanghe K (2001) An introduction to the IPEM toolbox for perception-based music analysis. " /><p class="c-article-references__text" id="ref-CR17">Leman M, Lesaffre M, Tanghe K (2001) An introduction to the IPEM toolbox for perception-based music analysis. Mikropolyphonie—The Online Contemporary Music Journal 7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mion L (2003) Application of Bayesian networks to automatic recognition of expressive content of piano improvi" /><p class="c-article-references__text" id="ref-CR18">Mion L (2003) Application of Bayesian networks to automatic recognition of expressive content of piano improvisations. In: Proceedings of the SMAC03 Stockholm music acoustics conference, Stockholm, Sweden, pp 557–560</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mion L, De Poli G (2004) Expressiveness detection of music performances in the kinematics energy space. In: Pr" /><p class="c-article-references__text" id="ref-CR19">Mion L, De Poli G (2004) Expressiveness detection of music performances in the kinematics energy space. In: Proceedings of sound and music computing conference (JIM/CIM 04), Paris, France, 20–22 October 2004, pp 257–261</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Repp, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Repp B (1990) Patterns of expressive timing in performances of a Beethoven minuet by nineteen pianists. J Acou" /><p class="c-article-references__text" id="ref-CR20">Repp B (1990) Patterns of expressive timing in performances of a Beethoven minuet by nineteen pianists. J Acoust Soc Am 88(2):622–641</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1121%2F1.399766" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=2212286" aria-label="View reference 20 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Patterns%20of%20expressive%20timing%20in%20performances%20of%20a%20Beethoven%20minuet%20by%20nineteen%20pianists&amp;journal=J%20Acoust%20Soc%20Am&amp;volume=88&amp;issue=2&amp;pages=622-641&amp;publication_year=1990&amp;author=Repp%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JA. Russell, " /><meta itemprop="datePublished" content="1980" /><meta itemprop="headline" content="Russell JA (1980) A circumplex model of affect. J Pers Soc Psychol 39:1161–1178" /><p class="c-article-references__text" id="ref-CR21">Russell JA (1980) A circumplex model of affect. J Pers Soc Psychol 39:1161–1178</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2Fh0077714" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20circumplex%20model%20of%20affect&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=39&amp;pages=1161-1178&amp;publication_year=1980&amp;author=Russell%2CJA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="N. Stanton, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Stanton N (1994) Human factors in alarm designs. Taylor &amp; Francis, London" /><p class="c-article-references__text" id="ref-CR22">Stanton N (1994) Human factors in alarm designs. Taylor &amp; Francis, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20factors%20in%20alarm%20designs&amp;publication_year=1994&amp;author=Stanton%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wanderley M, Battier M (2000) Trends in gestural control of music. Edition lectronique. IRCAM, Paris" /><p class="c-article-references__text" id="ref-CR23">Wanderley M, Battier M (2000) Trends in gestural control of music. Edition lectronique. IRCAM, Paris</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0029-3-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the European Network of Excellence “Enactive Interfaces” under the sixth framework program of the European Commission. We thank David Pirrò for developing part of the prototype.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Information Engineering—CSC/DEI, University of Padova, Padova, Italy</p><p class="c-article-author-affiliation__authors-list">Luca Mion &amp; Gianluca D’Incà</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Luca-Mion"><span class="c-article-authors-search__title u-h3 js-search-name">Luca Mion</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Luca+Mion&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Luca+Mion" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Luca+Mion%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Gianluca-D_Inc_"><span class="c-article-authors-search__title u-h3 js-search-name">Gianluca D’Incà</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Gianluca+D%E2%80%99Inc%C3%A0&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gianluca+D%E2%80%99Inc%C3%A0" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gianluca+D%E2%80%99Inc%C3%A0%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0029-3/email/correspondent/c1/new">Luca Mion</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Analysis%20of%20expression%20in%20simple%20musical%20gestures%20to%20enhance%20audio%20in%20interfaces&amp;author=Luca%20Mion%20et%20al&amp;contentID=10.1007%2Fs10055-006-0029-3&amp;publication=1359-4338&amp;publicationDate=2006-05-03&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Mion, L., D’Incà, G. Analysis of expression in simple musical gestures to enhance audio in interfaces.
                    <i>Virtual Reality</i> <b>10, </b>62 (2006). https://doi.org/10.1007/s10055-006-0029-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0029-3.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-12-22">22 December 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-04-03">03 April 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05-03">03 May 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0029-3" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0029-3</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Expression</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Audio interfaces</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sonification</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0029-3.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=29;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

