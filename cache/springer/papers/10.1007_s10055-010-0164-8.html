<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Digilog book for temple bell tolling experience based on interactive a"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="We first present the concept of the Digilog Book, an augmented paper book that provides additional multimedia content stimulating readers&#8217; five senses using augmented reality (AR)..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/15/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Digilog book for temple bell tolling experience based on interactive augmented reality"/>

    <meta name="dc.source" content="Virtual Reality 2010 15:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2010-06-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="We first present the concept of the Digilog Book, an augmented paper book that provides additional multimedia content stimulating readers&#8217; five senses using augmented reality (AR) technologies. We also develop a prototype to show the usefulness and effectiveness of the book. The Digilog book has the following characteristics: AR content descriptions for updatable multisensory AR contents; enhanced experience with multisensory feedback; and interactive experience with computerized vision-based manual input methods. As an example of an entertaining and interactive Digilog Book, this paper presents a &#8220;temple bell experience&#8221; book and its implementation details. Informal user observation and interviews were conducted to verify the feasibility of the prototype book. As a result, this case study of the Digilog book can be useful in guiding the design and implementation of other Digilog applications, including posters, pictures, newspapers, and sign boards."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2010-06-05"/>

    <meta name="prism.volume" content="15"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="295"/>

    <meta name="prism.endingPage" content="309"/>

    <meta name="prism.copyright" content="2010 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-010-0164-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-010-0164-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-010-0164-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-010-0164-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Digilog book for temple bell tolling experience based on interactive augmented reality"/>

    <meta name="citation_volume" content="15"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2011/11"/>

    <meta name="citation_online_date" content="2010/06/05"/>

    <meta name="citation_firstpage" content="295"/>

    <meta name="citation_lastpage" content="309"/>

    <meta name="citation_article_type" content="SI: Cultural Technology"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-010-0164-8"/>

    <meta name="DOI" content="10.1007/s10055-010-0164-8"/>

    <meta name="citation_doi" content="10.1007/s10055-010-0164-8"/>

    <meta name="description" content="We first present the concept of the Digilog Book, an augmented paper book that provides additional multimedia content stimulating readers&#8217; five sense"/>

    <meta name="dc.creator" content="Taejin Ha"/>

    <meta name="dc.creator" content="Youngho Lee"/>

    <meta name="dc.creator" content="Woontack Woo"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="ARToolKit, 
                    http://www.hitl.washington.edu/artoolkit
                    
                  . Access date: June 2010"/>

    <meta name="citation_reference" content="Azuma R (1997) A survey of augmented reality presence. 6, pp 355&#8211;385"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph; citation_title=The magic book: a transitional ar interface; citation_author=M Billinghurst, H Kato, I Poupyrev; citation_volume=25; citation_issue=5; citation_publication_date=2001; citation_pages=745-753; citation_doi=10.1016/S0097-8493(01)00117-0; citation_id=CR4"/>

    <meta name="citation_reference" content="Chen CH, Hsu MF, Lee IJ, Sun KW, Lin YC, Lee WH (2009) Applying augmented reality to visualize the history of traditional architecture in Taiwan. 22nd CIPA Symposium"/>

    <meta name="citation_reference" content="Cho K, Lee J, Soh J, Lee J, Yang H (2007) A realistic e-learning system based on mixed reality. In: The 13th Intl conference on virtual systems and multimedia. pp 57&#8211;64"/>

    <meta name="citation_reference" content="Cultural Heritage Administration of Korea, 
                    http://english.cha.go.kr
                    
                  . Access date: June 2010"/>

    <meta name="citation_reference" content="D&#252;nser A, Hornecker E (2007) An observational study of children interacting with an augmented story book. In: The 2nd international conference of e-learning and games (Edutainment). pp 305&#8211;315"/>

    <meta name="citation_reference" content="Grasset R, Duenser A, Billinghurst M (2008) Edutainment with a mixed reality book: a visually augmented illustrative childrens&#8217; book. In: International conference on advances in computer entertainment technology (ACE)"/>

    <meta name="citation_reference" content="Ha T, Woo W (2010) An empirical evaluation of virtual hand techniques for 3d object manipulation in a tangible augmented reality environment. IEEE 3D User Interfaces, pp 91&#8211;98"/>

    <meta name="citation_reference" content="Ha T, Kim Y, Ryu J, Woo W (2006) Enhancing Immersiveness in AR-based Product Design. LNCS (ICAT), pp 207&#8211;216"/>

    <meta name="citation_reference" content="H&#246;llerer T, Feiner S, Pavlik J (1999) Situated documentaries: embedding multimedia presentations in the real world. ISWC (Third Int. Symp. on Wearable Computers), pp 79&#8211;86"/>

    <meta name="citation_reference" content="Irawati S, Ahn S, Kim J, Ko H (2008) VARU Framework: enabling rapid prototyping of VR, AR and ubiquitous applications. IEEE Virtual Reality Conference. pp 201&#8211;208"/>

    <meta name="citation_reference" content="Korean bell&#8217;s home page, 
                    http://library.thinkquest.org/29141//main.htm
                    
                  , Access date: June 2010"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Languages Comput; citation_title=Immersive authoring of tangible augmented reality content: a user study; citation_author=G Lee, G Kim; citation_volume=20; citation_publication_date=2009; citation_pages=61-79; citation_doi=10.1016/j.jvlc.2008.07.001; citation_id=CR16"/>

    <meta name="citation_reference" content="Lee G, Billinghurst M, Kim J (2004) Occlusion based interaction methods for tangible augmented reality environments. In: ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry (VRCAI). pp 419&#8211;426"/>

    <meta name="citation_reference" content="Lee J, Ha T, Ryu J, Woo W (2009) Development of pen-type haptic user interface and haptic effect design for Digilog book authoring. KHCI, pp 402&#8211;405"/>

    <meta name="citation_reference" content="McKenzie J, Darnell D (2004) The eyeMagic book. A report into augmented reality storytelling in the context of a children&#8217;s workshop 2003. New Zealand Centre for Children&#8217;s Literature and Christchurch College of Education"/>

    <meta name="citation_reference" content="Open Dynamics Engine, 
                    http://www.ode.org
                    
                  , Access date: June 2010"/>

    <meta name="citation_reference" content="osgART, 
                    http://www.artoolworks.com/community/osgart
                    
                  , Access date: June 2010"/>

    <meta name="citation_reference" content="citation_title=From personal to cultural computing: how to assess a cultural experience; citation_inbook_title=uDayIV&#8211;Information nutzbar machen; citation_publication_date=2006; citation_pages=13-21; citation_id=CR24; citation_author=M Rauterberg; citation_publisher=Pabst Science Publisher"/>

    <meta name="citation_reference" content="Saso T, Iguchi K, Inakage M (2003) Little red: storytelling in mixed reality. SIGGRAPH Sketches and Applications, pp 1&#8211;1"/>

    <meta name="citation_reference" content="Scherrer C, Pilet J, Fua P, Lepetit V (2008) Haunted book. ISMAR, pp 163&#8211;164"/>

    <meta name="citation_reference" content="citation_title=The Myth of the paperless office; citation_publication_date=2003; citation_id=CR27; citation_author=A Sellen; citation_author=R Harper; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="Shelton B, Hedley N (2002) Using augmented reality for teaching Earth-Sun relationships to undergraduate geography students. First IEEE Intl. Augmented Reality Toolkit workshop. pp 1&#8211;8"/>

    <meta name="citation_reference" content="Shibata F, Yoshida Y, Furuno K, Sakai T, Kiguchi K, Kimura A, Tamura H (2004) Vivid encyclopedia: MR pictorial book of insects. In: the 9th VR society of Japan annual conference. pp 611&#8211;612"/>

    <meta name="citation_reference" content="Stricker D, Daehne P, Seibert F (2001) Design and development issues for archeoguide: a augmented reality based cultural heritage on-site guide. In: International conference on augmented, virtual environments and three-dimensional imaging"/>

    <meta name="citation_reference" content="Taketa N, Hayash K, Kato H, Nishida S (2007) Virtual pop-up book based on augmented reality. HCI, pp 475&#8211;484"/>

    <meta name="citation_reference" content="The Eye of Judgement, 
                    http://us.playstation.com/games-and-media/games/the-eye-of-judgment-ps3.html
                    
                  , 2007. Access date: June 2010"/>

    <meta name="citation_reference" content="citation_journal_title=Lect Notes Comput Sci; citation_title=Cultural computing with context-aware application: ZENetic computer; citation_author=N Tosa, S Matsuoka, B Ellis, H Ueda, R Nakatsu; citation_volume=3711; citation_publication_date=2005; citation_pages=13-23; citation_doi=10.1007/11558651_2; citation_id=CR33"/>

    <meta name="citation_reference" content="Wagner D, Barakonyi I (2003) Augmented reality kanji learning. In: Proceedings of the 2nd IEEE/ACM symposium on mixed and augmented reality (ISMAR). pp 335&#8211;336"/>

    <meta name="citation_reference" content="White M, Liarokapis F, Darcy J, Mourkoussis N, Petridis P, Lister PF (2003) Augmented reality for museum artefact visualization. In: 4th irish workshop on computer graphics, Eurographics Ireland Chapter. pp 75&#8211;80"/>

    <meta name="citation_reference" content="Wireless presenter: 3M WP-8000, 
                    http://www.3m.com
                    
                  . Access date: June 2010"/>

    <meta name="citation_reference" content="Woods E, Billinghurst M, Looser J, Aldridge G, Brown D, Garrie B, Nelles C (2004) Augmenting the science centre and museum experience. In: 2nd international conference on computer graphics and interactive techniques in Australasia and SouthEast Asia (Graphite). ACM Press, New York, 230&#8211;236"/>

    <meta name="citation_reference" content="Yang H, Cho K, Soh J, Jung J, Lee J (2008) Hybrid visual tracking for augmented books. In: the 7th international conference on entertainment computing. pp 161&#8211;166"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Virtual Real; citation_title=wIzQubesTM- a novel tangible interface for interactive storytelling in mixed reality; citation_author=Z Zhou, AD Cheok, J Tedjokusumo, GS Omer; citation_volume=7; citation_issue=4; citation_publication_date=2008; citation_pages=9-15; citation_id=CR40"/>

    <meta name="citation_author" content="Taejin Ha"/>

    <meta name="citation_author_email" content="tha@gist.ac.kr"/>

    <meta name="citation_author_institution" content="GIST U-VR Lab, Gwangju, South Korea"/>

    <meta name="citation_author" content="Youngho Lee"/>

    <meta name="citation_author_email" content="youngho@mokpo.ac.kr"/>

    <meta name="citation_author_institution" content="MNU U-VR Lab, Jeonnam, Korea"/>

    <meta name="citation_author" content="Woontack Woo"/>

    <meta name="citation_author_email" content="wwoo@gist.ac.kr"/>

    <meta name="citation_author_institution" content="GIST U-VR Lab, Gwangju, South Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-010-0164-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2011/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-010-0164-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Digilog book for temple bell tolling experience based on interactive augmented reality"/>
        <meta property="og:description" content="We first present the concept of the Digilog Book, an augmented paper book that provides additional multimedia content stimulating readers’ five senses using augmented reality (AR) technologies. We also develop a prototype to show the usefulness and effectiveness of the book. The Digilog book has the following characteristics: AR content descriptions for updatable multisensory AR contents; enhanced experience with multisensory feedback; and interactive experience with computerized vision-based manual input methods. As an example of an entertaining and interactive Digilog Book, this paper presents a “temple bell experience” book and its implementation details. Informal user observation and interviews were conducted to verify the feasibility of the prototype book. As a result, this case study of the Digilog book can be useful in guiding the design and implementation of other Digilog applications, including posters, pictures, newspapers, and sign boards."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Digilog book for temple bell tolling experience based on interactive augmented reality | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-010-0164-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Digilog book, Culture technology, User interaction, Multisensory experience, Augmented reality","kwrd":["Digilog_book","Culture_technology","User_interaction","Multisensory_experience","Augmented_reality"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-010-0164-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-010-0164-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=164;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-010-0164-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Digilog book for temple bell tolling experience based on interactive augmented reality
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0164-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0164-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">SI: Cultural Technology</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2010-06-05" itemprop="datePublished">05 June 2010</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Digilog book for temple bell tolling experience based on interactive augmented reality</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Taejin-Ha" data-author-popup="auth-Taejin-Ha">Taejin Ha</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="GIST U-VR Lab" /><meta itemprop="address" content="grid.61221.36, 0000000110339831, GIST U-VR Lab, Gwangju, 500-712, South Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Youngho-Lee" data-author-popup="auth-Youngho-Lee" data-corresp-id="c1">Youngho Lee<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="MNU U-VR Lab" /><meta itemprop="address" content="grid.411815.8, 0000 0000 9628 9654, MNU U-VR Lab, 61 Dorim-ri, Cheonggye-myeon, Muan-gun, Jeonnam, Korea" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Woontack-Woo" data-author-popup="auth-Woontack-Woo">Woontack Woo</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="GIST U-VR Lab" /><meta itemprop="address" content="grid.61221.36, 0000000110339831, GIST U-VR Lab, Gwangju, 500-712, South Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">295</span>–<span itemprop="pageEnd">309</span>(<span data-test="article-publication-year">2011</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">769 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">24 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-010-0164-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>We first present the concept of the Digilog Book, an augmented paper book that provides additional multimedia content stimulating readers’ five senses using augmented reality (AR) technologies. We also develop a prototype to show the usefulness and effectiveness of the book. The Digilog book has the following characteristics: AR content descriptions for updatable multisensory AR contents; enhanced experience with multisensory feedback; and interactive experience with computerized vision-based manual input methods. As an example of an entertaining and interactive Digilog Book, this paper presents a “temple bell experience” book and its implementation details. Informal user observation and interviews were conducted to verify the feasibility of the prototype book. As a result, this case study of the Digilog book can be useful in guiding the design and implementation of other Digilog applications, including posters, pictures, newspapers, and sign boards.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Electronic books (e-books) have been made available through computers, PDAs, and even mobile phones. The first e-books were produced in text formats (e.g., PDF, XML), while the second-generation e-books combined multimedia content. Despite early expectations that e-books would replace paper books, this has not been the case (Sellen and Harper <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Sellen A, Harper R (2003) The Myth of the paperless office. MIT Press, Cambridge" href="/article/10.1007/s10055-010-0164-8#ref-CR27" id="ref-link-section-d68916e335">2003</a>). There are several reasons that people still prefer paper books: physical presence (tangibility), possession, and the high quality of printed material. On the other hand, digital content has other advantages, including portability and additional information like multisensory feedback. To this end, augmented reality (AR) books integrate the advantages of paper books with digital content, so users can experience both analog aesthetic emotions and immersive digital multisensory feedback.</p><p>Many researchers have examined AR books, beginning with The MagicBook (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magic book: a transitional ar interface. IEEE Comput Graph 25(5):745–753" href="/article/10.1007/s10055-010-0164-8#ref-CR4" id="ref-link-section-d68916e341">2001</a>) that demonstrate the potential and usability of AR books as new-generation media. After the work was published, several implementations of AR books were created for education, storytelling, simulation, game, and artwork purposes. The most supported interactions in the AR book were viewing and listening with turning pages. In other works, various interactions involved 3D magnetic trackers, foldable cube, paddle, gaze tracking, mouse click, and turntable with a slide device.</p><p>This paper proposes Digilog Book, a next-generation publication that combines the analog sensibility of a paper book with digitized visual, auditory, and haptic feedback by exploiting AR technology. The Digilog Book consists of a conventional printed book, multimedia content, and a Digilog Book viewer that acquires images of the printed book from the camera and augments the multimedia content in the book. This allows the reader to experience the benefits of printed material enhanced by multimedia content through multisensory visual, auditory, and haptic experiences. In other words, a Digilog Book maintains the original functions of a printed book while adding the advantages of digital content.</p><p>The Digilog book has the following characteristics: AR content descriptions for updatable multisensory AR contents; enhanced experience with multisensory feedback; and interactive experience with computerized, vision-based, bare-hand input methods.</p><p>The publication of previously implemented digital content would be difficult to update given the changing requirements of conventional AR book applications. To solve this problem, this paper proposes AR content descriptions in which the AR content of the Digilog Book will be updated through an Internet connection if AR content in a remote database is updated. Moreover, this project adds vibration feedback to visual and audio feedbacks via a 3D manipulation tool in which a vibration module is embedded. This can create an immersive experience with visual, audio, and enhanced haptic feedback that is possible with a paper book. This project also proposes an input method for natural interaction with AR content, requiring only the user’s hands. In an early occlusion-based virtual button, the user manually occludes a fiducial marker on a page in order to trigger an event. This paper suggests an improved method in which unnatural fiducial markers do not need to be inserted on the book pages. Three particular types of virtual buttons are proposed that support various input types with respect to interaction scenarios.</p><p>As an example of an entertaining and interactive Digilog Book, this paper presents a “temple bell experience” book that explains Asian cultural heritage to users in a way that a conventional book cannot. For example, users can rotate and observe a virtual temple bell by covering virtual buttons on the book with their fingers or by tapping the 3D object with a manipulation tool; corresponding visuals, sounds, and vibration feedback are then initiated. Sequential start, emersion/withdrawal, interaction, and the transitional interpolation of the 3D object and visual special effects are also considered for natural augmentation. Through these interactions, the temple bell experience book is expected to encourage readers to explore other cultural heritages for education and entertainment.</p><p>In considering a practical running environment for typical users, this project designed a monitor-based display installation with a camera fixed on a table in line with the direction of a user for natural viewing. The typical USB camera used in this project is less expensive than a high-end camera, and the manipulation tool is simply a modification and extension of a conventional mouse. Therefore, this is an appropriate choice for home users. In order to verify the feasibility of implementation, we conducted informal user observation and interviews.</p><p>The remainder of this paper is organized as follows: Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0164-8#Sec2">2</a> explains the related work and characteristics of the Digilog Book. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0164-8#Sec11">3</a> describes the system design that enables AR interaction with multisensory feedback. As an example, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0164-8#Sec19">4</a> shows the implementation details and results for an AR-based temple bell experience book. Finally, lessons and future directions of the Digilog Book are summarized in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-010-0164-8#Sec29">5</a>.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Cultural technology in AR</h3><p>Over the past few decades, the computing paradigm has been changing from mainframe computing, network computing, and personal computing to social computing, or, with respect to the viewpoint of the user, user-centric computing. From such paradigm shifts, cultural technology (CT) has emerged, drawing a great deal of attention to cultural aspects in the social computing paradigm. In general, CT is, from a narrow point of view, the digitalizing of cultural contents through technology and, from a wider point of view, technology’s improving quality of life for humans by creating culture. CT-related industries are wide in range: film, broadcasting, gaming, music, art, cultural heritage, tourism, and computing (Tosa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2005) Cultural computing with context-aware application: ZENetic computer. Lect Notes Comput Sci 3711:13–23" href="/article/10.1007/s10055-010-0164-8#ref-CR33" id="ref-link-section-d68916e382">2005</a>; Rauterberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Rauterberg M (2006) From personal to cultural computing: how to assess a cultural experience. In: Kempter G, von Hellberg P (eds) uDayIV–Information nutzbar machen. Pabst Science Publisher, Lengerich, pp 13–21" href="/article/10.1007/s10055-010-0164-8#ref-CR24" id="ref-link-section-d68916e385">2006</a>).</p><p>Several AR-related projects have been launched to enhance humans’ imagination, creativity, and sensitivity with CT. In the Mobile Augmented Reality System (MARS) project (Höllerer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Höllerer T, Feiner S, Pavlik J (1999) Situated documentaries: embedding multimedia presentations in the real world. ISWC (Third Int. Symp. on Wearable Computers), pp 79–86" href="/article/10.1007/s10055-010-0164-8#ref-CR13" id="ref-link-section-d68916e391">1999</a>), users can experience images, video, narration, and 3D models related to a certain building on campus explaining its historical significance. The Archeoguide project provides an interactive AR guide for archaeological sites (Stricker et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Stricker D, Daehne P, Seibert F (2001) Design and development issues for archeoguide: a augmented reality based cultural heritage on-site guide. In: International conference on augmented, virtual environments and three-dimensional imaging" href="/article/10.1007/s10055-010-0164-8#ref-CR30" id="ref-link-section-d68916e394">2001</a>). In this project, users use a wearable AR system to visualize the Olympics 2,000 years ago and to understand significant events in Grecian heritage. In museum applications, 3D models of artifacts are created and rendered in an AR environment through photo-realistic and efficient 3D modeling (White et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="White M, Liarokapis F, Darcy J, Mourkoussis N, Petridis P, Lister PF (2003) Augmented reality for museum artefact visualization. In: 4th irish workshop on computer graphics, Eurographics Ireland Chapter. pp 75–80" href="/article/10.1007/s10055-010-0164-8#ref-CR35" id="ref-link-section-d68916e397">2003</a>). Viewing text, images, and 3D models in the same AR environment enhances the user’s experience. Additionally, in traditional architecture applications, users can understand the traditional structure and culture of the “Young’s Ancestral Hall” in Taiwan using AR technology (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Chen CH, Hsu MF, Lee IJ, Sun KW, Lin YC, Lee WH (2009) Applying augmented reality to visualize the history of traditional architecture in Taiwan. 22nd CIPA Symposium" href="/article/10.1007/s10055-010-0164-8#ref-CR5" id="ref-link-section-d68916e400">2009</a>).</p><h3 class="c-article__sub-heading" id="Sec4">Related work for Digilog book</h3><p>Many researchers have examined AR books, beginning with “The MagicBook” (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magic book: a transitional ar interface. IEEE Comput Graph 25(5):745–753" href="/article/10.1007/s10055-010-0164-8#ref-CR4" id="ref-link-section-d68916e411">2001</a>), which demonstrates the potential and usability of AR books as new-generation media. In this application, a user holding a handheld display that contains a small camera can experience the full reality of a virtual continuum through a paper book. Since Billinghurst’s work was published, other AR books have been created and proposed to teach details of earth-sun relationships in undergraduate geography courses (Shelton and Hedley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Shelton B, Hedley N (2002) Using augmented reality for teaching Earth-Sun relationships to undergraduate geography students. First IEEE Intl. Augmented Reality Toolkit workshop. pp 1–8" href="/article/10.1007/s10055-010-0164-8#ref-CR28" id="ref-link-section-d68916e414">2002</a>). In this system, viewing interaction uses the head-mounted display (HMD), which holds a USB camera. In educational software for Kanji Learning (Wagner and Barakonyi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Wagner D, Barakonyi I (2003) Augmented reality kanji learning. In: Proceedings of the 2nd IEEE/ACM symposium on mixed and augmented reality (ISMAR). pp 335–336" href="/article/10.1007/s10055-010-0164-8#ref-CR34" id="ref-link-section-d68916e417">2003</a>), multiple users holding PDA viewers can play a card game in a collaborative AR environment. Saso et al. propose an AR storytelling book, “Little Red,” which illustrates the story of Little Red Riding Hood (Saso et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Saso T, Iguchi K, Inakage M (2003) Little red: storytelling in mixed reality. SIGGRAPH Sketches and Applications, pp 1–1" href="/article/10.1007/s10055-010-0164-8#ref-CR25" id="ref-link-section-d68916e420">2003</a>). Users can partly control their interactions with the book by using their hands and wearing HMDs with USB cameras. The user can change the story and its ending. In the Vivid Encyclopedia’s “MR Pictorial Book of Insects” (Shibata et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Shibata F, Yoshida Y, Furuno K, Sakai T, Kiguchi K, Kimura A, Tamura H (2004) Vivid encyclopedia: MR pictorial book of insects. In: the 9th VR society of Japan annual conference. pp 611–612" href="/article/10.1007/s10055-010-0164-8#ref-CR29" id="ref-link-section-d68916e423">2004</a>), the user can interact with virtual insects using a 3D magnetic tracker. “EyeMagic Book” (McKenzie and Darnell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="McKenzie J, Darnell D (2004) The eyeMagic book. A report into augmented reality storytelling in the context of a children’s workshop 2003. New Zealand Centre for Children’s Literature and Christchurch College of Education" href="/article/10.1007/s10055-010-0164-8#ref-CR19" id="ref-link-section-d68916e427">2004</a>), a storytelling book application, uses augmented reality storytelling in the context of a children’s workshop. A user in front of the application’s kiosk can view a 3D object and various animations using a handheld display that has a small camera. In the “AR Volcano Kiosk” (Woods et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Woods E, Billinghurst M, Looser J, Aldridge G, Brown D, Garrie B, Nelles C (2004) Augmenting the science centre and museum experience. In: 2nd international conference on computer graphics and interactive techniques in Australasia and SouthEast Asia (Graphite). ACM Press, New York, 230–236" href="/article/10.1007/s10055-010-0164-8#ref-CR37" id="ref-link-section-d68916e430">2004</a>), a user can interact with augmented 3D objects through a turntable with a slider while holding a display that has a small camera. “Little Feet and Big Feet” and “Looking for the Sun,” by Dünser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Dünser A, Hornecker E (2007) An observational study of children interacting with an augmented story book. In: The 2nd international conference of e-learning and games (Edutainment). pp 305–315" href="/article/10.1007/s10055-010-0164-8#ref-CR8" id="ref-link-section-d68916e433">2007</a>), are AR storybook experiences created for young children. In this system, a web-cam is mounted on top of a computer screen. As such, it can be used in most modern classrooms or homes. For interaction, users use a paddle to trigger certain events and a mouse to click on navigation buttons that advance to the next or previous text page and allow the child to listen to the story.</p><p>Another AR book by Grasset et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Grasset R, Duenser A, Billinghurst M (2008) Edutainment with a mixed reality book: a visually augmented illustrative childrens’ book. In: International conference on advances in computer entertainment technology (ACE)" href="/article/10.1007/s10055-010-0164-8#ref-CR10" id="ref-link-section-d68916e439">2008</a>) is based on “The House that Jack Built;” with this storytelling book, a user can experience 2D and 3D animation and 3D sound through a handheld display and a small camera. Paddle interaction and gaze interaction are supported. Zhou et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Zhou Z, Cheok AD, Tedjokusumo J, Omer GS (2008) wIzQubesTM- a novel tangible interface for interactive storytelling in mixed reality. Int J Virtual Real 7(4):9–15" href="/article/10.1007/s10055-010-0164-8#ref-CR40" id="ref-link-section-d68916e442">2008</a>) present wIzQubes™, a novel user interface that supports interactive storytelling in an AR environment. It enhances the corporeal interactions of traditional AR books by using a foldable cube to “turn” the “page” in a monitor-based display system with different viewpoints between the user and the camera. Two cubes are used to navigate through various scenes of the story and choose different items that are needed in the story. The Eye of Judgment is an AR card game based on a monitor-based display that presents various viewpoints between the user and the camera (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference The Eye of Judgment" title="The Eye of Judgement, &#xA;                    http://us.playstation.com/games-and-media/games/the-eye-of-judgment-ps3.html&#xA;                    &#xA;                  , 2007. Access date: June 2010" href="/article/10.1007/s10055-010-0164-8#ref-CR32" id="ref-link-section-d68916e445">The Eye of Judgment</a>). Interaction is conducted by positioning cards in a layout with proximity of distance. The “Virtual pop-up book” by Taketa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Taketa N, Hayash K, Kato H, Nishida S (2007) Virtual pop-up book based on augmented reality. HCI, pp 475–484" href="/article/10.1007/s10055-010-0164-8#ref-CR31" id="ref-link-section-d68916e448">2007</a>) provides two kinds of display methods. The PC monitor display type is best suited for use by children who find HMD too cumbersome or for demonstrations in which many users see the content simultaneously. On the other hand, the HMD type gives the user a sense of immersion. Viewing with turning-page interaction is also supported. “The Haunted Book,” by Scherrer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Scherrer C, Pilet J, Fua P, Lepetit V (2008) Haunted book. ISMAR, pp 163–164" href="/article/10.1007/s10055-010-0164-8#ref-CR26" id="ref-link-section-d68916e451">2008</a>), is an interesting AR artwork application that is based on a poem. It features 2D animated illustration augmentation instead of 3D objects. In this installation, a computer screen is used to avoid the restrictions of wearing the HMD. This system also provides simple viewing (with turning pages) interaction. Cho et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Cho K, Lee J, Soh J, Lee J, Yang H (2007) A realistic e-learning system based on mixed reality. In: The 13th Intl conference on virtual systems and multimedia. pp 57–64" href="/article/10.1007/s10055-010-0164-8#ref-CR6" id="ref-link-section-d68916e455">2007</a>) and Yang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Yang H, Cho K, Soh J, Jung J, Lee J (2008) Hybrid visual tracking for augmented books. In: the 7th international conference on entertainment computing. pp 161–166" href="/article/10.1007/s10055-010-0164-8#ref-CR39" id="ref-link-section-d68916e458">2008</a>) have suggested creating an e-learning system application through AR books, targeting elementary school students. Viewing with turning pages is a supported interaction in the project.</p><h3 class="c-article__sub-heading" id="Sec5">Characteristics of Digilog book</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Features of the AR-based experience environment</h4><p>AR visually provides additional and meaningful virtual content (information) about a practically observed object in a current situation. On the other hand, in a virtual reality (VR)-based experience environment, all scenes are represented to the user as virtual objects in a specific computing space (e.g., CAVE or VR simulation environment). Therefore, AR can support more enhanced immersion and realism in real space because the input images used as background in an AR environment can achieve a more realistic and familiar effect than in VR. Additionally, a user can directly manipulate augmented information about a real object in a real environment in order to gain a more in-depth understanding of the real object. Many advantages of an AR-based experience system have been reported (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma R (1997) A survey of augmented reality presence. 6, pp 355–385" href="/article/10.1007/s10055-010-0164-8#ref-CR2" id="ref-link-section-d68916e473">1997</a>).</p><p>Based on the literature, this project has developed four characteristics for the Digilog book.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">AR content descriptions for updatable, multisensory AR content</h4><p>Content-sharing or collaborative content generation by Internet users has become common. Previously implemented virtual content publication, however, would be difficult to update in conventional AR book applications. If content modification occurred, then additional modeling and programing works would be inevitable.</p><p>To solve this problem, this project proposes a file format for AR content descriptions in which the AR content of the Digilog Book will be updatable through an Internet connection if AR content in a remote database is updated. The suggested file format also includes several properties for visual, audio, and haptic feedback. This is necessary for an immersive interactive experience with multisensory feedback.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Enhanced multisensory feedback to support an immersive experience</h4><p>Most of the feedback in the conventional AR book application is visual (e.g., seeing the book), spoken (e.g., narration), and tactile (e.g., turning pages). This paper adds vibration feedback via a vibration module embedded in the proposed 3D manipulation tool. This can motivate an immersive experience through visual, audio, and enhanced haptic feedback, which conventional paper books cannot provide.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">A natural, computerized, vision-based bare-hand input method for an interactive experience</h4><p>This paper proposes an input method for natural interaction with AR content that requires only the user’s bare hands. In an early iteration of an occlusion-based virtual button, the user occludes a fiducial marker with his or her hands in order to trigger an event (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lee G, Billinghurst M, Kim J (2004) Occlusion based interaction methods for tangible augmented reality environments. In: ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry (VRCAI). pp 419–426" href="/article/10.1007/s10055-010-0164-8#ref-CR17" id="ref-link-section-d68916e502">2004</a>). This project suggests an improved method in which unnatural markers need not be inserted on the book pages and proposes three additional types of virtual buttons that support various input types with respect to interaction characteristics.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Concrete interaction design</h4><p>The target users for this project are typical users with home PCs. While considering a practical running environment, a monitor-based display installation was designed with a USB camera fixed on a table in line with the direction of a user for natural ergonomic viewing. The monitor-based display system used is less expensive and more popular than a head-mounted display (HMD) system, and the designed manipulation tool is simply a modification and extension of a conventional mouse input device. Therefore, this is the proper choice for home users in their normal computer environments.</p><p>In order to verify the feasibility of implementation, this project conducted informal user observation and interviews. Lessons and future directions of the Digilog Book are summarized in the discussion section.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Digilog book</h2><div class="c-article-section__content" id="Sec11-content"><h3 class="c-article__sub-heading" id="Sec12">System overview</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig1">1</a> shows the Digilog Book system architecture based on the proposed interactive AR system. Based on input images from a camera, a computer vision-based tracking manager recognizes and tracks a paper book, a manipulation tool, and a hand object. The collision detector then inspects penetrations between a virtual line created by the manipulation tool and a bounding volume of the augmented 3D objects that are based on the book. The detector also checks an occluded area between the virtual buttons and the user’s hand objects. At this point, the interaction interpreter conducts examinations like 3D object pointing and hitting, movement interactions, or hand interactions for pushing virtual buttons.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Block diagram of Digilog Book viewer</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Next, the content manager composes proper multisensory content in order to react to the user input. The multisensory content consists of visual feedback (text, images, video clips, and 3D models), audio feedback (effects and background sounds), and haptic feedback (vibro-tactile interactions). Finally, a display device, a speaker, and a vibrator of the manipulation tool represent the multisensory content. Additionally, if AR content in a remote database is updated, then the AR content of the Digilog Book will be updatable through an Internet connection.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig2">2</a> shows a conceptual flow of Digilog Book usage, from purchase to interactive AR experience. A user purchases a Digilog Book and installs a Digilog Book viewer on a home computer. While the viewer is running, based on captured images from a camera, the computer detects a specific fiducial of the book, computes the camera’s coordinate relation from the book, and sets the basis coordinate. The 3D models, video clips, sounds, or other content are augmented based on the basis coordinate. While tracking the fiducial, the content is still augmented in the book. The user interacts with the augmented virtual content using a manipulation tool or virtual buttons, and he or she can simultaneously see, hear, and physically sense the augmented content from the paper book. The AR content is updatable if an Internet connection is available.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Conceptual figure of Digilog Book usage</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In this real demonstration, the setting, monitor, camera, and book were aligned, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig3">3</a> and were all in the user’s direct view. The camera view faced the book so that the entire book area was contained within the camera’s viewing angle.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Running environment of Digilog Book viewer; the monitor, camera, and book were aligned</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Target user/content/interaction</h4><p>The target user is an average computer user who has no related AR, computer vision- or graphics-related knowledge, or programing skills. Specifically, the target reader is a child in the elementary-school age range. As an example of a Digilog Book, this project proposes a temple bell experience book that explains Asian cultural heritage in a way that provides multisensory feedback and natural interaction. For example, users can translate or rotate a virtual temple bell, point to facets of the temple bell to play multimedia documents, or hear a tolling bell and sense vibration feedback by tapping the 3D bell model. This type of immersive experience can motivate readers to seek cultural heritages for education and entertainment purposes. For the user interface to be easy and intuitive, the user should be able to understand its potential simply by adopting familiar metaphors for interaction without resorting to a manual.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Environment/hardware</h4><p>The Digilog Book viewer is executed in an arbitrary indoor environment with sufficient lighting conditions to recognize the computer vision-based fiducial pattern. The only special requirements for the program are a desk (for laying out a book), a camera (for acquiring book images), and a computer (for processing the input images). An ordinary low-cost USB camera is fixed on a camera arm that is attached to the desk. This monitor-based AR display installation is suitable because it can be used in most home environments without any expensive additional hardware.</p><p>A wearable display type (e.g., optical see-through, video see-through) has several disadvantages; its cost is relatively high compared to that of the monitor-based AR display, and wearing the HMD or holding the device may cause arm or wrist fatigue or even dizziness. Instead, because the camera is moving, 3D object observation is possible from arbitrary positions. Tracking errors, however, can be increased as a result of camera movement.</p><p>For tracking the book and the manipulation tool, this project uses fiducial pattern markers [ARToolKit]. Although this method produces limited accuracy compared to an infrared optical or magnetic tracking method, its cost is much lower and it is easier for an average user to use.</p><h3 class="c-article__sub-heading" id="Sec15">System design enabling AR interaction with multisensory feedback</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">AR content descriptions for updatable multisensory AR content</h4><p>This paper proposes an AR content description that will update previously implemented virtual content from AR book publication in response to users’ changing requirements. Therefore, the AR content of a Digilog Book could be updatable through an Internet connection if AR content in a remote database is updated. The suggested file format also includes several properties for visual, audio, and haptic feedbacks. This is necessary for an immersive, interactive experience with multisensory feedback.</p><p>Previous works suggest XML-based data descriptions or customized markup language to construct an AR scene (Irawati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Irawati S, Ahn S, Kim J, Ko H (2008) VARU Framework: enabling rapid prototyping of VR, AR and ubiquitous applications. IEEE Virtual Reality Conference. pp 201–208" href="/article/10.1007/s10055-010-0164-8#ref-CR14" id="ref-link-section-d68916e633">2008</a>; Lee and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee J, Ha T, Ryu J, Woo W (2009) Development of pen-type haptic user interface and haptic effect design for Digilog book authoring. KHCI, pp 402–405" href="/article/10.1007/s10055-010-0164-8#ref-CR18" id="ref-link-section-d68916e636">2009</a>). However, there is a critical limitation on extension and compatibility with existing international standards. To solve this problem, this project exploits Extensible 3D (X3D). X3D is a software standard for describing an interactive virtual environment, and it is the successor of the Virtual Reality Modeling Language (VRML). X3D features 2D/3D graphics, animation, spatialized audio and video, user interaction, navigation, user-defined objects, scripting, networking, physical simulation, CAD geometry, programmable shaders, and particle systems. Among these functions, networking can compose a single X3D scene from assets located on a network. By setting a Uniform Resource Locator (URL) field and an activating condition, remote content can be loaded.</p><p>Visual descriptions of a Digilog Book could contain properties such as translation, rotation, scaling of the augmented 3D model, text, figures, video clips, and even color, texture, and lighting. For audio feedback, the Digilog Book should support auditory descriptions, such as effects, background sound, volume, sound direction, and tempo, in response to user interaction. 3D audio effects may be required for a more realistic or impressive experience. Most visual and audio effects can be described by functions of X3D support. Haptic feedback properties are not currently supported by X3D standard, but they are possible with the extended description of Digilog Book. The considered vibration modules are embedded internally in the manipulation tool. Vibration modules are activated when a user causes a collision between some virtual objects. In the Digilog Book, the vibration module of the manipulation tool can be activated depending on the collision direction and speed with respect to a 3D object. The intensity, direction, and duration time of each vibration should be described.</p><p>The Digilog Book could also include other properties, including a state and action trigger, events that occur with respect to distance between a 3D object and the manipulation prop, and collision detection so that when the user pushes the virtual buttons of the book, the corresponding reaction is activated.</p><p>By maintaining compatibility with existing X3D, the proposed description can be interpreted through the X3D viewer with limited functions. It can be viewed with full functions by adopting the proposed namespace, Extensible Markup Language (XML) schema definition (XSD), and implementation modules. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0164-8#Tab1">1</a> shows extended nodes and fields of the Digilog Book that are not contained in existing X3D. The X3D can be represented by a variety of encoding formats, including XML. The implementation section shows an XML example for an AR scene using the extended description.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Field definitions (specifications) for Digilog book data format (following the conventions of the ISO/IEC 19775)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0164-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Design of a manipulation tool that provides vibration feedback with respect to interactions</h4><p>For the average non-professional user in a normal computer environment, this project uses a cubical box with a virtual line that is attachable to a mouse input device. The mouse can easily be exploited in a normal computer environment. By using the mouse input buttons, a discrete input generates the event functions that are necessary for Digilog Book interaction. In addition, multiple fiducial patterns are printed on the box, enabling the manipulation tool tracking at arbitrary camera viewing angles and simultaneously providing continuous input for modifying a 3D object’s position and rotation properties (Ha and Woo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ha T, Woo W (2010) An empirical evaluation of virtual hand techniques for 3d object manipulation in a tangible augmented reality environment. IEEE 3D User Interfaces, pp 91–98" href="/article/10.1007/s10055-010-0164-8#ref-CR11" id="ref-link-section-d68916e1194">2010</a>).</p><p>This paper also suggests a vibro-tactile, haptic user interface. A mobile vibration module is used to enhance immersion when using the Digilog Book. Considering its portable, lightweight design, the vibration module can be embedded in the manipulation tool, where it directly stimulates the user’s skin by providing vibro-tactile feedback. The user can select several vibration patterns for clicking, dragging, translating, rotating, pointing, or tapping events. The patterns consist of vibration waves in various shapes, such as a square wave, a triangle wave, a sin wave, and other shapes of waves (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lee J, Ha T, Ryu J, Woo W (2009) Development of pen-type haptic user interface and haptic effect design for Digilog book authoring. KHCI, pp 402–405" href="/article/10.1007/s10055-010-0164-8#ref-CR18" id="ref-link-section-d68916e1200">2009</a>).</p><p>The users can obtain additional information about the specific elements of the 3D object by pointing to the corresponding parts with the manipulation tool. Visual annotations on various parts of the 3D object indicate a specific viewing point for figures, text, and video clips. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig4">4</a>(a) shows the translated/rotated annotation points from the 3D object and an augmented line tip (<b>M</b>
                              <sub>Tip</sub>: pose matrix of the tip) on the front part of the prop (manipulation tool). If the Euclidian distance between these points is within a threshold distance, then a collision can occur. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig4">4</a>, <b>R</b> is a rotation and <b>T</b> is a translation matrix.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Pointing and hitting interactions: <b>a</b> pointing method and <b>b</b> tapping method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>To design the hitting interaction, a rigid body from the Open Dynamics Engine (ODE) is fitted onto the virtual object A (obj1) and onto another object B (obj2), and a real-time collision process is conducted between them. Using a drag-and-drop technique familiar to the user, 3 axis positions and 3 axis rotation manipulations (6DOF manipulation) of a 3D object are easily and simultaneously performed in the book page plane, even in a 3D space such as air. Thus, a user can intuitively select and move the 3D object. If the B object collides with the A object, then the Digilog Book viewer system provides the corresponding visual, sound effect, and tactile vibration feedback to the user.</p><p>For the 6DOF manipulation, the collision detector first checks penetration depth between the virtual line at the front of the manipulation tool and a 3D object. Then, the user presses and holds a button for the manipulation tool to translate the 3D object to the wanted position. During the drag state, a 3D object position is obtained from the augmented sphere (<b>M</b>
                              <sub>Sphere</sub>) at the current frame, and rotation is set relative to the rotation of the sphere (<b>M</b>
                              <sub>Sphere</sub>) from the previous frame to the current frame. Equitation 1 shows these relationships, where <b>R</b> is a rotation and <b>T</b> is a translation matrix of the corresponding pose matrix (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig5">5</a>).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ {\text{M}}_{\text{Obj(i)}} = {\text{R}}\left( {{\text{M}}_{{{\text{Obj}}({\text{i}} - 1)}} } \right)\left[ {{\text{R}}\left( {{\text{M}}_{{{\text{Sphere}}({\text{i}} - 1)}} } \right)^{ - 1} {\text{R}}\left( {{\text{M}}_{{{\text{Sphere}}({\text{i}})}} } \right)} \right]{\text{T}}\left( {{\text{M}}_{{{\text{Sphere}}({\text{i}})}} } \right) $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>3D object translation and rotation through drag and drop</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Virtual button interaction using a user’s bare finger</h4><p>Normally, virtual objects are vertically augmented on the base plane of a book; the book is on a table, and a fixed camera faces the book. In this arrangement, however, the view direction of the camera is from top to bottom; users observe the top and the forward (or backward) side of the augmented 3D object. Therefore, interaction methods are required to rotate the 3D objects as well as to trigger other interaction scenarios.</p><p>This paper adopts a virtual button interface that is natural, simple, and easy to control because it requires only the user’s bare hands. In an early iteration of an occlusion-based virtual button, a user needed to occlude a fiducial marker with his or her hands in order to trigger an event (Lee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lee G, Billinghurst M, Kim J (2004) Occlusion based interaction methods for tangible augmented reality environments. In: ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry (VRCAI). pp 419–426" href="/article/10.1007/s10055-010-0164-8#ref-CR17" id="ref-link-section-d68916e1316">2004</a>). This project suggests an improved method in which unnatural markers need not be inserted into the book pages.</p><p>This method invisibly inserts virtual 3D buttons on the button area printed on the book. The buttons are projected onto a 2D image and are set by a region of interest (ROI). From this region, the finger objects segmentation process is run. Estimation of a color probability density function of the finger objects can then be conducted in real time to enable the stable segmentation of the finger objects under changing light conditions and to avoid the manual parameter settings (Ha et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ha T, Kim Y, Ryu J, Woo W (2006) Enhancing Immersiveness in AR-based Product Design. LNCS (ICAT), pp 207–216" href="/article/10.1007/s10055-010-0164-8#ref-CR12" id="ref-link-section-d68916e1322">2006</a>). If the finger object area is over a specific threshold, then the push button event is initiated, which triggers reactions among the 3D objects (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Flow chart for virtual button interaction using a user’s bare finger</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Three types of virtual buttons are also proposed to support various input types with respect to interaction characteristics. As shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-010-0164-8#Tab2">2</a>, three kinds of virtual buttons are considered for treating events in the Digilog Book: the toggle button, the push button, and the auto-releasable toggle button. Each button type is used in a particular situation. The toggle button changes the interaction mode, the push button is needed for an interaction that uses an instant time event, and the auto-releasable toggle button activates an interaction once the previous reaction completes (creating animation), similar to a synchronous event.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Three types of virtual buttons and their functions</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-010-0164-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Temple bell experience book</h2><div class="c-article-section__content" id="Sec19-content"><h3 class="c-article__sub-heading" id="Sec20">Making the temple bell experience book</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Concept of the temple bell experience book</h4><p>The temple bell experience book is expected to encourage readers to explore cultural heritages for education and entertainment purposes. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig7">7</a> presents the patterns, shapes, qualities, and carvings of Korea’s temple bells. The dragon head hangs on the bell tower, and the acoustic tube opposite the dragon head helps the sound of bell travel long distances. The lower band consists of repeated patterns. The echoing hollow is arranged to expand the reverberation with the least possible energy. The Yu-kwack consists of the Yu-kwack bands and the group of Yu-Tus, which resemble stalactites [Korean bells]. By using the Digilog book, readers can have enhanced multisensory and interactive experiences of the temple bells.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The Bell of King Seongdeok</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">Temple bell content</h4><p>In next step, multimedia content was created, including the Digilog Book, 3D models, 3D model mesh reductions, texture maps, sound effects, video clips, images, and other multimedia content. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig8">8</a> shows several 3D bell models that are used in this Temple Bell Experience Book. These scanned 3D models are supported by the documentation project for important movable cultural assets and by the cultural heritage administration of Korea [Cultural Heritage Administration of Korea].</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Several 3D models of “The Temple Bell of Korea” Digilog Book</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Book design</h4><p>Rectangular ARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="ARToolKit, &#xA;                    http://www.hitl.washington.edu/artoolkit&#xA;                    &#xA;                  . Access date: June 2010" href="/article/10.1007/s10055-010-0164-8#ref-CR1" id="ref-link-section-d68916e1506">2010</a>) markers often cause disjointed relationships in a book’s contents; this awkward visual display might diminish naturalism. From the book’s contents, this project registers a related figure as a fiducial marker so that it is as realistic as possible among the background text and figures. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig9">9</a> shows several of a total of 14 pages of the Temple Bell Experience Book. The left side shows text, a 4 × 4 cm marker, and figures, while the right side shows the temple bell figure and button images.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Several pages of “The Temple Bell of Korea” book: <b>a</b> cover, <b>b</b> outline, and <b>c</b>–<b>d</b> temple bell contents</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec24">Implementation and results of interaction techniques of the temple bell experience book</h3><p>This subchapter explains the implementation and results of the proposed interaction techniques. The program was executed in a normal indoor environment with no dramatic changes in lighting. The camera was a general-purpose USB camera that was fixed on the camera arm and captured 30 frames per second at 640 × 480 pixel resolution. The computer was equipped with a 2.40 GHz CPU and 4 GB of memory. The osgART library (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference osgART" title="osgART, &#xA;                    http://www.artoolworks.com/community/osgart&#xA;                    &#xA;                  , Access date: June 2010" href="/article/10.1007/s10055-010-0164-8#ref-CR23" id="ref-link-section-d68916e1552">osgART</a>) was used to support the rendering of scene-graph structured graphic models and the computer vision-based tracking functions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">AR content description with updatable multisensory feedback</h4><p>This prototype provides an ideal platform for testing AR authoring that allows designers to vary the graphic, audio, and haptic feedback. One example of a description file based on X3D is represented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig10">10</a>. In the Digilog Book node, if the load field is set to TRUE value, the X3D file described by the URL field can be loaded immediately.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>An example of node syntax, a file format for interactive AR with multisensory feedback based on X3D standard</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Sequential steps of the start-interaction-end process</h4><p>From the Digilog book viewer’s initial state, the visual special effects are enhanced. The temple bell model is naturally augmented from the page of a book through vertical translation and horizontal rotation interpolation. Lastly, in the interaction step, the user can receive a detailed explanation or can manipulate the augmented temple bell model by rotating or hitting the bell.</p><p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig11">11</a>, figures of books are 2D images, and augmentation objects are 3D models. For natural registration, geometrical consistency of appearance is required between the figures in the book and the 3D virtual objects. As a partial solution, this implementation interpolates rotations and translations between the 2D image figure and the augmented object. Specifically, the 3D temple bell model vertically rises from the 2D image at the same scale and then rotates frontward toward the user. During this interpolation process, the addition of a particle object enhances the lighting, making the visual effect more interesting and somewhat compensating for the inaccurate matching between the figures and the augmenting 3D models. This particle object is rendered through the alpha blending of a total of 150 texture sequences. In addition, the augmented 3D temple bell model repeatedly moves from left to right and vice versa, based on a vertical axis; this movement may compensate for the jittery visual effects caused by unstable marker tracking.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>The sequential steps of the start-interaction-end process for the temple bell experience book</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">Manipulation tool that supports vibration feedback</h4><p>A commercial pen mouse (wireless presenter) was adopted as a prototype for the implemented manipulation tool. This was attached to a box with multiple markers fixed on its front. The marker size was 3 × 3 cm, which was determined by the length between the user’s viewpoint and the recognizable marker when the user extended an arm while holding the manipulation tool. The internal components of the manipulation tool consisted of a vibration actuator, a Bluetooth communication module for wireless communication, and a microcontroller for tactile feedback generation and wireless communication (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>The internal components of the manipulation tool included a vibration actuator, a Bluetooth communication module, and a microcontroller</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig13">13</a>(a), white spheres are augmented on specific parts of the temple bell model, and these provide visual annotations on the heads-up display (HUD) in order to indicate areas that contain information about the bell, including figures, texts, and video clips. By pointing to them with a manipulation tool, users can obtain this additional information. In the event that a user hits the bell, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig13">13</a>(b), a cylindrical wooden model is selected and moved to the temple bell model with the manipulation tool. When the wooden model collides with the bell model, a bell sound is played.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Pointing and hitting interactions: <b>a</b> specified explanations obtained by pointing to specified parts of the temple bell; <b>b</b> using a manipulation tool to hit the temple bell</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec28">Virtual button input triggered by the user’s finger</h4><p>Four 3D buttons are projected onto a 2D image, setting the ROI as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig14">14</a>. If a finger covers more than 2/3 of the corresponding button area, then a push button event occurs. Computerized vision-based image processing is sequentially processed from the first button to the last button in an attempt to verify whether or not the user’s hand is covering the printed button area of the book. If the system determines that the user is pressing a certain button and an evaluation of the remaining buttons can be omitted, then the image processing time can be shortened.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Virtual button interaction: <b>a</b> virtual buttons are projected as a 2D image, which sets the region of interests (<i>red boxes</i>); <b>b</b> the corresponding chromatic image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In the proposed interaction scenario, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-010-0164-8#Fig15">15</a>, the first button is a toggle-type button; once it is selected, it remains in a selected state until another button is selected. This button is used to change interaction mode, i.e., showing detailed explanations of specific parts of the temple bell with a manipulation tool. The second and third buttons are push-button types that trigger the rotation of the 3D model as long as the buttons are being pushed. The fourth button is an auto-releasable toggle button that is released as soon as the hitting bell event is triggered.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-010-0164-8/MediaObjects/10055_2010_164_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Selection of virtual buttons, using a user’s finger: <b>a</b> The second and third buttons are push-button types that trigger the rotation of the 3D model (visible 3D buttons are on the book for confirmation); <b>b</b> The fourth button is an auto-releasable toggle button that triggers the hitting bell event (invisible 3D buttons for enhanced naturalism)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-010-0164-8/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec29"><div class="c-article-section" id="Sec29-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec29">Discussion and conclusion</h2><div class="c-article-section__content" id="Sec29-content"><h3 class="c-article__sub-heading" id="Sec30">Learning from informal user observation and interviews</h3><p>This project has collected valuable comments on the Temple Bell Experience Book during many demonstrations and exhibitions over the last 2 years. The section summarizes the learning gained from observation to interviews with informal users.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec31">Input device</h4><p>This paper discusses a fiducial marker-based manipulation tool as a prototype. Recognition and tracking of the manipulation tool could show better performance at a specific angle between the marker and camera, but this would require user learning time. In addition, prolonged use of the manipulation tool can cause wrist fatigue. As current technology develops, a novel and more comfortable pen-shaped manipulation tool will be available. Additionally, the manipulation tool should be inexpensive so that non-professional users can use it in a home environment, not just as a magnetic tracker-embedded pen in a laboratory.</p><p>Although this paper introduces the advantages of direct manipulation with the user’s bare hand and with the manipulation tool, another type of user input method is needed to consider the user’s requirements. For some readers, a 3D user interface with a manipulation tool would be difficult to control, and a 2D user interface with a mouse input device would be faster and easier to control. Therefore, user preferences and task characteristics for interaction scenarios should be reflected by AR input methods.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec32">Content and interaction</h4><p>A virtual 3D object should be naturally augmented on figures or layouts of a 2D page. The Digilog Book provides visual special effects and the translation, rotation, and interpolation of a 3D model from a 2D paper with corresponding position and scale in order to provide more natural augmentations. In addition, the background figure in a book may recede to a vanishing point; in such cases, an augmented 3D object should have a corresponding perspective so that augmentation appears natural.</p><p>The Digilog Book should support interesting functions that a paper book cannot provide. The main advantage of a Digilog Book is its provision of visual, audio, and haptic information to a reader. In addition to the methods proposed in this paper, more interesting interactions will be developed, such as explanations of magnified figures or a slice-side view of specific parts of the temple bells.</p><p>As a new interface and manipulation method, the Digilog Book can help to pique readers’ interest; conversely, however, a user could focus only on the 3D object manipulation rather than on the textual book content. To enforce the Digilog Book’s usefulness, manipulation and book content should be arranged in a complementary fashion and offered to the user.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec33">Running environment</h4><p>The environmental factors of the locations in which the Digilog Book will be used should be considered. Recognition and tracking of fiducial patterns mainly depends on lighting conditions, and also require fine-tuning of the position, distance, and angle between the camera and the fiducial patterns for good performance. One solution is to fix a camera and a book (with printed fiducial patterns) by providing some indications or marks for stable working. For example, in the AR game Eye of Judgment, the camera position is marked as a rectangle on the game pad.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec34">Tracking pattern</h4><p>Fiducial markers have been involved in many cases of unstable tracking, which occurs when a maker is occluded by a user’s hands or a manipulation tool on the book as the user interacts with the augmented objects. To address this issue, many related works have adopted natural feature tracking (NFT). However, depending on the book page’s layout and contents, it may be difficult to extract good features and proper tracking for similar patterns (e.g., text majority), dark figures, and reflective material. These are challenging research issues.</p><h3 class="c-article__sub-heading" id="Sec35">Future direction of the Digilog book</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec36">User and environment context awareness</h4><p>User context awareness, achieved through reader-sensing technologies, could provide personalized interactive stories according to the reader’s profile (age, gender, etc.), gesture recognition, eye tracking, physiological sensing, and more. For example, if a reader’s emotional states while reading a book could be recognized, then the book could unfold as a personalized interactive story. The Digilog Book is not fixed like paper books; rather, its story can change to remain interesting and relevant.</p><p>While a user reads a Digilog Book, input devices or the user’s hands could cast shadows on the book. This reduces naturalism in some situations because it can create conflict with the virtual shadows. To solve this problem, a light-source estimation technique may be useful. This technique measures a light’s position in order to generate adaptive shading that mimics the natural shadows in the room where a user is reading the book.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec37">Mobile platform</h4><p>The Digilog Book viewer is in its early stages of development, and it is currently configured for a desktop computer. As technologies advance in the near future, a portable device for mobile AR will become a general configuration. Using a portable device with any available camera and accessible wireless Internet, a reader could acquire multimedia content from a paper book at any time and in any location. New interaction techniques are also needed to relate to mobile devices with limited display sizes and slightly different input methods.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec38">AR authoring tool</h4><p>Production of a Digilog Book only can be accomplished by developers and researchers who have a technical background in AR programing. Typical users who lack technical knowledge about programing cannot easily make the Digilog Book. Existing AR authoring tools have been developed to create standard augmented reality applications. The programing-based AR authoring interface primarily uses script languages like programing API and XML for programmers; these are difficult to learn, and they require significant financial and time investment for normal users. Additionally, most users do not specialize in authoring functions for AR-based books. Therefore, this project is pursuing an effective method for producing the Digilog Book that is accessible even to users whose programing knowledge is non-professional.</p><h3 class="c-article__sub-heading" id="Sec39">Conclusion</h3><p>A Digilog Book stimulates a user’s visual, sound, and tactile sensations through the fusion of multimedia content. In particular, the Digilog Book can provide information that conventional paper books cannot. This project has added the following characteristics to the Digilog Book: multisensory AR content descriptions for updatable AR content; enhanced multisensory feedback to support an immersive experience; a natural, computerized, vision-based bare-hand input method for an interactive experience; and an interaction design. In order to verify the usability of these developments, this project observed and interviewed casual users. The paper also summarized the future directions of the Digilog Book.</p><p>As an example of a Digilog Book, this paper introduced the concept and interactive implementation of a temple bell experience book. The book allows users to explore Asian cultural heritage, and it is valuable and applicable because it provides digitalized information about cultural assets. The temple bell experience book gives its readers an interesting and interactive experience with multisensory feedback. Digilog Books are expected to be used in posters, pictures, newspapers, and sign boards.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ARToolKit, http://www.hitl.washington.edu/artoolkit. Access date: June 2010" /><p class="c-article-references__text" id="ref-CR1">ARToolKit, <a href="http://www.hitl.washington.edu/artoolkit">http://www.hitl.washington.edu/artoolkit</a>. Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Azuma R (1997) A survey of augmented reality presence. 6, pp 355–385" /><p class="c-article-references__text" id="ref-CR2">Azuma R (1997) A survey of augmented reality presence. 6, pp 355–385</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, H. Kato, I. Poupyrev, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I (2001) The magic book: a transitional ar interface. IEEE Comput Graph 25(5)" /><p class="c-article-references__text" id="ref-CR4">Billinghurst M, Kato H, Poupyrev I (2001) The magic book: a transitional ar interface. IEEE Comput Graph 25(5):745–753</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2801%2900117-0" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20magic%20book%3A%20a%20transitional%20ar%20interface&amp;journal=IEEE%20Comput%20Graph&amp;volume=25&amp;issue=5&amp;pages=745-753&amp;publication_year=2001&amp;author=Billinghurst%2CM&amp;author=Kato%2CH&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen CH, Hsu MF, Lee IJ, Sun KW, Lin YC, Lee WH (2009) Applying augmented reality to visualize the history of " /><p class="c-article-references__text" id="ref-CR5">Chen CH, Hsu MF, Lee IJ, Sun KW, Lin YC, Lee WH (2009) Applying augmented reality to visualize the history of traditional architecture in Taiwan. 22nd CIPA Symposium</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cho K, Lee J, Soh J, Lee J, Yang H (2007) A realistic e-learning system based on mixed reality. In: The 13th I" /><p class="c-article-references__text" id="ref-CR6">Cho K, Lee J, Soh J, Lee J, Yang H (2007) A realistic e-learning system based on mixed reality. In: The 13th Intl conference on virtual systems and multimedia. pp 57–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cultural Heritage Administration of Korea, http://english.cha.go.kr. Access date: June 2010" /><p class="c-article-references__text" id="ref-CR38">Cultural Heritage Administration of Korea, <a href="http://english.cha.go.kr">http://english.cha.go.kr</a>. Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dünser A, Hornecker E (2007) An observational study of children interacting with an augmented story book. In: " /><p class="c-article-references__text" id="ref-CR8">Dünser A, Hornecker E (2007) An observational study of children interacting with an augmented story book. In: The 2nd international conference of e-learning and games (Edutainment). pp 305–315</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grasset R, Duenser A, Billinghurst M (2008) Edutainment with a mixed reality book: a visually augmented illust" /><p class="c-article-references__text" id="ref-CR10">Grasset R, Duenser A, Billinghurst M (2008) Edutainment with a mixed reality book: a visually augmented illustrative childrens’ book. In: International conference on advances in computer entertainment technology (ACE)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ha T, Woo W (2010) An empirical evaluation of virtual hand techniques for 3d object manipulation in a tangible" /><p class="c-article-references__text" id="ref-CR11">Ha T, Woo W (2010) An empirical evaluation of virtual hand techniques for 3d object manipulation in a tangible augmented reality environment. IEEE 3D User Interfaces, pp 91–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ha T, Kim Y, Ryu J, Woo W (2006) Enhancing Immersiveness in AR-based Product Design. LNCS (ICAT), pp 207–216" /><p class="c-article-references__text" id="ref-CR12">Ha T, Kim Y, Ryu J, Woo W (2006) Enhancing Immersiveness in AR-based Product Design. LNCS (ICAT), pp 207–216</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Höllerer T, Feiner S, Pavlik J (1999) Situated documentaries: embedding multimedia presentations in the real w" /><p class="c-article-references__text" id="ref-CR13">Höllerer T, Feiner S, Pavlik J (1999) Situated documentaries: embedding multimedia presentations in the real world. ISWC (Third Int. Symp. on Wearable Computers), pp 79–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Irawati S, Ahn S, Kim J, Ko H (2008) VARU Framework: enabling rapid prototyping of VR, AR and ubiquitous appli" /><p class="c-article-references__text" id="ref-CR14">Irawati S, Ahn S, Kim J, Ko H (2008) VARU Framework: enabling rapid prototyping of VR, AR and ubiquitous applications. IEEE Virtual Reality Conference. pp 201–208</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Korean bell’s home page, http://library.thinkquest.org/29141//main.htm, Access date: June 2010" /><p class="c-article-references__text" id="ref-CR15">Korean bell’s home page, <a href="http://library.thinkquest.org/29141//main.htm">http://library.thinkquest.org/29141//main.htm</a>, Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Lee, G. Kim, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Lee G, Kim G (2009) Immersive authoring of tangible augmented reality content: a user study. J Vis Languages C" /><p class="c-article-references__text" id="ref-CR16">Lee G, Kim G (2009) Immersive authoring of tangible augmented reality content: a user study. J Vis Languages Comput 20:61–79</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jvlc.2008.07.001" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20authoring%20of%20tangible%20augmented%20reality%20content%3A%20a%20user%20study&amp;journal=J%20Vis%20Languages%20Comput&amp;volume=20&amp;pages=61-79&amp;publication_year=2009&amp;author=Lee%2CG&amp;author=Kim%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee G, Billinghurst M, Kim J (2004) Occlusion based interaction methods for tangible augmented reality environ" /><p class="c-article-references__text" id="ref-CR17">Lee G, Billinghurst M, Kim J (2004) Occlusion based interaction methods for tangible augmented reality environments. In: ACM SIGGRAPH international conference on virtual-reality continuum and its applications in industry (VRCAI). pp 419–426</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee J, Ha T, Ryu J, Woo W (2009) Development of pen-type haptic user interface and haptic effect design for Di" /><p class="c-article-references__text" id="ref-CR18">Lee J, Ha T, Ryu J, Woo W (2009) Development of pen-type haptic user interface and haptic effect design for Digilog book authoring. KHCI, pp 402–405</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McKenzie J, Darnell D (2004) The eyeMagic book. A report into augmented reality storytelling in the context of" /><p class="c-article-references__text" id="ref-CR19">McKenzie J, Darnell D (2004) The eyeMagic book. A report into augmented reality storytelling in the context of a children’s workshop 2003. New Zealand Centre for Children’s Literature and Christchurch College of Education</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Open Dynamics Engine, http://www.ode.org, Access date: June 2010" /><p class="c-article-references__text" id="ref-CR22">Open Dynamics Engine, <a href="http://www.ode.org">http://www.ode.org</a>, Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="osgART, http://www.artoolworks.com/community/osgart, Access date: June 2010" /><p class="c-article-references__text" id="ref-CR23">osgART, <a href="http://www.artoolworks.com/community/osgart">http://www.artoolworks.com/community/osgart</a>, Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Rauterberg, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Rauterberg M (2006) From personal to cultural computing: how to assess a cultural experience. In: Kempter G, v" /><p class="c-article-references__text" id="ref-CR24">Rauterberg M (2006) From personal to cultural computing: how to assess a cultural experience. In: Kempter G, von Hellberg P (eds) uDayIV–Information nutzbar machen. Pabst Science Publisher, Lengerich, pp 13–21</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=uDayIV%E2%80%93Information%20nutzbar%20machen&amp;pages=13-21&amp;publication_year=2006&amp;author=Rauterberg%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Saso T, Iguchi K, Inakage M (2003) Little red: storytelling in mixed reality. SIGGRAPH Sketches and Applicatio" /><p class="c-article-references__text" id="ref-CR25">Saso T, Iguchi K, Inakage M (2003) Little red: storytelling in mixed reality. SIGGRAPH Sketches and Applications, pp 1–1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Scherrer C, Pilet J, Fua P, Lepetit V (2008) Haunted book. ISMAR, pp 163–164" /><p class="c-article-references__text" id="ref-CR26">Scherrer C, Pilet J, Fua P, Lepetit V (2008) Haunted book. ISMAR, pp 163–164</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Sellen, R. Harper, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Sellen A, Harper R (2003) The Myth of the paperless office. MIT Press, Cambridge" /><p class="c-article-references__text" id="ref-CR27">Sellen A, Harper R (2003) The Myth of the paperless office. MIT Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Myth%20of%20the%20paperless%20office&amp;publication_year=2003&amp;author=Sellen%2CA&amp;author=Harper%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shelton B, Hedley N (2002) Using augmented reality for teaching Earth-Sun relationships to undergraduate geogr" /><p class="c-article-references__text" id="ref-CR28">Shelton B, Hedley N (2002) Using augmented reality for teaching Earth-Sun relationships to undergraduate geography students. First IEEE Intl. Augmented Reality Toolkit workshop. pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shibata F, Yoshida Y, Furuno K, Sakai T, Kiguchi K, Kimura A, Tamura H (2004) Vivid encyclopedia: MR pictorial" /><p class="c-article-references__text" id="ref-CR29">Shibata F, Yoshida Y, Furuno K, Sakai T, Kiguchi K, Kimura A, Tamura H (2004) Vivid encyclopedia: MR pictorial book of insects. In: the 9th VR society of Japan annual conference. pp 611–612</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stricker D, Daehne P, Seibert F (2001) Design and development issues for archeoguide: a augmented reality base" /><p class="c-article-references__text" id="ref-CR30">Stricker D, Daehne P, Seibert F (2001) Design and development issues for archeoguide: a augmented reality based cultural heritage on-site guide. In: International conference on augmented, virtual environments and three-dimensional imaging</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Taketa N, Hayash K, Kato H, Nishida S (2007) Virtual pop-up book based on augmented reality. HCI, pp 475–484" /><p class="c-article-references__text" id="ref-CR31">Taketa N, Hayash K, Kato H, Nishida S (2007) Virtual pop-up book based on augmented reality. HCI, pp 475–484</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="The Eye of Judgement, http://us.playstation.com/games-and-media/games/the-eye-of-judgment-ps3.html, 2007. Acce" /><p class="c-article-references__text" id="ref-CR32">The Eye of Judgement, <a href="http://us.playstation.com/games-and-media/games/the-eye-of-judgment-ps3.html">http://us.playstation.com/games-and-media/games/the-eye-of-judgment-ps3.html</a>, 2007. Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Tosa, S. Matsuoka, B. Ellis, H. Ueda, R. Nakatsu, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2005) Cultural computing with context-aware application: ZENet" /><p class="c-article-references__text" id="ref-CR33">Tosa N, Matsuoka S, Ellis B, Ueda H, Nakatsu R (2005) Cultural computing with context-aware application: ZENetic computer. Lect Notes Comput Sci 3711:13–23</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F11558651_2" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Cultural%20computing%20with%20context-aware%20application%3A%20ZENetic%20computer&amp;journal=Lect%20Notes%20Comput%20Sci&amp;volume=3711&amp;pages=13-23&amp;publication_year=2005&amp;author=Tosa%2CN&amp;author=Matsuoka%2CS&amp;author=Ellis%2CB&amp;author=Ueda%2CH&amp;author=Nakatsu%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner D, Barakonyi I (2003) Augmented reality kanji learning. In: Proceedings of the 2nd IEEE/ACM symposium o" /><p class="c-article-references__text" id="ref-CR34">Wagner D, Barakonyi I (2003) Augmented reality kanji learning. In: Proceedings of the 2nd IEEE/ACM symposium on mixed and augmented reality (ISMAR). pp 335–336</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="White M, Liarokapis F, Darcy J, Mourkoussis N, Petridis P, Lister PF (2003) Augmented reality for museum artef" /><p class="c-article-references__text" id="ref-CR35">White M, Liarokapis F, Darcy J, Mourkoussis N, Petridis P, Lister PF (2003) Augmented reality for museum artefact visualization. In: 4th irish workshop on computer graphics, Eurographics Ireland Chapter. pp 75–80</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wireless presenter: 3M WP-8000, http://www.3m.com. Access date: June 2010" /><p class="c-article-references__text" id="ref-CR36">Wireless presenter: 3M WP-8000, <a href="http://www.3m.com">http://www.3m.com</a>. Access date: June 2010</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Woods E, Billinghurst M, Looser J, Aldridge G, Brown D, Garrie B, Nelles C (2004) Augmenting the science centr" /><p class="c-article-references__text" id="ref-CR37">Woods E, Billinghurst M, Looser J, Aldridge G, Brown D, Garrie B, Nelles C (2004) Augmenting the science centre and museum experience. In: 2nd international conference on computer graphics and interactive techniques in Australasia and SouthEast Asia (Graphite). ACM Press, New York, 230–236</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yang H, Cho K, Soh J, Jung J, Lee J (2008) Hybrid visual tracking for augmented books. In: the 7th internation" /><p class="c-article-references__text" id="ref-CR39">Yang H, Cho K, Soh J, Jung J, Lee J (2008) Hybrid visual tracking for augmented books. In: the 7th international conference on entertainment computing. pp 161–166</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zhou, AD. Cheok, J. Tedjokusumo, GS. Omer, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Zhou Z, Cheok AD, Tedjokusumo J, Omer GS (2008) wIzQubesTM- a novel tangible interface for interactive storyte" /><p class="c-article-references__text" id="ref-CR40">Zhou Z, Cheok AD, Tedjokusumo J, Omer GS (2008) wIzQubesTM- a novel tangible interface for interactive storytelling in mixed reality. Int J Virtual Real 7(4):9–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=wIzQubesTM-%20a%20novel%20tangible%20interface%20for%20interactive%20storytelling%20in%20mixed%20reality&amp;journal=Int%20J%20Virtual%20Real&amp;volume=7&amp;issue=4&amp;pages=9-15&amp;publication_year=2008&amp;author=Zhou%2CZ&amp;author=Cheok%2CAD&amp;author=Tedjokusumo%2CJ&amp;author=Omer%2CGS">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-010-0164-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the CTI development project of KOCCA, MCST in S. Korea.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">GIST U-VR Lab, Gwangju, 500-712, South Korea</p><p class="c-article-author-affiliation__authors-list">Taejin Ha &amp; Woontack Woo</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">MNU U-VR Lab, 61 Dorim-ri, Cheonggye-myeon, Muan-gun, Jeonnam, Korea</p><p class="c-article-author-affiliation__authors-list">Youngho Lee</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Taejin-Ha"><span class="c-article-authors-search__title u-h3 js-search-name">Taejin Ha</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Taejin+Ha&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Taejin+Ha" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Taejin+Ha%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Youngho-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Youngho Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Youngho+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Youngho+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Youngho+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Woontack-Woo"><span class="c-article-authors-search__title u-h3 js-search-name">Woontack Woo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Woontack+Woo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Woontack+Woo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Woontack+Woo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-010-0164-8/email/correspondent/c1/new">Youngho Lee</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Digilog%20book%20for%20temple%20bell%20tolling%20experience%20based%20on%20interactive%20augmented%20reality&amp;author=Taejin%20Ha%20et%20al&amp;contentID=10.1007%2Fs10055-010-0164-8&amp;publication=1359-4338&amp;publicationDate=2010-06-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Ha, T., Lee, Y. &amp; Woo, W. Digilog book for temple bell tolling experience based on interactive augmented reality.
                    <i>Virtual Reality</i> <b>15, </b>295–309 (2011). https://doi.org/10.1007/s10055-010-0164-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-010-0164-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-04-29">29 April 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-05-19">19 May 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-06-05">05 June 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2011-11">November 2011</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-010-0164-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-010-0164-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Digilog book</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Culture technology</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Multisensory experience</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-010-0164-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=164;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

