<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Encoding immersive sessions for online, interactive VR analytics"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users&#8217; behavior, scene saliency and spatial..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Encoding immersive sessions for online, interactive VR analytics"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-09-25"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users&#8217; behavior, scene saliency and spatial affordances. Collected data can support effort prioritization in 3D modeling workflow or allow fine-tuning of locomotion models for time-constrained experiences. The web with its recent specifications (WebVR/WebXR) represents a valid solution to enable accessible, interactive and usable tools for remote VR analysis of recorded sessions. Performing immersive analytics through common browsers however presents different challenges, including limited rendering capabilities. Furthermore, interactive inspection of large session records is often problematic due to network bandwidth or may involve computationally intensive encoding/decoding routines. This work proposes, formalizes and investigates flexible dynamic models to volumetrically capture user states and scene saliency during running VR sessions using compact approaches. We investigate image-based encoding techniques and layouts targeting interactive and immersive WebVR remote inspection. We performed several experiments to validate and assess proposed encoding models applied to existing records and within networked scenarios through direct server-side encoding, using limited storage and computational resources."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-09-25"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="16"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00405-w"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00405-w"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00405-w.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00405-w"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Encoding immersive sessions for online, interactive VR analytics"/>

    <meta name="citation_online_date" content="2019/09/25"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="16"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00405-w"/>

    <meta name="DOI" content="10.1007/s10055-019-00405-w"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00405-w"/>

    <meta name="description" content="Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users&#8217; behavi"/>

    <meta name="dc.creator" content="Bruno Fanini"/>

    <meta name="dc.creator" content="Luigi Cinque"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Agus M, Marton F, Bettio F, Gobbetti E (2016) Interactive 3d exploration of a virtual sculpture collection: an analysis of user behavior in museum setting. In: Proceedings of the 13th eurographics workshop on graphics and cultural heritage"/>

    <meta name="citation_reference" content="Antal A et&#160;al (2016) A complete workflow from the data collection on the field to the deployment of a virtual museum: the case of virtual sarmizegetusa"/>

    <meta name="citation_reference" content="citation_journal_title=Int Arch Photogramm Remote Sens Spat Inf Sci; citation_title=The winckelmann300 project: Dissemination of culture with virtual reality at the capitoline museum in rome; citation_author=SG Barsanti, B Fanini; citation_volume=42; citation_issue=2; citation_publication_date=2018; citation_pages=371-378; citation_doi=10.5194/isprs-archives-XLII-2-371-2018; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Multimodal Technol Interact; citation_title=The new era of virtual reality locomotion: a systematic literature review of techniques and a proposed typology; citation_author=C Boletsis; citation_volume=1; citation_issue=4; citation_publication_date=2017; citation_pages=24; citation_doi=10.3390/mti1040024; citation_id=CR4"/>

    <meta name="citation_reference" content="Butcher PW, Roberts JC, Ritsos PD (2016) Immersive analytics with WebVR and Google cardboard. Posters of IEEE VIS"/>

    <meta name="citation_reference" content="Butcher PW, John NW, Ritsos PD (2018) Towards a framework for immersive analytics on the web"/>

    <meta name="citation_reference" content="citation_journal_title=Proceedings of the Human Factors and Ergonomics Society Annual Meeting; citation_title=Influence of altered visual feedback on neck movement for a virtual reality rehabilitative system; citation_author=Karen B. Chen, Kevin Ponto, Mary E. Sesto, Robert G. Radwin; citation_volume=58; citation_issue=1; citation_publication_date=2014; citation_pages=693-697; citation_doi=10.1177/1541931214581162; citation_id=CR7"/>

    <meta name="citation_reference" content="Cliquet G, Perreira M, Picarougne F, Pri&#233; Y, Vigier T (2017) Towards HMD-based immersive analytics. In: Immersive analytics workshop, IEEE VIS 2017"/>

    <meta name="citation_reference" content="Cook KA, Thomas JJ (2005) Illuminating the path: the research and development agenda for visual analytics. Technical report, Pacific Northwest National Lab (PNNL), Richland"/>

    <meta name="citation_reference" content="Dibbern C, Uhr M, Krupke D, Steinicke F (2018) Can WebVR further the adoption of virtual reality? Mensch und Computer 2018-Usability Professionals"/>

    <meta name="citation_reference" content="citation_title=Fast Encoding of Huge 3D Data Sets in Lossless PNG Format; citation_inbook_title=Advances in Intelligent Systems and Computing; citation_publication_date=2015; citation_pages=15-24; citation_id=CR11; citation_author=Daniel Dworak; citation_author=Maria Pietruszka; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="citation_title=Immersive Analytics: An Introduction; citation_inbook_title=Immersive Analytics; citation_publication_date=2018; citation_pages=1-23; citation_id=CR12; citation_author=Tim Dwyer; citation_author=Kim Marriott; citation_author=Tobias Isenberg; citation_author=Karsten Klein; citation_author=Nathalie Riche; citation_author=Falk Schreiber; citation_author=Wolfgang Stuerzlinger; citation_author=Bruce H. Thomas; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="Fanini B, d&#8217;Annibale E (2016) A framework for compact and improved panoramic VR dissemination. In: Proceedings of the 14th eurographics workshop on graphics and cultural heritage, Eurographics Association, pp 33&#8211;42"/>

    <meta name="citation_reference" content="Fanini B, Pescarin S, Palombini A (2019) A cloud-based architecture for processing and dissemination of 3D landscapes online. Digital Applications in Archaeology and Cultural Heritage p. e00100"/>

    <meta name="citation_reference" content="citation_journal_title=ACM Trans Graph (TOG); citation_title=Geometry images; citation_author=X Gu, SJ Gortler, H Hoppe; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=355-361; citation_doi=10.1145/566654.566589; citation_id=CR15"/>

    <meta name="citation_reference" content="Hadjar H, Meziane A, Gherbi R, Setitra I, Aouaa N (2018) WebVR based interactive visualization of open health data. In: Proceedings of the 2nd international conference on web studies. ACM, pp 56&#8211;63"/>

    <meta name="citation_reference" content="citation_journal_title=Pattern Recogn Lett; citation_title=Data clustering: 50 years beyond k-means; citation_author=AK Jain; citation_volume=31; citation_issue=8; citation_publication_date=2010; citation_pages=651-666; citation_doi=10.1016/j.patrec.2009.09.011; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=Trans GIS; citation_title=Discovering landmark preferences and movement patterns from photo postings; citation_author=P Jankowski, N Andrienko, G Andrienko, S Kisilevich; citation_volume=14; citation_issue=6; citation_publication_date=2010; citation_pages=833-852; citation_doi=10.1111/j.1467-9671.2010.01235.x; citation_id=CR18"/>

    <meta name="citation_reference" content="Knorr S, Ozcinar C, Fearghail CO, Smolic A (2018) Director&#8217;s cut-a combined dataset for visual attention analysis in cinematic VR content"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Fast, progressive loading of binary-encoded declarative-3d web content; citation_author=M Limper, Y Jung, J Behr, T Sturm, T Franke, K Schwenk, A Kuijper; citation_volume=33; citation_issue=5; citation_publication_date=2013; citation_pages=26-36; citation_doi=10.1109/MCG.2013.52; citation_id=CR20"/>

    <meta name="citation_reference" content="L&#246;we T, Stengel M, F&#246;rster EC, Grogorick S, Magnor M (2015) Visualization and analysis of head movement and gaze data for immersive video in head-mounted displays. In: Proceedings of the workshop on eye tracking and visualization (ETVIS), vol 1"/>

    <meta name="citation_reference" content="Maclntyre B, Smith TF (2019) Thoughts on the future of WebXR and the immersive web. In: 2018 IEEE international symposium on mixed and augmented reality adjunct (ISMAR-Adjunct). IEEE, pp 338&#8211;342"/>

    <meta name="citation_reference" content="citation_title=Approximate Frequency Counts over Data Streams; citation_inbook_title=VLDB &#39;02: Proceedings of the 28th International Conference on Very Large Databases; citation_publication_date=2002; citation_pages=346-357; citation_id=CR23; citation_author=Gurmeet Singh Manku; citation_author=Rajeev Motwani"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Cult Herit (JOCCH); citation_title=Ariadne: a research infrastructure for archaeology; citation_author=C Meghini, R Scopigno, J Richards, B Fanini; citation_volume=10; citation_issue=3; citation_publication_date=2017; citation_pages=18; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Saliency in VR: how do people explore virtual environments?; citation_author=V Sitzmann, A Serrano, A Pavel, M Agrawala, D Gutierrez, B Masia, G Wetzstein; citation_volume=24; citation_issue=4; citation_publication_date=2018; citation_pages=1633-1642; citation_doi=10.1109/TVCG.2018.2793599; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis; citation_title=Attentional synchrony in static and dynamic scenes; citation_author=T Smith, J Henderson; citation_volume=8; citation_issue=6; citation_publication_date=2008; citation_pages=773-773; citation_doi=10.1167/8.6.773; citation_id=CR26"/>

    <meta name="citation_reference" content="Upenik E, Ebrahimi T (2017) A simple method to obtain visual attention data in head mounted virtual reality. In: 2017 IEEE international conference on multimedia &amp; expo workshops (ICMEW). IEEE, pp 73&#8211;78"/>

    <meta name="citation_reference" content="Vincent C, Soroli E, Engemann H, Hendriks H, Hickmann M (2018) Tobii or not tobii? assessing the validity of eye tracking data: Challenges and solutions. In: Scandinavian workshop on applied eye tracking (SWAET)"/>

    <meta name="citation_reference" content="Wagner&#160;Filho JA, Rey MF, Freitas CM, Nedel L (2018) Immersive visualization of abstract information: an evaluation on dimensionally-reduced data scatterplots. In: Proceedings of the 25th IEEE conference on virtual reality and 3D user interfaces (March 2018), vol 2, p 4"/>

    <meta name="citation_reference" content="citation_journal_title=Radiographics; citation_title=Image file formats: past, present, and future; citation_author=RH Wiggins, HC Davidson, HR Harnsberger, JR Lauman, PA Goede; citation_volume=21; citation_issue=3; citation_publication_date=2001; citation_pages=789-798; citation_doi=10.1148/radiographics.21.3.g01ma25789; citation_id=CR30"/>

    <meta name="citation_reference" content="Wille M, Adolph L, Grauel B, Wischniewski S, Theis S, Alexander T (2014) Prolonged work with head mounted displays. In: Proceedings of the 2014 ACM international symposium on wearable computers: adjunct program. ACM, pp 221&#8211;224"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Visual analytics; citation_author=PC Wong, J Thomas; citation_volume=5; citation_publication_date=2004; citation_pages=20-21; citation_doi=10.1109/MCG.2004.39; citation_id=CR32"/>

    <meta name="citation_author" content="Bruno Fanini"/>

    <meta name="citation_author_email" content="bruno.fanini@gmail.com"/>

    <meta name="citation_author_institution" content="CNR ISPC/ITABC, Rome, Italy"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Sapienza University, Rome, Italy"/>

    <meta name="citation_author" content="Luigi Cinque"/>

    <meta name="citation_author_email" content="cinque@di.uniroma1.it"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Sapienza University, Rome, Italy"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00405-w&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00405-w"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Encoding immersive sessions for online, interactive VR analytics"/>
        <meta property="og:description" content="Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users’ behavior, scene saliency and spatial affordances. Collected data can support effort prioritization in 3D modeling workflow or allow fine-tuning of locomotion models for time-constrained experiences. The web with its recent specifications (WebVR/WebXR) represents a valid solution to enable accessible, interactive and usable tools for remote VR analysis of recorded sessions. Performing immersive analytics through common browsers however presents different challenges, including limited rendering capabilities. Furthermore, interactive inspection of large session records is often problematic due to network bandwidth or may involve computationally intensive encoding/decoding routines. This work proposes, formalizes and investigates flexible dynamic models to volumetrically capture user states and scene saliency during running VR sessions using compact approaches. We investigate image-based encoding techniques and layouts targeting interactive and immersive WebVR remote inspection. We performed several experiments to validate and assess proposed encoding models applied to existing records and within networked scenarios through direct server-side encoding, using limited storage and computational resources."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Encoding immersive sessions for online, interactive VR analytics | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00405-w","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Immersive analytics, Session encoding, Data quantization, WebVR, WebXR","kwrd":["Virtual_reality","Immersive_analytics","Session_encoding","Data_quantization","WebVR","WebXR"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00405-w","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00405-w","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=405;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00405-w">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Encoding immersive sessions for online, interactive VR analytics
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00405-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00405-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-09-25" itemprop="datePublished">25 September 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Encoding immersive sessions for online, interactive VR analytics</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Bruno-Fanini" data-author-popup="auth-Bruno-Fanini" data-corresp-id="c1">Bruno Fanini<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-4058-877X"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-4058-877X</a></span><sup class="u-js-hide"><a href="#Aff1">1</a>,<a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="CNR ISPC/ITABC" /><meta itemprop="address" content="CNR ISPC/ITABC, Rome, Italy" /></span><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sapienza University" /><meta itemprop="address" content="grid.7841.a, Department of Computer Science, Sapienza University, Rome, Italy" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Luigi-Cinque" data-author-popup="auth-Luigi-Cinque">Luigi Cinque</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sapienza University" /><meta itemprop="address" content="grid.7841.a, Department of Computer Science, Sapienza University, Rome, Italy" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">178 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00405-w/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users’ behavior, scene saliency and spatial affordances. Collected data can support effort prioritization in 3D modeling workflow or allow fine-tuning of locomotion models for time-constrained experiences. The web with its recent specifications (WebVR/WebXR) represents a valid solution to enable accessible, interactive and usable tools for remote VR analysis of recorded sessions. Performing immersive analytics through common browsers however presents different challenges, including limited rendering capabilities. Furthermore, interactive inspection of large session records is often problematic due to network bandwidth or may involve computationally intensive encoding/decoding routines. This work proposes, formalizes and investigates flexible dynamic models to volumetrically capture user states and scene saliency during running VR sessions using compact approaches. We investigate image-based encoding techniques and layouts targeting interactive and immersive WebVR remote inspection. We performed several experiments to validate and assess proposed encoding models applied to existing records and within networked scenarios through direct server-side encoding, using limited storage and computational resources.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Within cultural heritage, architecture and other fields, head-mounted displays (HMDs) are often employed in public spaces (museums, exhibits, etc.) to offer users immersive explorations of virtual environments, although limited by a number of constraints (e.g., short timings, serialization, etc.). Virtual 3D environments created for standard or non-stereoscopic visualization generally require vast re-adaption for immersive VR experiences consumed in public exhibits or on the web. Therefore, designing and crafting immersive virtual environments (IVEs) for such experiences may present bottlenecks and huge efforts in terms of 3D modeling and/or optimization tasks. Depending on locomotion models employed and spatial constraints, unexpected elements of the 3D scene may capture users’ attention.</p><p>In this sense, we may require an in-depth investigation of spatial affordances and saliency for a given 3D scene when explored through an HMD, in order to improve the overall immersive experience. Tracking, recording and visualizing the entire history of several VR sessions can provide valuable insights on users’ behavior and how they perceive the IVE. From the analyst point of view, in order to understand user sessions from quantitative and qualitative perspectives, collecting per-user fine-grained data is generally a strong requirement. When directly mapped to the original IVE, recorded data may offer an easier interpretation for spatial analysis, if supported by usable user interfaces. Furthermore, a remote analyst may need to inspect user sessions while the VR application is running (e.g., prolonged public events, permanent installations, online web applications, etc.).</p><p>Recent developments within online 3D presentation are fueling the expansion of WebVR (soon WebXR) open specification.<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> The main goal is to bring VR to the web with unified browser APIs, allowing for almost native performance using a common browser, without installing any third-party plug-in or software. Because of its inherent openness and accessibility, the web can represent an incredible opportunity to enable “universal” access to VR. Furthermore, it is already creating fertile ground for distributed and collaborative WebVR applications/tools, including immersive VR analytics.</p><p>Two major challenges to be addressed for WebVR deployment are: (A) large data exchange over the network and (B) a careful management of 3D resources due to rendering limitations inside the browser (including mobile). Within such a context, as VR analysts, we need novel approaches to encode and interactively inspect large collected records (user sessions) over the web.</p><p>The contributions of this work are:</p><ul class="u-list-style-bullet">
                <li>
                  <p>Formalization of compact image-based signals that allow both common off-line 2D image processing and interactive manipulation on modern desktop and mobile GPUs</p>
                </li>
                <li>
                  <p>Encoding techniques and layouts targeting web-based inspection and remote immersive WebVR analytics, using common browsers</p>
                </li>
                <li>
                  <p>Volumetric models to capture VR sessions (user state attributes and propensities) at runtime, with minimal computation also on limited hardware</p>
                </li>
                <li>
                  <p>Session signatures, to encode saliency and user propensities using image-based approaches</p>
                </li>
              </ul><p>The paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec2">2</a> discusses related work, and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec3">3</a> describes and formalizes proposed encoding techniques, volumetric quantization and image-based layouts to capture immersive VR sessions. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec12">4</a> discusses qualitative and quantitative results obtained from different experiments carried out on the open-source architecture developed.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p><i>Immersive VR on the Web</i> One of the issues for immersive VR nowadays is a lack of cross-platform support (Dibbern et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Dibbern C, Uhr M, Krupke D, Steinicke F (2018) Can WebVR further the adoption of virtual reality? Mensch und Computer 2018-Usability Professionals" href="/article/10.1007/s10055-019-00405-w#ref-CR10" id="ref-link-section-d9079e394">2018</a>): because of its inherent openness and accessibility, the web can represent a valid solution enabling a “universal” access to VR without requiring additional software. One of the most prominent examples is SketchFab,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> a well-established commercial platform to publish and present 3D content on different devices, including HMDs (immersive VR). WebVR API is playing a big role into democratizing VR, so more users can experience 3D content through low-cost (e.g., cardboards) or high-end headsets (HTC Vive, Oculus Rift, etc.). It is also fueling content creators, who need to test and deploy immersive VR scenes on the web, reaching a wide range of viewing platforms. The new WebXR Device API<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> (Maclntyre and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Maclntyre B, Smith TF (2019) Thoughts on the future of WebXR and the immersive web. In: 2018 IEEE international symposium on mixed and augmented reality adjunct (ISMAR-Adjunct). IEEE, pp 338–342" href="/article/10.1007/s10055-019-00405-w#ref-CR22" id="ref-link-section-d9079e421">2019</a>) aims to unify VR and AR worlds, with two main goals: (a) support a wider range of user inputs, such as voice and gestures, giving users options for navigating and interacting in virtual spaces and (b) create a technical foundation for development of AR experiences, letting creators integrate real-world media with contextual overlays.</p><p><i>Image-based encoding</i> can be used to transport spatial data over networked Web3D contexts in a compact manner (e.g., coordinates, vectors, etc.) exploiting common 2D images as containers. Regarding image formats, PNG (Wiggins et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Wiggins RH, Davidson HC, Harnsberger HR, Lauman JR, Goede PA (2001) Image file formats: past, present, and future. Radiographics 21(3):789–798" href="/article/10.1007/s10055-019-00405-w#ref-CR30" id="ref-link-section-d9079e429">2001</a>) is of particular interest, as it provides a network-friendly, patent-free, lossless compression scheme that is truly cross-platform. Previous research already employed the PNG format as an externalized mesh container (Limper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Limper M, Jung Y, Behr J, Sturm T, Franke T, Schwenk K, Kuijper A (2013) Fast, progressive loading of binary-encoded declarative-3d web content. IEEE Comput Graph Appl 33(5):26–36" href="/article/10.1007/s10055-019-00405-w#ref-CR20" id="ref-link-section-d9079e432">2013</a>; Dworak and Pietruszka <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Dworak D, Pietruszka M (2015) Fast encoding of huge 3D data sets in lossless PNG format. In: New research in multimedia and internet systems. Springer, Berlin, pp 15–24" href="/article/10.1007/s10055-019-00405-w#ref-CR11" id="ref-link-section-d9079e435">2015</a>) to efficiently transmit geometry data over the network to client browsers. Previous encoding models also studied “geometry images” (Gu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Gu X, Gortler SJ, Hoppe H (2002) Geometry images. ACM Trans Graph (TOG) 21(3):355–361" href="/article/10.1007/s10055-019-00405-w#ref-CR15" id="ref-link-section-d9079e438">2002</a>) as 2D arrays storing spatial information (<span class="mathjax-tex">\(\langle x,y,z\rangle\)</span>) as RGB data. Textures are also used within networked scenarios to transport quantized scalar values, subsequently decoded by the client GPU through vertex/fragment shaders. For instance in omnidirectional/panoramic visualization (Fanini and d’Annibale <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Fanini B, d’Annibale E (2016) A framework for compact and improved panoramic VR dissemination. In: Proceedings of the 14th eurographics workshop on graphics and cultural heritage, Eurographics Association, pp 33–42" href="/article/10.1007/s10055-019-00405-w#ref-CR13" id="ref-link-section-d9079e479">2016</a>), lightweight 8-bit-depth PNG images are exploited to transport depth information, decoded by the client GPU to restore an egocentric approximation of a 3D space, targeting WebVR visualization. Due to quantization (PNG with bit depth 8), a quadratic distribution is employed in order to maximize accuracy of depth restoration on closer surfaces.</p><p>Capturing, storing, mining and visualizing the whole history of all user sessions can provide valuable insights about spatial propensities, users behaviors, saliency and much more for a specific 3D environment. <i>Visual Analytics</i> (Wong and Thomas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wong PC, Thomas J (2004) Visual analytics. IEEE Comput Graph Appl 5:20–21" href="/article/10.1007/s10055-019-00405-w#ref-CR32" id="ref-link-section-d9079e488">2004</a>; Cook and Thomas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Cook KA, Thomas JJ (2005) Illuminating the path: the research and development agenda for visual analytics. Technical report, Pacific Northwest National Lab (PNNL), Richland" href="/article/10.1007/s10055-019-00405-w#ref-CR9" id="ref-link-section-d9079e491">2005</a>) is a proven approach to directly perceive patterns and extract knowledge from massive and dynamic information streams, enabling analysts to detect the expected and discover the unexpected. When dealing with permanent or long-term interactive installations, large amounts of recorded data related to users sessions are collected (Agus et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Agus M, Marton F, Bettio F, Gobbetti E (2016) Interactive 3d exploration of a virtual sculpture collection: an analysis of user behavior in museum setting. In: Proceedings of the 13th eurographics workshop on graphics and cultural heritage" href="/article/10.1007/s10055-019-00405-w#ref-CR1" id="ref-link-section-d9079e494">2016</a>). Data mining methods have been already applied to tourist activity records for discovering landmark preferences from photo mappings (Jankowski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Jankowski P, Andrienko N, Andrienko G, Kisilevich S (2010) Discovering landmark preferences and movement patterns from photo postings. Trans GIS 14(6):833–852" href="/article/10.1007/s10055-019-00405-w#ref-CR18" id="ref-link-section-d9079e497">2010</a>), and classical clustering methods (Jain <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Jain AK (2010) Data clustering: 50 years beyond k-means. Pattern Recogn Lett 31(8):651–666" href="/article/10.1007/s10055-019-00405-w#ref-CR17" id="ref-link-section-d9079e501">2010</a>) can be used to off-line analyze spatial behaviors. Within IVEs users are free to explore, and locomotion plays a central role (Boletsis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Boletsis C (2017) The new era of virtual reality locomotion: a systematic literature review of techniques and a proposed typology. Multimodal Technol Interact 1(4):24" href="/article/10.1007/s10055-019-00405-w#ref-CR4" id="ref-link-section-d9079e504">2017</a>); thus understanding users spatial preferences may support or validate the effectiveness of the interaction model employed. There is a growing interest in researching virtual environment saliency for immersive VR, although most of these works focus on 360 or omnidirectional visual content from fixed viewpoints. Sitzmann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Sitzmann V, Serrano A, Pavel A, Agrawala M, Gutierrez D, Masia B, Wetzstein G (2018) Saliency in VR: how do people explore virtual environments? IEEE Trans Vis Comput Graph 24(4):1633–1642" href="/article/10.1007/s10055-019-00405-w#ref-CR25" id="ref-link-section-d9079e507">2018</a>) analyze for instance how people explore virtual environments from a fixed viewpoint (360) collecting and visualizing gaze data. Upenik and Ebrahimi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Upenik E, Ebrahimi T (2017) A simple method to obtain visual attention data in head mounted virtual reality. In: 2017 IEEE international conference on multimedia &amp; expo workshops (ICMEW). IEEE, pp 73–78" href="/article/10.1007/s10055-019-00405-w#ref-CR27" id="ref-link-section-d9079e510">2017</a>) investigate a model to obtain fixation locations and maps from head direction, offering a good approximation when eye-tracking data are not available. Attention maps are often used to inspect gaze behavior: Löwe et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Löwe T, Stengel M, Förster EC, Grogorick S, Magnor M (2015) Visualization and analysis of head movement and gaze data for immersive video in head-mounted displays. In: Proceedings of the workshop on eye tracking and visualization (ETVIS), vol 1" href="/article/10.1007/s10055-019-00405-w#ref-CR21" id="ref-link-section-d9079e513">2015</a>) presented a visualization framework to analyze head movement and gaze data for immersive 360 movies. Regarding visual attention analysis, Knorr et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Knorr S, Ozcinar C, Fearghail CO, Smolic A (2018) Director’s cut-a combined dataset for visual attention analysis in cinematic VR content" href="/article/10.1007/s10055-019-00405-w#ref-CR19" id="ref-link-section-d9079e516">2018</a>) introduced an efficient metric and visualization method for similarity measures between a director’s cut and users scan paths using color-coded maps. Prolonged usage of HMDs is also a well-investigated topic in the literature (Wille et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Wille M, Adolph L, Grauel B, Wischniewski S, Theis S, Alexander T (2014) Prolonged work with head mounted displays. In: Proceedings of the 2014 ACM international symposium on wearable computers: adjunct program. ACM, pp 221–224" href="/article/10.1007/s10055-019-00405-w#ref-CR31" id="ref-link-section-d9079e520">2014</a>) and the effect on physiological (e.g., neck fatigue) and mental strain.</p><p><i>Immersive analytics</i> is an emerging research field investigating how new interaction models and display technologies can be employed to support analytical reasoning and decision making (Dwyer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Dwyer T, Marriott K, Isenberg T, Klein K, Riche N, Schreiber F, Stuerzlinger W, Thomas BH (2018) Immersive analytics: an introduction. In: Immersive analytics. Springer, Berlin, pp 1–23" href="/article/10.1007/s10055-019-00405-w#ref-CR12" id="ref-link-section-d9079e528">2018</a>). The main goal is to investigate advanced and usable user interfaces in order to support collaboration and offer VR analysts ways to immerse themselves in complex datasets. HMD-based inspection should exploit human visual perception and its factors: Cliquet et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Cliquet G, Perreira M, Picarougne F, Prié Y, Vigier T (2017) Towards HMD-based immersive analytics. In: Immersive analytics workshop, IEEE VIS 2017" href="/article/10.1007/s10055-019-00405-w#ref-CR8" id="ref-link-section-d9079e531">2017</a>) proposed five principles for designing immersive analytics and data representation. Wagner Filho et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Wagner Filho JA, Rey MF, Freitas CM, Nedel L (2018) Immersive visualization of abstract information: an evaluation on dimensionally-reduced data scatterplots. In: Proceedings of the 25th IEEE conference on virtual reality and 3D user interfaces (March 2018), vol 2, p 4" href="/article/10.1007/s10055-019-00405-w#ref-CR29" id="ref-link-section-d9079e534">2018</a>) investigated immersive inspection of multidimensional data, represented in 3D scatter plots as a result of dimensionality reduction. A few recent works focus their attention on immersive WebVR analytics, although facing the challenges related to online, web-based deployment. Butcher et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Butcher PW, Roberts JC, Ritsos PD (2016) Immersive analytics with WebVR and Google cardboard. Posters of IEEE VIS" href="/article/10.1007/s10055-019-00405-w#ref-CR5" id="ref-link-section-d9079e537">2016</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Butcher PW, John NW, Ritsos PD (2018) Towards a framework for immersive analytics on the web" href="/article/10.1007/s10055-019-00405-w#ref-CR6" id="ref-link-section-d9079e540">2018</a>) investigated some of the issues faced by developers in building effective and informative immersive web-based 3D visualizations. The combination of immersive analytics with WebVR API is fueling research in the field of information visualization, as it allows the VR analyst to better perceive complex data difficult to understand using traditional techniques (Hadjar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Hadjar H, Meziane A, Gherbi R, Setitra I, Aouaa N (2018) WebVR based interactive visualization of open health data. In: Proceedings of the 2nd international conference on web studies. ACM, pp 56–63" href="/article/10.1007/s10055-019-00405-w#ref-CR16" id="ref-link-section-d9079e544">2018</a>). From a WebVR analyst perspective, a clear gap is still present with respect to the desktop VR counterpart: performance, scene complexity and data streaming just to name a few. Visualizing or analyzing massive recorded data in a WebVR online context (e.g., user VR sessions being recorded in a remote location) presents several bottlenecks. The whole history of a single user session may contain a huge amount of information (locomotion, gaze, orientation, etc.) that may prevent online transport due to data bandwidth.</p><p>The next section will introduce and formalize the proposed encoding model to enable remote immersive analytics on the web using limited storage and computational resources.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">Session encoding</h2><div class="c-article-section__content" id="Sec3-content"><p>This section defines and describes an image-based encoding model to capture immersive VR sessions. Advantages in terms of portability, scalability, storage and data transport targeting modern web browsers are highlighted, including mobile devices and deployment on limited hardware.</p><h3 class="c-article__sub-heading" id="Sec4">User session</h3><p>Within this work, we represent a user state <i>s</i> as a collection of state attributes, here defined as <span class="mathjax-tex">\(s_{a}\)</span>, where <i>a</i> is the attribute we are dealing with (user location, orientation, focus, etc.). For instance, <span class="mathjax-tex">\(s_{p}\)</span> represent location (<span class="mathjax-tex">\(\langle x,y,z\rangle\)</span>), <span class="mathjax-tex">\(s_{o}\)</span> represent HMD orientation (for instance as quaternion <span class="mathjax-tex">\(\langle x,y,z,w\rangle\)</span>), and so on.</p><p>Each user performs different interactions in a given/limited time period (e.g., during a public expo or online experience), after activating the HMD to explore the virtual environment and before leaving it. We formally define the user session operator <i>S</i> in order to query a specific state attribute <span class="mathjax-tex">\(s_{a}\)</span> for user <i>u</i> at given time <i>t</i>:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} S_{a}(u,t)\rightarrow s_{a}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>Within immersive sessions, we are generally interested in specific state attributes like HMD location (<span class="mathjax-tex">\(s_{p}\)</span>), HMD orientation (<span class="mathjax-tex">\(s_{o}\)</span>), gaze direction (<span class="mathjax-tex">\(s_{d}\)</span>), focus (<span class="mathjax-tex">\(s_{f}\)</span>) or more sophisticated data. For instance, <span class="mathjax-tex">\(S_{p}(u,t)\)</span> can be employed to retrieve useful information on user locomotion or <span class="mathjax-tex">\(S_{f}(u,t)\)</span> to monitor user attention. When using an absolute temporal reference frame, returned <span class="mathjax-tex">\(s_a\)</span> can be undefined if the user <i>u</i> was not present at time <i>t</i> (e.g., he/she left the IVE). Regarding prolonged VR experiences, we may also be interested in measuring comfort or neck fatigue while wearing the HMD.</p><h3 class="c-article__sub-heading" id="Sec5">Quantized user session volume</h3><p>The general idea of a <i>Quantized User Session Volume</i> (QUSV) is to observe a well-defined portion of the 3D space, capturing user sessions and encoding specific users’ attributes. We define the QUSV as an AABB<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> structure defined by a unique ID, origin and 3D extents (width, length and height). The role of a single QUSV in a virtual space is thus to track and record user states within its volumetric extents at runtime. The volume is equipped with a set of <i>encoders</i>, able to write observed state attributes into compact images.</p><p>For the encoding of spatial attributes ranging inside a given volume <i>V</i> extents (e.g., user location, focus, etc.), we define a quantizer <span class="mathjax-tex">\(Q_V\)</span> that maps a 3D location <i>p</i> (<span class="mathjax-tex">\(\langle x,y,z\rangle\)</span>) into a specific RGB color:</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} Q_V(p) = \lfloor {{\mathrm{norm}}(p) \cdot (2^b - 1)}\rfloor \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where norm(<i>p</i>) is the normalized 3D location inside <i>V</i> extents and <i>b</i> is the bit depth employed for the RGB color (e.g., 8). Such an approach uniformly subdivides the volume into several voxels, depending on bit depth employed. Using a bit depth <span class="mathjax-tex">\(b=8\)</span> for instance, the volume is subdivided into <span class="mathjax-tex">\(256^{3}\)</span> voxels, and thus each 3D location within the given QUSV extents can be color-coded (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="246" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Two QUSVs with different extents arranged in a roman forum 3D scene, visible geometry color coded for clarity (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The extents of a QUSV and color bit depth have a huge impact on voxel sizes and quantization error. For instance in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig1">1</a>, the QUSV on the left has extents <span class="mathjax-tex">\(57\,{\mathrm{{m}}}\times 120\,{\mathrm{{m}}}\times 70\,{\mathrm{{m}}}\)</span> , and thus each voxel has a size of <span class="mathjax-tex">\(222\,{\mathrm{{cm}}}\times 468\,{\mathrm{{cm}}}\times 273\,{\mathrm{{cm}}}\)</span> to record and keep track of main forum locations. The second QUSV is observing instead a smaller volume, the temple stairs (<span class="mathjax-tex">\(40\,{\mathrm{{m}}}\times 10\,{\mathrm{{m}}}\times 10\,{\mathrm{{m}}}\)</span>) with each voxel being sized <span class="mathjax-tex">\(150\,{\mathrm{{cm}}}\times 3\,{\mathrm{{cm}}}\times 3\,{\mathrm{{cm}}}\)</span>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Maximum quantization error</h4><p>Given a session volume <i>V</i> and <span class="mathjax-tex">\(E_x(V)\)</span>, <span class="mathjax-tex">\(E_y(V)\)</span>, <span class="mathjax-tex">\(E_z(V)\)</span> to retrieve its extents (width, length and height, respectively) in the 3D scene, we can define the maximum quantization error <span class="mathjax-tex">\(\epsilon\)</span> for lossless RGB data with bit depth <i>b</i> along the three axes as:</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \epsilon = \langle \pm\, \frac{E_x(V)}{2^{b+1}}, \pm\, \frac{E_y(V)}{2^{b+1}}, \pm\, \frac{E_z(V)}{2^{b+1}} \rangle.\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>For instance, the left volume in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig1">1</a> has <span class="mathjax-tex">\(\epsilon = \langle\pm\, 111\,{\mathrm{{cm}}}, \pm\, 234\,{\mathrm{{cm}}}, \pm\, 136.5\,{\mathrm{{cm}}}\rangle\)</span> that has to be taken into account when decoding a 3D location. The model allows to deploy multiple QUSV at different scales in the same IVE; each capturing and encoding state attributes independently.</p><h3 class="c-article__sub-heading" id="Sec7">Time-driven session encoding</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Single-session signal</h4><p>For a single user <i>u</i>, we can now define the encoding of <span class="mathjax-tex">\(S_{a}(u,t)\)</span> for spatial attributes within a given QUSV (see Definition 2) as a lightweight one-dimensional stream of RGB(A) values, called quantized session signal (QSS). For instance, we can track and encode user HMD location (<span class="mathjax-tex">\(s_{p}\)</span>) as a discrete variation of RGB values over time (<i>x</i>-axis)—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig2">2</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="62" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A sample QSS encoding a single user location over time as a signal (variation of RGB values). Image has been vertically stretched for clarity (real height is 1 pixel) (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="256" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A few sample 2D filters applied to user locomotion QSS:<i> x</i>-axis blur (A), isolation of user HMD height variation by extraction of blue channel (B) and application of laplace operator to original QSS (C) to highlight quick HMD positional movements. Colors and sizes enhanced for clarity (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>With this approach, given a fixed temporal step increment, each pixel of the signal encodes a specific location within the QUSV boundaries at given time <i>t</i>. For instance, if spatial attribute recording was performed using 0.1 s step, ten contiguous pixels of the QSS represent variation in position within a single second. Notice that without QUSV location and extents, the client application cannot decode the signal (and its values). The alpha channel (A) can be used to encode user presence (fully opaque) in absolute temporal reference frames. Since the signal is a sequence of RGB(A) values, it can be stored as an image, and common 2D image processing algorithms can be optionally employed to perform further data manipulation (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig3">3</a>), such as:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>isolate a single channel (e.g., blue channel to analyze height variation)</p>
                    </li>
                    <li>
                      <p>shrink temporal data using advanced resampling methods (e.g., bicubic resampling, etc.)</p>
                    </li>
                    <li>
                      <p>apply one-dimensional blur filtering to smooth spatial data</p>
                    </li>
                    <li>
                      <p>employ common image compression algorithms</p>
                    </li>
                  </ul><p>Furthermore, we obtain a data layout suitable for fast encoding/decoding and direct manipulation on modern GPUs. For instance, we are able to exploit available OpenGL ES texture filtering for automatic interpolation of spatial attributes at a given <i>t</i> via common texel fetching routines. When using formats such as PNG, this layout also facilitates compression since temporal data will be more likely continuous between neighboring pixels (Limper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Limper M, Jung Y, Behr J, Sturm T, Franke T, Schwenk K, Kuijper A (2013) Fast, progressive loading of binary-encoded declarative-3d web content. IEEE Comput Graph Appl 33(5):26–36" href="/article/10.1007/s10055-019-00405-w#ref-CR20" id="ref-link-section-d9079e2064">2013</a>)—for instance, locomotion data.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Multiple sessions</h4><p>At this point, multiple session signals (QSS) can be vertically stacked into a 2D atlas—called quantized session atlas (QSA): the QUSV will take care of encoding and writing the entire <span class="mathjax-tex">\(S_{a}(u,t)\)</span> for a given state attribute <i>a</i>. We propose a 2D layout where the <i>x</i>-axis still represents time and <i>y</i>-axis represents user ID (<i>u</i>). Such layout still allows off-line image processing operations and interactive manipulation on modern GPUs.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="174" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>A sample QSA encoding locomotion data for 14 users and temporal compression of 1/2 and 1/5, respectively. Session atlas has been vertically stretched for clarity</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>From a storage perspective, a QSA can be shrinked along the <i>x</i>-axis (temporal compression) using nearest-neighbor algorithm, thus allowing a coherent (but lossy) data reduction, where needed. Such basic 2D operation can be employed for different reasons, for instance compressing old QSAs stored on disk, maintaining an approximation of original <span class="mathjax-tex">\(S_{a}(u,t)\)</span>. From a runtime perspective, the time-driven layout in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig4">4</a> can be easily manipulated by common desktop and mobile GPUs as texture data (fetching and decoding texels).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Session paging</h4><p>The stream size along <i>x</i>-axis (time) for QSA so far raises a limitation regarding session duration and texture size: for instance given a time step of 0.1 s per pixel and an available texture size of 4096 (4K), the maximum user session duration we are able to record is around 6 min. Users in public exhibits have generally limited timing for VR sessions due to serialization of the experience on a single machine, and thus under certain circumstances, this can be sufficient. For prolonged sessions though, we can employ <i>temporal paging</i> techniques: once texture space is fully consumed, a new page can be created and continue the recording.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="414" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Paging technique for prolonged sessions, using a resolution of <span class="mathjax-tex">\(4096\times 4096\)</span> for each page. Figure shows lower resolution for clarity</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The QUSV encoder is thus able to allocate new pages (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig5">5</a>) and write them on disk, while a separate client application can request a specific page TP<i>k</i> depending on simulated time. Due to the lightweight layout, such segmentation is specifically suitable for both prolonged recording contexts and large amount of users. The layout compactness suits online scenarios where a collector service encodes running user sessions during the exhibit, while remote clients (e.g., web applications) request temporal pages. The latter approach can be exploited by analysts for direct visual inspection or third-party data mining applications, while the VR installation is running elsewhere (museum, exhibit or on a remote server).</p><h3 class="c-article__sub-heading" id="Sec11">Session signatures</h3><p>Spatial attributes (user location, target, focus, etc.) recorded over time can be exploited to compute a set of <i>salient</i> voxels in a given QUSV, providing valuable intel on users spatial interactions. Depending on persistence over time and other contributing factors (see for instance Agus et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Agus M, Marton F, Bettio F, Gobbetti E (2016) Interactive 3d exploration of a virtual sculpture collection: an analysis of user behavior in museum setting. In: Proceedings of the 13th eurographics workshop on graphics and cultural heritage" href="/article/10.1007/s10055-019-00405-w#ref-CR1" id="ref-link-section-d9079e2278">2016</a>), elements or portions of the 3D scene may receive unexpected interests when perceived through an HMD. The large amount of data collected from exhibits or running WebVR online applications can offer a good and valuable estimate of user behaviors and scene saliency. Such data can be encoded for interactive visual analytics, supporting locomotion adjustments or fine-tuning interaction model. Furthermore, volumetric saliency encoded as image represents a “signature” of the session (or several sessions) for a specific QUSV. The general idea applied to spatial attributes (e.g., HMD world location, user focus, etc.) is to maintain a running record of VOIs (voxels of interest) inside a given QUSV, sorting the list depending on a rank value. We use the same mapping method described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec5">3.2</a> (definition 2) to encode the location into RGB value, while using the alpha channel to encode rank (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="794" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>A sample locomotion signature (<span class="mathjax-tex">\(\sigma _{p}\)</span>) for a set of recorded users in a given IVE evaluated with different ranges. Leftmost values are most ranked voxels of the QUSV (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The algorithm is based on Manku and Motwani (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Manku GS, Motwani R (2002) Approximate frequency counts over data streams. In: VLDB’02 proceedings of the 28th international conference on very large databases. Elsevier, Amsterdam, pp 346–357" href="/article/10.1007/s10055-019-00405-w#ref-CR23" id="ref-link-section-d9079e2332">2002</a>) using a table to compute frequency counts (voxel hits) while maintaining a constant memory footprint (<i>k</i>, the size of table) and a running record approach. The latter guarantees an evolving “fingerprint” of the tracked spatial attribute during the whole life of the QUSV. In order to produce and update such a volumetric “session signature” for a given state attribute, voxels are sorted by rank: for instance with focus (<span class="mathjax-tex">\(s_{f}\)</span>), the leftmost pixel of the signature encodes the voxel that received higher attention (Smith and Henderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Smith T, Henderson J (2008) Attentional synchrony in static and dynamic scenes. J Vis 8(6):773–773" href="/article/10.1007/s10055-019-00405-w#ref-CR26" id="ref-link-section-d9079e2362">2008</a>) (computed from persistence over time and distance). Such sorted layout allows partial evaluation of the signature directly on the GPU by discarding rightmost values (lower rank). For instance with <span class="mathjax-tex">\(k=512\)</span>, we can partially evaluate the first half of the signature (256 voxels) or a portion of leftmost voxels (most ranked locations). The running signature computed recording a state attribute <span class="mathjax-tex">\(s_{a}\)</span> is thus defined as:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sigma _{a}(\langle v_{\mathrm{{{min}}}},v_{\mathrm{{max}}}\rangle) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>returning a set of voxels, where <span class="mathjax-tex">\(\langle v_{\mathrm{{min}}},v_{\mathrm{{max}}}\rangle\)</span> is the evaluation range (normalized values) thus <span class="mathjax-tex">\(\langle 0.0,0.0\rangle\)</span> means evaluating just the first voxel; <span class="mathjax-tex">\(\langle 1.0,1.0\rangle\)</span> evaluates just the last voxel, while <span class="mathjax-tex">\(\langle 0.0,0.5\rangle\)</span> evaluates the first half. A signature can be generated from a single session to measure salient locations for a given user, although more useful results are generally produced by multiple sessions. Furthermore, this solution abstracts completely from scene complexity since it is not bound to geometry (e.g., vertices) or texturing of the 3D scene; instead its accuracy depends solely on QUSV extents, image format and compression method used (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig7_HTML.jpg" alt="figure7" loading="lazy" width="685" height="1160" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>A focus signature (<span class="mathjax-tex">\(\sigma _{f}\)</span>) produced by multiple WebVR sessions, evaluated with <span class="mathjax-tex">\(\langle 0.0,0.04\rangle\)</span>, <span class="mathjax-tex">\(\langle 0.0,0.2\rangle\)</span> and <span class="mathjax-tex">\(\langle 0.0,0.5\rangle\)</span> (top to bottom). The signature is transmitted and visualized at runtime by the remote analyst using a common browser: <span class="mathjax-tex">\(\sigma _{f}\)</span> is decoded interactively by the GPU, highlighting and mapping users attention</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>There are several advantages for signatures and their encoding layout:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>the signature is ultra-compact (see results in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec12">4</a>), suitable for online contexts and remote analytics</p>
                  </li>
                  <li>
                    <p>sorting allows progressive evaluation, starting from most ranked voxels: <span class="mathjax-tex">\(\sigma _{a}(\langle 0.0,x\rangle)\)</span></p>
                  </li>
                  <li>
                    <p>rightmost values of the image (low-ranked voxels) can be trimmed, maintaining good approximation</p>
                  </li>
                  <li>
                    <p>multiple signatures (e.g., different state attributes) can be stacked vertically (like QSA layout) to transport several spatial propensities for a QUSV, using a single lightweight image</p>
                  </li>
                </ul><p>One use for session signatures is for instance the automatic generation of progressive recommended locomotion graph inside a QUSV, produced by multiple sessions (<span class="mathjax-tex">\(\sigma _{p}\)</span>). Another application is fast image-based comparison of different <span class="mathjax-tex">\(\sigma\)</span> with low range to evaluate similarity between session sets with respect to certain spatial attribute (e.g., focus). A few interesting results regarding session signatures will be shown in the next section.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Experiments and results</h2><div class="c-article-section__content" id="Sec12-content"><p>This section discusses different experiments including visual, qualitative and quantitative results obtained from presented image-based encoding methods in online WebVR contexts. For all experiments, we used lossless PNG as output format for QSAs and session signatures. For the latter, a size of <span class="mathjax-tex">\(k = 1024\)</span> was used, consuming around 3 Kb of storage per signature.</p><h3 class="c-article__sub-heading" id="Sec13">WebVR Front End</h3><p>In order to validate and assess the presented model for online WebVR contexts, different web components for QUSV management, GPU encoding/decoding and user session handling were developed on top of existing open-source WebVR project<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> (Fanini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Fanini B, Pescarin S, Palombini A (2019) A cloud-based architecture for processing and dissemination of 3D landscapes online. Digital Applications in Archaeology and Cultural Heritage p. e00100" href="/article/10.1007/s10055-019-00405-w#ref-CR14" id="ref-link-section-d9079e2979">2019</a>; Barsanti et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Barsanti SG, Fanini B et al (2018) The winckelmann300 project: Dissemination of culture with virtual reality at the capitoline museum in rome. Int Arch Photogramm Remote Sens Spat Inf Sci 42(2):371–378. &#xA;                  https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/371/2018/&#xA;                  &#xA;                &#xA;                     " href="/article/10.1007/s10055-019-00405-w#ref-CR3" id="ref-link-section-d9079e2982">2018</a>; Meghini et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Meghini C, Scopigno R, Richards J, Fanini B et al (2017) Ariadne: a research infrastructure for archaeology. J Comput Cult Herit (JOCCH) 10(3):18" href="/article/10.1007/s10055-019-00405-w#ref-CR24" id="ref-link-section-d9079e2985">2017</a>; Antal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Antal A et al (2016) A complete workflow from the data collection on the field to the deployment of a virtual museum: the case of virtual sarmizegetusa" href="/article/10.1007/s10055-019-00405-w#ref-CR2" id="ref-link-section-d9079e2988">2016</a>). Specifically, the front end offered two distinct immersive interfaces and interaction models for participants (exploration of a 3D scene) and analysts (inspection). The developed components will become part of the open-source project.</p><h3 class="c-article__sub-heading" id="Sec14">Experiment A</h3><p>For the first experiment, we applied presented encoding model to previously recorded user session datasets. We investigated data quantization for obtained QSAs and session signatures, also comparing storage to other common approaches. Data was collected and stored during a past public exhibit<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> focused on digital heritage, using a desktop VR application equipped with recording component. The latter used standard CSV<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup> files to anonymously record and store per-user session data. We off-line encoded all data using different QUSVs for each virtual environment and compared the obtained results in terms of disk storage. The basic user state originally collected by the desktop application was composed by:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>Timestamp</i> including also year, month and day of the record</p>
                  </li>
                  <li>
                    <p><i>HMD position</i> in world coordinates, including translation matrix from tracking sensors</p>
                  </li>
                  <li>
                    <p><i>HMD orientation</i> as quaternion</p>
                  </li>
                </ul><p>The recording component role was to silently write data into cumulative ASCII files via non-blocking callbacks, to avoid unintended latency or skip HMD frames. Each user session record was initiated by the component when user put on the HMD and closed when they removed it.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Setup</h4><p>The in situ installation was composed by:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>1 Workstation equipped with NVIDIA 980 GTX and i7 CPU</p>
                    </li>
                    <li>
                      <p>1 Oculus Rift CV1 + Joypad</p>
                    </li>
                  </ul><p>The analyst setup (lab) included:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>1 Workstation equipped with NVIDIA 980 GTX and i7 CPU + Firefox Quantum v64</p>
                    </li>
                    <li>
                      <p>1 Oculus Rift CV1 + Oculus Touch controllers</p>
                    </li>
                    <li>
                      <p>1 PC running a web server, serving encoded QSAs and session signatures</p>
                    </li>
                  </ul><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Participants</h4><p>Around 300 casual users were recorded during the three-day exhibit, using a time step of 0.1s for each selected virtual environment. CSV files were produced for each user stored on local machine.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">VR locomotion model</h4><p>The navigation system offered by the VR desktop application was carried out using free linear locomotion (gravity enabled) using a common joypad. Such locomotion model is very prone to motion sickness due to the well-known conflict between visual and vestibular system (Boletsis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Boletsis C (2017) The new era of virtual reality locomotion: a systematic literature review of techniques and a proposed typology. Multimodal Technol Interact 1(4):24" href="/article/10.1007/s10055-019-00405-w#ref-CR4" id="ref-link-section-d9079e3103">2017</a>), although it was part of the investigation. Users were thus free to explore large sections of the scene and stop to have a look around, only limited by basic colliders placed in correspondence of walls and restricted zones.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Results</h4><p>Collected records were encoded into presented image-based layouts (QSA and session signatures) in order to:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>Perform storage comparisons (QSA against standard CSV and raw binary data formats) with a good number of casual users</p>
                    </li>
                    <li>
                      <p>Validate decoding routines for WebVR front end (analyst side) and inspect several user sessions interactively inside a browser</p>
                    </li>
                    <li>
                      <p>Evaluate session signatures at runtime for HMD location (<span class="mathjax-tex">\(\sigma _{p}\)</span>) for each QUSV</p>
                    </li>
                  </ul><p>In order to investigate quantitative results on described image-based encoding, we converted original data (CSV) into uncompressed binary formats and QSA (using lossless PNG format). Two binary solutions were evaluated to encode HMD location coordinates within the 3D scene: double (4 byte)- and single-byte precision (inside the session volume extents).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="526" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>A storage comparison (Mb) between original locomotion records in CSV (blue square) for each 3D scene, binary (double precision, orange square) and binary (byte precision, ash square) on top. Bottom compares binary (byte precision) and QSA (light green square) using lossless PNG (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The second (single-byte encoding) can be compared directly against QSA since it encodes identical coordinates stored into the atlas using a lossless approach; thus they exhibit the same quantization error upon decoding (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec5">3.2</a>). CSV records were encoded into paged QSAs, arranging one QUSV per scene enclosing users sessions, employing the same time step used for original recording (0.1 s). As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig8">8,</a> the QSA encoding (using lossless PNG format) is really lightweight compared to the other approaches: for instance, original session records (CSV) for scene 1 did take up 25.7 Mb against 100 Kb (all QSA pages).</p><p>Regarding signatures (<span class="mathjax-tex">\(\sigma _{p}\)</span>) produced by all sessions, we used short evaluation ranges (<span class="mathjax-tex">\(\sigma _{p}(\langle 0.0,x\rangle)\)</span> with <i>x</i> ranging between 0.01 and 0.1) to synthesize key locations for each 3D scene, directly resulting from users spatial persistence over time. This allowed analysts to automatically decode for each QUSV a sorted set of salient locations (voxels) representing locomotion fixations for all users.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig9_HTML.jpg" alt="figure9" loading="lazy" width="685" height="457" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A QUSV arranged to record locomotion data inside a library of scene 2 “Forum Pacis” (top left, color coded for clarity) and real-time decoding of session signature <span class="mathjax-tex">\(\sigma _{p}\)</span> (HMD location) evaluated at increasing ranges: <span class="mathjax-tex">\(\sigma _{p}(\langle 0.0,x\rangle)\)</span> with <span class="mathjax-tex">\(x=0.007\hbox { (A)}, x=0.125\hbox { (B)}\)</span> and <span class="mathjax-tex">\(x=0.5\hbox { (C)}\)</span> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig9">9</a> shows obtained locations from <span class="mathjax-tex">\(\sigma _{p}\)</span> of scene 2: the WebVR front end can decode locomotion fixations at runtime inside a standard browser, using different ranges (A, B and C) to progressively control the number of nodes. Such results were employed by 3D modelers and digital heritage professionals to support selective improvements of 3D scene details.</p><h3 class="c-article__sub-heading" id="Sec19">Experiment B</h3><p>This experiment focuses on <i>direct</i> encoding of QSAs and signatures (<span class="mathjax-tex">\(\sigma\)</span>), performed by a web service running on a remote server node. The main goal was to assess live inspection carried out from a remote WebVR analyst over the network. Within such context, we wanted to deploy and evaluate a completely web-based architecture—based on node.js<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup>—including direct encoding (server side) and remote WebVR clients to validate our techniques in a real scenario: the full setup is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig10">10</a>. A client component (participant) was developed to send HMD states to the encoder service (Q) via HTML5 WebSockets. A single user state consists of:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>ID</i> (User ID)</p>
                  </li>
                  <li>
                    <p><i>HMD position</i> in world coordinates, including translation matrix from tracking sensors</p>
                  </li>
                  <li>
                    <p><i>HMD orientation</i> as quaternion</p>
                  </li>
                  <li>
                    <p><i>Focal point</i> in world coordinates</p>
                  </li>
                </ul><p>The focal point was decoupled from HMD orientation to support HMDs equipped with eye-tracking devices (e.g., Tobii VR<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup>) for accurate user focus. The resulting 3D location is computed from the first intersection along the view direction, thus automatically taking care of occlusion: for the experiments using Oculus Rift devices, we approximated such interest vector with HMD direction. Experiments were carried out by volunteers on a couple of sample 3D scenes freely available (CC BY-SA 4.0) on SketchFab by the Hallwyl Museum.<sup><a href="#Fn10"><span class="u-visually-hidden">Footnote </span>10</a></sup></p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec20">Setup</h4><p>For this set of experiments, we used two VR workstations (NVIDIA 980 GTX and i7 CPU, with Firefox Quantum v64) remotely located: one for participants and one for the analyst, both equipped with HMDs (Oculus Rift CV1)—see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig10">10</a>. A node (server) ran the web services, including QUSV encoders and content streaming functionalities. Setup included:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>2 Oculus Rift CV1 and Oculus Touch controllers</p>
                    </li>
                    <li>
                      <p>1 Workstation (P) for participants</p>
                    </li>
                    <li>
                      <p>1 Workstation (A) for the analyst</p>
                    </li>
                    <li>
                      <p>1 Node (server) running content service (C) and encoding service (Q) serving QSAs and session signatures to analyst</p>
                    </li>
                  </ul><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig10_HTML.png?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig10_HTML.png" alt="figure10" loading="lazy" width="685" height="355" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Setup for direct encoding and remote immersive analytics. Participants (blue) carried out WebVR sessions on a few 3D scenes. The role of content service (<i>C</i>) was to serve the 3D scene + front end (once) to participants and remote analyst, while the role of encoder service (<i>Q</i>) was to produce and stream running QSAs and signatures to the analyst for immersive inspection. Both services were running on a public node.js server in the laboratory. Both workstations used firefox browser, without any third-party software (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec21">Participants</h4><p>A total of 25 users (students from Dept. of Computer Science at Sapienza University and interns from VHLab) participated in the experiments.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec22">VR locomotion model</h4><p>The WebVR front end offered users (participants and analysts) a simple teleport-based locomotion model (Boletsis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Boletsis C (2017) The new era of virtual reality locomotion: a systematic literature review of techniques and a proposed typology. Multimodal Technol Interact 1(4):24" href="/article/10.1007/s10055-019-00405-w#ref-CR4" id="ref-link-section-d9079e3644">2017</a>) using HMD direction and common VR controllers. The actual location where users were able to teleport was based on the intersection between HMD direction and planar surfaces. For the experiments, we defined a surface <i>eligible</i> for teleport if its normal<i> z</i>-component satisfies the condition <span class="mathjax-tex">\(\&gt;0.8\)</span>; a similar locomotion model is currently implemented also in the SketchFab WebVR viewer.</p><p>We arranged a QUSV (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig11">11</a>) to capture running sessions over the local network. From an accuracy point of view, the main QUSV extents were set to <span class="mathjax-tex">\(16.87\,{\mathrm{{m}}}\times 16.87\,{\mathrm{{m}}}\times 16.87\,{\mathrm{{m}}}\)</span>, thus leading to a maximum quantization error <span class="mathjax-tex">\(\epsilon =\pm 3\,{\mathrm{{cm}}}\)</span> for spatial attributes (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec6">3.2.1</a>). For user <i>locomotion</i> analysis in this case (a room), such error is a good trade-off, while regarding <i>focus</i> accuracy it highly depends on object/area sizes the analyst is interested in (e.g., a fork on the table or very small items).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig11_HTML.jpg" alt="figure11" loading="lazy" width="685" height="553" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Top: Arranged QUSV to capture user sessions (color coded, left) and participant VR interface with teleport-based locomotion. Middle: analyst UI to inspect decoded locomotion data from QSA using web sliders or VR controllers (left) and <span class="mathjax-tex">\(\sigma _{p}\)</span> (right) to inspect spatial persistence over time. Bottom: <span class="mathjax-tex">\(\sigma _{f}\)</span> evaluated interactively inside the browser with ranges 0.01 and 0.25, respectively, to inspect users interest after two sessions (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>A smaller QUSV in this case could be arranged to enclose the table and capture user interactions with higher accuracy. During the experiment, the WebVR analyst (located in a different place) was able to interactively inspect both signatures (<span class="mathjax-tex">\(\sigma _{p}\)</span> and <span class="mathjax-tex">\(\sigma _{f}\)</span>) as they were evolving after each session. Final signatures (lossless PNG) after all sessions had sizes of 2.67 Kb and 3.66 Kb, respectively, thus served without any issues to the remote WebVR analyst.</p><p>For the latter, we measured a frame rate ranging between 74 and 90 fps during immersive inspection (3D scene rendering + GPU decoding of QSAs and signatures). The WebVR front end allowed the remote analyst to interact with live data using VR controllers: specifically time line (QSA) and signature evaluation range (<span class="mathjax-tex">\(\sigma _{p}\)</span> and <span class="mathjax-tex">\(\sigma _{f}\)</span>).</p><h3 class="c-article__sub-heading" id="Sec23">Experiment C</h3><p>We arranged a third experiment involving a different 3D scene for participants of the previous experiment <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec19">4.3</a> evaluating encoding performance with limited resources. The objectives of the experiment were as follows:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Evaluate performance of encoding services running on limited hardware (Raspberry Pi 3, a low-cost single-board computer)</p>
                  </li>
                  <li>
                    <p>Capture and encode several state attributes at once into multiple QSAs, including HMD ergonomics</p>
                  </li>
                </ul><p>This time we deployed both content service (C) and encoding services (Q) on a Raspberry Pi 3 board<sup><a href="#Fn11"><span class="u-visually-hidden">Footnote </span>11</a></sup> (quad-core 1.4 GHz 64-bit ARMv8 CPU, 1 Gb RAM, micro-SD storage) connected to the local network. Participants and analyst used the same workstations for experiment B. We set the usual time step of 0.1 s and evaluated resulting QSAs and signatures produced on the micro-SD, also monitoring memory and storage consumption during the sessions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig12_HTML.jpg" alt="figure12" loading="lazy" width="685" height="950" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>From top: The arranged QUSV to capture user sessions (color coded); decoding focus for multiple users from a single QSA (using temporal slider UI); immersive inspection of decoded focal points from multiple sessions using VR controllers to move through the time line (QSA <i>x</i>-axis). Bottom: <span class="mathjax-tex">\(\sigma _{f}\)</span> and <span class="mathjax-tex">\(\sigma _{p}\)</span> ,respectively, to interactively visualize overall interest and spatial persistence after ten sessions (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Web services running on the board during the experiment did not show any particular issue: incoming HMD states were encoded into multiple QSAs and signatures, maintaining a stable memory footprint (<span class="mathjax-tex">\(\sim 100\)</span> Mb). From a storage perspective, we tracked ten user sessions (5 minutes each) capturing following state attributes inside the QUSV: <i>P</i> (HMD location), <i>F</i> (focal point), <i>O</i> (HMD orientation) and <i>N</i> (neck comfort). For QSAs, final obtained sizes were 8.9 Kb (<i>P</i>), 16.7 Kb (<i>F</i>), 23 Kb (<i>O</i>) and 3.5 Kb (<i>N</i>). Regarding session signatures, generated lossless PNGs were 3.18 Kb (<span class="mathjax-tex">\(\sigma _{p}\)</span>) and 3,37Kb (<span class="mathjax-tex">\(\sigma _{f}\)</span>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig13_HTML.png?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig13_HTML.png" alt="figure13" loading="lazy" width="685" height="700" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>QSAs (portions) generated for each state attribute during user sessions. Bottom shows QSA for neck comfort (light green square = very comfortable, yellow square = quite comfortable and red square = uncomfortable) and 2D image processing steps (A and B) to obtain weighted average neck comfort (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig13">13</a> shows portions of obtained QSAs. Notice how QSA for HMD position is “blocky” while focus QSA shows finer variations: this is due to locomotion model employed (teleport). Regarding neck comfort, thanks to well-known ranges in HMD literature (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Chen KB, Ponto K, Sesto ME, Radwin RG (2014) Influence of altered visual feedback on neck movement for a virtual reality rehabilitative system. In: Proceedings of the human factors and ergonomics society annual meeting, vol 58. SAGE Publications Sage, Los Angeles, pp 693–697" href="/article/10.1007/s10055-019-00405-w#ref-CR7" id="ref-link-section-d9079e4177">2014</a>) we mapped physical strain of users necks to three different levels: sporadic “uncomfortable” head orientations (red) were due to some details on gallery rooves, capturing users’ attention. The array value in (A) is produced by applying a basic 2D resampling filter <span class="mathjax-tex">\((\hbox {along x-axis} = \hbox {time})\)</span> thus obtaining the weighted average neck comfort for each user (row) or a global average for all sessions (B). Analyst WebVR front end reported a frame rate ranging between 78 and 90 fps during immersive inspection (3D scene rendering + GPU decoding of QSAs and signatures).</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Discussion</h2><div class="c-article-section__content" id="Sec24-content"><p>We performed different experiments to assess and validate our proposed model by encoding previously collected datasets (ASCII, CSV) and performing direct encoding on server nodes. For all the experiments, lossless PNG format with bit depth 8 was employed to encode QSA and session signatures. For the first experiment <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec14">4.2</a>, we applied the encoding model to locomotion datasets (CSV) of casual visitors previously recorded during public events. This allowed a comparison between image-based encoding, ASCII and binary encoding in terms of data size for online transmission. We evaluated quantization error and accuracy trade-offs related to spatial attributes. As formalized in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec5">3.2</a> (Definition 3), a maximum location error <span class="mathjax-tex">\(\epsilon\)</span> can be determined for lossless image formats, depending on QUSV extents and image bit depth. The QSA (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec9">3.3.2</a>) offered a lightweight and compact storage usage for the experiments, providing a suitable session recording container and data transport over networked scenarios (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig8">8</a>). The time-driven layout (<span class="mathjax-tex">\(time \times users\)</span>) of the image atlas guarantees high PNG compression ratios for spatial attributes also thanks to inherently smooth data variations—i.e., lexicographically sorted values (see for instance Limper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Limper M, Jung Y, Behr J, Sturm T, Franke T, Schwenk K, Kuijper A (2013) Fast, progressive loading of binary-encoded declarative-3d web content. IEEE Comput Graph Appl 33(5):26–36" href="/article/10.1007/s10055-019-00405-w#ref-CR20" id="ref-link-section-d9079e4294">2013</a>). Location session signatures (<span class="mathjax-tex">\(\sigma _{p}\)</span>) were computed from all CSV datasets using a voxel-persistence policy. (A linear locomotion model was offered to visitors.) This allowed analysts to automatically and progressively decode from each <span class="mathjax-tex">\(\sigma _{p}\)</span> a sorted list of key locations representing users locomotion fixations, within a specific QUSV. Such locations were employed by 3D modelers and digital heritage professionals to support selective improvements of 3D scene details, including applied VR games within a few European projects such as REVEAL.<sup><a href="#Fn12"><span class="u-visually-hidden">Footnote </span>12</a></sup></p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00405-w/MediaObjects/10055_2019_405_Fig14_HTML.jpg" alt="figure14" loading="lazy" width="685" height="916" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Analyst WebVR front end while remotely inspecting the focus signature <span class="mathjax-tex">\(\sigma _{f}\)</span> from experiment B at runtime, using VR controllers to dynamically extend the <span class="mathjax-tex">\(\sigma _{f}\)</span> evaluation range (top to bottom)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00405-w/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The experiments B <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec19">4.3</a> and C <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00405-w#Sec23">4.4</a> allowed the assessment of the model for direct, running encoding in a developed web-based architecture, comprising a participant WebVR workstation and a remote WebVR workstation for the analyst. During the experiment B, the remote analyst was able to interactively inspect QSAs and session signatures while participants were exploring the virtual environments. We also measured good frame rates (74–90) on analyst WebVR browser while he was inspecting the GPU-decoded atlases (fragment shader) using an HMD (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig14">14</a>) and VR controllers to interact with data (time line and signature evaluation range). The experiment C specifically targeted the efficiency of server-side encoding on limited hardware (Raspberry Pi 3) when handling multiple attributes (QSAs and signatures) per QUSV at runtime. This was very useful to assess the encoding model performance and memory footprint in a real scenario using a stand-alone node with low computational resources. The image-based approach did prove to be very useful also in terms of off-line 2D manipulation: basic image resampling operations were carried out on neck comfort QSA (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00405-w#Fig13">13</a>) to retrieve weighted average user comfort during the immersive session. More generally, the QSA layout offers analysts a wide range of 2D image operations—not available with raw binary or compressed data—to extract or combine multiple image atlases with ease.</p></div></div></section><section aria-labelledby="Sec25"><div class="c-article-section" id="Sec25-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec25">Conclusions and future work</h2><div class="c-article-section__content" id="Sec25-content"><p>We presented, formalized and evaluated lightweight image-based encoding techniques and web-oriented layouts to capture immersive VR sessions for remote interactive inspection (immersive WebVR analytics). <i>Quantized User Session Volumes</i> (QUSV) are defined and described, to observe and capture user attributes and behaviors inside a volumetric portion of the 3D scene. We show advantages of the encoding model in terms of compactness of data transport over the network and disadvantages related to data quantization depending on QUSV extents and image bit depth. The proposed techniques and layouts can be employed for fast, direct encoding/decoding routines targeting real-time immersive WebVR analytics. Regarding quantized session atlas (QSA), we show advantages in terms of storage and web deployment using temporal paging for prolonged sessions. They can be used to track specific state attributes or features including location and orientation, focus, HMD neck comfort, 6DOF controllers, and much more. We show how image-based layouts also offer several advantages by leveraging on existing lossless web-oriented formats—such as PNG—allowing common off-line 2D image manipulation and real-time GPU encoding/decoding for remote VR analytics. We then describe <i>session signatures</i> (<span class="mathjax-tex">\(\sigma\)</span>) as compact image-based layouts to maintain a running “fingerprint” of the QUSV as sorted list of encoded VOIs (voxels of interest) capturing salient elements of a 3D scene and users’ behaviors during the immersive session, easily transmitted to remote WebVR analysts over the network. We discuss also applications of signatures for unconstrained locomotion models and casual visitors during public events. Regarding focus, presented methods offer also a direct application to HMDs equipped with eye tracking, like <i>Tobii</i> (Vincent et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Vincent C, Soroli E, Engemann H, Hendriks H, Hickmann M (2018) Tobii or not tobii? assessing the validity of eye tracking data: Challenges and solutions. In: Scandinavian workshop on applied eye tracking (SWAET)" href="/article/10.1007/s10055-019-00405-w#ref-CR28" id="ref-link-section-d9079e4474">2018</a>) or similar devices, providing a compact volumetric encoding of gaze behaviors for online WebVR analysis. Described techniques can be easily replicated using other 3D libraries and devices, ranging from desktop VR to mobile browsers. Results of this research and web components implemented will be open-sourced for reuse in other open-source projects.</p><p>Future developments will focus on testing alternative layouts for QSAs and signatures, including for instance user-centric atlases (carrying all quantized attributes for a single user). We will also focus on exploiting signatures to compare immersive sessions by means of 2D image-based manipulations (similarity). Encoding techniques for handheld 6DOF controllers can be further optimized to perform closer inspection on manipulation tasks: due to their limited distance to user HMD during the session, they may employ a dependent quantization model to improve spatial accuracy.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p><a href="https://immersive-web.github.io/webxr/">https://immersive-web.github.io/webxr/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p><a href="https://sketchfab.com/">https://sketchfab.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p><a href="https://www.w3.org/TR/webxr/">https://www.w3.org/TR/webxr/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>Axis-Aligned Bounding Box.</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p><a href="http://osiris.itabc.cnr.it/scenebaker/index.php/projects/aton/">http://osiris.itabc.cnr.it/scenebaker/index.php/projects/aton/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p>“TourismA” 2018 event - <a href="http://www.tourisma.it/home-2/">http://www.tourisma.it/home-2/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p>comma-separated values, ASCII file.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p><a href="https://nodejs.org/en/">https://nodejs.org/en/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9"><span class="c-article-footnote--listed__index">9.</span><div class="c-article-footnote--listed__content"><p><a href="https://vr.tobii.com/">https://vr.tobii.com/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn10"><span class="c-article-footnote--listed__index">10.</span><div class="c-article-footnote--listed__content"><p><a href="https://sketchfab.com/TheHallwylMuseum">https://sketchfab.com/TheHallwylMuseum</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn11"><span class="c-article-footnote--listed__index">11.</span><div class="c-article-footnote--listed__content"><p><a href="https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/">https://www.raspberrypi.org/products/raspberry-pi-3-model-b-plus/</a>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn12"><span class="c-article-footnote--listed__index">12.</span><div class="c-article-footnote--listed__content"><p><a href="https://revealvr.eu/">https://revealvr.eu/</a>.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Agus M, Marton F, Bettio F, Gobbetti E (2016) Interactive 3d exploration of a virtual sculpture collection: an" /><p class="c-article-references__text" id="ref-CR1">Agus M, Marton F, Bettio F, Gobbetti E (2016) Interactive 3d exploration of a virtual sculpture collection: an analysis of user behavior in museum setting. In: Proceedings of the 13th eurographics workshop on graphics and cultural heritage</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Antal A et al (2016) A complete workflow from the data collection on the field to the deployment of a virtual " /><p class="c-article-references__text" id="ref-CR2">Antal A et al (2016) A complete workflow from the data collection on the field to the deployment of a virtual museum: the case of virtual sarmizegetusa</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SG. Barsanti, B. Fanini, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Barsanti SG, Fanini B et al (2018) The winckelmann300 project: Dissemination of culture with virtual reality a" /><p class="c-article-references__text" id="ref-CR3">Barsanti SG, Fanini B et al (2018) The winckelmann300 project: Dissemination of culture with virtual reality at the capitoline museum in rome. Int Arch Photogramm Remote Sens Spat Inf Sci 42(2):371–378. <a href="https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/371/2018/">https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2/371/2018/</a>
                     </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.5194%2Fisprs-archives-XLII-2-371-2018" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20winckelmann300%20project%3A%20Dissemination%20of%20culture%20with%20virtual%20reality%20at%20the%20capitoline%20museum%20in%20rome&amp;journal=Int%20Arch%20Photogramm%20Remote%20Sens%20Spat%20Inf%20Sci&amp;volume=42&amp;issue=2&amp;pages=371-378&amp;publication_year=2018&amp;author=Barsanti%2CSG&amp;author=Fanini%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Boletsis, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Boletsis C (2017) The new era of virtual reality locomotion: a systematic literature review of techniques and " /><p class="c-article-references__text" id="ref-CR4">Boletsis C (2017) The new era of virtual reality locomotion: a systematic literature review of techniques and a proposed typology. Multimodal Technol Interact 1(4):24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fmti1040024" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20new%20era%20of%20virtual%20reality%20locomotion%3A%20a%20systematic%20literature%20review%20of%20techniques%20and%20a%20proposed%20typology&amp;journal=Multimodal%20Technol%20Interact&amp;volume=1&amp;issue=4&amp;publication_year=2017&amp;author=Boletsis%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Butcher PW, Roberts JC, Ritsos PD (2016) Immersive analytics with WebVR and Google cardboard. Posters of IEEE " /><p class="c-article-references__text" id="ref-CR5">Butcher PW, Roberts JC, Ritsos PD (2016) Immersive analytics with WebVR and Google cardboard. Posters of IEEE VIS</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Butcher PW, John NW, Ritsos PD (2018) Towards a framework for immersive analytics on the web" /><p class="c-article-references__text" id="ref-CR6">Butcher PW, John NW, Ritsos PD (2018) Towards a framework for immersive analytics on the web</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Karen B.. Chen, Kevin. Ponto, Mary E.. Sesto, Robert G.. Radwin, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Chen KB, Ponto K, Sesto ME, Radwin RG (2014) Influence of altered visual feedback on neck movement for a virtu" /><p class="c-article-references__text" id="ref-CR7">Chen KB, Ponto K, Sesto ME, Radwin RG (2014) Influence of altered visual feedback on neck movement for a virtual reality rehabilitative system. In: Proceedings of the human factors and ergonomics society annual meeting, vol 58. SAGE Publications Sage, Los Angeles, pp 693–697</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F1541931214581162" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Influence%20of%20altered%20visual%20feedback%20on%20neck%20movement%20for%20a%20virtual%20reality%20rehabilitative%20system&amp;journal=Proceedings%20of%20the%20Human%20Factors%20and%20Ergonomics%20Society%20Annual%20Meeting&amp;volume=58&amp;issue=1&amp;pages=693-697&amp;publication_year=2014&amp;author=Chen%2CKaren%20B.&amp;author=Ponto%2CKevin&amp;author=Sesto%2CMary%20E.&amp;author=Radwin%2CRobert%20G.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cliquet G, Perreira M, Picarougne F, Prié Y, Vigier T (2017) Towards HMD-based immersive analytics. In: Immers" /><p class="c-article-references__text" id="ref-CR8">Cliquet G, Perreira M, Picarougne F, Prié Y, Vigier T (2017) Towards HMD-based immersive analytics. In: Immersive analytics workshop, IEEE VIS 2017</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cook KA, Thomas JJ (2005) Illuminating the path: the research and development agenda for visual analytics. Tec" /><p class="c-article-references__text" id="ref-CR9">Cook KA, Thomas JJ (2005) Illuminating the path: the research and development agenda for visual analytics. Technical report, Pacific Northwest National Lab (PNNL), Richland</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dibbern C, Uhr M, Krupke D, Steinicke F (2018) Can WebVR further the adoption of virtual reality? Mensch und C" /><p class="c-article-references__text" id="ref-CR10">Dibbern C, Uhr M, Krupke D, Steinicke F (2018) Can WebVR further the adoption of virtual reality? Mensch und Computer 2018-Usability Professionals</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Daniel. Dworak, Maria. Pietruszka, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Dworak D, Pietruszka M (2015) Fast encoding of huge 3D data sets in lossless PNG format. In: New research in m" /><p class="c-article-references__text" id="ref-CR11">Dworak D, Pietruszka M (2015) Fast encoding of huge 3D data sets in lossless PNG format. In: New research in multimedia and internet systems. Springer, Berlin, pp 15–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20Intelligent%20Systems%20and%20Computing&amp;pages=15-24&amp;publication_year=2015&amp;author=Dworak%2CDaniel&amp;author=Pietruszka%2CMaria">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Tim. Dwyer, Kim. Marriott, Tobias. Isenberg, Karsten. Klein, Nathalie. Riche, Falk. Schreiber, Wolfgang. Stuerzlinger, Bruce H.. Thomas, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Dwyer T, Marriott K, Isenberg T, Klein K, Riche N, Schreiber F, Stuerzlinger W, Thomas BH (2018) Immersive ana" /><p class="c-article-references__text" id="ref-CR12">Dwyer T, Marriott K, Isenberg T, Klein K, Riche N, Schreiber F, Stuerzlinger W, Thomas BH (2018) Immersive analytics: an introduction. In: Immersive analytics. Springer, Berlin, pp 1–23</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Immersive%20Analytics&amp;pages=1-23&amp;publication_year=2018&amp;author=Dwyer%2CTim&amp;author=Marriott%2CKim&amp;author=Isenberg%2CTobias&amp;author=Klein%2CKarsten&amp;author=Riche%2CNathalie&amp;author=Schreiber%2CFalk&amp;author=Stuerzlinger%2CWolfgang&amp;author=Thomas%2CBruce%20H.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fanini B, d’Annibale E (2016) A framework for compact and improved panoramic VR dissemination. In: Proceedings" /><p class="c-article-references__text" id="ref-CR13">Fanini B, d’Annibale E (2016) A framework for compact and improved panoramic VR dissemination. In: Proceedings of the 14th eurographics workshop on graphics and cultural heritage, Eurographics Association, pp 33–42</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fanini B, Pescarin S, Palombini A (2019) A cloud-based architecture for processing and dissemination of 3D lan" /><p class="c-article-references__text" id="ref-CR14">Fanini B, Pescarin S, Palombini A (2019) A cloud-based architecture for processing and dissemination of 3D landscapes online. Digital Applications in Archaeology and Cultural Heritage p. e00100</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Gu, SJ. Gortler, H. Hoppe, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Gu X, Gortler SJ, Hoppe H (2002) Geometry images. ACM Trans Graph (TOG) 21(3):355–361" /><p class="c-article-references__text" id="ref-CR15">Gu X, Gortler SJ, Hoppe H (2002) Geometry images. ACM Trans Graph (TOG) 21(3):355–361</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F566654.566589" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometry%20images&amp;journal=ACM%20Trans%20Graph%20%28TOG%29&amp;volume=21&amp;issue=3&amp;pages=355-361&amp;publication_year=2002&amp;author=Gu%2CX&amp;author=Gortler%2CSJ&amp;author=Hoppe%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hadjar H, Meziane A, Gherbi R, Setitra I, Aouaa N (2018) WebVR based interactive visualization of open health " /><p class="c-article-references__text" id="ref-CR16">Hadjar H, Meziane A, Gherbi R, Setitra I, Aouaa N (2018) WebVR based interactive visualization of open health data. In: Proceedings of the 2nd international conference on web studies. ACM, pp 56–63</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AK. Jain, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Jain AK (2010) Data clustering: 50 years beyond k-means. Pattern Recogn Lett 31(8):651–666" /><p class="c-article-references__text" id="ref-CR17">Jain AK (2010) Data clustering: 50 years beyond k-means. Pattern Recogn Lett 31(8):651–666</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.patrec.2009.09.011" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Data%20clustering%3A%2050%20years%20beyond%20k-means&amp;journal=Pattern%20Recogn%20Lett&amp;volume=31&amp;issue=8&amp;pages=651-666&amp;publication_year=2010&amp;author=Jain%2CAK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Jankowski, N. Andrienko, G. Andrienko, S. Kisilevich, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Jankowski P, Andrienko N, Andrienko G, Kisilevich S (2010) Discovering landmark preferences and movement patte" /><p class="c-article-references__text" id="ref-CR18">Jankowski P, Andrienko N, Andrienko G, Kisilevich S (2010) Discovering landmark preferences and movement patterns from photo postings. Trans GIS 14(6):833–852</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-9671.2010.01235.x" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Discovering%20landmark%20preferences%20and%20movement%20patterns%20from%20photo%20postings&amp;journal=Trans%20GIS&amp;volume=14&amp;issue=6&amp;pages=833-852&amp;publication_year=2010&amp;author=Jankowski%2CP&amp;author=Andrienko%2CN&amp;author=Andrienko%2CG&amp;author=Kisilevich%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knorr S, Ozcinar C, Fearghail CO, Smolic A (2018) Director’s cut-a combined dataset for visual attention analy" /><p class="c-article-references__text" id="ref-CR19">Knorr S, Ozcinar C, Fearghail CO, Smolic A (2018) Director’s cut-a combined dataset for visual attention analysis in cinematic VR content</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Limper, Y. Jung, J. Behr, T. Sturm, T. Franke, K. Schwenk, A. Kuijper, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Limper M, Jung Y, Behr J, Sturm T, Franke T, Schwenk K, Kuijper A (2013) Fast, progressive loading of binary-e" /><p class="c-article-references__text" id="ref-CR20">Limper M, Jung Y, Behr J, Sturm T, Franke T, Schwenk K, Kuijper A (2013) Fast, progressive loading of binary-encoded declarative-3d web content. IEEE Comput Graph Appl 33(5):26–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2013.52" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%2C%20progressive%20loading%20of%20binary-encoded%20declarative-3d%20web%20content&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=33&amp;issue=5&amp;pages=26-36&amp;publication_year=2013&amp;author=Limper%2CM&amp;author=Jung%2CY&amp;author=Behr%2CJ&amp;author=Sturm%2CT&amp;author=Franke%2CT&amp;author=Schwenk%2CK&amp;author=Kuijper%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Löwe T, Stengel M, Förster EC, Grogorick S, Magnor M (2015) Visualization and analysis of head movement and ga" /><p class="c-article-references__text" id="ref-CR21">Löwe T, Stengel M, Förster EC, Grogorick S, Magnor M (2015) Visualization and analysis of head movement and gaze data for immersive video in head-mounted displays. In: Proceedings of the workshop on eye tracking and visualization (ETVIS), vol 1</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Maclntyre B, Smith TF (2019) Thoughts on the future of WebXR and the immersive web. In: 2018 IEEE internationa" /><p class="c-article-references__text" id="ref-CR22">Maclntyre B, Smith TF (2019) Thoughts on the future of WebXR and the immersive web. In: 2018 IEEE international symposium on mixed and augmented reality adjunct (ISMAR-Adjunct). IEEE, pp 338–342</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Gurmeet Singh. Manku, Rajeev. Motwani, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Manku GS, Motwani R (2002) Approximate frequency counts over data streams. In: VLDB’02 proceedings of the 28th" /><p class="c-article-references__text" id="ref-CR23">Manku GS, Motwani R (2002) Approximate frequency counts over data streams. In: VLDB’02 proceedings of the 28th international conference on very large databases. Elsevier, Amsterdam, pp 346–357</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=VLDB%20%2702%3A%20Proceedings%20of%20the%2028th%20International%20Conference%20on%20Very%20Large%20Databases&amp;pages=346-357&amp;publication_year=2002&amp;author=Manku%2CGurmeet%20Singh&amp;author=Motwani%2CRajeev">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Meghini, R. Scopigno, J. Richards, B. Fanini, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Meghini C, Scopigno R, Richards J, Fanini B et al (2017) Ariadne: a research infrastructure for archaeology. J" /><p class="c-article-references__text" id="ref-CR24">Meghini C, Scopigno R, Richards J, Fanini B et al (2017) Ariadne: a research infrastructure for archaeology. J Comput Cult Herit (JOCCH) 10(3):18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ariadne%3A%20a%20research%20infrastructure%20for%20archaeology&amp;journal=J%20Comput%20Cult%20Herit%20%28JOCCH%29&amp;volume=10&amp;issue=3&amp;publication_year=2017&amp;author=Meghini%2CC&amp;author=Scopigno%2CR&amp;author=Richards%2CJ&amp;author=Fanini%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="V. Sitzmann, A. Serrano, A. Pavel, M. Agrawala, D. Gutierrez, B. Masia, G. Wetzstein, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Sitzmann V, Serrano A, Pavel A, Agrawala M, Gutierrez D, Masia B, Wetzstein G (2018) Saliency in VR: how do pe" /><p class="c-article-references__text" id="ref-CR25">Sitzmann V, Serrano A, Pavel A, Agrawala M, Gutierrez D, Masia B, Wetzstein G (2018) Saliency in VR: how do people explore virtual environments? IEEE Trans Vis Comput Graph 24(4):1633–1642</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2018.2793599" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Saliency%20in%20VR%3A%20how%20do%20people%20explore%20virtual%20environments%3F&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=24&amp;issue=4&amp;pages=1633-1642&amp;publication_year=2018&amp;author=Sitzmann%2CV&amp;author=Serrano%2CA&amp;author=Pavel%2CA&amp;author=Agrawala%2CM&amp;author=Gutierrez%2CD&amp;author=Masia%2CB&amp;author=Wetzstein%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Smith, J. Henderson, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Smith T, Henderson J (2008) Attentional synchrony in static and dynamic scenes. J Vis 8(6):773–773" /><p class="c-article-references__text" id="ref-CR26">Smith T, Henderson J (2008) Attentional synchrony in static and dynamic scenes. J Vis 8(6):773–773</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1167%2F8.6.773" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Attentional%20synchrony%20in%20static%20and%20dynamic%20scenes&amp;journal=J%20Vis&amp;volume=8&amp;issue=6&amp;pages=773-773&amp;publication_year=2008&amp;author=Smith%2CT&amp;author=Henderson%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Upenik E, Ebrahimi T (2017) A simple method to obtain visual attention data in head mounted virtual reality. I" /><p class="c-article-references__text" id="ref-CR27">Upenik E, Ebrahimi T (2017) A simple method to obtain visual attention data in head mounted virtual reality. In: 2017 IEEE international conference on multimedia &amp; expo workshops (ICMEW). IEEE, pp 73–78</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vincent C, Soroli E, Engemann H, Hendriks H, Hickmann M (2018) Tobii or not tobii? assessing the validity of e" /><p class="c-article-references__text" id="ref-CR28">Vincent C, Soroli E, Engemann H, Hendriks H, Hickmann M (2018) Tobii or not tobii? assessing the validity of eye tracking data: Challenges and solutions. In: Scandinavian workshop on applied eye tracking (SWAET)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wagner Filho JA, Rey MF, Freitas CM, Nedel L (2018) Immersive visualization of abstract information: an evalua" /><p class="c-article-references__text" id="ref-CR29">Wagner Filho JA, Rey MF, Freitas CM, Nedel L (2018) Immersive visualization of abstract information: an evaluation on dimensionally-reduced data scatterplots. In: Proceedings of the 25th IEEE conference on virtual reality and 3D user interfaces (March 2018), vol 2, p 4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RH. Wiggins, HC. Davidson, HR. Harnsberger, JR. Lauman, PA. Goede, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Wiggins RH, Davidson HC, Harnsberger HR, Lauman JR, Goede PA (2001) Image file formats: past, present, and fut" /><p class="c-article-references__text" id="ref-CR30">Wiggins RH, Davidson HC, Harnsberger HR, Lauman JR, Goede PA (2001) Image file formats: past, present, and future. Radiographics 21(3):789–798</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1148%2Fradiographics.21.3.g01ma25789" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Image%20file%20formats%3A%20past%2C%20present%2C%20and%20future&amp;journal=Radiographics&amp;volume=21&amp;issue=3&amp;pages=789-798&amp;publication_year=2001&amp;author=Wiggins%2CRH&amp;author=Davidson%2CHC&amp;author=Harnsberger%2CHR&amp;author=Lauman%2CJR&amp;author=Goede%2CPA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wille M, Adolph L, Grauel B, Wischniewski S, Theis S, Alexander T (2014) Prolonged work with head mounted disp" /><p class="c-article-references__text" id="ref-CR31">Wille M, Adolph L, Grauel B, Wischniewski S, Theis S, Alexander T (2014) Prolonged work with head mounted displays. In: Proceedings of the 2014 ACM international symposium on wearable computers: adjunct program. ACM, pp 221–224</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PC. Wong, J. Thomas, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Wong PC, Thomas J (2004) Visual analytics. IEEE Comput Graph Appl 5:20–21" /><p class="c-article-references__text" id="ref-CR32">Wong PC, Thomas J (2004) Visual analytics. IEEE Comput Graph Appl 5:20–21</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2004.39" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20analytics&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=5&amp;pages=20-21&amp;publication_year=2004&amp;author=Wong%2CPC&amp;author=Thomas%2CJ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00405-w-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>Authors want to thank the Hallwyl Museum and their 3D models freely available (CC BY-SA 4.0) on SketchFab; VisionLab at Computer Science department (Sapienza University) and volunteers; D. Ferdani and E. Demetrescu (CNR ISPC/ITABC) for samples of 3D models used for Keys2Rome project (<a href="http://keys2rome.eu/">http://keys2rome.eu/</a>); and A. Palombini, I. Cerato and L. Rescic (CNR ISPC/ITABC) for VR installation support during “TourismA 2018” event.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CNR ISPC/ITABC, Rome, Italy</p><p class="c-article-author-affiliation__authors-list">Bruno Fanini</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Computer Science, Sapienza University, Rome, Italy</p><p class="c-article-author-affiliation__authors-list">Bruno Fanini &amp; Luigi Cinque</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Bruno-Fanini"><span class="c-article-authors-search__title u-h3 js-search-name">Bruno Fanini</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Bruno+Fanini&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Bruno+Fanini" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Bruno+Fanini%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Luigi-Cinque"><span class="c-article-authors-search__title u-h3 js-search-name">Luigi Cinque</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Luigi+Cinque&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Luigi+Cinque" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Luigi+Cinque%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00405-w/email/correspondent/c1/new">Bruno Fanini</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="Sec26"><div class="c-article-section" id="Sec26-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec26">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec26-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv1344496"><script src="https://www.edge-cdn.net/videojs_1344496?jsdiv=mijsvdiv1344496&amp;playerskin=37016" defer="defer"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>Supplementary material 1 (mp4 88877 KB)</p></div></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Encoding%20immersive%20sessions%20for%20online%2C%20interactive%20VR%20analytics&amp;author=Bruno%20Fanini%20et%20al&amp;contentID=10.1007%2Fs10055-019-00405-w&amp;publication=1359-4338&amp;publicationDate=2019-09-25&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00405-w" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00405-w" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Fanini, B., Cinque, L. Encoding immersive sessions for online, interactive VR analytics.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00405-w</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00405-w.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-02-06">06 February 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-09-13">13 September 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-09-25">25 September 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00405-w" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00405-w</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive analytics</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Session encoding</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Data quantization</span></li><li class="c-article-subject-list__subject"><span itemprop="about">WebVR</span></li><li class="c-article-subject-list__subject"><span itemprop="about">WebXR</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00405-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=405;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

