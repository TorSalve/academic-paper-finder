<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="An automatic method for motion capture-based exaggeration of facial ex"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Facial expressions have always attracted considerable attention as a form of nonverbal communication. In visual applications such as movies, games, and animations, people tend to be interested in..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="An automatic method for motion capture-based exaggeration of facial expressions with personality types"/>

    <meta name="dc.source" content="Virtual Reality 2013 17:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-08-28"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Facial expressions have always attracted considerable attention as a form of nonverbal communication. In visual applications such as movies, games, and animations, people tend to be interested in exaggerated expressions rather than regular expressions since the exaggerated ones deliver more vivid emotions. In this paper, we propose an automatic method for exaggeration of facial expressions from motion-captured data with a certain personality type. The exaggerated facial expressions are generated by using the exaggeration mapping (EM) that transforms facial motions into exaggerated motions. As all individuals do not have identical personalities, a conceptual mapping of the individual&#8217;s personality type for exaggerating facial expressions needs to be considered. The Myers&#8211;Briggs type indicator, which is a popular method for classifying personality types, is employed to define the personality-type-based EM. Further, we have experimentally validated the EM and simulations of facial expressions."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-08-28"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="219"/>

    <meta name="prism.endingPage" content="237"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0227-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0227-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0227-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0227-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="An automatic method for motion capture-based exaggeration of facial expressions with personality types"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2013/09"/>

    <meta name="citation_online_date" content="2013/08/28"/>

    <meta name="citation_firstpage" content="219"/>

    <meta name="citation_lastpage" content="237"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0227-8"/>

    <meta name="DOI" content="10.1007/s10055-013-0227-8"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0227-8"/>

    <meta name="description" content="Facial expressions have always attracted considerable attention as a form of nonverbal communication. In visual applications such as movies, games, and ani"/>

    <meta name="dc.creator" content="Seongah Chin"/>

    <meta name="dc.creator" content="Chung Yeon Lee"/>

    <meta name="dc.creator" content="Jaedong Lee"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Ahn S, Ozawa S (2004) Generating facial expressions based on estimation of muscular contraction parameters from facial feature points. In: Proceedings of IEEE international conference on systems, man and cybernetics, vol 1, pp 660&#8211;665"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGGRAPH 2002 Trans Graph; citation_title=Turning to the masters: motion capturing cartoons; citation_author=C Bregler, L Loeb, E Chuang, H Deshpande; citation_volume=21; citation_issue=3; citation_publication_date=2002; citation_pages=399-407; citation_id=CR2"/>

    <meta name="citation_reference" content="Brennan SE (1982) Caricature generator: the dynamic exaggeration of faces by computer. Leonardo, The MIT Press, 18(3):170&#8211;178"/>

    <meta name="citation_reference" content="Calder AJ, Young AW, Rowland D, Perrett DI (1997) Computer-enhanced emotion in facial expressions. In: Proceedings of the royal society b: biological sciences, vol 264, no 1383, pp 919&#8211;925"/>

    <meta name="citation_reference" content="citation_journal_title=Cognition; citation_title=Caricaturing facial expressions; citation_author=AJ Caldera, D Rowland, AW Young, I Nimmo-Smith, J Keane, DI Perrett; citation_volume=76; citation_issue=2; citation_publication_date=2000; citation_pages=105-146; citation_doi=10.1016/S0010-0277(00)00074-3; citation_id=CR5"/>

    <meta name="citation_reference" content="Cao Y, Faloutsos P, Pighin F (2003) Unsupervised learning for speech motion editing. In: Proceedings of ACM SIGGRAPH/eurographics symposium on Computer animation, pp 225&#8211;231"/>

    <meta name="citation_reference" content="Chai JX, Xiao J, Hodgins J (2003) Vision-based control of 3D facial animation. In: Eurographics/SIGGRAPH symposium on computer animation"/>

    <meta name="citation_reference" content="Chen H, Liu Z, Rose C, Xu Y, Shum H, Salesin D (2004) Example-Based Composite Sketching of Human Portraits. In: Proceedings of the 3rd international symposium on non-photorealistic animation and rendering, pp 95&#8211;153"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern C; citation_title=Emotional intensity-based facial expression cloning for low polygonal applications; citation_author=S Chin, KY Kim; citation_volume=39; citation_issue=3; citation_publication_date=2009; citation_pages=315-330; citation_doi=10.1109/TSMCC.2008.2011283; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Chin Opt Lett; citation_title=Exaggeration of facial expressions from motion capture data; citation_author=S Chin, CY Lee; citation_volume=8; citation_issue=1; citation_publication_date=2010; citation_pages=29-32; citation_doi=10.3788/COL20100801.0029; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Rob Syst; citation_title=Personality trait and facial expression filter-based brain-computer interface; citation_author=S Chin, CY Lee; citation_volume=10; citation_publication_date=2013; citation_pages=138; citation_id=CR11"/>

    <meta name="citation_reference" content="Chin S, Lee CY, Lee J (2009) Personal Style and non-negative matrix factorization based exaggerative expressions of face. In: Proceedings of the 2009 international conference on computer graphics and virtual Reality, pp 91&#8211;96"/>

    <meta name="citation_reference" content="Chuang ES (2004) Analysis, synthesis, and retargeting of facial expressions. Ph.D. dissertation, Stanford University, Palo Alto, CA"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Automatic generation of 3D caricatures based on artistic deformation styles; citation_author=L Clarke, M Chen; citation_volume=17; citation_issue=6; citation_publication_date=2011; citation_pages=808-821; citation_doi=10.1109/TVCG.2010.76; citation_id=CR14"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Imag Underst; citation_title=Facial expression recognition from video sequences: temporal and static modeling; citation_author=I Cohen, N Sebe, A Garg, LS Chen, TS Huang; citation_volume=91; citation_issue=1&#8211;2; citation_publication_date=2003; citation_pages=160-187; citation_doi=10.1016/S1077-3142(03)00081-X; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Active appearance models; citation_author=TF Cootes, GJ Edwards, CJ Taylor; citation_volume=23; citation_issue=6; citation_publication_date=2001; citation_pages=681-685; citation_doi=10.1109/34.927467; citation_id=CR16"/>

    <meta name="citation_reference" content="Deng Z, Neumann U (2006) eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls. In: Proceedings of ACM SIGGRAPH/EG symposium on computer animation, pp 251&#8211;259"/>

    <meta name="citation_reference" content="Ekman P (1972) Universal and cultural differences in facial expressions of emotion. In: Nebraska symposium on motivation, vol 38, pp 207&#8211;283"/>

    <meta name="citation_reference" content="citation_title=Unmasking the face: a guide to recognizing emotions from facial clues; citation_publication_date=2003; citation_id=CR19; citation_author=P Ekman; citation_author=WV Friesen; citation_publisher=Malor Books"/>

    <meta name="citation_reference" content="citation_journal_title=J Manag; citation_title=Using the Meyers&#8211;Briggs Type Indicator to study managers: a literature review and research agenda; citation_author=W Gardner, M Martinko; citation_volume=22; citation_issue=1; citation_publication_date=1996; citation_pages=45-83; citation_doi=10.1177/014920639602200103; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=J Mach Learn Res; citation_title=Non-negative Matrix Factorization with Sparseness Constraints; citation_author=PO Hoyer; citation_volume=5; citation_publication_date=2004; citation_pages=1457-1469; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Cogn Emot; citation_title=Affective intensity and emotional responses; citation_author=D Keltner, P Ekman; citation_volume=10; citation_issue=3; citation_publication_date=1996; citation_pages=323-328; citation_doi=10.1080/026999396380277; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=J Res Pers; citation_title=Emotional expression, and the art of empirical epiphany; citation_author=D Keltner, P Ekman; citation_volume=38; citation_publication_date=2004; citation_pages=37-44; citation_doi=10.1016/j.jrp.2003.09.006; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Imag Process; citation_title=Facial expression recognition in image sequences using geometric deformation features and support vector machines; citation_author=I Kotsia, I Pitas; citation_volume=16; citation_issue=1; citation_publication_date=2007; citation_pages=172-187; citation_doi=10.1109/TIP.2006.884954; citation_id=CR24"/>

    <meta name="citation_reference" content="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: Proceedings of 2nd international symposium on smart graphics, ACM Press, pp 107&#8211;115"/>

    <meta name="citation_reference" content="citation_journal_title=Nature; citation_title=Learning the parts of objects by non-negative matrix factorization; citation_author=DD Lee, HS Seung; citation_volume=401; citation_publication_date=1999; citation_pages=788-791; citation_doi=10.1038/44565; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Interactive 3D caricature from harmonic exaggeration; citation_author=T Lewiner, T Vieira, D Mart&#237;nez, A Peixoto, V Mello, L Velho; citation_volume=35; citation_issue=3; citation_publication_date=2011; citation_pages=586-595; citation_doi=10.1016/j.cag.2011.03.005; citation_id=CR27"/>

    <meta name="citation_reference" content="Lewis JP, Cordner M, Fong N (2000) Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: Proceedings of ACM SIGGRAPH 2000 international conference on computer graphics and interactive techniques, New Orleans, LO, pp 165&#8211;172"/>

    <meta name="citation_reference" content="Liang L, Chen H, Xu Y, Shum H (2002) Example-based caricature generation with exaggeration. In: Proceedings of the 10th Pacific conference of computer graphics and application, p 386"/>

    <meta name="citation_reference" content="Liu S, Wang J, Zhang M, Wang Z (2012) Three-dimensional cartoon facial animation based on art rules. The visual computer, ISSN: 0178-2789, pp 1&#8211;15"/>

    <meta name="citation_reference" content="Ma X, Le B, Deng Z (2009) Style learning and transferring for facial animation editing. In: Proceedings of ACM SIGGRAPH/Eurographics symposium on computer animation, pp 123&#8211;132"/>

    <meta name="citation_reference" content="citation_journal_title=J Res Pers; citation_title=Emotion perception threshold: individual differences in emotional sensitivity; citation_author=RA Martin, GE Berry, T Dobranski, M Horne, PG Dodgson; citation_volume=30; citation_issue=2; citation_publication_date=1996; citation_pages=290-305; citation_doi=10.1006/jrpe.1996.0019; citation_id=CR32"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Sketch express: a sketching interface for facial animation; citation_author=JC Miranda, X Alvarez, J Orvalho, D Gutierrez, AA Sousa, V Orvalho; citation_volume=36; citation_issue=6; citation_publication_date=2012; citation_pages=585-595; citation_doi=10.1016/j.cag.2012.03.002; citation_id=CR33"/>

    <meta name="citation_reference" content="Mo Z, Lewis JP, Neumann U (2004) Improved Automatic Caricature by Feature Normalization and Exaggeration. In: Proceedings of ACM SIGGRAPH 2004 sketches international conference on computer graphics and interactive techniques, pp 57&#8211;59"/>

    <meta name="citation_reference" content="citation_journal_title=J Bus Psychol; citation_title=The emotional intelligence of managers: assessing the construct validity of a nonverbal measure of people skill; citation_author=DA Morand; citation_volume=16; citation_issue=1; citation_publication_date=2001; citation_pages=21-33; citation_doi=10.1023/A:1007831603825; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_title=In MBTI manual: a guide to the development and use of the Myers&#8211;Briggs type indicator; citation_publication_date=1998; citation_id=CR36; citation_author=I Myers; citation_author=M McCaulley; citation_author=N Quenk; citation_author=A Hammer; citation_publisher=Consulting Psychologists Press"/>

    <meta name="citation_reference" content="Noh JY, Neumann U (2001) Expression cloning. In: Proceedings of ACM SIGGRAPH 2001 international conference on computer graphics and interactive techniques, pp 277&#8211;288"/>

    <meta name="citation_reference" content="citation_journal_title=Environmetrics; citation_title=Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values; citation_author=P Paatero, U Tapper; citation_volume=5; citation_issue=2; citation_publication_date=1994; citation_pages=111-126; citation_doi=10.1002/env.3170050203; citation_id=CR38"/>

    <meta name="citation_reference" content="citation_title=In MPEG-4 facial animation the standard, implementation and application; citation_publication_date=2002; citation_id=CR39; citation_author=IS Pandzic; citation_author=R Forchheimer; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="Parke FI (1972) Computer generated animation of faces. In: Proceedings of ACM annual conference, vol 1, pp 451&#8211;457"/>

    <meta name="citation_reference" content="citation_title=Computer facial animation; citation_publication_date=2008; citation_id=CR41; citation_author=FI Parke; citation_author=K Waters; citation_publisher=Wellesley, MA"/>

    <meta name="citation_reference" content="Pighin F, Hecker J, Lischinski D, Szeliski R, Salesin D (1995) &#8220;Synthesizing realistic facial expressions from photographs. In: Proceedings of SIGGRAPH 1998 international conference on computer graphics and interactive techniques, San Antonio, TX, pp 75&#8211;84"/>

    <meta name="citation_reference" content="Platt SM, Badler NI (1981) Animating facial expression. In: Proceedings of ACM SIGGRAPH computer graphics, vol 15, no 3, pp 245&#8211;252"/>

    <meta name="citation_reference" content="citation_title=3-D human modeling and animation; citation_publication_date=2003; citation_id=CR44; citation_author=P Ratner; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_title=How to draw caricatures; citation_publication_date=1984; citation_id=CR45; citation_author=L Redman; citation_publisher=McGraw-Hill"/>

    <meta name="citation_reference" content="Rhodes G (1997) Superportraits. Psychology Press, Hove, East Sussex, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Imaginat Cognit Pers; citation_title=Emotional intelligence; citation_author=P Salovey, JD Mayer; citation_volume=9; citation_issue=3; citation_publication_date=1990; citation_pages=185-211; citation_doi=10.2190/DUGG-P24E-52WK-6CDG; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_title=Nonlinguistic vocal indicators of emotion and psychopathology; citation_publication_date=1979; citation_id=CR48; citation_author=KR Scherer; citation_publisher=Emot Pers Psychopathol"/>

    <meta name="citation_reference" content="citation_journal_title=ACM SIGGRAPH 2005 Trans Graph; citation_title=Automatic Determination of Facial Muscle Activations from Sparse Motion Capture Marker Data; citation_author=E Sifakis, I Neverov, R Fedkiw; citation_volume=24; citation_issue=3; citation_publication_date=2005; citation_pages=417-425; citation_doi=10.1145/1073204.1073208; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Vis Comput; citation_title=Shape-based detail-preserving exaggeration of extremely accurate 3D faces; citation_author=A Soon, WS Lee; citation_volume=22; citation_issue=7; citation_publication_date=2006; citation_pages=478-492; citation_doi=10.1007/s00371-006-0023-5; citation_id=CR50"/>

    <meta name="citation_reference" content="citation_journal_title=Am J Orthod Dentofac Orthop; citation_title=The spontaneous smile in dynamic motion; citation_author=VV Tarantili, DJ Halazonetis, MN Spyropoulos; citation_volume=128; citation_issue=1; citation_publication_date=2005; citation_pages=8-15; citation_doi=10.1016/j.ajodo.2004.03.042; citation_id=CR51"/>

    <meta name="citation_reference" content="citation_journal_title=J Vis Computer Anim; citation_title=Physically-based facial modeling, analysis, and animation; citation_author=D Terzopoulos, K Waters; citation_volume=1; citation_issue=4; citation_publication_date=1990; citation_pages=73-80; citation_doi=10.1002/vis.4340010208; citation_id=CR52"/>

    <meta name="citation_reference" content="citation_journal_title=Pac Graph; citation_title=Manifold-based 3D face caricature generation with individualized facial feature extraction; citation_author=SF Wang, SH Lai; citation_volume=29; citation_issue=7; citation_publication_date=2010; citation_pages=2161-2168; citation_id=CR53"/>

    <meta name="citation_reference" content="citation_journal_title=Eurograph 2004 Comput Graph Forum; citation_title=High resolution acquisition, learning and transfer of dynamic 3-D facial expressions; citation_author=Y Wang, X Huang, CS Lee, S Zhang, Z Li, D Samaras, D Metaxas, A Elgammal, P Huang; citation_volume=23; citation_issue=3; citation_publication_date=2004; citation_pages=677-686; citation_doi=10.1111/j.1467-8659.2004.00800.x; citation_id=CR54"/>

    <meta name="citation_reference" content="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17&#8211;24"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Geometry-driven photorealistic facial expression synthesis; citation_author=Q Zhang, Z Liu, B Guo, D Terzopoulos, H Shum; citation_volume=12; citation_issue=1; citation_publication_date=2006; citation_pages=48-60; citation_doi=10.1109/TVCG.2006.9; citation_id=CR56"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Geometry-driven photorealistic facial expression synthesis; citation_author=Q Zhang, Z Liu, B Guo, D Terzopoulos, HY Shum; citation_volume=12; citation_issue=1; citation_publication_date=2006; citation_pages=48-60; citation_doi=10.1109/TVCG.2006.9; citation_id=CR57"/>

    <meta name="citation_author" content="Seongah Chin"/>

    <meta name="citation_author_email" content="solideochin@gmail.com"/>

    <meta name="citation_author_institution" content="Division of Multimedia, College of Engineering, Sungkyul University, Anyang, South Korea"/>

    <meta name="citation_author" content="Chung Yeon Lee"/>

    <meta name="citation_author_email" content="jamixlee@gmail.com"/>

    <meta name="citation_author_institution" content="Biointelligence Laboratory, School of Computer Science and Engineering, Seoul National University, Seoul, South Korea"/>

    <meta name="citation_author" content="Jaedong Lee"/>

    <meta name="citation_author_email" content="eyewater3199@gmail.com"/>

    <meta name="citation_author_institution" content="DXP Lab., Department of Computer Science, College of Engineering, Korea University, Seoul, South Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0227-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0227-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="An automatic method for motion capture-based exaggeration of facial expressions with personality types"/>
        <meta property="og:description" content="Facial expressions have always attracted considerable attention as a form of nonverbal communication. In visual applications such as movies, games, and animations, people tend to be interested in exaggerated expressions rather than regular expressions since the exaggerated ones deliver more vivid emotions. In this paper, we propose an automatic method for exaggeration of facial expressions from motion-captured data with a certain personality type. The exaggerated facial expressions are generated by using the exaggeration mapping (EM) that transforms facial motions into exaggerated motions. As all individuals do not have identical personalities, a conceptual mapping of the individual’s personality type for exaggerating facial expressions needs to be considered. The Myers–Briggs type indicator, which is a popular method for classifying personality types, is employed to define the personality-type-based EM. Further, we have experimentally validated the EM and simulations of facial expressions."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>An automatic method for motion capture-based exaggeration of facial expressions with personality types | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0227-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Facial expressions, Exaggeration, Facial motion capture, Facial motion cloning, Personality, MBTI, Nonnegative matrix factorization","kwrd":["Facial_expressions","Exaggeration","Facial_motion_capture","Facial_motion_cloning","Personality","MBTI","Nonnegative_matrix_factorization"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0227-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0227-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=227;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0227-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            An automatic method for motion capture-based exaggeration of facial expressions with personality types
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0227-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0227-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-08-28" itemprop="datePublished">28 August 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">An automatic method for motion capture-based exaggeration of facial expressions with personality types</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Seongah-Chin" data-author-popup="auth-Seongah-Chin" data-corresp-id="c1">Seongah Chin<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Sungkyul University" /><meta itemprop="address" content="grid.443733.4, 0000000405333955, Division of Multimedia, College of Engineering, Sungkyul University, Anyang, South Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chung_Yeon-Lee" data-author-popup="auth-Chung_Yeon-Lee">Chung Yeon Lee</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Seoul National University" /><meta itemprop="address" content="grid.31501.36, 0000000404705905, Biointelligence Laboratory, School of Computer Science and Engineering, Seoul National University, Seoul, South Korea" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jaedong-Lee" data-author-popup="auth-Jaedong-Lee">Jaedong Lee</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Korea University" /><meta itemprop="address" content="grid.222754.4, 0000000108402678, DXP Lab., Department of Computer Science, College of Engineering, Korea University, Seoul, South Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">219</span>–<span itemprop="pageEnd">237</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">492 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0227-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Facial expressions have always attracted considerable attention as a form of nonverbal communication. In visual applications such as movies, games, and animations, people tend to be interested in exaggerated expressions rather than regular expressions since the exaggerated ones deliver more vivid emotions. In this paper, we propose an automatic method for exaggeration of facial expressions from motion-captured data with a certain personality type. The exaggerated facial expressions are generated by using the exaggeration mapping (EM) that transforms facial motions into exaggerated motions. As all individuals do not have identical personalities, a conceptual mapping of the individual’s personality type for exaggerating facial expressions needs to be considered. The Myers–Briggs type indicator, which is a popular method for classifying personality types, is employed to define the personality-type-based EM. Further, we have experimentally validated the EM and simulations of facial expressions.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Facial expressions of characters, digital actors, and virtual humans have been widely used for delivering nonverbal messages based on their personalities in various fields such as video games, movies, social agents, commercials, and animations. The increasing role of entertainment in digital media is highlighted by the increased popularity of personalized services on the Internet and avatars designed to represent participants in social communities, homepages, and video games. Further, a character’s personality needs to be critically considered while designing the story of animations, video games, and movies. For instance, some actors can be assigned an extremely extroverted personality, whereas others are introverted. Their roles in reality would be contingent on their personalities. In general, it is desired that a specific personality be assigned to a character when designing the characters of animations or movies. If viewers are familiar with the character, they would anticipate the character to behave in a certain manner (Ratner <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ratner P (2003) 3-D human modeling and animation, 2nd edn. Wiley, New York" href="/article/10.1007/s10055-013-0227-8#ref-CR44" id="ref-link-section-d58766e377">2003</a>).</p><p>In addition, human beings communicate with each other by making facial expressions. The magnitude of an expression’s intensity causes different interpretations (Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books, Cambridge, MA" href="/article/10.1007/s10055-013-0227-8#ref-CR19" id="ref-link-section-d58766e383">2003</a>), even in only one facial expression (e.g., angry face and happy face). This principle lets us intuitively recognize that diverse levels of intensity exist for even a single expression. This fact is also supported by different research groups working in the fields of cognitive study, orthodontic research (Tarantili et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Tarantili VV, Halazonetis DJ, Spyropoulos MN (2005) The spontaneous smile in dynamic motion. Am J Orthod Dentofac Orthop 128(1):8–15" href="/article/10.1007/s10055-013-0227-8#ref-CR51" id="ref-link-section-d58766e386">2005</a>), and facial recognition (Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Cohen I, Sebe N, Garg A, Chen LS, Huang TS (2003) Facial expression recognition from video sequences: temporal and static modeling. Comput Vis Imag Underst 91(1–2):160–187" href="/article/10.1007/s10055-013-0227-8#ref-CR15" id="ref-link-section-d58766e389">2003</a>). Further, it is easier to understand such an abstract message in real life than in cyberspace or digital media because of the awareness of the atmosphere. Most researchers in the area of facial expression synthesis seem to focus on realistic expressions or retargeting rather than exaggerating expressions associated with a certain personality (Zhang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum H (2006a) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Vis Comput Graph 12(1):48–60" href="/article/10.1007/s10055-013-0227-8#ref-CR56" id="ref-link-section-d58766e392">2006</a>; Chuang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Chuang ES (2004) Analysis, synthesis, and retargeting of facial expressions. Ph.D. dissertation, Stanford University, Palo Alto, CA" href="/article/10.1007/s10055-013-0227-8#ref-CR13" id="ref-link-section-d58766e395">2004</a>; Kotsia and Pitas <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Kotsia I, Pitas I (2007) Facial expression recognition in image sequences using geometric deformation features and support vector machines. IEEE Trans Imag Process 16(1):172–187" href="/article/10.1007/s10055-013-0227-8#ref-CR24" id="ref-link-section-d58766e399">2007</a>; Noh and Neumann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Noh JY, Neumann U (2001) Expression cloning. In: Proceedings of ACM SIGGRAPH 2001 international conference on computer graphics and interactive techniques, pp 277–288" href="/article/10.1007/s10055-013-0227-8#ref-CR37" id="ref-link-section-d58766e402">2001</a>), and facial motion and animation editing schemes were reported (Deng and Neumann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Deng Z, Neumann U (2006) eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls. In: Proceedings of ACM SIGGRAPH/EG symposium on computer animation, pp 251–259" href="/article/10.1007/s10055-013-0227-8#ref-CR17" id="ref-link-section-d58766e405">2006</a>; Ma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Ma X, Le B, Deng Z (2009) Style learning and transferring for facial animation editing. In: Proceedings of ACM SIGGRAPH/Eurographics symposium on computer animation, pp 123–132" href="/article/10.1007/s10055-013-0227-8#ref-CR31" id="ref-link-section-d58766e408">2009</a>; Cao et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Cao Y, Faloutsos P, Pighin F (2003) Unsupervised learning for speech motion editing. In: Proceedings of ACM SIGGRAPH/eurographics symposium on Computer animation, pp 225–231" href="/article/10.1007/s10055-013-0227-8#ref-CR6" id="ref-link-section-d58766e411">2003</a>). Also, very recent works (Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Liu S, Wang J, Zhang M, Wang Z (2012) Three-dimensional cartoon facial animation based on art rules. The visual computer, ISSN: 0178-2789, pp 1–15" href="/article/10.1007/s10055-013-0227-8#ref-CR30" id="ref-link-section-d58766e414">2012</a>; Miranda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Miranda JC, Alvarez X, Orvalho J, Gutierrez D, Sousa AA, Orvalho V (2012) Sketch express: a sketching interface for facial animation. Comput Graph 36(6):585–595" href="/article/10.1007/s10055-013-0227-8#ref-CR33" id="ref-link-section-d58766e418">2012</a>; Lewiner et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Lewiner T, Vieira T, Martínez D, Peixoto A, Mello V, Velho L (2011) Interactive 3D caricature from harmonic exaggeration. Comput Graph 35(3):586–595" href="/article/10.1007/s10055-013-0227-8#ref-CR27" id="ref-link-section-d58766e421">2011</a>) have been reported by addressing the exaggerative methods of facial expressions. However, they seem to just take into consideration geometrical aspects of a face rather than personality types. We can easily imagine that the difference between twins can be found even in the same expression because of their personality even if the geometrical features of the faces are almost similar.</p><p>Interestingly, most viewers watching movies or animations and playing games tend to be interested in over-exaggerated expressions rather than real expressions. For instance, in real life, expressions usually are very subtle, while exaggerated expressions are observed clearly. Also, subtle expressions from avatars are probably more difficult to be recognized than exaggerated ones. Therefore, an automatic method for exaggerating facial expressions is required to make digital actors more alive, expressive, and attentive. However, the exaggeration tends to be considered as rather nonreality-based exaggeration, mostly contingent on artists’ skills than reality-based exaggeration, reproduced on the basis of real expressions.</p><p>Very few published articles have reported exaggeration of facial expressions that depend largely on the personality type of each individual; the authors have reported a preliminary method for exaggerating facial expressions in a previous article (Chin and Lee <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Chin S, Lee CY (2010) Exaggeration of facial expressions from motion capture data. Chin Opt Lett 8(1):29–32" href="/article/10.1007/s10055-013-0227-8#ref-CR10" id="ref-link-section-d58766e429">2010</a>).</p><p>In this article, we have enhanced the previous approach by adding a personality type to make the method to be advanced. It is considerably imperative to bridge the gap between a technique of exaggerations of facial expressions for human characters and a personality type being considered for each personal expression pattern. Moreover, drawing people’s attention is a rather fascinating idea.</p><p>In order to deal with a facial motion acquired by motion capture that can be decomposed, we employ a nonnegative matrix factorization (NMF) capable of computation for the exaggeration (Paatero and Tapper <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Paatero P, Tapper U (1994) Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values. Environmetrics 5(2):111–126" href="/article/10.1007/s10055-013-0227-8#ref-CR38" id="ref-link-section-d58766e438">1994</a>; Lee and Seung <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401:788–791" href="/article/10.1007/s10055-013-0227-8#ref-CR26" id="ref-link-section-d58766e441">1999</a>). The Myers–Briggs type indicator (MBTI) classification is used for determining a personality type (Myers et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Myers I, McCaulley M, Quenk N, Hammer A (1998) In MBTI manual: a guide to the development and use of the Myers–Briggs type indicator. Consulting Psychologists Press, Palo Alto, CA" href="/article/10.1007/s10055-013-0227-8#ref-CR36" id="ref-link-section-d58766e444">1998</a>; Morand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Morand DA (2001) The emotional intelligence of managers: assessing the construct validity of a nonverbal measure of people skill. J Bus Psychol 16(1):21–33" href="/article/10.1007/s10055-013-0227-8#ref-CR35" id="ref-link-section-d58766e447">2001</a>; Martin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Martin RA, Berry GE, Dobranski T, Horne M, Dodgson PG (1996) Emotion perception threshold: individual differences in emotional sensitivity. J Res Pers 30(2):290–305" href="/article/10.1007/s10055-013-0227-8#ref-CR32" id="ref-link-section-d58766e450">1996</a>; Salovey and Mayer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Salovey P, Mayer JD (1990) Emotional intelligence. Imaginat Cognit Pers 9(3):185–211" href="/article/10.1007/s10055-013-0227-8#ref-CR47" id="ref-link-section-d58766e454">1990</a>) when defining a proper exaggeration rate. Exaggerated motion data as an input for facial expressions are then cloned. Finally, we experimentally show the validity of the proposed method by implementing facial expressions.</p><p>The schematic approach is illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig1">1</a>. At first, we begin with the capturing of facial motions. Then, personality-type mapping is defined by the classification of MBTI. Facial motions are exaggerated by the exaggeration mapping (EM) defined by NMF. The detailed procedures are discussed in the following sections.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Schematic overview of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The rest of this paper is organized as follows: The next section consists of reviews of literatures of previous facial expression synthesis and personality types classified by MBTI. Then, we present a method of exaggerating facial motions, enabling the exaggerations of details. Later, the mapping of the personality type for the exaggeration is formularized, and we give an account of the conducted experiments. Finally, the paper is concluded. The preliminary conference version was presented in Chin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Chin S, Lee CY, Lee J (2009) Personal Style and non-negative matrix factorization based exaggerative expressions of face. In: Proceedings of the 2009 international conference on computer graphics and virtual Reality, pp 91–96" href="/article/10.1007/s10055-013-0227-8#ref-CR12" id="ref-link-section-d58766e484">2009</a>).</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background and literature review</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Facial expressions</h3><p>Facial expressions have played a key role in the field of nonverbal communication. Cognitive studies have shown that facial expressions are an indication of an individual’s emotional state. Emotions are transferred by internal signals to physically expressive parts of the body, such as the face, through the implementation of a magnitude of delicate functions. The varying intensity of a single facial expression can convey slightly different mimic nuances (Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books, Cambridge, MA" href="/article/10.1007/s10055-013-0227-8#ref-CR19" id="ref-link-section-d58766e499">2003</a>; Chin and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Chin S, Kim KY (2009) Emotional intensity-based facial expression cloning for low polygonal applications. IEEE Trans Syst Man Cybern C 39(3):315–330" href="/article/10.1007/s10055-013-0227-8#ref-CR9" id="ref-link-section-d58766e502">2009</a>).</p><p>General approaches for facial expression synthesis can be classified into the following three categories: interpolation, muscle-based expression, and performance-driven animation and retargeting. We consider interpolation-based facial expression to be a widely used technique because of the convenience of its use and its simplicity (Pighin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Pighin F, Hecker J, Lischinski D, Szeliski R, Salesin D (1995) “Synthesizing realistic facial expressions from photographs. In: Proceedings of SIGGRAPH 1998 international conference on computer graphics and interactive techniques, San Antonio, TX, pp 75–84" href="/article/10.1007/s10055-013-0227-8#ref-CR42" id="ref-link-section-d58766e508">1995</a>; Parke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1972" title="Parke FI (1972) Computer generated animation of faces. In: Proceedings of ACM annual conference, vol 1, pp 451–457" href="/article/10.1007/s10055-013-0227-8#ref-CR40" id="ref-link-section-d58766e511">1972</a>; Lewis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Lewis JP, Cordner M, Fong N (2000) Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: Proceedings of ACM SIGGRAPH 2000 international conference on computer graphics and interactive techniques, New Orleans, LO, pp 165–172" href="/article/10.1007/s10055-013-0227-8#ref-CR28" id="ref-link-section-d58766e514">2000</a>). The basic approach of this technique is to specify a key frame of an expression for a certain time and repeat the process for the other vertices in time. All frames between these key frames are automatically computed by an interpolation. A muscle-based facial expression is fundamentally based on the detailed anatomy of the face (Terzopoulos and Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Terzopoulos D, Waters K (1990) Physically-based facial modeling, analysis, and animation. J Vis Computer Anim 1(4):73–80" href="/article/10.1007/s10055-013-0227-8#ref-CR52" id="ref-link-section-d58766e517">1990</a>; Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17–24" href="/article/10.1007/s10055-013-0227-8#ref-CR55" id="ref-link-section-d58766e520">1987</a>; Platt and Badler <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Platt SM, Badler NI (1981) Animating facial expression. In: Proceedings of ACM SIGGRAPH computer graphics, vol 15, no 3, pp 245–252" href="/article/10.1007/s10055-013-0227-8#ref-CR43" id="ref-link-section-d58766e524">1981</a>; Sifakis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sifakis E, Neverov I, Fedkiw R (2005) Automatic Determination of Facial Muscle Activations from Sparse Motion Capture Marker Data. ACM SIGGRAPH 2005 Trans Graph 24(3):417–425" href="/article/10.1007/s10055-013-0227-8#ref-CR49" id="ref-link-section-d58766e527">2005</a>; Ahn and Ozawa <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Ahn S, Ozawa S (2004) Generating facial expressions based on estimation of muscular contraction parameters from facial feature points. In: Proceedings of IEEE international conference on systems, man and cybernetics, vol 1, pp 660–665" href="/article/10.1007/s10055-013-0227-8#ref-CR1" id="ref-link-section-d58766e530">2004</a>). Anatomically, the face is composed of a complex assembly of bones, muscles, nerves, blood vessels, connective tissue, and skin. The method manipulates facial muscles by assigning contraction values that assign muscle movements created by embedded muscles. Waters (Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17–24" href="/article/10.1007/s10055-013-0227-8#ref-CR55" id="ref-link-section-d58766e533">1987</a>) introduces two types of muscles: linear muscles that pull and sphincter muscles that squeeze. The influence of a particular muscle is determined by certain parameters. The upgrade version to Waters reports that the facial tissues employ a three-layer deformable lattice structure. Platt and Badler (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1981" title="Platt SM, Badler NI (1981) Animating facial expression. In: Proceedings of ACM SIGGRAPH computer graphics, vol 15, no 3, pp 245–252" href="/article/10.1007/s10055-013-0227-8#ref-CR43" id="ref-link-section-d58766e536">1981</a>) propose a muscle-based facial expression generation method by implementing a face surface in which the muscles are elastically interconnected with modeled springs. Sifakis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sifakis E, Neverov I, Fedkiw R (2005) Automatic Determination of Facial Muscle Activations from Sparse Motion Capture Marker Data. ACM SIGGRAPH 2005 Trans Graph 24(3):417–425" href="/article/10.1007/s10055-013-0227-8#ref-CR49" id="ref-link-section-d58766e539">2005</a>) propose a method of automatically determining muscle activations that track a sparse set of surface landmarks obtained from the motion capture marker data. The animation is obtained via a three-dimensional nonlinear finite element method showing visually plausible and anatomically correct deformations with a spatial and temporal coherence. Performance-driven animation and retargeting have been widely used in the field of facial expressions. The determination of facial animation control parameters seems to be a tedious process because a considerable number of these parameters have to be specified in order to generate even a simple expression sequence. Therefore, capturing the motion of live performers is necessary to reduce the time consumed by the above-mentioned process. The motion of the synthetic characters is very realistic, and additional effort is not required to determine the key frames. The original expressions of a performance are a good source for synthesizing facial expressions, and a retargeting process from a source face to a target face is required to create plausible expressions. Zhang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum HY (2006b) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Vis Comput Graph 12(1):48–60" href="/article/10.1007/s10055-013-0227-8#ref-CR57" id="ref-link-section-d58766e543">2006</a>) has developed a geometry-driven facial expression synthesis system. Given the feature-point positions of a facial expression, this system automatically synthesizes the corresponding expression image. Editing is allowed in order to create facial expressions. Chai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chai JX, Xiao J, Hodgins J (2003) Vision-based control of 3D facial animation. In: Eurographics/SIGGRAPH symposium on computer animation" href="/article/10.1007/s10055-013-0227-8#ref-CR7" id="ref-link-section-d58766e546">2003</a>) propose a combination of the strengths of a vision-based interface with those of the motion capture data for the interactive control of 3D facial animations. The user can interactively control actions by capturing the desired motions with a video camera. Bregler et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Bregler C, Loeb L, Chuang E, Deshpande H (2002) Turning to the masters: motion capturing cartoons. ACM SIGGRAPH 2002 Trans Graph 21(3):399–407" href="/article/10.1007/s10055-013-0227-8#ref-CR2" id="ref-link-section-d58766e549">2002</a>) developed a technique called cartoon capture and retargeting, which is used for tracking the motion of traditionally animated cartoons and retargeting this motion onto 3D models, 2D drawings, and photographs. By using animation as the source, these researchers produce new animations that are expressive, exaggerated, and nonrealistic. Wang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Wang Y, Huang X, Lee CS, Zhang S, Li Z, Samaras D, Metaxas D, Elgammal A, Huang P (2004) High resolution acquisition, learning and transfer of dynamic 3-D facial expressions. Eurograph 2004 Comput Graph Forum 23(3):677–686" href="/article/10.1007/s10055-013-0227-8#ref-CR54" id="ref-link-section-d58766e552">2004</a>) address fundamental issues regarding the use of high-quality dense 3D data samples depicting motions at various video speeds, e.g., human facial expressions, by exploring correspondences established between data in different frames of the same faces as well as between different faces.</p><p>In addition, exaggerations of facial expressions have been addressed in some researches. Liang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Liang L, Chen H, Xu Y, Shum H (2002) Example-based caricature generation with exaggeration. In: Proceedings of the 10th Pacific conference of computer graphics and application, p 386" href="/article/10.1007/s10055-013-0227-8#ref-CR29" id="ref-link-section-d58766e558">2002</a>) developed a system that automatically generates caricatures in 2D from input face images. In this approach, shape exaggeration and a transfer of texture styles were explored. However, the method seemed to emphasize more on the caricatures of the face than on natural-looking exaggerations. An example-based sketching method for human portraits in 2D was developed by employing two subsystems—one for the face and the other for the hair (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Chen H, Liu Z, Rose C, Xu Y, Shum H, Salesin D (2004) Example-Based Composite Sketching of Human Portraits. In: Proceedings of the 3rd international symposium on non-photorealistic animation and rendering, pp 95–153" href="/article/10.1007/s10055-013-0227-8#ref-CR8" id="ref-link-section-d58766e561">2004</a>). Facial exaggeration was handled by introducing separable subproblems for the eyes, mouth, and nose. This method emphasized on making the sketch of a face shape in 2D rather than on exaggerating facial expressions. By using a vector-based caricature algorithm, Soon and Lee (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Soon A, Lee WS (2006) Shape-based detail-preserving exaggeration of extremely accurate 3D faces. Vis Comput 22(7):478–492" href="/article/10.1007/s10055-013-0227-8#ref-CR50" id="ref-link-section-d58766e564">2006</a>) proposed that a working model’s characteristics are exaggerated. This algorithm automatically enhances the prominent facial features by comparing the working model to an average face. The method seemed to emphasize on 3D face modeling techniques rather than on creating real-time expressions. Recently, some approaches also tried to caricature facial expressions using artistic deformation styles (Liu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Liu S, Wang J, Zhang M, Wang Z (2012) Three-dimensional cartoon facial animation based on art rules. The visual computer, ISSN: 0178-2789, pp 1–15" href="/article/10.1007/s10055-013-0227-8#ref-CR30" id="ref-link-section-d58766e567">2012</a>; Clarke and Chen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Clarke L, Chen M (2011) Automatic generation of 3D caricatures based on artistic deformation styles. IEEE Trans Vis Comput Graph 17(6):808–821" href="/article/10.1007/s10055-013-0227-8#ref-CR14" id="ref-link-section-d58766e570">2011</a>). These caricatured facial expressions are well generated, but still require manual adjustments from the artist. On the other hand, Wang and Lai (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Wang SF, Lai SH (2010) Manifold-based 3D face caricature generation with individualized facial feature extraction. Pac Graph 29(7):2161–2168" href="/article/10.1007/s10055-013-0227-8#ref-CR53" id="ref-link-section-d58766e574">2010</a>) presented an automatic caricature generation method based on unique/specialized features of a person’s face and facial expressions extracted from different subjects.</p><h3 class="c-article__sub-heading" id="Sec4">Personality type</h3><p>MBTI (Myers et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Myers I, McCaulley M, Quenk N, Hammer A (1998) In MBTI manual: a guide to the development and use of the Myers–Briggs type indicator. Consulting Psychologists Press, Palo Alto, CA" href="/article/10.1007/s10055-013-0227-8#ref-CR36" id="ref-link-section-d58766e585">1998</a>), the most popular inventory of personality types in the United States today, is based on the work of the Swiss psychologist Carl Jung, as developed by Katherine Briggs and Isabel Briggs Myers. Jung’s theories—based on his observations of normal, healthy people—suggest that our behavioral differences are based on our inborn tendencies to use our minds in different ways. Four dichotomous dimensions classify individuals as extroverted (E) or introverted (I); sensing (S) or intuitive (N); thinking (T) or feeling (F); and judging (J) or perceiving (P). Combinations of the four preferences determine personality types. Each individual is classified into one of the possible four-letter codes. The effect of personality and emotion will also have an effect on expression (speech intonation, facial expressions, etc.) is partly dealt within previous research by Kshirsagar and Magnenat-Thalmann (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: Proceedings of 2nd international symposium on smart graphics, ACM Press, pp 107–115" href="/article/10.1007/s10055-013-0227-8#ref-CR25" id="ref-link-section-d58766e588">2002</a>) which describes a system that simulates personalized facial animation with speech and expressions, modulated through mood. Besides, Chin and Lee (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chin S, Lee CY (2013) Personality trait and facial expression filter-based brain-computer interface. Int J Adv Rob Syst 10:138" href="/article/10.1007/s10055-013-0227-8#ref-CR11" id="ref-link-section-d58766e591">2013</a>) presented how facial expressions convey different personality types in order to better realize facial expressions for the brain–computer interface.</p><h3 class="c-article__sub-heading" id="Sec5">Emotional intelligence</h3><p>Emotional intelligence is broadly defined as references of the ability to monitor one’s own and others’ feelings and emotions. Sensing others’ emotional states is obviously a skill central to the entire notion of interpersonal communicative competence. Martin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Martin RA, Berry GE, Dobranski T, Horne M, Dodgson PG (1996) Emotion perception threshold: individual differences in emotional sensitivity. J Res Pers 30(2):290–305" href="/article/10.1007/s10055-013-0227-8#ref-CR32" id="ref-link-section-d58766e602">1996</a>) presents a technique for measuring the emotion-decoding ability that is sensitive to individual differences with respect to immediate, experiential emotional awareness, or knowledge-by-acquaintance. The idea is to determine the exposure duration threshold at which individuals are able to distinguish facial expressions of pleasant versus unpleasant emotions at a level better than chance. Thresholds are significantly related to the thinking/feeling scale of the Myers–Briggs type indicator.</p><p>Salovey and Mayer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Salovey P, Mayer JD (1990) Emotional intelligence. Imaginat Cognit Pers 9(3):185–211" href="/article/10.1007/s10055-013-0227-8#ref-CR47" id="ref-link-section-d58766e608">1990</a>) propose the concept of emotional intelligence as a set of skills related to the ability to monitor one’s own and others’ feelings and emotions, to discriminate among them, and to use this information to guide one’s thinking and actions. Keltner and Ekman (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Keltner D, Ekman P (2004) Emotional expression, and the art of empirical epiphany. J Res Pers 38:37–44" href="/article/10.1007/s10055-013-0227-8#ref-CR23" id="ref-link-section-d58766e611">2004</a>) show that individual differences in facial expressions of emotions are related in theoretically coherent ways to personal adjustment in response to loss, in interpersonal relationships, and in the broad context of psychopathology. The findings show a primary correlation in nature. Therefore, it is important to address how facial expressions contribute to personal adjustment.</p></div></div></section><section aria-labelledby="Sec6"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">Exaggerative facial expressions</h2><div class="c-article-section__content" id="Sec6-content"><p>In order to make exaggerated facial expressions with personality types, we design four critical procedures as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig2">2</a>. At first, a 3D customized model is introduced to obtain a personalized facial motion capture data via facial motion cloning (FMC), which will be described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0227-8#Sec12">3.3.1</a>. Second, EM is defined to transform facial motion capture data into exaggerated ones in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0227-8#Sec7">3.1</a>. Then, personality-based exaggeration mapping (PEM) in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0227-8#Sec8">3.2</a> is defined to take into account the individual’s personality type. Finally, exaggerated facial expressions with personality types are created by using muscle-based techniques.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Block diagram of the proposed method</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>We start with capturing facial motion capture data called facial motion (FM) from the six facial expressions of an actor, including surprise, fear, disgust, anger, sadness, and happiness. An example-based motion capture data were collected from the actor who had displayed six facial expressions, including those of surprise, fear, disgust, anger, sadness, and happiness. Cross-cultural research presents six universal expressions (Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books, Cambridge, MA" href="/article/10.1007/s10055-013-0227-8#ref-CR19" id="ref-link-section-d58766e656">2003</a>). We assume facial expressions become exaggerated given an emotion not taking into consideration situational contexts. The purpose of the approach is to tell exaggerative differences in exaggerating of facial expressions from personality types given six emotions. We have focused on the expression itself rather than the situational contexts.</p><h3 class="c-article__sub-heading" id="Sec7">Exaggeration mapping</h3><p>Exaggeration mapping (EM) transforms a facial motion (FM) into an exaggerated FM. In short, EM takes FM as an input to make exaggerated FM as a output. In order to appropriately compute the exaggeration of the FMs of six expressions, a sequence of FMs needs to be factorized. Each FM consists of 3D movements of 35 markers attached to the principal facial muscles of the actor. The fundamental idea of the approach to EM is to place more emphasis on the features where movements are relatively great. The first step is to seek appropriate data factorization. A task to acquire the optimal structure of feature analysis is thought to be a crucial step. FMs need to be factorized into two matrices. Once the decomposition matrices are obtained, a transformation should be applied on the matrices. We employ NMF, which is successfully applied to a variety of data sets to find part-based linear representations of nonnegative data because of its advantage over principal component analysis (PCA) and independent component analysis (ICA). We know that ICA, a variant of PCA assumes that the hidden variables are statistically independent and non-Gaussian. In addition, applying ICA to the facial images to make the encodings independent represents in basis images that are not suitable for learning parts-based representation. As an alternative way, PCA computes a set of orthogonal subspace basis images and projects the image into the compressed subspace. However, the encodings are not sparse. Fortunately, the NMF representation contains sparseness, which is critical for a part-based representation. The details can be explained further (Paatero and Tapper <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Paatero P, Tapper U (1994) Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values. Environmetrics 5(2):111–126" href="/article/10.1007/s10055-013-0227-8#ref-CR38" id="ref-link-section-d58766e666">1994</a>; Lee and Seung <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401:788–791" href="/article/10.1007/s10055-013-0227-8#ref-CR26" id="ref-link-section-d58766e669">1999</a>; Hoyer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Hoyer PO (2004) Non-negative Matrix Factorization with Sparseness Constraints. J Mach Learn Res 5:1457–1469" href="/article/10.1007/s10055-013-0227-8#ref-CR21" id="ref-link-section-d58766e672">2004</a>).</p><p>The mapping method primarily comprises two procedures. First, NMF decomposes FMs into a basis vector and its weight matrix. Then, an FM is exaggerated by multiplying both the weights and residuals acquired from the NMF decomposition with a specific exaggeration rate.</p><p>The original FM acquired from an optical motion capture system (Motion Analysis Co., Santa Rosa, CA, USA, 120 frames/s) includes the movements of markers on the facial muscles and consists of either positive values or negative values. Therefore, we first translate by adding the minimum value of the data to make all the nonnegative values applicable to NMF.</p><p>The FMs are regarded as an <i>n</i> × <i>m</i> matrix <i>M</i>; each column of this matrix consists of the <i>x</i>, <i>y</i>, and <i>z</i> coordinates of the markers. Given an FM denoted by <i>M</i>, NMF decomposes <i>M</i> into two matrices <i>B</i> and <i>E</i> as given in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ1">1</a>) in order to approximate the FM.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ M_{iu} \approx \left( {BE} \right)_{iu} = \sum\limits_{a = 1}^{r} {B_{ia} E_{au} } $$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div>
                        <p>Each column of matrix <i>B</i> contains a basis vector, while each column of <i>E</i> includes the weights corresponding to the measurement column in <i>M</i> using the bases from <i>B</i>. The dimensions of the factorized matrices <i>B</i> and <i>E</i> are <i>n</i> × <i>r</i> and <i>r</i> × <i>m</i>, respectively, with <i>r</i> satisfying the condition that <i>(n</i> + <i>m)r</i> &lt; <i>nm</i>. In order to estimate the factorization matrices, an objective function has to be defined. This objective function works out the likelihood of computing the FMs in <i>M</i> from the basis <i>B</i> and encodings <i>E</i>. A possible objective function for optimization that we have used is given in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ2">2</a>).</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ H = \sum\limits_{i = 1}^{n} {\sum\limits_{u = 1}^{m} {\left[ {M_{iu} \log \left( {BE} \right)_{iu} - \left( {BE} \right)_{iu} } \right]} } $$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div>
                        <p>Solutions for NMF begin with the initialization of nonnegative conditions for <i>B</i> and <i>E</i>. With a continuation of the iteration of the update rules in Eqs. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ3">3</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ4">4</a>), M finds an approximate factorization <i>M</i> ≈ <i>BE</i> by converging to a local maximum of the objective function given in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ2">2</a>).</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ B_{ia} \leftarrow B_{ia} \sum\limits_{u} {({{M_{iu} } \mathord{\left/ {\vphantom {{M_{iu} } {BE_{iu} }}} \right. \kern-0pt} {BE_{iu} }})} E_{au} $$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div>
                           <div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ E_{au} \leftarrow E_{au} \sum\limits_{i} {B_{ia} (M_{iu} /BE_{iu} )} $$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div>
                        <p>The fundamental idea of exaggerating an FM is likely to employ an exaggeration of the difference from the mean in which it emphasizes movements for the distinctive feature points of the FM (Mo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Mo Z, Lewis JP, Neumann U (2004) Improved Automatic Caricature by Feature Normalization and Exaggeration. In: Proceedings of ACM SIGGRAPH 2004 sketches international conference on computer graphics and interactive techniques, pp 57–59" href="/article/10.1007/s10055-013-0227-8#ref-CR34" id="ref-link-section-d58766e1313">2004</a>; Redman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1984" title="Redman L (1984) How to draw caricatures. McGraw-Hill, New York" href="/article/10.1007/s10055-013-0227-8#ref-CR45" id="ref-link-section-d58766e1316">1984</a>; Brennan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Brennan SE (1982) Caricature generator: the dynamic exaggeration of faces by computer. Leonardo, The MIT Press, 18(3):170–178" href="/article/10.1007/s10055-013-0227-8#ref-CR3" id="ref-link-section-d58766e1319">1982</a>; Rhodes <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Rhodes G (1997) Superportraits. Psychology Press, Hove, East Sussex, UK" href="/article/10.1007/s10055-013-0227-8#ref-CR46" id="ref-link-section-d58766e1322">1997</a>). Moreover, an additional residual has to be exaggerated in order to reduce errors.</p><p>For the optimization, 10,000 iterations are executed when we compute <i>BE</i> with an average of −0.000526 and a standard deviation of 0.1443 of residual <span class="mathjax-tex">\( \vec{r} \)</span>.</p><p>Provided that the objective function stops after the iteration in which <i>E</i> is properly acquired, <i>E</i> needs to be divided again into the mean <i>m</i>
                           <sub>
                    <i>i</i>
                  </sub> and the deviation <i>d</i>
                           <sub>
                    <i>i</i>
                  </sub> of each column of <i>E</i> for its exaggeration. Each dimension is composed of a basis vector <i>b</i>
                           <sub>
                    <i>i</i>
                  </sub> (the <i>i</i>-th column of matrix <i>B</i>) and its weights, including expectation mi and deviation <i>d</i>
                           <sub>
                    <i>i</i>
                  </sub> of the <i>i</i>-th row in matrix <i>E</i>. The residual <span class="mathjax-tex">\( \vec{r} \)</span> can be thought of as distinctive FMs that are not expressed in the facial motions. The FM, <span class="mathjax-tex">\( \overrightarrow {f} \)</span> in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ5">5</a>), is in the form of a nonnegative linear combination of the basis and a residual.</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overrightarrow {f} = \sum\limits_{i} {e_{i} \cdot \overrightarrow {{b_{i} }} + \overrightarrow {r} } = \sum\limits_{i} {\left( {m_{i} + d_{i} } \right) \cdot \overrightarrow {{b_{i} }} + \overrightarrow {r} } $$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div>
                        <p>The computation model of an exaggerated FM is shown in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ6">6</a>). <span class="mathjax-tex">\( \overrightarrow {{f^{\prime } }} \)</span> called EM is calculated by scaling the deviation <i>d</i>
                           <sub>
                    <i>i</i>
                  </sub> and <span class="mathjax-tex">\( \vec{r} \)</span> with a personality-type-based exaggeration rate denoted by <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>). A more detailed definition of <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) will be given in the next section.</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \overrightarrow {{f^{\prime } }} = \sum {_{i} \left( {m_{i} + t \cdot d_{i} } \right) \cdot \overrightarrow {{b_{i} }} } + \theta_{t} \left( \omega \right) \cdot \overrightarrow {r} , $$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <i>t</i> = 1 if |<i>d</i>
                           <sub>
                    <i>i</i>
                  </sub>| &lt; 2 <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub> or <i>t</i> = <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) with an initial exaggeration rate <i>ω</i> chosen by a user if |<i>d</i>
                           <sub>
                    <i>i</i>
                  </sub>| ≥ 2 <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub> with |<i>d</i>
                           <sub>
                    <i>i</i>
                  </sub>| = |<i>e</i>
                           <sub>
                    <i>i</i>
                  </sub> <i>−</i> <i>m</i>
                           <sub>
                    <i>i</i>
                  </sub>| and standard deviation <i>s</i>
                           <sub>
                    <i>i</i>
                  </sub>.</p><h3 class="c-article__sub-heading" id="Sec8">Personality-type-based exaggeration mapping</h3><p>Personality-type-based exaggeration mapping (PEM) is a function that transforms the exaggeration rate denoted by <i>ω</i> into a personality-type-based exaggeration rate, <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>). In other words, facial expressions must deliver fairly distinctive muscle movements of a face. Hence, we need to convert a mapping exaggeration rate into a personality-type-based exaggeration rate.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Personality-type classification</h4><p>Psychologists have researched implications between facial expressions and emotions for more than 30 years (Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1972" title="Ekman P (1972) Universal and cultural differences in facial expressions of emotion. In: Nebraska symposium on motivation, vol 38, pp 207–283" href="/article/10.1007/s10055-013-0227-8#ref-CR18" id="ref-link-section-d58766e2086">1972</a>; Scherer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1979" title="Scherer KR (1979) Nonlinguistic vocal indicators of emotion and psychopathology. Emot Pers Psychopathol, New York, pp 493–529" href="/article/10.1007/s10055-013-0227-8#ref-CR48" id="ref-link-section-d58766e2089">1979</a>; Keltner and Ekman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Keltner D, Ekman P (1996) Affective intensity and emotional responses. Cogn Emot 10(3):323–328" href="/article/10.1007/s10055-013-0227-8#ref-CR22" id="ref-link-section-d58766e2092">1996</a>). How do we seek a relation between personality type and facial expressions? Each individual has slightly unique characteristics even in terms of emotions and feelings. Similarly, facial expressions tend to deliver internal emotions of human beings via fairly unique muscle movements of a face. Hence, to find common features of facial expression behaviors, we need to classify facial expressions into meaningful and coherent homogeneous groups.</p><p>In order to classify individuals into coherently meaning groups, which mostly show a similar personality type, we refer to previous research indicating that emotional intelligence is broadly defined as references of the ability to monitor one’s own and others’ feelings and emotions (Morand <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Morand DA (2001) The emotional intelligence of managers: assessing the construct validity of a nonverbal measure of people skill. J Bus Psychol 16(1):21–33" href="/article/10.1007/s10055-013-0227-8#ref-CR35" id="ref-link-section-d58766e2098">2001</a>; Martin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Martin RA, Berry GE, Dobranski T, Horne M, Dodgson PG (1996) Emotion perception threshold: individual differences in emotional sensitivity. J Res Pers 30(2):290–305" href="/article/10.1007/s10055-013-0227-8#ref-CR32" id="ref-link-section-d58766e2101">1996</a>; Salovey and Mayer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Salovey P, Mayer JD (1990) Emotional intelligence. Imaginat Cognit Pers 9(3):185–211" href="/article/10.1007/s10055-013-0227-8#ref-CR47" id="ref-link-section-d58766e2104">1990</a>). The previous research derives implications between facial measures and the Myers–Briggs feeling or thinking score. The MBTI, Form G, is a self-reporting, personality-type indicator widely used in the field of behavioral science.</p><p>In order to derive appropriate PEM, we first need to analyze the feature movements of facial expressions. The ultimate goal is to construct a PEM method, which represents curved behaviors to similar FMs, implying that an acceptable PEM has to convey the change rates of FMs.</p><p>In our approach, extroversion and feeling (EF), extroversion and thinking (ET), introversion and feeling (IF), and introversion and thinking (IT) are used for classifying the personality type of facial expressions. Emotional perception affects mostly extroversion or introversion and thinking or feeling dimensions (Gardner and Martinko <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Gardner W, Martinko M (1996) Using the Meyers–Briggs Type Indicator to study managers: a literature review and research agenda. J Manag 22(1):45–83" href="/article/10.1007/s10055-013-0227-8#ref-CR20" id="ref-link-section-d58766e2112">1996</a>).</p><p>In order to develop a PEM method, we begin with gathering both the six facial expression samples and the MBTI information from 30 Korean participants (18 males and 12 females). The subjects (age from 20 to 27) are asked to respond to an MBTI GS form in Korean and perform the six universal expressions. We assume that posed expressions taken from participants have been collected for the experiments. To fill the unnatural gaps caused by being performing posed expressions, participants were beforehand asked to bring internal emotion by carrying out training with sample expression pictures as natural as they could when making expressions. For taking the pictures of the performers, we use a Nikon D80 with an 800 × 600 pixel resolution captured from a distance of 100 cm. For the purpose of preprocessing, we conduct the transformation of the samples, including skew correction and scale, using Adobe Photoshop in order to minimize errors. In order to take into consideration maintenance of reusability as well as creation of facial expression effectively, we have defined facial feature points by referring to MPEG-4 (Pandzic and Forchheimer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Pandzic IS, Forchheimer R (2002) In MPEG-4 facial animation the standard, implementation and application. Wiley, Southern Gate" href="/article/10.1007/s10055-013-0227-8#ref-CR39" id="ref-link-section-d58766e2119">2002</a>) which satisfies the positions of muscles of a face. We also refer to FACS (Ekman and Friesen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books, Cambridge, MA" href="/article/10.1007/s10055-013-0227-8#ref-CR19" id="ref-link-section-d58766e2122">2003</a>) for movements of muscles even if we have employed Water’s method (Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17–24" href="/article/10.1007/s10055-013-0227-8#ref-CR55" id="ref-link-section-d58766e2125">1987</a>; Parke and Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Parke FI, Waters K (2008) Computer facial animation, 2nd edn. Wellesley, MA, A K Peters" href="/article/10.1007/s10055-013-0227-8#ref-CR41" id="ref-link-section-d58766e2128">2008</a>) in making expressions.</p><p>Feature movements of facial expressions need to be analyzed in order to make PEM. Given an expression sample obtained from pictures is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig3">3</a>. The layout of 30 markers is the same as we have used in facial motion capture except five markers attached to the boundary of head. We then compute the distance between the feature points of the six expressions and neutral expressions. The distance function between the neutral expression vector and a facial expressions vector is defined in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ7">7</a>)</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \delta = \left( {\sum\limits_{s = 1}^{k} {\left( {m_{s} - m_{Ns} } \right)^{p} } } \right)^{{{1 \mathord{\left/ {\vphantom {1 p}} \right. \kern-0pt} p}}} , $$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>where the index for markers is 1 ≤ <i>s</i> ≤ 30 and <i>p</i> = 2 <i>m</i>
                              <sub>
                      <i>s</i>
                    </sub> is the component of feature vectors for an expression and <i>m</i>
                              <sub>
                      <i>Ns</i>
                    </sub> is the component of feature vectors for a neural expression.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Sample of pictures taken with denoted markers with index 8; the pictures display six universal expressions—sadness (<b>a</b>), happiness (<b>b</b>), anger (<b>c</b>), surprise (<b>d</b>), fear (<b>e</b>), and disgust (<b>f</b>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The average distances of the six expressions associated with the above-mentioned four personality types are listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0227-8#Tab1">1</a>. For example, surprise has the largest value of 144.77 at type EF, indicating that surprise at EF is rather intensive in terms of the expression as compared to the other styles.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Average distance for various personality types</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0227-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The initial personality-type weight is defined in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ8">8</a>) with <i>g</i> = 1.5 and expression index <i>i</i>. Large values of <i>δ</i> imply that the expression has a high intensity for that particular personality type. For example, EF type has the largest values for happiness, surprise, sadness, anger, and disgust, indicating that the EF personality type tends to have bigger expressions than the other types. Therefore, we need to combine this property according to how the personality types relate to the magnitudes of expressions.</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \varphi_{i} = (\Updelta_{\hbox{max} } - \Updelta_{i} )/\Updelta_{\hbox{max} } + \Updelta_{i} /(\Updelta_{\hbox{max} } \cdot g), $$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where ∆<sub>max</sub> = δ<sub>max</sub> − δ<sub>min</sub>, and ∆<sub>
                      <i>i</i>
                    </sub> = δ<sub>max</sub> − δ<sub>i</sub>.</p><p>Finally, an initial exaggeration rate <i>ω</i> (given in terms of percentage) chosen by a user needs to be mapped via PEM by conveying each magnitude value computed using Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ8">8</a>).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Analytical approach to PEM</h4><p>The magnitude of an expression’s intensity causes different interpretations, even in the case of only one facial expression (e.g., that of anger or happiness). This principle lets us intuitively recognize that diverse levels of intensity exist for even a single expression. This fact is also supported by different research groups in the fields of cognitive study, orthodontic research, and facial recognition as aforementioned in the introduction.</p><p>Accordingly, based on the fact that diverse levels of intensity affect the detailed interpretations of nonverbal communication, an analytical approach needs to be investigated to precisely define PEM. In short, PEM is required to be defined as an analytical method carrying out levels of intensity rather than a simple linearly scaling method. In order to make this method successful, we analyze FMs to suitably model PEM.</p><p>It is observed that vector norms computed by a neutral motion and each frame of FMs for each expression in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ7">7</a>) display a curve indicating different levels of intensity. In order to derive a standard PEM, illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig4">4</a>, we calculate average curves obtained by the aforementioned computation. We use this average curve as a prototype PEM that represents the final PEM shape and determines an exaggeration parameter value taking into account each personality type.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Mean motion path obtained from the distance between each frame in six motions and the neutral frame with <i>vertical lines</i> indicating the standard deviations of 20 samples</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The PEM is defined in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ9">9</a>).</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ \theta_{t} (\omega ) = \left\{ {\begin{array}{*{20}c} c \hfill &amp; {for \; \omega \ge 1} \hfill \\ {k\left( {\omega \cdot (1/\varphi )} \right)^{n} \cdot e^{{ - \{ \left( {\omega + 1} \right) \cdot n\} /\varphi }} + 1} \hfill &amp; {for \; 0 \le \omega &lt; 1} \hfill \\ \end{array} } \right., $$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>where <i>t</i> is the personality type index, <i>n</i> is the exponent parameter, and <i>k</i> is the scale parameter.</p><p>The <i>ω</i> initial exaggeration rate is <i>ω</i> (0–1), and <i>C</i> is the convergent value. If 0 ≤ <i>ω</i> &lt; 1, then PEM is achieved by using the second definition in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ9">9</a>), and if <i>ω</i> ≥ 1, then PEM returns the convergence <i>C</i>. In addition, we calculate the scaling constant <i>k</i> in order to ensure that 1 ≤ <i>θ</i>
                              <sub>
                      <i>t</i>
                    </sub> (<i>ω</i>) ≤ 1.5 (<i>k</i> = 203.654). Users are asked to input MBTI. The initial exaggeration rate in which <i>ω</i> = 0 (0 %) implies that no exaggeration is applied; when <i>ω</i> = 1 (100 %), this indicates that the highest exaggeration rate is demanded.</p><p>The horizontal axis indicates the initial exaggeration rate <i>ω</i> selected by a user, and the vertical axis indicates the PEM, <i>θ</i>
                              <sub>
                      <i>t</i>
                    </sub> (<i>ω</i>). Once <i>ω</i> ≥ 1, all the PEM stay on the different convergent values for surprise PEM, such as 1.5 for EF, 1.371 for ET, 1.328 for IT, and 1.260 for IF, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig5">5</a>. In a similar manner, the PEMs for other expressions are obtained.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>PEM plots for six expressions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec11">Facial expression synthesis</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Facial motion cloning</h4><p>FMs are captured from a performer who is asked to display certain facial expressions. Therefore, the geometric coordinates of FMs that we obtain initially are appropriate for the performer’s facial shape and features. Hence, FMs need to be converted into FMs for a personalized face without any loss of the characteristics of the expressions of the performer. As a solution to the above problem, FMC that we describe here is a procedure for transforming exaggerated FMs into FMs suitable for a personalized face. The fundamental approach to finding a solution of FMC is based on the fact that each person has a unique facial geometry. Hence, facial cloning (FC) is employed in our method to generate exaggerative FMs by reflecting the geometric configurations of a personalized face. Then, motion cloning for an input face is introduced. In this approach, we enhance the FC algorithm with respect to utilizing the automatic extraction of facial features. Active appearance models (AAM) that are generated by combining a model of shape variations with a model of texture variations are employed to position facial features (Cootes et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Cootes TF, Edwards GJ, Taylor CJ (2001) Active appearance models. IEEE Trans Pattern Anal Mach Intell 23(6):681–685" href="/article/10.1007/s10055-013-0227-8#ref-CR16" id="ref-link-section-d58766e3099">2001</a>).</p><p>A training set of annotated images with feature points of a face that determine a shape is required to build a model. The mean shape of each training image acquired by using a warping process is used in the fitting stage to make a shape-free patch. The final matching process for an input image is completed by tuning the shape and appearance coefficients obtained from a PCA.</p><p>In this experiment, we take ten sample pictures (resolution 800 × 600 pixels) captured by Nikon D80 as the training data of AAM. Finally, the warping process that we developed in a previous study is applied on the facial features that we have acquired via AAM. Now, we have completed FC. Once an FC model is created, exaggerative FMs are ready to be customized for this model. FMC is an imperative process carried out before generating facial expressions. We developed an FMC technique in a previous study; further details on FMC can be found in (Chin and Kim <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Chin S, Kim KY (2009) Emotional intensity-based facial expression cloning for low polygonal applications. IEEE Trans Syst Man Cybern C 39(3):315–330" href="/article/10.1007/s10055-013-0227-8#ref-CR9" id="ref-link-section-d58766e3107">2009</a>). After completing FC, exaggerative FMs are generated as well. As a consequence, exaggerative FMs that we aim at obtaining can be computed by using the FMC technique.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec13">Facial expression</h4><p>In this research, we employ Water’s method to simulate exaggerated facial expressions. The muscles need to be embedded into a 3D face model in order to realize exaggerated facial expressions. About 1,876 vertices are used for building our generic face model with 21 muscles, including linear and sphincter muscles embedded as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig6">6</a>. In addition, virtual muscles are positioned not on the surface of the model but into the skin in order to hold the properties of real muscle movements, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig6">6</a> (left and right). We expect to express more accurate and real muscle motions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Virtual linear and sphincter muscles positioned not on the surface but inside the skin-like real muscles</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>A sequence of exaggerated FMs calculated by the FMC technique by varying the exaggeration rates to four values ranging from 0 to 1 (0–100 %) and denoted by small dots can be visualized on the facial expressions determined by <i>θ</i>
                              <sub>
                      <i>t</i>
                    </sub> (<i>ω</i>) = 1.5 from our generic model, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig7">7</a>. On the basis of the FMs, we have on a face a set of muscle contractions that can be utilized to generate facial expressions by setting the initial setting once. This means that contractions of the facial expressions of maximally exaggerated FMs determined by heuristic experiments are completed once by manual setting; then, facial expressions of the exaggerated FMs with a scalable exaggeration rate are easily realized merely by manipulating the contractions. Heuristic experiments have been conducted by displaying exaggerated FMs on a generic face model by varying the exaggeration rate parameters; we find the maximal exaggeration rate at <i>θ</i>
                              <sub>
                      <i>t</i>
                    </sub> (<i>ω</i>) = 1.5 as mentioned before.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Flow of FMs determined by PEMs of six facial expressions with <i>θ</i>
                                          <sub>
                              <i>t</i>
                            </sub> (<i>ω</i>) = 1.5</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>As a consequence, facial expressions of exaggerated FMs are acquired from a simple-scaling transformation of the set of contractions of muscles of maximally exaggerated FMs.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Experiments</h2><div class="c-article-section__content" id="Sec14-content"><p>In order to validate the presented method, we made a facial animation system by using Microsoft MFC and OpenGL 8 and analyzed the data by using MATLAB (Mathworks, Natick, MA).</p><p>We show facial animation system interface shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig8">8</a> in which we display the anger expression on the left image with six expression menus on the pop-up window, while viewing the surprise expression on the right image. In this paper, four experimental approaches are presented to validate the proposed method. The remaining subsections thoroughly describe the experiments.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig8_HTML.gif?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig8_HTML.gif" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Facial animation system interface</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <h3 class="c-article__sub-heading" id="Sec15">Validation of exaggeration mapping</h3><p>We asked a participant to act the six universal expressions taking into consideration three expression phases (i.e., an initial attack phase, a sustaining phase, and a relaxation phase) in order to appropriately acquire facial expressions. The markers’ positions were determined by carefully considering muscle movements (Terzopoulos and Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Terzopoulos D, Waters K (1990) Physically-based facial modeling, analysis, and animation. J Vis Computer Anim 1(4):73–80" href="/article/10.1007/s10055-013-0227-8#ref-CR52" id="ref-link-section-d58766e3246">1990</a>; Waters <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17–24" href="/article/10.1007/s10055-013-0227-8#ref-CR55" id="ref-link-section-d58766e3249">1987</a>); we found that any error from marker data extraction was insignificant. To minimize errors caused by wrinkles, skin freckles, facial flaccidity, and props, only actors with good skin conditions were selected.</p><p>The captured facial motion data were then analyzed to understand the facial muscle movement of the individual for six facial expressions; the data provided the core input of exaggerated FMs. FMs defined by a sequence of set marker locations at a time frame had to be validated with respect to relative motions with various exaggeration rates.</p><p>The fundamental idea of this approach using NMF for the EM of facial motions is to place more emphasis on features with relatively more movements.</p><p>To show comparisons between the original FMs and exaggerated FMs, we display the movement of facial features numbered 8 in (b) and 35 in (c) by plotting as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig9">9</a>. This shows consistent curve patterns of muscle movements even though different PEMs (<i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.0, 1.2 and 1.5) transformed the motion values. The motions are divided into initial, attack, peak, and fade-out stages. The peak marked by a circle indicates the highest intensity position of these markers. It distinctively places a high emphasis on the red plot (<i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.5) among the motion paths with an exaggeration rate [<i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.0 (original), 1.2 and 1.5], as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig9">9</a>. Similarly, movements of all other markers hold the same properties while maintaining the consistent curve patterns and distinctions between exaggerated FMs as well.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Facial marker positions are shown in (<b>a</b>). Curve patterns of the marker numbered 8 are shown in (<b>b</b>), and the curve patterns of the marker numbered 35 are shown (<b>c</b>) for surprise motions with <i>θ</i>
                                       <sub>
                            <i>t</i>
                          </sub> (<i>ω</i>) = 1.0, 1.2, and 1.5</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec16">Residual parameter optimization</h3><p>Exaggerations have to be affected by not only <i>d</i>
                           <sub>
                    <i>i</i>
                  </sub> but also <span class="mathjax-tex">\( \vec{r} \)</span> in Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-013-0227-8#Equ6">6</a>), implying that an exaggeration of residual needs to be taken into account along with PEM. Through experiments, we ascertain the optimal residual weight should be 1.0. The motion curve of marker number 9, which is placed on the bottom of left eye, and marker number 33, which is placed on the bottom of lips with various residual weights, including 0 (original residual without PEM, red), 0.5 (magenta), 1.0 (blue), and the case that the residual has not been added (green) in anger FMs, is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig10">10</a>, indicating that the residual weight is chosen to be 1.0 where a smooth curve is found with the optimal parameter for the residual in blue lines on the right in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig10">10</a>. As a consequence, we found that PEM, <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub>
                           <i>(ω)</i> should be applied to the residual part as well in order to further minimize the errors.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Motion curve of the marker number 9 and 33 with varying residual weights requiring residual parameter optimization</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec17">Facial expression synthesis</h3><p>In order to simulate the exaggerated facial expression synthesis, the personality-type information of a participant is required. As described earlier, PEM is introduced in the previous section by taking the personality type as a parameter. In order to follow the proposed algorithm, a user is initially asked to respond to the MBTI test. Once the MBTI test result is obtained, the user needs to input an initial exaggeration rate (0–100 %), which is computed by PEM, to obtain the ultimate exaggeration rate. If PEM is determined, then we apply the EM described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0227-8#Sec7">3.1</a>.</p><p>If the PEM is not determined in which a user does not know his (her) personality type, the algorithm ignores the personality type and simply provides an exaggeration rate without personality types. With the PEM, appropriately converted FMs for the user face can be obtained by conducting FMC, as described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0227-8#Sec12">3.3.1</a>, that is exaggerated using NMF. Now, we have the exaggerated FMs of the user whose facial expressions we want to generate. Finally, on the basis of the exaggerated FMs, we manipulate muscles by selecting appropriate contraction values to achieve the ultimate facial expression synthesis.</p><p>Figures <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig11">11</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig12">12</a> show numerical distinctions of some parts on the face including eyes, nose, glabella, and mouth for the fear and anger expressions. Distance denoted by norm was measured from a neutral expression to each expression on some markers attached to muscles which exert influence on each expression. For instance, even if both expressions with IF and IT of fear are completely exaggerated meaning that exaggeration rate is <i>ω</i> = 1.0, the final appearances look different when we consider the details. Because of the effect of the bigger PEM on the frontalis and orbicularis oculi muscles, eyes are open wider in the case of the IF-type personality. The mouth also becomes wider because of the manipulation of the zygomaticus major, depressor anguli oris, and orbicularis oris. Levator labii superioris and nasalis make the nose work as well. Six facial expressions and their exaggerated expressions were synthesized with respect to the exaggeration rates (<i>ω</i> = 0.0, 0.5, 0.75, and 1.0) and four personality types in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig13">13</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig14">14</a>. Each row in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig13">13</a> corresponds to synthesized facial expressions with <i>ω</i> = 0.0 (0 %), 0.5 (50 %), 0.75 (75 %), and 1.0 (100 %) at the maximal personality type of each expression. We find that each intensity of facial expressions increased slightly with an increase in PEM. Besides, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig14">14</a>, we notice that facial expressions concerning different personality types with their maximal PEMs. It is observed that the intensity of facial expressions is contingent on PEMs determined by a personality type.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Comparison between IT and IF on fear expression</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Comparison between ET and EF on anger expression</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Exaggerated facial expressions given a personality type with exaggeration rate parameter <i>ω</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig14_HTML.gif?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig14_HTML.gif" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Exaggerated facial expressions with four personality types</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>On the basis of the observations, we conclude that the personality type EF shows maximal intensities on facial expressions except in the case of the expression of fear (IF exhibits the maximal intensity in this case). Therefore, it can be inferred that an introvert person can show a rather pronounced expression of fear than other expressions.</p><h3 class="c-article__sub-heading" id="Sec18">User study</h3><p>In addition, we performed two user studies. At first, an offline user study was carried out from 30 subjects who participated in the survey of questionnaire which contains images of the exaggerated facial expressions. They viewed static images of facial expressions shown in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig13">13</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig14">14</a> which were created by the animation system in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig8">8</a>. The participants evaluated their quality of facial expressions (satisfaction) and whether the images show characteristic differences (distinction) with regard to personality types.</p><p>The scores range 0–5 scale points representing the higher scores indicate more reliable judgment than the lower scores. The result scores for the satisfaction of expressions show the averages of happiness (4.13), surprise (4.35), sadness (4.26), anger (4.61), fear (3.74), and disgust (3.74) with the grand average 4.14 and standard deviation 0.35. The results for degree of distinctions between exaggerated expressions with personality types deliver the averages of happiness (3.39), surprise (4.00), sadness (3.81), anger (3.87), fear (3.68), and disgust (4.06) with grand average 3.80 and standard deviation 0.25 as well. We also conducted another online user study from 30 participants (20 males and 10 females) with a mean age of 25.80 ± 4.20, and their average degrees of left and right eyesight are 1.00 ± 0.35 and 0.99 ± 0.32. Participants were asked to grade their contentment about each of the facial expressions using a 5-point Likert scale ranging from “never contented (1 point)” to “strongly contented (5 point)”. There were three issues:</p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>distinction between non-exaggerated facial expression (<i>ω</i> = 0.0) and exaggerated facial expressions (<i>ω</i> = 0.5, 1.0);</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>degrees of looking natural of exaggerated facial expressions;</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>distinction between exaggerated expressions of each personality types (EF, ET, IF, and IT).</p>
                      
                    </li>
                  </ol>
                        <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig15">15</a>a, all the average grades are bigger than 3 and the grand average is 3.9667. It shows participants mostly perceived the distinction of exaggeration degree at anger and surprise expressions. Besides, happiness and disgust were not perceived as well as others. Furthermore, we ran a one-way ANOVA for each exaggerated expression. While there was no effect of (b) degree of looking natural on exaggerated facial expressions (<i>F</i> = .822, <i>p</i> &gt; .535) and (c) distinction of exaggerated facial expressions with personality types (<i>F</i> = 1.910, <i>p</i> &gt; .095), there was a significant effect for (a) distinction of exaggerated facial expressions with PEM (<i>F</i> = 6.310, <i>p</i> &lt; .000) whereby participants perceived the exaggerated facial expressions inconsistently in (a).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig15_HTML.gif?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig15_HTML.gif" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Score on user study for the exaggerated facial expression (<i>error bars</i> based on standard error)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig15">15</a>b, the scores graded by participants (mean = 3.2444, std = 0.1515) at a question about degrees of looking natural on exaggerated facial expressions show that the exaggerated facial expressions are somewhat reasonable and believable.</p><p>However, at the question about the distinction of exaggerated facial expressions in different personality types, participants made response of quite disagreement (mean = 2.8000, std = 0.2805) as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig15">15</a>c. It seems reasonable because difference of PEM between the exaggerated expressions with personality types is very subtle.</p><h3 class="c-article__sub-heading" id="Sec19">Validation of equivalence between FMs and facial expressions</h3><p>Here, we need to validate the equivalence between facial motions and animation because the above-mentioned methods first generate exaggerate facial motions that are composed of markers and then apply other vertices to create expressions by manipulating the muscle contractions.</p><p>In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig16">16</a>, we have shown two curves for the anger expression, representing facial motions and expressions in 3D vertices denoted by EFACE varying at different exaggeration rates such as <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.0, 1,25, and 1.5. The results show identical curves. We not only show the validation of the exaggeration of facial motions but also demonstrate the fact that the equivalent exaggeration of facial expressions is identical to that of facial motion. The locations of the markers vary slightly because of the changes in PEMs. In order to verify identical movements between exaggerated facial expressions and exaggerated FMs, we analyze the distance between the neutral face motion and an expression sample of the anger expression with <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.0, 1.25, and 1.5.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0227-8/MediaObjects/10055_2013_227_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Validation for exaggerated FMs versus facial expression on a 3D face</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0227-8/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In order to compute error rates of corresponding samples, we scale expression coordinates in proportion to FMs and then compute their distances. As we see in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0227-8#Fig16">16</a>a–c, the curves are considerably identical showing FMs marked by a red curve, the mean 4.6069 (std = 5.0280) denoted by a green curve, and facial animations for anger marked by a blue curve.</p><p>Similarly, the validation at <i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.5 is shown in (c). In addition, (d) shows that the curves vary with respect to PEMs (<i>θ</i>
                           <sub>
                    <i>t</i>
                  </sub> (<i>ω</i>) = 1.0, 1.25, and 1.5).</p><h3 class="c-article__sub-heading" id="Sec20">Discussion</h3><p>Previous researches have addressed the fact that the intensity of facial expressions is critical while measuring the magnitude of facial expressions and making exaggerations (Caldera et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Caldera AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cognition 76(2):105–146" href="/article/10.1007/s10055-013-0227-8#ref-CR5" id="ref-link-section-d58766e3778">2000</a>; Calder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Calder AJ, Young AW, Rowland D, Perrett DI (1997) Computer-enhanced emotion in facial expressions. In: Proceedings of the royal society b: biological sciences, vol 264, no 1383, pp 919–925" href="/article/10.1007/s10055-013-0227-8#ref-CR4" id="ref-link-section-d58766e3781">1997</a>).</p><p>Caldera et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Caldera AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cognition 76(2):105–146" href="/article/10.1007/s10055-013-0227-8#ref-CR5" id="ref-link-section-d58766e3787">2000</a>) demonstrated that caricaturing facial expressions of the basic emotions increases their intensity. Emotional intensity has been thought of as an intrinsic component of facial expressions. Their results indicate that computer-generated caricatures offer a good alternative to varying intensity in natural facial expressions. Caldera also studied that the exaggeration of facial expression could be considered to be relative to a picture of the same person holding a neutral expression (Calder et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Calder AJ, Young AW, Rowland D, Perrett DI (1997) Computer-enhanced emotion in facial expressions. In: Proceedings of the royal society b: biological sciences, vol 264, no 1383, pp 919–925" href="/article/10.1007/s10055-013-0227-8#ref-CR4" id="ref-link-section-d58766e3790">1997</a>). One finding is that facial expression caricaturing works by enhancing an expression’s emotional intensity.</p><p>In this approach, we aim at synthesizing exaggerated facial expressions with personality types. For instance, one has very similar shape and geometrical features of the face to his friend. In general, we would not expect the exactly same expressions between them because facial expressions can be made by internal personality type and his emotion that he personally perceives. Similarly, some expressions cannot be distinctive even if they are not same at all, while others may be more clearly observed. In the research, we proposed exaggeration of facial expressions with personality types, which sometimes convey subtle difference on expressions while clear distinctions are found in a certain case.</p><p>The final look of a natural-looking exaggeration is determined by PEM. The fact that some expressions, in which an introvert person expresses fear at a high intensity or an extrovert displays the same expression at a low intensity are observed to be similar, can be considered to be reasonable. In the experiments, the examples shown are based on the fact that facial expressions produced by the movements of the facial muscles called rapid signals change flash on the face for a matter of seconds or fractions of a second. In this approach, we have only considered six facial expressions captured from a performer. However, this methodology can be extended to any facial motions provided facial motion capture data that are available.</p><p>Some limitations of the proposed method are described here. The technical limitations, including slight gaps between the upper and lower lips, may be caused by the muscle algorithms. However, the proposed method is still good for real-time animations.</p><p>A greater variety of personality traits would be considered in a future work that would take into consideration factors such as gender, age, and role. Gestures with personality traits also would be a cue for nonverbal communication.</p><p>In addition, the performance of a real actor with different personality types in a digital movie needs to be investigated in the context of the background of before–after story telling as well. This model can be extended to cover also other than six basic expressions as future work as well.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">Conclusion</h2><div class="c-article-section__content" id="Sec21-content"><p>Exaggeration can be thought of as placing more emphasis on the key features of an object. The main features need to be highlighted more than the others. However, the exaggeration of facial expressions on a digital human face is noticeably different from character expressions in cartoon animation in which the traditional approaches seem rather surrealistic than realistic or natural. Hence, a reality-based exaggeration of the human face is imperative. Our approach indicates that exaggeration techniques offer a good alternative to the varying intensity in natural facial expressions. We employ NMF. Then, MBTI classification is used for determining the personality type in order to define PEM. Further, we experimentally validate the proposed method by implementing facial expression synthesis.</p><p>A particular advantage of the proposed method is that in this method, the reality-based exaggeration of facial expressions has been explored. The proposed method has challenged to solve this problem by providing techniques for the automatic generation of exaggerations from reality-based facial motion capture data. Furthermore, one in general wants to assign a specific personality to a character when making the character. It is obvious that the character exhibits its own unique traits. These personality traits are also taken into consideration while generating the exaggerated expressions.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ahn S, Ozawa S (2004) Generating facial expressions based on estimation of muscular contraction parameters fro" /><p class="c-article-references__text" id="ref-CR1">Ahn S, Ozawa S (2004) Generating facial expressions based on estimation of muscular contraction parameters from facial feature points. In: Proceedings of IEEE international conference on systems, man and cybernetics, vol 1, pp 660–665</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Bregler, L. Loeb, E. Chuang, H. Deshpande, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Bregler C, Loeb L, Chuang E, Deshpande H (2002) Turning to the masters: motion capturing cartoons. ACM SIGGRAP" /><p class="c-article-references__text" id="ref-CR2">Bregler C, Loeb L, Chuang E, Deshpande H (2002) Turning to the masters: motion capturing cartoons. ACM SIGGRAPH 2002 Trans Graph 21(3):399–407</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Turning%20to%20the%20masters%3A%20motion%20capturing%20cartoons&amp;journal=ACM%20SIGGRAPH%202002%20Trans%20Graph&amp;volume=21&amp;issue=3&amp;pages=399-407&amp;publication_year=2002&amp;author=Bregler%2CC&amp;author=Loeb%2CL&amp;author=Chuang%2CE&amp;author=Deshpande%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brennan SE (1982) Caricature generator: the dynamic exaggeration of faces by computer. Leonardo, The MIT Press" /><p class="c-article-references__text" id="ref-CR3">Brennan SE (1982) Caricature generator: the dynamic exaggeration of faces by computer. Leonardo, The MIT Press, 18(3):170–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Calder AJ, Young AW, Rowland D, Perrett DI (1997) Computer-enhanced emotion in facial expressions. In: Proceed" /><p class="c-article-references__text" id="ref-CR4">Calder AJ, Young AW, Rowland D, Perrett DI (1997) Computer-enhanced emotion in facial expressions. In: Proceedings of the royal society b: biological sciences, vol 264, no 1383, pp 919–925</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="AJ. Caldera, D. Rowland, AW. Young, I. Nimmo-Smith, J. Keane, DI. Perrett, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Caldera AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Co" /><p class="c-article-references__text" id="ref-CR5">Caldera AJ, Rowland D, Young AW, Nimmo-Smith I, Keane J, Perrett DI (2000) Caricaturing facial expressions. Cognition 76(2):105–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0010-0277%2800%2900074-3" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Caricaturing%20facial%20expressions&amp;journal=Cognition&amp;volume=76&amp;issue=2&amp;pages=105-146&amp;publication_year=2000&amp;author=Caldera%2CAJ&amp;author=Rowland%2CD&amp;author=Young%2CAW&amp;author=Nimmo-Smith%2CI&amp;author=Keane%2CJ&amp;author=Perrett%2CDI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cao Y, Faloutsos P, Pighin F (2003) Unsupervised learning for speech motion editing. In: Proceedings of ACM SI" /><p class="c-article-references__text" id="ref-CR6">Cao Y, Faloutsos P, Pighin F (2003) Unsupervised learning for speech motion editing. In: Proceedings of ACM SIGGRAPH/eurographics symposium on Computer animation, pp 225–231</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chai JX, Xiao J, Hodgins J (2003) Vision-based control of 3D facial animation. In: Eurographics/SIGGRAPH sympo" /><p class="c-article-references__text" id="ref-CR7">Chai JX, Xiao J, Hodgins J (2003) Vision-based control of 3D facial animation. In: Eurographics/SIGGRAPH symposium on computer animation</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen H, Liu Z, Rose C, Xu Y, Shum H, Salesin D (2004) Example-Based Composite Sketching of Human Portraits. In" /><p class="c-article-references__text" id="ref-CR8">Chen H, Liu Z, Rose C, Xu Y, Shum H, Salesin D (2004) Example-Based Composite Sketching of Human Portraits. In: Proceedings of the 3rd international symposium on non-photorealistic animation and rendering, pp 95–153</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Chin, KY. Kim, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Chin S, Kim KY (2009) Emotional intensity-based facial expression cloning for low polygonal applications. IEEE" /><p class="c-article-references__text" id="ref-CR9">Chin S, Kim KY (2009) Emotional intensity-based facial expression cloning for low polygonal applications. IEEE Trans Syst Man Cybern C 39(3):315–330</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCC.2008.2011283" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20intensity-based%20facial%20expression%20cloning%20for%20low%20polygonal%20applications&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20C&amp;volume=39&amp;issue=3&amp;pages=315-330&amp;publication_year=2009&amp;author=Chin%2CS&amp;author=Kim%2CKY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Chin, CY. Lee, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Chin S, Lee CY (2010) Exaggeration of facial expressions from motion capture data. Chin Opt Lett 8(1):29–32" /><p class="c-article-references__text" id="ref-CR10">Chin S, Lee CY (2010) Exaggeration of facial expressions from motion capture data. Chin Opt Lett 8(1):29–32</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3788%2FCOL20100801.0029" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Exaggeration%20of%20facial%20expressions%20from%20motion%20capture%20data&amp;journal=Chin%20Opt%20Lett&amp;volume=8&amp;issue=1&amp;pages=29-32&amp;publication_year=2010&amp;author=Chin%2CS&amp;author=Lee%2CCY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Chin, CY. Lee, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chin S, Lee CY (2013) Personality trait and facial expression filter-based brain-computer interface. Int J Adv" /><p class="c-article-references__text" id="ref-CR11">Chin S, Lee CY (2013) Personality trait and facial expression filter-based brain-computer interface. Int J Adv Rob Syst 10:138</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Personality%20trait%20and%20facial%20expression%20filter-based%20brain-computer%20interface&amp;journal=Int%20J%20Adv%20Rob%20Syst&amp;volume=10&amp;publication_year=2013&amp;author=Chin%2CS&amp;author=Lee%2CCY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chin S, Lee CY, Lee J (2009) Personal Style and non-negative matrix factorization based exaggerative expressio" /><p class="c-article-references__text" id="ref-CR12">Chin S, Lee CY, Lee J (2009) Personal Style and non-negative matrix factorization based exaggerative expressions of face. In: Proceedings of the 2009 international conference on computer graphics and virtual Reality, pp 91–96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chuang ES (2004) Analysis, synthesis, and retargeting of facial expressions. Ph.D. dissertation, Stanford Univ" /><p class="c-article-references__text" id="ref-CR13">Chuang ES (2004) Analysis, synthesis, and retargeting of facial expressions. Ph.D. dissertation, Stanford University, Palo Alto, CA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Clarke, M. Chen, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Clarke L, Chen M (2011) Automatic generation of 3D caricatures based on artistic deformation styles. IEEE Tran" /><p class="c-article-references__text" id="ref-CR14">Clarke L, Chen M (2011) Automatic generation of 3D caricatures based on artistic deformation styles. IEEE Trans Vis Comput Graph 17(6):808–821</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2010.76" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20generation%20of%203D%20caricatures%20based%20on%20artistic%20deformation%20styles&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=17&amp;issue=6&amp;pages=808-821&amp;publication_year=2011&amp;author=Clarke%2CL&amp;author=Chen%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Cohen, N. Sebe, A. Garg, LS. Chen, TS. Huang, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Cohen I, Sebe N, Garg A, Chen LS, Huang TS (2003) Facial expression recognition from video sequences: temporal" /><p class="c-article-references__text" id="ref-CR15">Cohen I, Sebe N, Garg A, Chen LS, Huang TS (2003) Facial expression recognition from video sequences: temporal and static modeling. Comput Vis Imag Underst 91(1–2):160–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1077-3142%2803%2900081-X" aria-label="View reference 15">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1134.90466" aria-label="View reference 15 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20expression%20recognition%20from%20video%20sequences%3A%20temporal%20and%20static%20modeling&amp;journal=Comput%20Vis%20Imag%20Underst&amp;volume=91&amp;issue=1%E2%80%932&amp;pages=160-187&amp;publication_year=2003&amp;author=Cohen%2CI&amp;author=Sebe%2CN&amp;author=Garg%2CA&amp;author=Chen%2CLS&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="TF. Cootes, GJ. Edwards, CJ. Taylor, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Cootes TF, Edwards GJ, Taylor CJ (2001) Active appearance models. IEEE Trans Pattern Anal Mach Intell 23(6):68" /><p class="c-article-references__text" id="ref-CR16">Cootes TF, Edwards GJ, Taylor CJ (2001) Active appearance models. IEEE Trans Pattern Anal Mach Intell 23(6):681–685</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F34.927467" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Active%20appearance%20models&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=23&amp;issue=6&amp;pages=681-685&amp;publication_year=2001&amp;author=Cootes%2CTF&amp;author=Edwards%2CGJ&amp;author=Taylor%2CCJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Deng Z, Neumann U (2006) eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls" /><p class="c-article-references__text" id="ref-CR17">Deng Z, Neumann U (2006) eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls. In: Proceedings of ACM SIGGRAPH/EG symposium on computer animation, pp 251–259</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ekman P (1972) Universal and cultural differences in facial expressions of emotion. In: Nebraska symposium on " /><p class="c-article-references__text" id="ref-CR18">Ekman P (1972) Universal and cultural differences in facial expressions of emotion. In: Nebraska symposium on motivation, vol 38, pp 207–283</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Ekman, WV. Friesen, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books," /><p class="c-article-references__text" id="ref-CR19">Ekman P, Friesen WV (2003) Unmasking the face: a guide to recognizing emotions from facial clues. Malor Books, Cambridge, MA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unmasking%20the%20face%3A%20a%20guide%20to%20recognizing%20emotions%20from%20facial%20clues&amp;publication_year=2003&amp;author=Ekman%2CP&amp;author=Friesen%2CWV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="W. Gardner, M. Martinko, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Gardner W, Martinko M (1996) Using the Meyers–Briggs Type Indicator to study managers: a literature review and" /><p class="c-article-references__text" id="ref-CR20">Gardner W, Martinko M (1996) Using the Meyers–Briggs Type Indicator to study managers: a literature review and research agenda. J Manag 22(1):45–83</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F014920639602200103" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20the%20Meyers%E2%80%93Briggs%20Type%20Indicator%20to%20study%20managers%3A%20a%20literature%20review%20and%20research%20agenda&amp;journal=J%20Manag&amp;volume=22&amp;issue=1&amp;pages=45-83&amp;publication_year=1996&amp;author=Gardner%2CW&amp;author=Martinko%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PO. Hoyer, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Hoyer PO (2004) Non-negative Matrix Factorization with Sparseness Constraints. J Mach Learn Res 5:1457–1469" /><p class="c-article-references__text" id="ref-CR21">Hoyer PO (2004) Non-negative Matrix Factorization with Sparseness Constraints. J Mach Learn Res 5:1457–1469</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2248024" aria-label="View reference 21 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1222.68218" aria-label="View reference 21 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Non-negative%20Matrix%20Factorization%20with%20Sparseness%20Constraints&amp;journal=J%20Mach%20Learn%20Res&amp;volume=5&amp;pages=1457-1469&amp;publication_year=2004&amp;author=Hoyer%2CPO">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Keltner, P. Ekman, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Keltner D, Ekman P (1996) Affective intensity and emotional responses. Cogn Emot 10(3):323–328" /><p class="c-article-references__text" id="ref-CR22">Keltner D, Ekman P (1996) Affective intensity and emotional responses. Cogn Emot 10(3):323–328</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F026999396380277" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Affective%20intensity%20and%20emotional%20responses&amp;journal=Cogn%20Emot&amp;volume=10&amp;issue=3&amp;pages=323-328&amp;publication_year=1996&amp;author=Keltner%2CD&amp;author=Ekman%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Keltner, P. Ekman, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Keltner D, Ekman P (2004) Emotional expression, and the art of empirical epiphany. J Res Pers 38:37–44" /><p class="c-article-references__text" id="ref-CR23">Keltner D, Ekman P (2004) Emotional expression, and the art of empirical epiphany. J Res Pers 38:37–44</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jrp.2003.09.006" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20expression%2C%20and%20the%20art%20of%20empirical%20epiphany&amp;journal=J%20Res%20Pers&amp;volume=38&amp;pages=37-44&amp;publication_year=2004&amp;author=Keltner%2CD&amp;author=Ekman%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Kotsia, I. Pitas, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Kotsia I, Pitas I (2007) Facial expression recognition in image sequences using geometric deformation features" /><p class="c-article-references__text" id="ref-CR24">Kotsia I, Pitas I (2007) Facial expression recognition in image sequences using geometric deformation features and support vector machines. IEEE Trans Imag Process 16(1):172–187</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=2460156" aria-label="View reference 24 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIP.2006.884954" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Facial%20expression%20recognition%20in%20image%20sequences%20using%20geometric%20deformation%20features%20and%20support%20vector%20machines&amp;journal=IEEE%20Trans%20Imag%20Process&amp;volume=16&amp;issue=1&amp;pages=172-187&amp;publication_year=2007&amp;author=Kotsia%2CI&amp;author=Pitas%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: Proceedings of 2nd international " /><p class="c-article-references__text" id="ref-CR25">Kshirsagar S, Magnenat-Thalmann N (2002) A multilayer personality model. In: Proceedings of 2nd international symposium on smart graphics, ACM Press, pp 107–115</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DD. Lee, HS. Seung, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401:788–791" /><p class="c-article-references__text" id="ref-CR26">Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401:788–791</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1038%2F44565" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Learning%20the%20parts%20of%20objects%20by%20non-negative%20matrix%20factorization&amp;journal=Nature&amp;volume=401&amp;pages=788-791&amp;publication_year=1999&amp;author=Lee%2CDD&amp;author=Seung%2CHS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Lewiner, T. Vieira, D. Martínez, A. Peixoto, V. Mello, L. Velho, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Lewiner T, Vieira T, Martínez D, Peixoto A, Mello V, Velho L (2011) Interactive 3D caricature from harmonic ex" /><p class="c-article-references__text" id="ref-CR27">Lewiner T, Vieira T, Martínez D, Peixoto A, Mello V, Velho L (2011) Interactive 3D caricature from harmonic exaggeration. Comput Graph 35(3):586–595</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2011.03.005" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%203D%20caricature%20from%20harmonic%20exaggeration&amp;journal=Comput%20Graph&amp;volume=35&amp;issue=3&amp;pages=586-595&amp;publication_year=2011&amp;author=Lewiner%2CT&amp;author=Vieira%2CT&amp;author=Mart%C3%ADnez%2CD&amp;author=Peixoto%2CA&amp;author=Mello%2CV&amp;author=Velho%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lewis JP, Cordner M, Fong N (2000) Pose space deformation: a unified approach to shape interpolation and skele" /><p class="c-article-references__text" id="ref-CR28">Lewis JP, Cordner M, Fong N (2000) Pose space deformation: a unified approach to shape interpolation and skeleton-driven deformation. In: Proceedings of ACM SIGGRAPH 2000 international conference on computer graphics and interactive techniques, New Orleans, LO, pp 165–172</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liang L, Chen H, Xu Y, Shum H (2002) Example-based caricature generation with exaggeration. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR29">Liang L, Chen H, Xu Y, Shum H (2002) Example-based caricature generation with exaggeration. In: Proceedings of the 10th Pacific conference of computer graphics and application, p 386</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liu S, Wang J, Zhang M, Wang Z (2012) Three-dimensional cartoon facial animation based on art rules. The visua" /><p class="c-article-references__text" id="ref-CR30">Liu S, Wang J, Zhang M, Wang Z (2012) Three-dimensional cartoon facial animation based on art rules. The visual computer, ISSN: 0178-2789, pp 1–15</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ma X, Le B, Deng Z (2009) Style learning and transferring for facial animation editing. In: Proceedings of ACM" /><p class="c-article-references__text" id="ref-CR31">Ma X, Le B, Deng Z (2009) Style learning and transferring for facial animation editing. In: Proceedings of ACM SIGGRAPH/Eurographics symposium on computer animation, pp 123–132</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RA. Martin, GE. Berry, T. Dobranski, M. Horne, PG. Dodgson, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Martin RA, Berry GE, Dobranski T, Horne M, Dodgson PG (1996) Emotion perception threshold: individual differen" /><p class="c-article-references__text" id="ref-CR32">Martin RA, Berry GE, Dobranski T, Horne M, Dodgson PG (1996) Emotion perception threshold: individual differences in emotional sensitivity. J Res Pers 30(2):290–305</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fjrpe.1996.0019" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%20perception%20threshold%3A%20individual%20differences%20in%20emotional%20sensitivity&amp;journal=J%20Res%20Pers&amp;volume=30&amp;issue=2&amp;pages=290-305&amp;publication_year=1996&amp;author=Martin%2CRA&amp;author=Berry%2CGE&amp;author=Dobranski%2CT&amp;author=Horne%2CM&amp;author=Dodgson%2CPG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JC. Miranda, X. Alvarez, J. Orvalho, D. Gutierrez, AA. Sousa, V. Orvalho, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Miranda JC, Alvarez X, Orvalho J, Gutierrez D, Sousa AA, Orvalho V (2012) Sketch express: a sketching interfac" /><p class="c-article-references__text" id="ref-CR33">Miranda JC, Alvarez X, Orvalho J, Gutierrez D, Sousa AA, Orvalho V (2012) Sketch express: a sketching interface for facial animation. Comput Graph 36(6):585–595</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2012.03.002" aria-label="View reference 33">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 33 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sketch%20express%3A%20a%20sketching%20interface%20for%20facial%20animation&amp;journal=Comput%20Graph&amp;volume=36&amp;issue=6&amp;pages=585-595&amp;publication_year=2012&amp;author=Miranda%2CJC&amp;author=Alvarez%2CX&amp;author=Orvalho%2CJ&amp;author=Gutierrez%2CD&amp;author=Sousa%2CAA&amp;author=Orvalho%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mo Z, Lewis JP, Neumann U (2004) Improved Automatic Caricature by Feature Normalization and Exaggeration. In: " /><p class="c-article-references__text" id="ref-CR34">Mo Z, Lewis JP, Neumann U (2004) Improved Automatic Caricature by Feature Normalization and Exaggeration. In: Proceedings of ACM SIGGRAPH 2004 sketches international conference on computer graphics and interactive techniques, pp 57–59</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Morand, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Morand DA (2001) The emotional intelligence of managers: assessing the construct validity of a nonverbal measu" /><p class="c-article-references__text" id="ref-CR35">Morand DA (2001) The emotional intelligence of managers: assessing the construct validity of a nonverbal measure of people skill. J Bus Psychol 16(1):21–33</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1023%2FA%3A1007831603825" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20emotional%20intelligence%20of%20managers%3A%20assessing%20the%20construct%20validity%20of%20a%20nonverbal%20measure%20of%20people%20skill&amp;journal=J%20Bus%20Psychol&amp;volume=16&amp;issue=1&amp;pages=21-33&amp;publication_year=2001&amp;author=Morand%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="I. Myers, M. McCaulley, N. Quenk, A. Hammer, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Myers I, McCaulley M, Quenk N, Hammer A (1998) In MBTI manual: a guide to the development and use of the Myers" /><p class="c-article-references__text" id="ref-CR36">Myers I, McCaulley M, Quenk N, Hammer A (1998) In MBTI manual: a guide to the development and use of the Myers–Briggs type indicator. Consulting Psychologists Press, Palo Alto, CA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20MBTI%20manual%3A%20a%20guide%20to%20the%20development%20and%20use%20of%20the%20Myers%E2%80%93Briggs%20type%20indicator&amp;publication_year=1998&amp;author=Myers%2CI&amp;author=McCaulley%2CM&amp;author=Quenk%2CN&amp;author=Hammer%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Noh JY, Neumann U (2001) Expression cloning. In: Proceedings of ACM SIGGRAPH 2001 international conference on " /><p class="c-article-references__text" id="ref-CR37">Noh JY, Neumann U (2001) Expression cloning. In: Proceedings of ACM SIGGRAPH 2001 international conference on computer graphics and interactive techniques, pp 277–288</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Paatero, U. Tapper, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Paatero P, Tapper U (1994) Positive matrix factorization: a non-negative factor model with optimal utilization" /><p class="c-article-references__text" id="ref-CR38">Paatero P, Tapper U (1994) Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values. Environmetrics 5(2):111–126</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fenv.3170050203" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Positive%20matrix%20factorization%3A%20a%20non-negative%20factor%20model%20with%20optimal%20utilization%20of%20error%20estimates%20of%20data%20values&amp;journal=Environmetrics&amp;volume=5&amp;issue=2&amp;pages=111-126&amp;publication_year=1994&amp;author=Paatero%2CP&amp;author=Tapper%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="IS. Pandzic, R. Forchheimer, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Pandzic IS, Forchheimer R (2002) In MPEG-4 facial animation the standard, implementation and application. Wile" /><p class="c-article-references__text" id="ref-CR39">Pandzic IS, Forchheimer R (2002) In MPEG-4 facial animation the standard, implementation and application. Wiley, Southern Gate</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 39 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20MPEG-4%20facial%20animation%20the%20standard%2C%20implementation%20and%20application&amp;publication_year=2002&amp;author=Pandzic%2CIS&amp;author=Forchheimer%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Parke FI (1972) Computer generated animation of faces. In: Proceedings of ACM annual conference, vol 1, pp 451" /><p class="c-article-references__text" id="ref-CR40">Parke FI (1972) Computer generated animation of faces. In: Proceedings of ACM annual conference, vol 1, pp 451–457</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="FI. Parke, K. Waters, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Parke FI, Waters K (2008) Computer facial animation, 2nd edn. Wellesley, MA, A K Peters" /><p class="c-article-references__text" id="ref-CR41">Parke FI, Waters K (2008) Computer facial animation, 2nd edn. Wellesley, MA, A K Peters</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 41 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20facial%20animation&amp;publication_year=2008&amp;author=Parke%2CFI&amp;author=Waters%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pighin F, Hecker J, Lischinski D, Szeliski R, Salesin D (1995) “Synthesizing realistic facial expressions from" /><p class="c-article-references__text" id="ref-CR42">Pighin F, Hecker J, Lischinski D, Szeliski R, Salesin D (1995) “Synthesizing realistic facial expressions from photographs. In: Proceedings of SIGGRAPH 1998 international conference on computer graphics and interactive techniques, San Antonio, TX, pp 75–84</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Platt SM, Badler NI (1981) Animating facial expression. In: Proceedings of ACM SIGGRAPH computer graphics, vol" /><p class="c-article-references__text" id="ref-CR43">Platt SM, Badler NI (1981) Animating facial expression. In: Proceedings of ACM SIGGRAPH computer graphics, vol 15, no 3, pp 245–252</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Ratner, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ratner P (2003) 3-D human modeling and animation, 2nd edn. Wiley, New York" /><p class="c-article-references__text" id="ref-CR44">Ratner P (2003) 3-D human modeling and animation, 2nd edn. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3-D%20human%20modeling%20and%20animation&amp;publication_year=2003&amp;author=Ratner%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="L. Redman, " /><meta itemprop="datePublished" content="1984" /><meta itemprop="headline" content="Redman L (1984) How to draw caricatures. McGraw-Hill, New York" /><p class="c-article-references__text" id="ref-CR45">Redman L (1984) How to draw caricatures. McGraw-Hill, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=How%20to%20draw%20caricatures&amp;publication_year=1984&amp;author=Redman%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rhodes G (1997) Superportraits. Psychology Press, Hove, East Sussex, UK" /><p class="c-article-references__text" id="ref-CR46">Rhodes G (1997) Superportraits. Psychology Press, Hove, East Sussex, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Salovey, JD. Mayer, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Salovey P, Mayer JD (1990) Emotional intelligence. Imaginat Cognit Pers 9(3):185–211" /><p class="c-article-references__text" id="ref-CR47">Salovey P, Mayer JD (1990) Emotional intelligence. Imaginat Cognit Pers 9(3):185–211</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2190%2FDUGG-P24E-52WK-6CDG" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotional%20intelligence&amp;journal=Imaginat%20Cognit%20Pers&amp;volume=9&amp;issue=3&amp;pages=185-211&amp;publication_year=1990&amp;author=Salovey%2CP&amp;author=Mayer%2CJD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="KR. Scherer, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Scherer KR (1979) Nonlinguistic vocal indicators of emotion and psychopathology. Emot Pers Psychopathol, New Y" /><p class="c-article-references__text" id="ref-CR48">Scherer KR (1979) Nonlinguistic vocal indicators of emotion and psychopathology. Emot Pers Psychopathol, New York, pp 493–529</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nonlinguistic%20vocal%20indicators%20of%20emotion%20and%20psychopathology&amp;pages=493-529&amp;publication_year=1979&amp;author=Scherer%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Sifakis, I. Neverov, R. Fedkiw, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Sifakis E, Neverov I, Fedkiw R (2005) Automatic Determination of Facial Muscle Activations from Sparse Motion " /><p class="c-article-references__text" id="ref-CR49">Sifakis E, Neverov I, Fedkiw R (2005) Automatic Determination of Facial Muscle Activations from Sparse Motion Capture Marker Data. ACM SIGGRAPH 2005 Trans Graph 24(3):417–425</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F1073204.1073208" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Automatic%20Determination%20of%20Facial%20Muscle%20Activations%20from%20Sparse%20Motion%20Capture%20Marker%20Data&amp;journal=ACM%20SIGGRAPH%202005%20Trans%20Graph&amp;volume=24&amp;issue=3&amp;pages=417-425&amp;publication_year=2005&amp;author=Sifakis%2CE&amp;author=Neverov%2CI&amp;author=Fedkiw%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Soon, WS. Lee, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Soon A, Lee WS (2006) Shape-based detail-preserving exaggeration of extremely accurate 3D faces. Vis Comput 22" /><p class="c-article-references__text" id="ref-CR50">Soon A, Lee WS (2006) Shape-based detail-preserving exaggeration of extremely accurate 3D faces. Vis Comput 22(7):478–492</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00371-006-0023-5" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Shape-based%20detail-preserving%20exaggeration%20of%20extremely%20accurate%203D%20faces&amp;journal=Vis%20Comput&amp;volume=22&amp;issue=7&amp;pages=478-492&amp;publication_year=2006&amp;author=Soon%2CA&amp;author=Lee%2CWS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="VV. Tarantili, DJ. Halazonetis, MN. Spyropoulos, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Tarantili VV, Halazonetis DJ, Spyropoulos MN (2005) The spontaneous smile in dynamic motion. Am J Orthod Dento" /><p class="c-article-references__text" id="ref-CR51">Tarantili VV, Halazonetis DJ, Spyropoulos MN (2005) The spontaneous smile in dynamic motion. Am J Orthod Dentofac Orthop 128(1):8–15</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ajodo.2004.03.042" aria-label="View reference 51">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 51 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20spontaneous%20smile%20in%20dynamic%20motion&amp;journal=Am%20J%20Orthod%20Dentofac%20Orthop&amp;volume=128&amp;issue=1&amp;pages=8-15&amp;publication_year=2005&amp;author=Tarantili%2CVV&amp;author=Halazonetis%2CDJ&amp;author=Spyropoulos%2CMN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Terzopoulos, K. Waters, " /><meta itemprop="datePublished" content="1990" /><meta itemprop="headline" content="Terzopoulos D, Waters K (1990) Physically-based facial modeling, analysis, and animation. J Vis Computer Anim " /><p class="c-article-references__text" id="ref-CR52">Terzopoulos D, Waters K (1990) Physically-based facial modeling, analysis, and animation. J Vis Computer Anim 1(4):73–80</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fvis.4340010208" aria-label="View reference 52">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Physically-based%20facial%20modeling%2C%20analysis%2C%20and%20animation&amp;journal=J%20Vis%20Computer%20Anim&amp;volume=1&amp;issue=4&amp;pages=73-80&amp;publication_year=1990&amp;author=Terzopoulos%2CD&amp;author=Waters%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SF. Wang, SH. Lai, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Wang SF, Lai SH (2010) Manifold-based 3D face caricature generation with individualized facial feature extract" /><p class="c-article-references__text" id="ref-CR53">Wang SF, Lai SH (2010) Manifold-based 3D face caricature generation with individualized facial feature extraction. Pac Graph 29(7):2161–2168</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Manifold-based%203D%20face%20caricature%20generation%20with%20individualized%20facial%20feature%20extraction&amp;journal=Pac%20Graph&amp;volume=29&amp;issue=7&amp;pages=2161-2168&amp;publication_year=2010&amp;author=Wang%2CSF&amp;author=Lai%2CSH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Wang, X. Huang, CS. Lee, S. Zhang, Z. Li, D. Samaras, D. Metaxas, A. Elgammal, P. Huang, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Wang Y, Huang X, Lee CS, Zhang S, Li Z, Samaras D, Metaxas D, Elgammal A, Huang P (2004) High resolution acqui" /><p class="c-article-references__text" id="ref-CR54">Wang Y, Huang X, Lee CS, Zhang S, Li Z, Samaras D, Metaxas D, Elgammal A, Huang P (2004) High resolution acquisition, learning and transfer of dynamic 3-D facial expressions. Eurograph 2004 Comput Graph Forum 23(3):677–686</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1111%2Fj.1467-8659.2004.00800.x" aria-label="View reference 54">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=High%20resolution%20acquisition%2C%20learning%20and%20transfer%20of%20dynamic%203-D%20facial%20expressions&amp;journal=Eurograph%202004%20Comput%20Graph%20Forum&amp;volume=23&amp;issue=3&amp;pages=677-686&amp;publication_year=2004&amp;author=Wang%2CY&amp;author=Huang%2CX&amp;author=Lee%2CCS&amp;author=Zhang%2CS&amp;author=Li%2CZ&amp;author=Samaras%2CD&amp;author=Metaxas%2CD&amp;author=Elgammal%2CA&amp;author=Huang%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGG" /><p class="c-article-references__text" id="ref-CR55">Waters K (1987) A muscle model for animating three-dimensional facial expressions. In: Proceedings of ACM SIGGRAPH 1987 computer graphics, vol 21, no 4, pp 17–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Zhang, Z. Liu, B. Guo, D. Terzopoulos, H. Shum, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum H (2006a) Geometry-driven photorealistic facial expression synthesi" /><p class="c-article-references__text" id="ref-CR56">Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum H (2006a) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Vis Comput Graph 12(1):48–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2006.9" aria-label="View reference 56">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometry-driven%20photorealistic%20facial%20expression%20synthesis&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=12&amp;issue=1&amp;pages=48-60&amp;publication_year=2006&amp;author=Zhang%2CQ&amp;author=Liu%2CZ&amp;author=Guo%2CB&amp;author=Terzopoulos%2CD&amp;author=Shum%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Q. Zhang, Z. Liu, B. Guo, D. Terzopoulos, HY. Shum, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum HY (2006b) Geometry-driven photorealistic facial expression synthes" /><p class="c-article-references__text" id="ref-CR57">Zhang Q, Liu Z, Guo B, Terzopoulos D, Shum HY (2006b) Geometry-driven photorealistic facial expression synthesis. IEEE Trans Vis Comput Graph 12(1):48–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2006.9" aria-label="View reference 57">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Geometry-driven%20photorealistic%20facial%20expression%20synthesis&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=12&amp;issue=1&amp;pages=48-60&amp;publication_year=2006&amp;author=Zhang%2CQ&amp;author=Liu%2CZ&amp;author=Guo%2CB&amp;author=Terzopoulos%2CD&amp;author=Shum%2CHY">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0227-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was partially supported by the Korea Research Foundation Grant fund (KRF-521-D00398).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Division of Multimedia, College of Engineering, Sungkyul University, Anyang, South Korea</p><p class="c-article-author-affiliation__authors-list">Seongah Chin</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Biointelligence Laboratory, School of Computer Science and Engineering, Seoul National University, Seoul, South Korea</p><p class="c-article-author-affiliation__authors-list">Chung Yeon Lee</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">DXP Lab., Department of Computer Science, College of Engineering, Korea University, Seoul, South Korea</p><p class="c-article-author-affiliation__authors-list">Jaedong Lee</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Seongah-Chin"><span class="c-article-authors-search__title u-h3 js-search-name">Seongah Chin</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Seongah+Chin&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Seongah+Chin" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Seongah+Chin%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Chung_Yeon-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Chung Yeon Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Chung Yeon+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chung Yeon+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chung Yeon+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jaedong-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Jaedong Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jaedong+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jaedong+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jaedong+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0227-8/email/correspondent/c1/new">Seongah Chin</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=An%20automatic%20method%20for%20motion%20capture-based%20exaggeration%20of%20facial%20expressions%20with%20personality%20types&amp;author=Seongah%20Chin%20et%20al&amp;contentID=10.1007%2Fs10055-013-0227-8&amp;publication=1359-4338&amp;publicationDate=2013-08-28&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Chin, S., Lee, C.Y. &amp; Lee, J. An automatic method for motion capture-based exaggeration of facial expressions with personality types.
                    <i>Virtual Reality</i> <b>17, </b>219–237 (2013). https://doi.org/10.1007/s10055-013-0227-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0227-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2012-12-27">27 December 2012</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-08">08 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-28">28 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-09">September 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0227-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0227-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Facial expressions</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Exaggeration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Facial motion capture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Facial motion cloning</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Personality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">MBTI</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Nonnegative matrix factorization</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0227-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=227;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

