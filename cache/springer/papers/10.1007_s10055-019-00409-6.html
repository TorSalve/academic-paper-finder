<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="The impact of the input interface in a virtual environment: the Vive c"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of such devices on the user experience has to..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="The impact of the input interface in a virtual environment: the Vive controller and the Myo armband"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-11-21"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of such devices on the user experience has to be evaluated, especially if we consider that most users are more familiar with classical handheld gaming controllers. In virtual reality applications, they influence not only the traditional usability, but also the user perception related to some peculiarities of immersive environments. In this paper, we evaluate both these aspects by comparing the user experience with the Myo armband touchless interface and the Vive controller distributed with the HTC Vive headset. We focused on a virtual navigator we developed for HTC Vive to allow users exploring the organs of the human body and navigating inside them. We recruited 78 subjects to test the virtual environment and asked them to fill in a questionnaire: we combined two generic purpose questionnaires focusing on the system usability (UMUX and SUS) and a presence questionnaire, which was specifically designed for virtual environments. We conducted a statistical analysis to study the effects of a touchless interaction on the user experience. The results revealed a better usability of the Vive controller, even though the effort to learn how to use the two devices is similar. In particular, difficulties in using Myo have a significant impact on immersion and adaptation in the virtual environment."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-11-21"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="20"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00409-6"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00409-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00409-6.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00409-6"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="The impact of the input interface in a virtual environment: the Vive controller and the Myo armband"/>

    <meta name="citation_online_date" content="2019/11/21"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="20"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00409-6"/>

    <meta name="DOI" content="10.1007/s10055-019-00409-6"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00409-6"/>

    <meta name="description" content="Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of s"/>

    <meta name="dc.creator" content="Lucio Tommaso De Paolis"/>

    <meta name="dc.creator" content="Valerio De Luca"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Anwar S, Sinha SK, Vivek S, Ashank V (2019) Hand gesture recognition: a survey. In: Lecture notes in electrical engineering, pp 365&#8211;371"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph (Pergamon); citation_title=A survey of 3D object selection techniques for virtual environments; citation_author=F Argelaguet, C Andujar; citation_volume=37; citation_issue=3; citation_publication_date=2013; citation_pages=121-136; citation_doi=10.1016/j.cag.2012.12.003; citation_id=CR2"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Sci Inf Technol (eJCSIT); citation_title=Standardized usability questionnaires: features and quality focus; citation_author=A Assila, K Mar&#231;al De Oliveira, H Ezzedine; citation_volume=6; citation_publication_date=2016; citation_pages=15-31; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Review of three-dimensional human&#8211;computer interaction with focus on the leap motion controller; citation_author=D Bachmann, F Weichert, G Rinkenauer; citation_volume=18; citation_publication_date=2018; citation_pages=2194; citation_doi=10.3390/s18072194; citation_id=CR4"/>

    <meta name="citation_reference" content="Bailey SK, Johnson CI, Sims VK (2019) Using natural gesture interactions leads to higher usability and presence in a computer lesson. In: Advances in intelligent systems and computing, pp 663&#8211;671"/>

    <meta name="citation_reference" content="citation_journal_title=Proc R Soc Lond Ser A Math Phys Sci; citation_title=Properties of sufficiency and statistical tests; citation_author=MS Bartlett; citation_volume=160; citation_issue=901; citation_publication_date=1937; citation_pages=268-282; citation_id=CR6"/>

    <meta name="citation_reference" content="Bhattacharyya A, Mazumder O, Chakravarty K, Chatterjee D, Sinha A, Gavas R (2018) Development of an interactive gaming solution using MYO sensor for rehabilitation. In: 2018 international conference on advances in computing, communications and informatics, ICACCI 2018, pp 2127&#8211;2130"/>

    <meta name="citation_reference" content="Borges M, Symington A, Coltin B, Smith T, Ventura R (2018) HTC Vive: analysis and accuracy improvement. In: IEEE international conference on intelligent robots and systems, pp 2610&#8211;2615"/>

    <meta name="citation_reference" content="citation_journal_title=Games Health J; citation_title=Comparison of Oculus Rift and HTC Vive: feasibility for virtual reality-based exploration, navigation, exergaming, and rehabilitation; citation_author=A Borrego, J Latorre, M Alca&#241;iz, R Llorens; citation_volume=7; citation_issue=3; citation_publication_date=2018; citation_pages=151-156; citation_doi=10.1089/g4h.2017.0114; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_journal_title=Cognit Process; citation_title=On the dimensionality of the system usability scale: a test of alternative measurement models; citation_author=S Borsci, S Federici, M Lauriola; citation_volume=10; citation_issue=3; citation_publication_date=2009; citation_pages=193-197; citation_doi=10.1007/s10339-009-0268-9; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Virtual reality: how much immersion is enough?; citation_author=DA Bowman, RP McMahan; citation_volume=40; citation_issue=7; citation_publication_date=2007; citation_pages=36-43; citation_doi=10.1109/MC.2007.257; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=3D user interfaces: new directions and perspectives; citation_author=DA Bowman, S Coquillart, B Froehlich, M Hirose, Y Kitamura, K Kiyokawa, W Stuerzlinger; citation_volume=28; citation_issue=6; citation_publication_date=2008; citation_pages=20-36; citation_doi=10.1109/MCG.2008.109; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_title=SUS&#8212;a quick and dirty usability scale; citation_inbook_title=Usability evaluation in industry; citation_publication_date=1996; citation_id=CR13; citation_author=J Brooke; citation_publisher=CRC Press"/>

    <meta name="citation_reference" content="Caggianese G, Gallo L, Neroni P (2019) The vive controllers vs. leap motion for interactions in virtual environments: a comparative evaluation. In: smart innovation, systems and technologies, pp 24&#8211;33"/>

    <meta name="citation_reference" content="Cain B (2004) A review of the mental workload literature. NATO RTO-TR-HFM-121-Part-II"/>

    <meta name="citation_reference" content="citation_title=Evaluation of basic object manipulation modes for low-cost immersive virtual reality; citation_publication_date=2015; citation_id=CR16; citation_author=FM Caputo; citation_author=A Giachetti; citation_publisher=ACM"/>

    <meta name="citation_reference" content="Chen MY, Tung YC, Wu PJ, Hsu CY, Chyou S, Valstar A, Wang HY, Lin JW (2015) User-defined game input for smart glasses in public space"/>

    <meta name="citation_reference" content="Cook H, Nguyen QV, Simoff S, Trescak T, Preston D (2015) A close-range gesture interaction with Kinect. In: 2015 big data visual analytics, BDVA 2015, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Csapo AB, Nagy H, Kristjansson A, Wersenyi G (2017) Evaluation of human-Myo gesture control capabilities in continuous search and select operations. In: 7th IEEE international conference on cognitive infocommunications, CogInfoCom 2016"/>

    <meta name="citation_reference" content="De Paolis LT (2016) A touchless gestural platform for the interaction with the patients data. In: XIV Mediterranean conference on medical and biological engineering and computing 2016 (MEDICON 2016), March 31st&#8211;April 2nd 2016, Paphos, Cyprus, IFMBE Proceedings. Springer, Berlin, pp 880&#8211;884"/>

    <meta name="citation_reference" content="De Paolis LT (2018) Augmented visualization and touchless interaction with virtual organs. In: International conference on bioinformatics and biomedical engineering (IWBBIO 2018) Granada, Spain, April 25&#8211;27, 2018. Lecture notes in bioinformatics, LNBI 10814. Springer, Berlin, pp 118&#8211;127"/>

    <meta name="citation_reference" content="citation_journal_title=Med Biol Eng Comput; citation_title=Augmented visualization with depth perception cues to improve the surgeon&#8217;s performance in minimally invasive surgery; citation_author=LT Paolis, V Luca; citation_volume=57; citation_issue=5; citation_publication_date=2019; citation_pages=995-1013; citation_doi=10.1007/s11517-018-1929-6; citation_id=CR22"/>

    <meta name="citation_reference" content="citation_journal_title=Stud Health Technol Inform; citation_title=Virtual model of the human brain for neurosurgical simulation; citation_author=LT Paolis, A Mauro, J Raczkowsky, G Aloisio; citation_volume=150; citation_publication_date=2009; citation_pages=811-815; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Inf Technol; citation_title=Advanced visualization and interaction systems for surgical pre-operative planning; citation_author=LT Paolis, M Pulimeno, G Aloisio; citation_volume=18; citation_issue=4; citation_publication_date=2010; citation_pages=385-392; citation_doi=10.2498/cit.1001878; citation_id=CR24"/>

    <meta name="citation_reference" content="De&#160;Paolis LT, De&#160;Luca V, Paladini GI (2019) Touchless navigation in a multimedia application: the effects perceived in an educational context. In: Sixth international conference augmented and virtual reality, and computer graphics (AVR 2019), Santa Maria al Bagno, Italy, June 24&#8211;27, 2019. Lecture notes in computer science, LNCS 11614. Springer, Berlin, pp 348&#8211;367"/>

    <meta name="citation_reference" content="Dong H, Figueroa N, El Saddik A (2016) An elicitation study on gesture attitudes and preferences towards an interactive hand-gesture vocabulary"/>

    <meta name="citation_reference" content="citation_journal_title=BioMed Eng Online; citation_title=Performance of the Emotiv Epoc headset for P300-based applications; citation_author=M Duvinage, T Castermans, M Petieau, T Hoellinger, G Cheron, T Dutoit; citation_volume=12; citation_issue=1; citation_publication_date=2013; citation_pages=56; citation_doi=10.1186/1475-925X-12-56; citation_id=CR27"/>

    <meta name="citation_reference" content="Emotiv (2019) Emotiv EPOC+. Retrieved 15 November 2019, from 
                https://www.emotiv.com
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph (Pergamon); citation_title=A comparative evaluation of direct hand and wand interactions on consumer devices; citation_author=L Figueiredo, E Rodrigues, J Teixeira, V Techrieb; citation_volume=77; citation_publication_date=2018; citation_pages=108-121; citation_doi=10.1016/j.cag.2018.10.006; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=Interact Comput; citation_title=The usability metric for user experience; citation_author=K Finstad; citation_volume=22; citation_issue=5; citation_publication_date=2010; citation_pages=323-327; citation_doi=10.1016/j.intcom.2010.04.004; citation_id=CR30"/>

    <meta name="citation_reference" content="citation_journal_title=Cognit Process; citation_title=The language of landmarks: the role of background knowledge in indoor wayfinding; citation_author=J Frankenstein, S Br&#252;ssow, F Ruzzoli, C H&#246;lscher; citation_volume=13; citation_issue=1; citation_publication_date=2012; citation_pages=165-170; citation_doi=10.1007/s10339-012-0482-8; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Biometrika; citation_title=The biplot graphic display of matrices with application to principal component analysis; citation_author=KR Gabriel; citation_volume=58; citation_issue=3; citation_publication_date=1971; citation_pages=453-467; citation_doi=10.1093/biomet/58.3.453; citation_id=CR32"/>

    <meta name="citation_reference" content="Gamer PC (2019) Valve index review. Retrieved 15 November 2019, from 
                https://www.pcgamer.com/valve-index-review/
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Gestural technology: moving interfaces in a new direction; citation_author=L Garber; citation_volume=46; citation_issue=10; citation_publication_date=2013; citation_pages=22-25; citation_doi=10.1109/MC.2013.352; citation_id=CR34"/>

    <meta name="citation_reference" content="Grandhi SA, Joue G, Mittelberg I (2011) Understanding naturalness and intuitiveness in gesture production&#8212;insights for touchless gestural interfaces. In: Proceedings of the international conference on human factors in computing systems (CHI&#8217;11), New York, NY, USA, pp 821&#8211;824"/>

    <meta name="citation_reference" content="Gusai E, Bassano C, Solari F, Chessa M (2017) Interaction in an immersive collaborative virtual reality environment: a comparison between Leap Motion and HTC controllers. In: Lecture notes in computer science, pp 290&#8211;300"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Suitability of the Kinect sensor and Leap Motion controller&#8212;a literature review; citation_author=T Guzsvinecz, V Szucs, C Sik-Lanyi; citation_volume=19; citation_publication_date=2019; citation_pages=1072; citation_doi=10.3390/s19051072; citation_id=CR37"/>

    <meta name="citation_reference" content="Hauser N, Wade E (2018) Detecting reach to grasp activities using motion and muscle activation data. In: Proceedings of the annual international conference of the IEEE engineering in medicine and biology society, EMBS, pp 3264&#8211;3267"/>

    <meta name="citation_reference" content="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display Otmar. In: Proceedings of the 2012 ACM annual conference on human factors in computing systems-CHI &#8217;12, New York, NY, USA, pp 2421&#8211;2430"/>

    <meta name="citation_reference" content="
HTC (2019a) HTC Vive controller. Retrieved 15 November 2019, from 
                https://www.vive.com/eu/accessory/controller/
                
              
"/>

    <meta name="citation_reference" content="HTC (2019b) HTC Vive. Retrieved 15 November 2019, from 
                https://www.vive.com/us/product/vive-virtual-reality-system/
                
              
"/>

    <meta name="citation_reference" content="
HTC (2019c) VIVE wireless adapter. Retrieved 15 November 2019, from 
                https://www.vive.com/us/wireless-adapter/
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=J Bus Res; citation_title=With or without you? Interaction and immersion in a virtual reality experience; citation_author=S Hudson, S Matson-Barkat, N Pallamin, G Jegou; citation_volume=100; citation_publication_date=2019; citation_pages=459-468; citation_doi=10.1016/j.jbusres.2018.10.062; citation_id=CR43"/>

    <meta name="citation_reference" content="Indraccolo C, De Paolis LT (2017) Augmented reality and MYO for a touchless interaction with virtual organs. In: Fourth international conference augmented and virtual reality, and computer graphics (AVR 2017), Ugento, Italy, June 12&#8211;15, 2017. Lecture notes in computer science, LNCS 10325, pp 63&#8211;73"/>

    <meta name="citation_reference" content="Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2015) Interactive entertainment, virtual motion training and brain ergonomy. In: 7th international conference on intelligent technologies for interactive entertainment (INTETAIN 2015), Torino, Italy, June 10&#8211;12, 2015, pp 88&#8211;94"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Haptic, virtual interaction and motor imagery: entertainment tools and psychophysiological testing; citation_author=S Invitto, C Faggiano, S Sammarco, V Luca, LT Paolis; citation_volume=16; citation_issue=3; citation_publication_date=2016; citation_pages=394; citation_doi=10.3390/s16030394; citation_id=CR46"/>

    <meta name="citation_reference" content="citation_journal_title=Psychometrika; citation_title=The varimax criterion for analytic rotation in factor analysis; citation_author=HF Kaiser; citation_volume=23; citation_issue=3; citation_publication_date=1958; citation_pages=187-200; citation_doi=10.1007/BF02289233; citation_id=CR47"/>

    <meta name="citation_reference" content="citation_journal_title=J Am Stat Assoc; citation_title=Use of ranks in one-criterion variance analysis; citation_author=WH Kruskal, WA Wallis; citation_volume=47; citation_issue=260; citation_publication_date=1952; citation_pages=583-621; citation_doi=10.1080/01621459.1952.10483441; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_journal_title=Remote Sens; citation_title=Assessment and calibration of a RGB-D camera (Kinect v2 Sensor) towards a potential use for close-range 3D modeling; citation_author=E Lachat, H Macher, T Landes, P Grussenmeyer; citation_volume=7; citation_issue=10; citation_publication_date=2015; citation_pages=13070-13097; citation_doi=10.3390/rs71013070; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Interact; citation_title=Measuring perceived usability: the CSUQ, SUS, and UMUX; citation_author=JR Lewis; citation_volume=34; citation_issue=12; citation_publication_date=2018; citation_pages=1148-1156; citation_doi=10.1080/10447318.2017.1418805; citation_id=CR50"/>

    <meta name="citation_reference" content="Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer science, pp 94&#8211;103"/>

    <meta name="citation_reference" content="Lewis JR, Utesch BS, Maher DE (2015) Investigating the correspondence between UMUX-LITE and SUS scores. In: Lecture notes in computer science, pp 204&#8211;211"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Ind Electron; citation_title=Stereoscopic visualization and 3-D technologies in medical endoscopic teleoperation; citation_author=S Livatino, LT Paolis, M D&#8217;Agostino, A Zocco, A Agrimi, A Santis, LV Bruno, M Lapresa; citation_volume=62; citation_issue=1; citation_publication_date=2015; citation_pages=525-535; citation_doi=10.1109/TIE.2014.2334675; citation_id=CR53"/>

    <meta name="citation_reference" content="Lucas JF, Kim JS, Bowman DA (2005) Resizing beyond widgets: object resizing techniques for immersive virtual environments. In: Proceedings of ACM CHI 2005 conference on human factors in computing systems, New York, NY, USA, pp 1601&#8211;1604"/>

    <meta name="citation_reference" content="Lund BAM (2001) Measuring usability with the USE questionnaire. STC usability SIG newsletter"/>

    <meta name="citation_reference" content="McMahan RP, Gorton D, Gresock J, McConnell W, Bowman DA (2007) Separating the effects of level of immersion and 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 108&#8211;111"/>

    <meta name="citation_reference" content="citation_journal_title=Univ Access Inf Soc; citation_title=Natural interaction in virtual TV sets through the synergistic operation of low-cost sensors; citation_author=R M&#233;ndez, J Flores, E Castell&#243;, JR Viqueira; citation_volume=18; citation_issue=1; citation_publication_date=2019; citation_pages=17-29; citation_doi=10.1007/s10209-017-0586-0; citation_id=CR57"/>

    <meta name="citation_reference" content="Microsoft (2019a) Azure Kinect DK. Retrieved 15 November 2019, from 
                https://azure.microsoft.com/en-us/services/kinect-dk/
                
              
"/>

    <meta name="citation_reference" content="Microsoft (2019b) Microsoft Hololens. Retrieved 15 November 2019, from 
                https://www.microsoft.com/en-IE/hololens
                
              
"/>

    <meta name="citation_reference" content="Motion Leap (2019) Leap Motion. Retrieved 15 November 2019, from 
                https://www.leapmotion.com/
                
              
"/>

    <meta name="citation_reference" content="Moustafa K, Luz S, Longo L (2017) Assessment of mental workload: a comparison of machine learning methods and subjective assessment techniques. In: Communications in computer and information science, pp 30&#8211;50"/>

    <meta name="citation_reference" content="citation_journal_title=i-Perception; citation_title=The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research; citation_author=DC Niehorster, L Li, M Lappe; citation_volume=8; citation_issue=3; citation_publication_date=2017; citation_pages=2041669517708205; citation_id=CR62"/>

    <meta name="citation_reference" content="Nintendo (2019) Wii Remote. Retrieved 15 November 2019, from 
                https://www.nintendo.com
                
              
"/>

    <meta name="citation_reference" content="Oculus VR (2019a) Oculus Rift. Retrieved 15 November 2019, from 
                https://www.oculus.com/
                
              
"/>

    <meta name="citation_reference" content="Oculus VR (2019b) Oculus Touch. Retrieved 15 November 2019, from 
                https://www.oculus.com/rift/accessories/
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Assessing hands-free interactions for VR using eye gaze and electromyography; citation_author=YS Pai, T Dingler, K Kunze; citation_volume=23; citation_publication_date=2018; citation_pages=119-131; citation_doi=10.1007/s10055-018-0371-2; citation_id=CR66"/>

    <meta name="citation_reference" content="Polygon (2019) Oculus Go review. Retrieved 15 November 2019, from 
                https://www.polygon.com/virtual-reality/2018/5/1/17284454/oculus-go-review
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=J Ambient Intell Humaniz Comput; citation_title=Impact of commercial sensors in human computer interaction: a review; citation_author=EJ Rechy-Ramirez, A Marin-Hernandez, HV Rios-Figueroa; citation_volume=9; citation_publication_date=2018; citation_pages=1479-1496; citation_doi=10.1007/s12652-017-0568-3; citation_id=CR68"/>

    <meta name="citation_reference" content="citation_title=Review: 3D user interfaces: theory and practice; citation_inbook_title=Presence: teleoperators and virtual environments; citation_publication_date=2006; citation_id=CR69; citation_author=R Ruddle; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="Samsung (2019) Samsung Gear VR. Retrieved 15 November 2019, from 
                https://www.samsung.com/us/mobile/virtual-reality/gear-vr/gear-vr-with-controller-sm-r324nzaaxar/
                
              
"/>

    <meta name="citation_reference" content="citation_title=A usability scale for handheld augmented reality; citation_publication_date=2014; citation_id=CR71; citation_author=MEC Santos; citation_author=C Sandor; citation_author=H Kato; citation_author=G Yamamoto; citation_author=T Taketomi; citation_author=J Polvi; citation_publisher=ACM"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Toward standard usability questionnaires for handheld augmented reality; citation_author=MEC Santos, J Polvi, T Taketomi, G Yamamoto, C Sandor, H Kato; citation_volume=35; citation_issue=5; citation_publication_date=2015; citation_pages=66-75; citation_doi=10.1109/MCG.2015.94; citation_id=CR72"/>

    <meta name="citation_reference" content="Santos-Torres A, Zarraonandia T, D&#237;az P, Aedo I (2018) Exploring interaction mechanisms for map interfaces in virtual reality environments. In: Proceedings of the XIX international conference on human computer interaction. ACM, New York, pp 1&#8211;7"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Kinect range sensing: structured-light versus Time-of-Flight Kinect; citation_author=H Sarbolandi, D Lefloch, A Kolb; citation_volume=139; citation_publication_date=2015; citation_pages=1-20; citation_doi=10.1016/j.cviu.2015.05.006; citation_id=CR74"/>

    <meta name="citation_reference" content="Sayin FS, Ozen S, Baspinar U (2018) Hand gesture recognition by using sEMG signals for human machine interaction applications. In: Signal processing&#8212;algorithms, architectures, arrangements, and applications conference proceedings, SPA, pp 27&#8211;30"/>

    <meta name="citation_reference" content="citation_journal_title=Biometrika; citation_title=An analysis of variance test for normality (complete samples); citation_author=SS Shapiro, MB Wilk; citation_volume=52; citation_issue=3/4; citation_publication_date=1965; citation_pages=591-611; citation_doi=10.2307/2333709; citation_id=CR76"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Do virtual reality head-mounted displays make a difference? A comparison of presence and self-efficacy between head-mounted displays and desktop computer-facilitated virtual environments; citation_author=Y Shu, YZ Huang, SH Chang, MY Chen; citation_publication_date=2018; citation_doi=10.1007/s10055-018-0376-x; citation_id=CR77"/>

    <meta name="citation_reference" content="citation_journal_title=Future Gener Comput Syst; citation_title=Estimating VR sickness and user experience using different HMD technologies: an evaluation study; citation_author=A Somrak, I Humar, MS Hossain, MF Alhamid, MA Hossain, J Guna; citation_volume=94; citation_publication_date=2019; citation_pages=302-316; citation_doi=10.1016/j.future.2018.11.041; citation_id=CR78"/>

    <meta name="citation_reference" content="Sony (2019) PlayStation Move. Retrieved15 November 2019,  from 
                https://www.playstation.com/en-us/explore/accessories/vr-accessories/playstation-move/
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=J Biomech; citation_title=Feasibility of using a fully immersive virtual reality system for kinematic data collection; citation_author=KA Spitzley, AR Karduna; citation_volume=87; citation_publication_date=2019; citation_pages=172-176; citation_doi=10.1016/j.jbiomech.2019.02.015; citation_id=CR80"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Hum Comput Stud; citation_title=Usability engineering of virtual environments (VEs): Identifying multiple criteria that drive effective VE system design; citation_author=KM Stanney, M Mollaghasemi, L Reeves, R Breaux, DA Graeber; citation_volume=58; citation_issue=4; citation_publication_date=2003; citation_pages=447-481; citation_doi=10.1016/S1071-5819(03)00015-6; citation_id=CR81"/>

    <meta name="citation_reference" content="Steam (2019) SteamVR. Retrieved 15 November 2019, from 
                https://steamcommunity.com/steamvr
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=The effect of a virtual reality learning environment on learners&#8217; spatial ability; citation_author=R Sun, YJ Wu, Q Cai; citation_publication_date=2018; citation_doi=10.1007/s10055-018-0355-2; citation_id=CR83"/>

    <meta name="citation_reference" content="citation_journal_title=J Comput Des Eng; citation_title=Navigation modes, operation methods, observation scales and background options in UI design for high learning performance in VR-based architectural applications; citation_author=C Sun, W Hu, D Xu; citation_volume=6; citation_issue=2; citation_publication_date=2019; citation_pages=189-196; citation_id=CR84"/>

    <meta name="citation_reference" content="Tcha-Tokey K, Loup-Escande E, Christmann O, Richir S (2017) Effects on user experience in an edutainment virtual environment"/>

    <meta name="citation_reference" content="Thalmic Labs (2019) Myo armband. Retrieved 15 November 2019, from 
                https://support.getmyo.com/hc/en-us/articles/203398347-Getting-started-with-your-Myo-armband
                
              
"/>

    <meta name="citation_reference" content="citation_title=Measuring the user experience: collecting, analyzing, and presenting usability metrics; citation_publication_date=2013; citation_id=CR88; citation_author=T Tullis; citation_author=B Albert; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="citation_journal_title=IBM Syst J; citation_title=Emerging frameworks for tangible user interfaces; citation_author=B Ullmer, H Ishii; citation_volume=39; citation_publication_date=2010; citation_pages=915-931; citation_doi=10.1147/sj.393.0915; citation_id=CR89"/>

    <meta name="citation_reference" content="citation_journal_title=Medicina (Lithuania); citation_title=Subjective visual vertical assessment with mobile virtual reality system; citation_author=I Uloziene, M Totiliene, A Paulauskas, T Bla&#382;auskas, V Marozas, D Kaski, V Ulozas; citation_volume=53; citation_issue=6; citation_publication_date=2017; citation_pages=394-402; citation_id=CR90"/>

    <meta name="citation_reference" content="Unity Technologies (2019) Unity3D. Retrieved 15 November 2019, from 
                https://unity3d.com
                
              
"/>

    <meta name="citation_reference" content="UploadVR (2019a) Oculus Quest review. Retrieved 15 November 2019, from 
                https://uploadvr.com/oculus-quest-review/
                
              
"/>

    <meta name="citation_reference" content="UploadVR (2019b) OC6: Oculus Quest is getting camera-based finger tracking early next year. Retrieved 15 November 2019, from 
                https://uploadvr.com/oculus-quest-finger-tracking/
                
              
"/>

    <meta name="citation_reference" content="UploadVR (2019c) Oculus Rift S is official. Retrieved 15 November 2019, from 
                https://uploadvr.com/oculus-rift-s-official/
                
              
"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Evaluation of visual feedback techniques for virtual grasping with bare hands using Leap Motion and Oculus Rift; citation_author=S Vosinakis, P Koutsabasis; citation_volume=22; citation_issue=1; citation_publication_date=2018; citation_pages=47-62; citation_doi=10.1007/s10055-017-0313-4; citation_id=CR94"/>

    <meta name="citation_reference" content="Vrellis I, Moutsioulis A, Mikropoulos TA (2014) Primary school students&#8217; attitude towards gesture based interaction: a comparison between Microsoft Kinect and mouse. In: Proceedings&#8212;IEEE 14th international conference on advanced learning technologies, ICALT 2014, pp 678&#8211;682"/>

    <meta name="citation_reference" content="Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift&#174; DK2 and Samsung Gear VR&#174;. In: 2017 ASEE annual conference &amp; exposition, ASEE conferences, Columbus, Ohio"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=Measuring presence in virtual environments: a presence questionnaire; citation_author=BG Witmer, MJ Singer; citation_volume=7; citation_issue=3; citation_publication_date=1998; citation_pages=225-240; citation_doi=10.1162/105474698565686; citation_id=CR97"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The factor structure of the presence questionnaire; citation_author=BG Witmer, CJ Jerome, MJ Singer; citation_volume=14; citation_publication_date=2005; citation_pages=298-312; citation_doi=10.1162/105474605323384654; citation_id=CR98"/>

    <meta name="citation_reference" content="citation_title=Maximizing the guessability of symbolic input; citation_publication_date=2005; citation_id=CR99; citation_author=JO Wobbrock; citation_author=HH Aung; citation_author=B Rothrock; citation_author=BA Myers; citation_publisher=ACM"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Ergon; citation_title=An evaluation for VR glasses system user experience: the influence factors of interactive operation and motion sickness; citation_author=M Yu, R Zhou, H Wang, W Zhao; citation_volume=74; citation_publication_date=2019; citation_pages=206-213; citation_doi=10.1016/j.apergo.2018.08.012; citation_id=CR100"/>

    <meta name="citation_author" content="Lucio Tommaso De Paolis"/>

    <meta name="citation_author_email" content="lucio.depaolis@unisalento.it"/>

    <meta name="citation_author_institution" content="Department of Engineering for Innovation, University of Salento, Lecce, Italy"/>

    <meta name="citation_author" content="Valerio De Luca"/>

    <meta name="citation_author_email" content="valerio.deluca@unisalento.it"/>

    <meta name="citation_author_institution" content="Department of Engineering for Innovation, University of Salento, Lecce, Italy"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00409-6&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00409-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="The impact of the input interface in a virtual environment: the Vive controller and the Myo armband"/>
        <meta property="og:description" content="Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of such devices on the user experience has to be evaluated, especially if we consider that most users are more familiar with classical handheld gaming controllers. In virtual reality applications, they influence not only the traditional usability, but also the user perception related to some peculiarities of immersive environments. In this paper, we evaluate both these aspects by comparing the user experience with the Myo armband touchless interface and the Vive controller distributed with the HTC Vive headset. We focused on a virtual navigator we developed for HTC Vive to allow users exploring the organs of the human body and navigating inside them. We recruited 78 subjects to test the virtual environment and asked them to fill in a questionnaire: we combined two generic purpose questionnaires focusing on the system usability (UMUX and SUS) and a presence questionnaire, which was specifically designed for virtual environments. We conducted a statistical analysis to study the effects of a touchless interaction on the user experience. The results revealed a better usability of the Vive controller, even though the effort to learn how to use the two devices is similar. In particular, difficulties in using Myo have a significant impact on immersion and adaptation in the virtual environment."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>The impact of the input interface in a virtual environment: the Vive controller and the Myo armband | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00409-6","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Touchless interaction, Gesture, User experience, Usability, Presence, Virtual environment","kwrd":["Touchless_interaction","Gesture","User_experience","Usability","Presence","Virtual_environment"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00409-6","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00409-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=409;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00409-6">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            The impact of the input interface in a virtual environment: the Vive controller and the Myo armband
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00409-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00409-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-11-21" itemprop="datePublished">21 November 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">The impact of the input interface in a virtual environment: the Vive controller and the Myo armband</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Lucio_Tommaso-De_Paolis" data-author-popup="auth-Lucio_Tommaso-De_Paolis">Lucio Tommaso De Paolis</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Salento" /><meta itemprop="address" content="grid.9906.6, 0000 0001 2289 7785, Department of Engineering for Innovation, University of Salento, Lecce, Italy" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Valerio-De_Luca" data-author-popup="auth-Valerio-De_Luca" data-corresp-id="c1">Valerio De Luca<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-3018-7251"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-3018-7251</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Salento" /><meta itemprop="address" content="grid.9906.6, 0000 0001 2289 7785, Department of Engineering for Innovation, University of Salento, Lecce, Italy" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">198 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00409-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of such devices on the user experience has to be evaluated, especially if we consider that most users are more familiar with classical handheld gaming controllers. In virtual reality applications, they influence not only the traditional usability, but also the user perception related to some peculiarities of immersive environments. In this paper, we evaluate both these aspects by comparing the user experience with the Myo armband touchless interface and the Vive controller distributed with the HTC Vive headset. We focused on a virtual navigator we developed for HTC Vive to allow users exploring the organs of the human body and navigating inside them. We recruited 78 subjects to test the virtual environment and asked them to fill in a questionnaire: we combined two generic purpose questionnaires focusing on the system usability (UMUX and SUS) and a presence questionnaire, which was specifically designed for virtual environments. We conducted a statistical analysis to study the effects of a touchless interaction on the user experience. The results revealed a better usability of the Vive controller, even though the effort to learn how to use the two devices is similar. In particular, difficulties in using Myo have a significant impact on immersion and adaptation in the virtual environment.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Touchless interfaces, which allow giving commands without touching anything, have become more and more popular in ICT. In virtual reality applications, they enable the possibility to move or manipulate virtual objects by means of gestures or even by simulating the act of grasping objects. They bring important benefits in several application scenarios. For instance, during surgical operations touchless interaction allows to meet health and hygiene standards, which prohibit coming in contact with nonsterilized objects and thus prevent the use of keyboards, mouse and touchscreens. In such contexts, gesture-based touchless interaction can allow doctors to consult medical data during interventions (De Paolis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="De Paolis LT (2016) A touchless gestural platform for the interaction with the patients data. In: XIV Mediterranean conference on medical and biological engineering and computing 2016 (MEDICON 2016), March 31st–April 2nd 2016, Paphos, Cyprus, IFMBE Proceedings. Springer, Berlin, pp 880–884" href="/article/10.1007/s10055-019-00409-6#ref-CR20" id="ref-link-section-d27611e279">2016</a>). Moreover, it can be employed in augmented reality applications, which superimpose 3D models reconstructed from CT slices on the patient body to guide surgical procedures (De Paolis and De Luca <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="De Paolis LT, De Luca V (2019) Augmented visualization with depth perception cues to improve the surgeon’s performance in minimally invasive surgery. Med Biol Eng Comput 57(5):995–1013" href="/article/10.1007/s10055-019-00409-6#ref-CR22" id="ref-link-section-d27611e282">2019</a>): while the interaction with 3D models of the organs can still exploit touch-based devices in virtual training (De Paolis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="De Paolis LT, De Mauro A, Raczkowsky J, Aloisio G (2009) Virtual model of the human brain for neurosurgical simulation. Stud Health Technol Inform 150:811–815" href="/article/10.1007/s10055-019-00409-6#ref-CR23" id="ref-link-section-d27611e285">2009</a>) or during the preoperative planning (De Paolis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="De Paolis LT, Pulimeno M, Aloisio G (2010) Advanced visualization and interaction systems for surgical pre-operative planning. J Comput Inf Technol 18(4):385–392" href="/article/10.1007/s10055-019-00409-6#ref-CR24" id="ref-link-section-d27611e288">2010</a>), for intraoperative support it should be based on touchless modalities (De Paolis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="De Paolis LT (2018) Augmented visualization and touchless interaction with virtual organs. In: International conference on bioinformatics and biomedical engineering (IWBBIO 2018) Granada, Spain, April 25–27, 2018. Lecture notes in bioinformatics, LNBI 10814. Springer, Berlin, pp 118–127" href="/article/10.1007/s10055-019-00409-6#ref-CR21" id="ref-link-section-d27611e291">2018</a>; Indraccolo and De Paolis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Indraccolo C, De Paolis LT (2017) Augmented reality and MYO for a touchless interaction with virtual organs. In: Fourth international conference augmented and virtual reality, and computer graphics (AVR 2017), Ugento, Italy, June 12–15, 2017. Lecture notes in computer science, LNCS 10325, pp 63–73" href="/article/10.1007/s10055-019-00409-6#ref-CR44" id="ref-link-section-d27611e295">2017</a>) to be fully compliant with sanitary regulations.</p><p>Unfortunately, many gestures have been defined mainly according to what a physical device can recognize. Therefore, gesture-based interaction closely depends on the device working principle. For instance, the Myo armband can detect five basic gestures that are related to specific arm impulses it can recognize. Other gestures are combinations of these ones. For these reasons, gesture-based control is not always perceived as a natural form of interaction by users. Compared to arbitrary gestures, natural gestures can enhance both the system usability and the feeling of presence perceived by users. Some authors introduced also the concept of guessability (Wobbrock et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="&#xA;Wobbrock JO, Aung HH, Rothrock B, Myers BA (2005) Maximizing the guessability of symbolic input. In: CHI ’05 extended abstracts on human factors in computing systems. ACM, New York, pp 1869–1872" href="/article/10.1007/s10055-019-00409-6#ref-CR99" id="ref-link-section-d27611e301">2005</a>), which refers to the ability to guess symbolic input without any prior knowledge or learning phase. In this sense, guessability is a stronger concept than immediate usability, which does not exclude the need for a brief learning period. A redesign of gesture interfaces based on such concepts is an important and challenging issue, which brought to the definition of interactive gesture vocabularies based on users’ attitudes and preferences (Dong et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Dong H, Figueroa N, El Saddik A (2016) An elicitation study on gesture attitudes and preferences towards an interactive hand-gesture vocabulary" href="/article/10.1007/s10055-019-00409-6#ref-CR26" id="ref-link-section-d27611e304">2016</a>).</p><p>The sense of presence is a key aspect for the effectiveness of virtual environments. It denotes the subjective experience of being in one place or environment, independently of where a subject is actually located (Witmer and Singer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Witmer BG, Singer MJ (1998) Measuring presence in virtual environments: a presence questionnaire. Presence Teleoper Virtual Environ 7(3):225–240" href="/article/10.1007/s10055-019-00409-6#ref-CR97" id="ref-link-section-d27611e310">1998</a>). Two necessary conditions for experiencing presence are immersion and involvement. Immersion denotes the perception of being surrounded by an environment that provides a stream of sensorial experiences. Therefore, the immersion level is correlated with the fidelity in reproducing real-world sensory modalities. Immersion depends on isolation from the physical environment, perception of self-inclusion in the virtual environment, natural modes of interaction and control and perception of self-movement. Involvement refers to users focusing their attention on stimuli provided by the virtual environment.</p><p>Our work is contemporary with the above-mentioned debate on natural gestures. In particular, we try to assess how touchless interaction can influence not only the usability, but also the sense of presence in a virtual environment. The study aims at detecting whether a touchless interface like the Myo armband is mature enough for immersive virtual reality scenarios. To this aim, we conducted an experimental study in a virtual environment to compare the Myo armband touchless device and the Vive controller bundled with the HTC Vive headset (HTC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="HTC (2019b) HTC Vive. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/us/product/vive-virtual-reality-system/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR41" id="ref-link-section-d27611e316">2019b</a>) in terms of impact on the user experience. The two considered devices represent the classical handheld gaming controllers and the more recent gesture-based interfaces, respectively. The subjects we recruited for tests are between 20 and 40 years old: most of them have already tried or seen traditional game controllers that were popular during the 1980s and the 1990s in the entertainment industry. On the other hand, gesture-based devices are more recent and have a lower popularity. For this reason, most users could be unfamiliar with them. Moreover, we guess most of Myo’s gestures could not be perceived as natural. In general, we expect that Myo could provide a lower usability compared to the traditional gamepad controllers young users have confidence with. This study aims at verifying such hypothesis and evaluating which aspects of the user experience in a virtual environment are mainly influenced by gesture-based interaction.</p><p>The reference application chosen for the experimental study is a virtual reality navigator we developed for HTC Vive headset that allows the user to explore the organs of the human body and navigate inside them.</p><p>The rest of the paper is structured in this way: Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec2">2</a> discusses related work about the assessment of the user experience; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec3">3</a> discusses the benefits deriving from immersion in a virtual environment; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec4">4</a> introduces natural interaction; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec7">5</a> describes the virtual reality headset employed in our scenario; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec9">6</a> describes the reference scenario that is the surgical navigator we developed for the HTC Vive headset; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec11">7</a> describes the methodology we adopted to assess the user experience; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec15">8</a> presents the results; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec19">9</a> discusses the main findings of the experiment; Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec20">10</a> concludes the paper.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Several studies were conducted to determine which gestures are perceived as natural. Some experiments (Grandhi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Grandhi SA, Joue G, Mittelberg I (2011) Understanding naturalness and intuitiveness in gesture production—insights for touchless gestural interfaces. In: Proceedings of the international conference on human factors in computing systems (CHI’11), New York, NY, USA, pp 821–824" href="/article/10.1007/s10055-019-00409-6#ref-CR35" id="ref-link-section-d27611e362">2011</a>) showed dynamic pantomimic gestures consisting in explicitly holding an imagined object are the most intuitive. Other experiments (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Chen MY, Tung YC, Wu PJ, Hsu CY, Chyou S, Valstar A, Wang HY, Lin JW (2015) User-defined game input for smart glasses in public space" href="/article/10.1007/s10055-019-00409-6#ref-CR17" id="ref-link-section-d27611e365">2015</a>) collected some statistics on users’ preferences about input modalities when using smart glasses: they revealed palm interaction is preferred over the use of wearable devices; in particular, in-air gestures are mostly preferred.</p><p>The study presented in De Paolis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="De Paolis LT, De Luca V, Paladini GI (2019) Touchless navigation in a multimedia application: the effects perceived in an educational context. In: Sixth international conference augmented and virtual reality, and computer graphics (AVR 2019), Santa Maria al Bagno, Italy, June 24–27, 2019. Lecture notes in computer science, LNCS 11614. Springer, Berlin, pp 348–367" href="/article/10.1007/s10055-019-00409-6#ref-CR25" id="ref-link-section-d27611e371">2019</a>) evaluated the effects of a Kinect-based touchless navigation in a multimedia learning application: the work analyzed the perceived usability of Kinect-based touchless interface according to the System Usability Scale (SUS) (Brooke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="&#xA;Brooke J (1996) SUS—a quick and dirty usability scale. In: Jordan PW, Thomas B, McClelland IL, Weerdmeester B (eds) Usability evaluation in industry. CRC Press, Boca Raton" href="/article/10.1007/s10055-019-00409-6#ref-CR13" id="ref-link-section-d27611e374">1996</a>) and USE (Usefulness, Satisfaction, and Ease of use) (Lund <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lund BAM (2001) Measuring usability with the USE questionnaire. STC usability SIG newsletter" href="/article/10.1007/s10055-019-00409-6#ref-CR55" id="ref-link-section-d27611e377">2001</a>) questionnaire and the consequences in the learning activity according to a custom questionnaire.</p><p>Usability for hand interaction based on the Leap Motion controller was evaluated in Vosinakis and Koutsabasis (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Vosinakis S, Koutsabasis P (2018) Evaluation of visual feedback techniques for virtual grasping with bare hands using Leap Motion and Oculus Rift. Virtual Real 22(1):47–62" href="/article/10.1007/s10055-019-00409-6#ref-CR94" id="ref-link-section-d27611e383">2018</a>) in terms of execution time, accuracy and user satisfaction for grasp-and-release tasks concerning object coloring, connecting line, shadow and object halo. The comparison between the virtual environment and the desktop environment revealed better user performance in the former case and highlighted the importance of visual feedback in both the scenarios.</p><p>In Figueiredo et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Figueiredo L, Rodrigues E, Teixeira J, Techrieb V (2018) A comparative evaluation of direct hand and wand interactions on consumer devices. Comput Graph (Pergamon) 77:108–121" href="/article/10.1007/s10055-019-00409-6#ref-CR29" id="ref-link-section-d27611e389">2018</a>), the Leap Motion Controller and the HTC Vive were compared for near and far virtual object interaction according to the SUS scale. Such study proved natural interaction works well when objects are close to the user, but it needs some improvement for scenarios dealing with distant objects. It differs from the analysis described in this paper mainly for the following reasons:</p><ol class="u-list-style-none">
              <li>
                <span class="u-custom-list-number">1.</span>
                
                  <p>It focused on abstract interaction scenarios, while in our analysis gestures are contextualized in a concrete immersive use case (a virtual simulator consisting of three scenarios of the human body);</p>
                
              </li>
              <li>
                <span class="u-custom-list-number">2.</span>
                
                  <p>It employed only the System Usability Scale, whereas in our analysis we made a comparison among three different metrics; in particular, the Presence Questionnaire allowed us to evaluate also the consequences of the interaction modality on some specific aspects characterizing a virtual experience.</p>
                
              </li>
            </ol><p>Other studies in literature compared body-based and device-based interaction in various contexts by considering both objective measurements and qualitative observations. The most common reference parameters are the time spent to perform the requested actions, the error count and the perceived usability.</p><p>The study presented in Santos-Torres et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Santos-Torres A, Zarraonandia T, Díaz P, Aedo I (2018) Exploring interaction mechanisms for map interfaces in virtual reality environments. In: Proceedings of the XIX international conference on human computer interaction. ACM, New York, pp 1–7" href="/article/10.1007/s10055-019-00409-6#ref-CR73" id="ref-link-section-d27611e421">2018</a>) compared two different interaction modalities for map interfaces in Geographical Information Systems. The former exploits hands, detected by a Leap Motion device, and gaze, while the latter is based on the Oculus Touch devices. The results suggested that device-based interaction provides better performance in selection tasks and is perceived as more usable.</p><p>The analysis conducted in Caggianese et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Caggianese G, Gallo L, Neroni P (2019) The vive controllers vs. leap motion for interactions in virtual environments: a comparative evaluation. In: smart innovation, systems and technologies, pp 24–33" href="/article/10.1007/s10055-019-00409-6#ref-CR14" id="ref-link-section-d27611e427">2019</a>) compared the Vive controller and the Leap Motion device as interfaces for manipulation tasks. The results revealed the Vive controller allows a faster and more accurate interaction. Users encountered some difficulties with Leap Motion, probably due to the absence of a tactile feedback and due to tracking interruptions when hands move out of the sensor field of view.</p><p>While such work considered an abstract interaction scenario, another study (Gusai et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Gusai E, Bassano C, Solari F, Chessa M (2017) Interaction in an immersive collaborative virtual reality environment: a comparison between Leap Motion and HTC controllers. In: Lecture notes in computer science, pp 290–300" href="/article/10.1007/s10055-019-00409-6#ref-CR36" id="ref-link-section-d27611e433">2017</a>) compared the two devices for object manipulation in a collaborative environment. The results revealed a strong user preference toward the Vive controller, which provides better performance in terms of task execution time and accuracy.</p><p>The system described in Pai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Pai YS, Dingler T, Kunze K (2018) Assessing hands-free interactions for VR using eye gaze and electromyography. Virtual Real 23:119–131" href="/article/10.1007/s10055-019-00409-6#ref-CR66" id="ref-link-section-d27611e439">2018</a>) proved the combination of gaze tracking for cursor movement and forearm contractions for selection can achieve better performance than Vive controller, Xbox gamepad and simple dwell gaze.</p><p>While such comparative studies focused on the perceived usability of devices and interaction modalities, our work tries also to analyze the correlation between usability and sense of presence. Moreover, our work considers gesture-based interaction metaphors instead of accurate selection tasks.</p><p>In another work (Uloziene et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Uloziene I, Totiliene M, Paulauskas A, Blažauskas T, Marozas V, Kaski D, Ulozas V (2017) Subjective visual vertical assessment with mobile virtual reality system. Medicina (Lithuania) 53(6):394–402" href="/article/10.1007/s10055-019-00409-6#ref-CR90" id="ref-link-section-d27611e448">2017</a>), the SUS scale was used to compare two methods for controlling objects based on the Myo armband. The target scenario was an immersive real-world environment dealing with a boat in the sea. However, the analysis focused only on the subjective visual vertical, i.e., the ability to indicate what is perceived to be an Earth vertical line. In Bailey et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Bailey SK, Johnson CI, Sims VK (2019) Using natural gesture interactions leads to higher usability and presence in a computer lesson. In: Advances in intelligent systems and computing, pp 663–671" href="/article/10.1007/s10055-019-00409-6#ref-CR5" id="ref-link-section-d27611e451">2019</a>), natural gestures were compared to arbitrary gestures by employing the Kinect device in a computer-based science lesson about telescope and lenses: perceived usability and presence were evaluated by means of the SUS scale and the Presence Questionnaire (Witmer and Singer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Witmer BG, Singer MJ (1998) Measuring presence in virtual environments: a presence questionnaire. Presence Teleoper Virtual Environ 7(3):225–240" href="/article/10.1007/s10055-019-00409-6#ref-CR97" id="ref-link-section-d27611e454">1998</a>), but the considered scenario was not a really a immersive virtual environment.</p><p>Some works (Santos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Santos MEC, Sandor C, Kato H, Yamamoto G, Taketomi T, Polvi J (2014) A usability scale for handheld augmented reality. In: Proceedings of the 20th ACM symposium on virtual reality software and technology. ACM, New York, pp 167–176" href="/article/10.1007/s10055-019-00409-6#ref-CR71" id="ref-link-section-d27611e461">2014</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Santos MEC, Polvi J, Taketomi T, Yamamoto G, Sandor C, Kato H (2015) Toward standard usability questionnaires for handheld augmented reality. IEEE Comput Graph Appl 35(5):66–75" href="/article/10.1007/s10055-019-00409-6#ref-CR72" id="ref-link-section-d27611e464">2015</a>) focused on the design of a questionnaire for assessing usability of smartphones and tablets in augmented reality applications: the result was the HARUS questionnaire, made up of a comprehensibility scale and a manipulability scale.</p><p>Other usability studies (Yu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Yu M, Zhou R, Wang H, Zhao W (2019) An evaluation for VR glasses system user experience: the influence factors of interactive operation and motion sickness. Appl Ergon 74:206–213" href="/article/10.1007/s10055-019-00409-6#ref-CR100" id="ref-link-section-d27611e470">2019</a>; Somrak et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Somrak A, Humar I, Hossain MS, Alhamid MF, Hossain MA, Guna J (2019) Estimating VR sickness and user experience using different HMD technologies: an evaluation study. Future Gener Comput Syst 94:302–316" href="/article/10.1007/s10055-019-00409-6#ref-CR78" id="ref-link-section-d27611e473">2019</a>; Livatino et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Livatino S, De Paolis LT, D’Agostino M, Zocco A, Agrimi A, De Santis A, Bruno LV, Lapresa M (2015) Stereoscopic visualization and 3-D technologies in medical endoscopic teleoperation. IEEE Trans Ind Electron 62(1):525–535" href="/article/10.1007/s10055-019-00409-6#ref-CR53" id="ref-link-section-d27611e476">2015</a>; Webster and Dues <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift® DK2 and Samsung Gear VR®. In: 2017 ASEE annual conference &amp; exposition, ASEE conferences, Columbus, Ohio" href="/article/10.1007/s10055-019-00409-6#ref-CR96" id="ref-link-section-d27611e479">2017</a>; Tcha-Tokey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Tcha-Tokey K, Loup-Escande E, Christmann O, Richir S (2017) Effects on user experience in an edutainment virtual environment" href="/article/10.1007/s10055-019-00409-6#ref-CR85" id="ref-link-section-d27611e482">2017</a>) focused on a comparative analysis among various visualization systems for virtual environments.</p><p>The user experience with VR glasses was evaluated in Yu et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Yu M, Zhou R, Wang H, Zhao W (2019) An evaluation for VR glasses system user experience: the influence factors of interactive operation and motion sickness. Appl Ergon 74:206–213" href="/article/10.1007/s10055-019-00409-6#ref-CR100" id="ref-link-section-d27611e488">2019</a>), where different questionnaires were employed for hardware, interface, motion sickness and interactive operations. The presented analysis revealed that a good application design, aimed at improving the user experience, can reduce the perceived motion sickness. However, such study did not consider other aspects such as immersion or presence (which is the main topic of this paper).</p><p>The study described in Somrak et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Somrak A, Humar I, Hossain MS, Alhamid MF, Hossain MA, Guna J (2019) Estimating VR sickness and user experience using different HMD technologies: an evaluation study. Future Gener Comput Syst 94:302–316" href="/article/10.1007/s10055-019-00409-6#ref-CR78" id="ref-link-section-d27611e494">2019</a>) employed several questionnaires to assess the influence of VR sickness symptoms on the user experience for two panoramic (360) videos. The results proved head-mounted displays cause a higher level of sickness than TV. The same work revealed the user experience is influenced by display condition and degrades in the presence of VR sickness.</p><p>The study in Livatino et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Livatino S, De Paolis LT, D’Agostino M, Zocco A, Agrimi A, De Santis A, Bruno LV, Lapresa M (2015) Stereoscopic visualization and 3-D technologies in medical endoscopic teleoperation. IEEE Trans Ind Electron 62(1):525–535" href="/article/10.1007/s10055-019-00409-6#ref-CR53" id="ref-link-section-d27611e500">2015</a>) focused on stereoscopic 3D visualization in medical endoscopic teleoperation: some questionnaires were designed to assess depth impression, presence and comfort. The results showed the user experience is significantly improved when stereo vision is used to provide depth perception.</p><p>The study presented in Webster and Dues (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift® DK2 and Samsung Gear VR®. In: 2017 ASEE annual conference &amp; exposition, ASEE conferences, Columbus, Ohio" href="/article/10.1007/s10055-019-00409-6#ref-CR96" id="ref-link-section-d27611e506">2017</a>) compared Oculus Rift DK2 and Samsung Gear VR (Samsung <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Samsung (2019) Samsung Gear VR. Retrieved 15 November 2019, from &#xA;                https://www.samsung.com/us/mobile/virtual-reality/gear-vr/gear-vr-with-controller-sm-r324nzaaxar/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR70" id="ref-link-section-d27611e509">2019</a>) in terms of usability by means of the SUS scale.</p><p>Some works (Tcha-Tokey et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Tcha-Tokey K, Loup-Escande E, Christmann O, Richir S (2017) Effects on user experience in an edutainment virtual environment" href="/article/10.1007/s10055-019-00409-6#ref-CR85" id="ref-link-section-d27611e516">2017</a>) compared the effects induced by CAVE and head-mounted display in the edutainment field: it seems the CAVE can provide the best user experience in terms of presence, engagement, flow, skill, judgement and experience consequence, while there is no relevant difference in immersion, usability, emotion and technology adoption.</p><p>The experimental work presented in Hudson et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Hudson S, Matson-Barkat S, Pallamin N, Jegou G (2019) With or without you? Interaction and immersion in a virtual reality experience. J Bus Res 100:459–468" href="/article/10.1007/s10055-019-00409-6#ref-CR43" id="ref-link-section-d27611e522">2019</a>) studied the effects of social interaction in a virtual reality environment reproducing an underwater seascape. It showed the interaction with the virtual environment can increase immersion, with significant effects on satisfaction and loyalty. Despite the positive influence on the overall satisfaction, social interactions limit the impact of immersion on satisfaction, probably due to interferences on the perceived immersion.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">The role of immersion in 3D environments</h2><div class="c-article-section__content" id="Sec3-content"><p>Besides enabling applications such as training and phobia therapy, immersion can bring several other important benefits (Bowman and McMahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" href="/article/10.1007/s10055-019-00409-6#ref-CR11" id="ref-link-section-d27611e533">2007</a>). It improves spatial understanding, which can have a great impact on many applications such as scientific visualization, design review and virtual prototyping. Moreover, a higher level of immersion can decrease information clutter and enhance the environment understandability: in particular, it is possible to achieve this benefit by increasing the field of view, the field of regard (i.e., the visual field surrounding the user) and the display resolution. Other important benefits are a higher peripheral awareness and a larger useful information bandwidth.</p><p>However, for simpler tasks with less complex visualization requirements high levels of immersion do not provide substantial advantages.</p><p>For object manipulation at higher distances (McMahan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="McMahan RP, Gorton D, Gresock J, McConnell W, Bowman DA (2007) Separating the effects of level of immersion and 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 108–111" href="/article/10.1007/s10055-019-00409-6#ref-CR56" id="ref-link-section-d27611e542">2007</a>), neither stereoscopy nor an increase in the field of regard can provide significant benefits, while the interaction modality becomes a fundamental aspect: in such cases, spatial understanding has a secondary importance and the predominant task is to find the optimal position and orientation of objects (Bowman and McMahan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" href="/article/10.1007/s10055-019-00409-6#ref-CR11" id="ref-link-section-d27611e545">2007</a>).</p><p>The diffusion of immersive virtual reality has fostered the design of new 3D user interfaces, which allow the interaction with virtual objects in the physical or virtual space. The main obstacle hampering a wider diffusion of such systems is the training required before an effective use (Bowman et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Bowman DA, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interfaces: new directions and perspectives. IEEE Comput Graph Appl 28(6):20–36" href="/article/10.1007/s10055-019-00409-6#ref-CR12" id="ref-link-section-d27611e551">2008</a>). Most users adapt more quickly even to 2D input devices (such as mouse and keyboard), despite the additional overhead of mapping 2D movements to 3D motion on the screen. However, 3D interaction devices are faster and more accurate than 2D counterparts for 6-DOF manipulation tasks involving distant objects (McMahan et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="McMahan RP, Gorton D, Gresock J, McConnell W, Bowman DA (2007) Separating the effects of level of immersion and 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 108–111" href="/article/10.1007/s10055-019-00409-6#ref-CR56" id="ref-link-section-d27611e554">2007</a>).</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Natural interaction in 3D environments</h2><div class="c-article-section__content" id="Sec4-content"><p>Natural interaction refers to new communication modalities between users and computers based on gestures (Anwar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Anwar S, Sinha SK, Vivek S, Ashank V (2019) Hand gesture recognition: a survey. In: Lecture notes in electrical engineering, pp 365–371" href="/article/10.1007/s10055-019-00409-6#ref-CR1" id="ref-link-section-d27611e565">2019</a>), voice or even brain waves. It makes computing contextually aware and able to interpret actions that are similar to the ones performed to achieve the same results in the real world. Devices specifically designed to enable this kind of interaction are called Natural User Interfaces (NUIs): they employ various technologies, ranging from infrared cameras to depth sensors, to detect the user movements, which are interpreted as commands. Such innovations have contributed to the rise of the User Experience concept (Tullis and Albert <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Tullis T, Albert B (2013) Measuring the user experience: collecting, analyzing, and presenting usability metrics, 2nd edn. Morgan Kaufmann, Los Altos" href="/article/10.1007/s10055-019-00409-6#ref-CR88" id="ref-link-section-d27611e568">2013</a>), which goes beyond the traditional usability by including emotional and aesthetical factors (Bachmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus on the leap motion controller. Sensors 18:2194" href="/article/10.1007/s10055-019-00409-6#ref-CR4" id="ref-link-section-d27611e571">2018</a>).</p><p>Multimodal interactions, which involve multiple senses, play an important role in virtual reality environments. The sense of immersion derives from the integration of real-time vision, hearing and other perceptive stimuli. In such contexts, natural user interfaces provide an important contribution.</p><p>A typical interaction with 3D user interfaces consists of navigation, selection/manipulation tasks and system control (Ruddle <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ruddle R (2006) Review: 3D user interfaces: theory and practice. In: Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (eds) Presence: teleoperators and virtual environments. Addison-Wesley, Boston" href="/article/10.1007/s10055-019-00409-6#ref-CR69" id="ref-link-section-d27611e580">2006</a>).</p><p>Games for HTC Vive typically exploit the flying or the fishing navigation mode (Sun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Sun C, Hu W, Xu D (2019) Navigation modes, operation methods, observation scales and background options in UI design for high learning performance in VR-based architectural applications. J Comput Des Eng 6(2):189–196" href="/article/10.1007/s10055-019-00409-6#ref-CR84" id="ref-link-section-d27611e586">2019</a>). The former has been inherited from traditional first role jet fighting games. The latter allows the user to reach a destination point instantaneously by raising the controller.</p><p>Object selection, which usually influences manipulation, usually exploits virtual hand or virtual pointing metaphors (Argelaguet and Andujar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Argelaguet F, Andujar C (2013) A survey of 3D object selection techniques for virtual environments. Comput Graph (Pergamon) 37(3):121–136" href="/article/10.1007/s10055-019-00409-6#ref-CR2" id="ref-link-section-d27611e593">2013</a>). According to usability studies, virtual pointing offers a better selection effectiveness, as it allows object selection in a wider area and requires less physical movements (Argelaguet and Andujar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Argelaguet F, Andujar C (2013) A survey of 3D object selection techniques for virtual environments. Comput Graph (Pergamon) 37(3):121–136" href="/article/10.1007/s10055-019-00409-6#ref-CR2" id="ref-link-section-d27611e596">2013</a>; Ruddle <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Ruddle R (2006) Review: 3D user interfaces: theory and practice. In: Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (eds) Presence: teleoperators and virtual environments. Addison-Wesley, Boston" href="/article/10.1007/s10055-019-00409-6#ref-CR69" id="ref-link-section-d27611e599">2006</a>).</p><p>Object manipulation can be direct (Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display Otmar. In: Proceedings of the 2012 ACM annual conference on human factors in computing systems-CHI ’12, New York, NY, USA, pp 2421–2430" href="/article/10.1007/s10055-019-00409-6#ref-CR39" id="ref-link-section-d27611e605">2012</a>) or based on an augmented tangible interface (Ullmer and Ishii <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Ullmer B, Ishii H (2010) Emerging frameworks for tangible user interfaces. IBM Syst J 39:915–931" href="/article/10.1007/s10055-019-00409-6#ref-CR89" id="ref-link-section-d27611e608">2010</a>). In the former, a virtual object is associated directly with the position of the user hand detected by a sensor device; some complex techniques (Hilliges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display Otmar. In: Proceedings of the 2012 ACM annual conference on human factors in computing systems-CHI ’12, New York, NY, USA, pp 2421–2430" href="/article/10.1007/s10055-019-00409-6#ref-CR39" id="ref-link-section-d27611e611">2012</a>) reproduce also physics-inspired interactions and compute collision and friction forces between real and virtual objects. In the latter, a real element manipulated by the user is tracked by an infrared camera and replaced by a virtual object in the same position.</p><p>The modalities chosen for manipulating objects play an important role due to the different cognition effects deriving from their adoption (Caputo and Giachetti <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Caputo FM, Giachetti A (2015) Evaluation of basic object manipulation modes for low-cost immersive virtual reality. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, New York, pp 74–77" href="/article/10.1007/s10055-019-00409-6#ref-CR16" id="ref-link-section-d27611e617">2015</a>). The study presented in Caputo and Giachetti (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Caputo FM, Giachetti A (2015) Evaluation of basic object manipulation modes for low-cost immersive virtual reality. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, New York, pp 74–77" href="/article/10.1007/s10055-019-00409-6#ref-CR16" id="ref-link-section-d27611e620">2015</a>) compared the efficiency and usability of various manipulation modalities, involving one or two hands: besides direct manipulation, the authors analyzed interaction through abstract GUI elements such as finger pointers and handlebars. According to other experimental tests (Méndez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Méndez R, Flores J, Castelló E, Viqueira JR (2019) Natural interaction in virtual TV sets through the synergistic operation of low-cost sensors. Univ Access Inf Soc 18(1):17–29" href="/article/10.1007/s10055-019-00409-6#ref-CR57" id="ref-link-section-d27611e623">2019</a>), a tangible interface seems to provide the most effective and natural interaction modality, since the absence of a physical reference makes the interaction less intuitive.</p><p>For the system control factor, two important elements are the possibility to change the observation scale (Lucas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Lucas JF, Kim JS, Bowman DA (2005) Resizing beyond widgets: object resizing techniques for immersive virtual environments. In: Proceedings of ACM CHI 2005 conference on human factors in computing systems, New York, NY, USA, pp 1601–1604" href="/article/10.1007/s10055-019-00409-6#ref-CR54" id="ref-link-section-d27611e629">2005</a>) and the background options (Frankenstein et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Frankenstein J, Brüssow S, Ruzzoli F, Hölscher C (2012) The language of landmarks: the role of background knowledge in indoor wayfinding. Cognit Process 13(1):165–170" href="/article/10.1007/s10055-019-00409-6#ref-CR31" id="ref-link-section-d27611e632">2012</a>), which can help users in finding their orientation in unfamiliar environments (even though their effects on the space cognition have not been studied yet).</p><h3 class="c-article__sub-heading" id="Sec5">Devices for 3D user interaction</h3><p>Several devices have been designed for 3D user interaction (Bachmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus on the leap motion controller. Sensors 18:2194" href="/article/10.1007/s10055-019-00409-6#ref-CR4" id="ref-link-section-d27611e642">2018</a>; Rechy-Ramirez et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Rechy-Ramirez EJ, Marin-Hernandez A, Rios-Figueroa HV (2018) Impact of commercial sensors in human computer interaction: a review. J Ambient Intell Humaniz Comput 9:1479–1496" href="/article/10.1007/s10055-019-00409-6#ref-CR68" id="ref-link-section-d27611e645">2018</a>). A first category is represented by handheld controllers. Some of them, such as Wii Remote (Nintendo <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Nintendo (2019) Wii Remote. Retrieved 15 November 2019, from &#xA;                https://www.nintendo.com&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR63" id="ref-link-section-d27611e648">2019</a>) and Playstation Move (Sony <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Sony (2019) PlayStation Move. Retrieved15 November 2019,  from &#xA;                https://www.playstation.com/en-us/explore/accessories/vr-accessories/playstation-move/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR79" id="ref-link-section-d27611e651">2019</a>), were bundled with game consoles. Later, other devices, specifically designed for VR headsets, were introduced, such as the Oculus Touch (Oculus VR <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="Oculus VR (2019b) Oculus Touch. Retrieved 15 November 2019, from &#xA;                https://www.oculus.com/rift/accessories/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR65" id="ref-link-section-d27611e654">2019b</a>) and the HTC Vive controllers (HTC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="HTC (2019b) HTC Vive. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/us/product/vive-virtual-reality-system/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR41" id="ref-link-section-d27611e658">2019b</a>).</p><p>As an alternative to traditional handheld controllers, several devices have been designed to enable touchless interaction modalities.</p><p>The Microsoft Kinect controller is a camera-based device based on the detection of human body movements. The second version of Kinect, known as Kinect 2.0 or Kinect One, released in 2013, is based on a new architecture providing better performance (Sarbolandi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Sarbolandi H, Lefloch D, Kolb A (2015) Kinect range sensing: structured-light versus Time-of-Flight Kinect. Comput Vis Image Underst 139:1–20" href="/article/10.1007/s10055-019-00409-6#ref-CR74" id="ref-link-section-d27611e667">2015</a>). Such device can track up to six human skeletons and detect 25 joints per person (Lachat et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lachat E, Macher H, Landes T, Grussenmeyer P (2015) Assessment and calibration of a RGB-D camera (Kinect v2 Sensor) towards a potential use for close-range 3D modeling. Remote Sens 7(10):13070–13097" href="/article/10.1007/s10055-019-00409-6#ref-CR49" id="ref-link-section-d27611e670">2015</a>). Recently, Microsoft launched the Kinect’s successor, Azure Kinect DK (Microsoft <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="Microsoft (2019a) Azure Kinect DK. Retrieved 15 November 2019, from &#xA;                https://azure.microsoft.com/en-us/services/kinect-dk/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR58" id="ref-link-section-d27611e673">2019a</a>), which is connected to the azure cloud and exploits artificial intelligence for computer vision and speech models.</p><p>The Leap Motion controller (Bachmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus on the leap motion controller. Sensors 18:2194" href="/article/10.1007/s10055-019-00409-6#ref-CR4" id="ref-link-section-d27611e679">2018</a>; Leap Motion <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Motion Leap (2019) Leap Motion. Retrieved 15 November 2019, from &#xA;                https://www.leapmotion.com/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR60" id="ref-link-section-d27611e682">2019</a>) exploits an infrared camera to detect and track the user hands and fingers, but only within a restricted pyramidal interaction volume. On the other hand, it provides higher precision and accuracy than Kinect (Guzsvinecz et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Guzsvinecz T, Szucs V, Sik-Lanyi C (2019) Suitability of the Kinect sensor and Leap Motion controller—a literature review. Sensors 19:1072" href="/article/10.1007/s10055-019-00409-6#ref-CR37" id="ref-link-section-d27611e685">2019</a>), which may fail in properly detecting hands and fingers in the presence of abrupt movements (Cook et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Cook H, Nguyen QV, Simoff S, Trescak T, Preston D (2015) A close-range gesture interaction with Kinect. In: 2015 big data visual analytics, BDVA 2015, pp 1–8" href="/article/10.1007/s10055-019-00409-6#ref-CR18" id="ref-link-section-d27611e688">2015</a>).</p><p>In general, the restricted interaction area of camera-based systems, which is limited to a specific field of view near the device, hampers their adoption in large virtual reality environments. Furthermore, lenses or sensors issues, lighting problems, environment conditions and the presence of objects or other people in the scene foreground/background could significantly affect their accuracy and reliability (Garber <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Garber L (2013) Gestural technology: moving interfaces in a new direction. Computer 46(10):22–25" href="/article/10.1007/s10055-019-00409-6#ref-CR34" id="ref-link-section-d27611e695">2013</a>). For instance, Kinect’s performance gets better in the presence of an opaque floor and worse with a light reflecting floor (Vrellis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Vrellis I, Moutsioulis A, Mikropoulos TA (2014) Primary school students’ attitude towards gesture based interaction: a comparison between Microsoft Kinect and mouse. In: Proceedings—IEEE 14th international conference on advanced learning technologies, ICALT 2014, pp 678–682" href="/article/10.1007/s10055-019-00409-6#ref-CR95" id="ref-link-section-d27611e698">2014</a>). Moreover, at least 2–3 m of empty space between the Kinect and the user is required for an accurate gesture detection.</p><p>The Myo armband (Thalmic Labs <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Thalmic Labs (2019) Myo armband. Retrieved 15 November 2019, from &#xA;                https://support.getmyo.com/hc/en-us/articles/203398347-Getting-started-with-your-Myo-armband&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR87" id="ref-link-section-d27611e704">2019</a>; Csapo et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Csapo AB, Nagy H, Kristjansson A, Wersenyi G (2017) Evaluation of human-Myo gesture control capabilities in continuous search and select operations. In: 7th IEEE international conference on cognitive infocommunications, CogInfoCom 2016" href="/article/10.1007/s10055-019-00409-6#ref-CR19" id="ref-link-section-d27611e707">2017</a>) is a wearable device that exploits electromyographic signals to detect forearm and palm movements with no spatial limitations. Unfortunately, the performance of such device varies greatly according to the physical morphology of the user forearm. However, even though the Leap Motion controller provides a more accurate tracking and gesture detection, we chose the Myo armband, which allows the interaction in a wider space. While the Leap Motion controller should be connected via USB, the Myo armband exploits a Bluetooth interface, which makes the user movements even more free.</p><p>Another class of devices for natural user interaction is based on the interpretation of electroencephalographic signals. The Emotiv EPOC+ headset (Emotiv <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Emotiv (2019) Emotiv EPOC+. Retrieved 15 November 2019, from &#xA;                https://www.emotiv.com&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR28" id="ref-link-section-d27611e713">2019</a>) is a more portable and less intrusive alternative to traditional uncomfortable electroencephalography caps. It can detect head movements, facial expressions, thoughts and emotions, but its lower resolution (Duvinage et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Duvinage M, Castermans T, Petieau M, Hoellinger T, Cheron G, Dutoit T (2013) Performance of the Emotiv Epoc headset for P300-based applications. BioMed Eng Online 12(1):56" href="/article/10.1007/s10055-019-00409-6#ref-CR27" id="ref-link-section-d27611e716">2013</a>) makes it suitable only for applications and not for EEG signal analysis.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Myo armband</h4><p>The Myo armband (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig1">1</a>) exploits eight radially distributed sensors to collect electromyographic signals (i.e., electrical activity) from four muscle groups (Hauser and Wade <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Hauser N, Wade E (2018) Detecting reach to grasp activities using motion and muscle activation data. In: Proceedings of the annual international conference of the IEEE engineering in medicine and biology society, EMBS, pp 3264–3267" href="/article/10.1007/s10055-019-00409-6#ref-CR38" id="ref-link-section-d27611e729">2018</a>):</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>The brachioradialis, the extensor carpi radialis longus and the brevis;</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>The extensor digitorum and extensor carpi ulnaris;</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>The flexor carpi radialis, flexor digitorum superficialis and the pronator teres;</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>The flexor carpi ulnaris and the palmaris longus.</p>
                    
                  </li>
                </ol><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig1_HTML.jpg" alt="figure1" loading="lazy" width="472" height="448" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Myo armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>
The device has a sampling rate of 200 Hz and applies a filter to discard frequencies higher than 500 Hz or lower than 20 Hz (Sayin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Sayin FS, Ozen S, Baspinar U (2018) Hand gesture recognition by using sEMG signals for human machine interaction applications. In: Signal processing—algorithms, architectures, arrangements, and applications conference proceedings, SPA, pp 27–30" href="/article/10.1007/s10055-019-00409-6#ref-CR75" id="ref-link-section-d27611e796">2018</a>).</p><p>It can detect the arm motion and angle through a 9-axis Intertial Measurement Unit (IMU), which includes a triaxial accelerometer, a triaxial gyroscope and a triaxial magnetometer (Bhattacharyya et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bhattacharyya A, Mazumder O, Chakravarty K, Chatterjee D, Sinha A, Gavas R (2018) Development of an interactive gaming solution using MYO sensor for rehabilitation. In: 2018 international conference on advances in computing, communications and informatics, ICACCI 2018, pp 2127–2130" href="/article/10.1007/s10055-019-00409-6#ref-CR7" id="ref-link-section-d27611e802">2018</a>).</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Virtual reality headsets</h2><div class="c-article-section__content" id="Sec7-content"><p>HTC Vive (HTC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="&#xA;HTC (2019a) HTC Vive controller. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/eu/accessory/controller/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR40" id="ref-link-section-d27611e815">2019a</a>) and Oculus Rift (Oculus VR <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="Oculus VR (2019a) Oculus Rift. Retrieved 15 November 2019, from &#xA;                https://www.oculus.com/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR64" id="ref-link-section-d27611e818">2019a</a>) are two of the most popular head-mounted displays for virtual reality: they provide immersive experiences based on the user position estimated by their built-in head tracking systems. According to some comparative tests (Borrego et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Borrego A, Latorre J, Alcañiz M, Llorens R (2018) Comparison of Oculus Rift and HTC Vive: feasibility for virtual reality-based exploration, navigation, exergaming, and rehabilitation. Games Health J 7(3):151–156" href="/article/10.1007/s10055-019-00409-6#ref-CR9" id="ref-link-section-d27611e821">2018</a>), the working area of HTC Vive is twice as large as that of Oculus Rift, even though Oculus Rift provides a better accuracy. For our target scenario, we chose HTC Vive, whose technical specifications are described in detail in the following subsection.</p><p>However, perhaps the most advanced VR headset is the Valve Index (PC Gamer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Gamer PC (2019) Valve index review. Retrieved 15 November 2019, from &#xA;                https://www.pcgamer.com/valve-index-review/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR33" id="ref-link-section-d27611e827">2019</a>), launched in June, 2019, with a display delivering a <span class="mathjax-tex">\(1440\times 1600\)</span> per-eye resolution at a refresh rate of 120 Hz.</p><p>A recent innovation for 6-DOF devices can be found in the new Oculus Rift S (UploadVR <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019c" title="UploadVR (2019c) Oculus Rift S is official. Retrieved 15 November 2019, from &#xA;                https://uploadvr.com/oculus-rift-s-official/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR93" id="ref-link-section-d27611e857">2019c</a>), launched in May, 2019: it uses inside-out tracking, which exploits sensors on the helmet instead of external cameras placed around the room.</p><p>Wireless interfaces (HTC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019c" title="&#xA;HTC (2019c) VIVE wireless adapter. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/us/wireless-adapter/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR42" id="ref-link-section-d27611e863">2019c</a>) have been introduced to free some of these devices from the burden of cables connecting them to a PC.</p><p>Cardboards such as Samsung Gear VR, designed to enable mobile VR, are a low-cost alternative to a VR headset, but the latter can provide a higher level of immersion thanks to more advanced tracking capabilities (Webster and Dues <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift® DK2 and Samsung Gear VR®. In: 2017 ASEE annual conference &amp; exposition, ASEE conferences, Columbus, Ohio" href="/article/10.1007/s10055-019-00409-6#ref-CR96" id="ref-link-section-d27611e870">2017</a>).</p><p>A standalone VR headset, which does not need to be connected to any PC, is Oculus Go, a low-cost alternative to Oculus Rift. It provides only 3 degrees of freedom, against the 6 degrees of freedom provided by standard headsets. The biggest limitation affecting the user experience is that the user head appears “locked” in the 3D space: the user can look around, above and below him/her, but he/she cannot get a closer look at virtual objects (Polygon <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Polygon (2019) Oculus Go review. Retrieved 15 November 2019, from &#xA;                https://www.polygon.com/virtual-reality/2018/5/1/17284454/oculus-go-review&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR67" id="ref-link-section-d27611e876">2019</a>). After such experience, the user can feel sick when he/she gets back to real-world movements. The bundled handheld controller works only as a laser pointer without enabling any physical movement in the virtual space.</p><p>A more powerful standalone VR headset, called Oculus Quest (UploadVR <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="UploadVR (2019a) Oculus Quest review. Retrieved 15 November 2019, from &#xA;                https://uploadvr.com/oculus-quest-review/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR91" id="ref-link-section-d27611e882">2019a</a>), was launched in May, 2019. It provides 6 degrees of freedom as Oculus Rift and HTC Vive. Moreover, a software upgrade (announced for 2020) will enable the recognition of hands’ pose and location based on machine learning algorithms (UploadVR <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="UploadVR (2019b) OC6: Oculus Quest is getting camera-based finger tracking early next year. Retrieved 15 November 2019, from &#xA;                https://uploadvr.com/oculus-quest-finger-tracking/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR92" id="ref-link-section-d27611e885">2019b</a>).</p><h3 class="c-article__sub-heading" id="Sec8">HTC Vive</h3><p>HTC Vive (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig2">2</a>) is a virtual reality system consisting of a lightweight headset, providing a fully immersive experience, two infrared light emitting boxes, whose size measures <span class="mathjax-tex">\(9\times 9\times 6\)</span> cm, and two handheld controllers. The headset exploits two <span class="mathjax-tex">\(1080\times 1200\)</span> displays, updated at 90 Hz, which provides a field of view of about <span class="mathjax-tex">\(110^{\circ }\)</span> and a pixel density of about <span class="mathjax-tex">\(12\hbox { pixels}/^{\circ }\)</span> (Niehorster et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Niehorster DC, Li L, Lappe M (2017) The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research. i-Perception 8(3):2041669517708205" href="/article/10.1007/s10055-019-00409-6#ref-CR62" id="ref-link-section-d27611e1005">2017</a>).</p><p>The infrared light emitters, positioned one in front of the other, alternatively send out horizontal and vertical sweeps covering <span class="mathjax-tex">\(120^{\circ }\)</span> in each direction. In this way, the position and orientation of the headset and of the controllers can be detected thanks to some photodiodes on their surface hit by the laser beams (Niehorster et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Niehorster DC, Li L, Lappe M (2017) The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research. i-Perception 8(3):2041669517708205" href="/article/10.1007/s10055-019-00409-6#ref-CR62" id="ref-link-section-d27611e1033">2017</a>). Moreover, an integrated Inertial Measurement Unit (IMU) allows maintaining a smooth and continuous trajectory (Borges et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Borges M, Symington A, Coltin B, Smith T, Ventura R (2018) HTC Vive: analysis and accuracy improvement. In: IEEE international conference on intelligent robots and systems, pp 2610–2615" href="/article/10.1007/s10055-019-00409-6#ref-CR8" id="ref-link-section-d27611e1036">2018</a>).</p><p>A downward-facing camera on the front of the headset provides <span class="mathjax-tex">\(1280\times 720\)</span> frames at 60 Hz. There are no limits on the detection range, even though in Vive-based games physical movements are limited within a <span class="mathjax-tex">\(4\times 4\)</span> m area (Niehorster et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Niehorster DC, Li L, Lappe M (2017) The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research. i-Perception 8(3):2041669517708205" href="/article/10.1007/s10055-019-00409-6#ref-CR62" id="ref-link-section-d27611e1090">2017</a>).</p><p>The manufacturer declared a static accuracy of <span class="mathjax-tex">\(0.15^{\circ }\)</span> and 0.76 mm. According to experimental tests (Spitzley and Karduna <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Spitzley KA, Karduna AR (2019) Feasibility of using a fully immersive virtual reality system for kinematic data collection. J Biomech 87:172–176" href="/article/10.1007/s10055-019-00409-6#ref-CR80" id="ref-link-section-d27611e1125">2019</a>), the tracker has a mean rotational error of <span class="mathjax-tex">\(0.13\pm 0.08^{\circ }\)</span> and a mean translational error of <span class="mathjax-tex">\(1.7\pm 0.4\hbox { mm}\)</span>, while the controller has a mean rotational error of <span class="mathjax-tex">\(0.3^{\circ }\pm 0.07^{\circ }\)</span> and a mean translational error of <span class="mathjax-tex">\(2.0\pm 0.8\hbox { mm}\)</span>.</p><p>After the setup of the Vive components, a calibration procedure should be carried out with the bundled software package: in this way, the system is able to detect the position of the floor and define the “play area” of the Vive environment.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig2_HTML.jpg" alt="figure2" loading="lazy" width="685" height="599" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>HTC Vive VR system: two handheld controllers, a headset and two infrared emitters</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">The reference scenario: a surgical navigator</h2><div class="c-article-section__content" id="Sec9-content"><p>The application was developed in Unity3D (Unity Technologies <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Unity Technologies (2019) Unity3D. Retrieved 15 November 2019, from &#xA;                https://unity3d.com&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR86" id="ref-link-section-d27611e1292">2019</a>) with the SteamVR API (Steam <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Steam (2019) SteamVR. Retrieved 15 November 2019, from &#xA;                https://steamcommunity.com/steamvr&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR82" id="ref-link-section-d27611e1295">2019</a>) for the HTC Vive headset (HTC <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="&#xA;HTC (2019a) HTC Vive controller. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/eu/accessory/controller/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR40" id="ref-link-section-d27611e1298">2019a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="HTC (2019b) HTC Vive. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/us/product/vive-virtual-reality-system/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR41" id="ref-link-section-d27611e1301">b</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference c" title="&#xA;HTC (2019c) VIVE wireless adapter. Retrieved 15 November 2019, from &#xA;                https://www.vive.com/us/wireless-adapter/&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR42" id="ref-link-section-d27611e1304">c</a>) and the Thalmic Labs API for implementing the gestures recognized by the Myo armband. Virtual models were reconstructed from CT images.</p><p>At startup, the application displays a menu (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig3">3</a>) where the user can choose by means of a gaze controller one among these three virtual scenarios:</p><ul class="u-list-style-bullet">
              <li>
                <p>cardio explorer, which focuses on a case of study for mitral valve surgery;</p>
              </li>
              <li>
                <p>neuro explorer, which focuses on a case of cerebral aneurysm in a non-hemorrhagic state;</p>
              </li>
              <li>
                <p>body explorer, which represents most of the remaining organs of the human body.</p>
              </li>
            </ul><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig3_HTML.jpg" alt="figure3" loading="lazy" width="685" height="325" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Gaze controller menu for the selection of virtual scenarios</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>
After this selection, the chosen scenario is rendered on the HTC Vive headset: the user can interact by means of the Vive controller (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig4">4</a>) or by means of gestures detected by the Myo armband (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig5">5</a>). When the user gets closer to the organs, he/she has also the possibility to enter them and navigate inside their internal structure. The 3D models are intersected by a plane, called TAC slicer, representing the CT images used to reconstruct the visualized virtual organs.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig4_HTML.jpg" alt="figure4" loading="lazy" width="685" height="500" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>User interaction through the Vive controller</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig5_HTML.jpg" alt="figure5" loading="lazy" width="685" height="480" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>User interaction through the Myo armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>We associated the commands for model zoom and rotation with the central button of the Vive controller (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig6">6</a>): pressing the lower or the upper side of the button allows increasing (zoom in) or decreasing (zoom out) the model dimensions, while pressing the left or the right side enables a left or right rotation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="578" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Trackpad on the Vive controller</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The plane containing CT slices can be moved by keeping the grab pinch button (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig7">7</a>) pressed and moving the controller on the left and on the right.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="582" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Trigger on the Vive controller</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The same operations performed by means of the Vive controller are associated with some basic gestures detected by the Myo armband. An initial activation gesture, which consists in the inch and the middle finger getting in touch twice quickly (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig8">8</a>), is necessary to turn on the recognition of the following control gestures:</p><ul class="u-list-style-bullet">
              <li>
                <p>a fist rotation (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig9">9</a>) triggers zoom-in and zoom-out actions on the 3D models;</p>
              </li>
              <li>
                <p>a forearm rotation with an open hand and spread fingers (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig10">10</a>) triggers translations of the plain containing CT slices that are sections of the 3D models;</p>
              </li>
              <li>
                <p>the wave left and wave right gestures (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig11">11</a>), consisting in 90 degrees rotations of the hand palm on the left and on the right, trigger a rotation of the 3D models.</p>
              </li>
            </ul><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig8_HTML.jpg" alt="figure8" loading="lazy" width="685" height="428" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Myo activation gesture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig9_HTML.jpg" alt="figure9" loading="lazy" width="685" height="428" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Myo zoom gesture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig10_HTML.jpg" alt="figure10" loading="lazy" width="685" height="428" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Myo tac slicer gesture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig11_HTML.jpg" alt="figure11" loading="lazy" width="685" height="427" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Myo rotation gesture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec10">System setup</h3><p>We put the Vive infrared units at 2 m from the ground on the top of two perches 3 m far from each other. With this setup, we achieved a wide enough interaction area for the considered VR scenario. We connected the Vive headset to a VR ready desktop PC and performed the calibration routine provided by the software bundled with the device. The desktop PC is equipped with a Core i7 4770 3.4 GHz CPU, 8 GB of DDR3 RAM and a NVIDIA GTX 1060 6 GB graphic card.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Assessing the user experience</h2><div class="c-article-section__content" id="Sec11-content"><p>Usability evaluation methods for user interfaces can be classified depending on the involvement of representative users, the evaluation context and the qualitative or quantitative nature of the test results (Bachmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus on the leap motion controller. Sensors 18:2194" href="/article/10.1007/s10055-019-00409-6#ref-CR4" id="ref-link-section-d27611e1558">2018</a>).</p><p>Compared to traditional usability, user experience includes additional aspects such as usefulness, emotional factors and design elegance (Vosinakis and Koutsabasis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Vosinakis S, Koutsabasis P (2018) Evaluation of visual feedback techniques for virtual grasping with bare hands using Leap Motion and Oculus Rift. Virtual Real 22(1):47–62" href="/article/10.1007/s10055-019-00409-6#ref-CR94" id="ref-link-section-d27611e1564">2018</a>).</p><p>User task performance concerns specific tasks such as navigation, object placement and object selection. An important factor influencing the user performance is the mental workload (Cain <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Cain B (2004) A review of the mental workload literature. NATO RTO-TR-HFM-121-Part-II" href="/article/10.1007/s10055-019-00409-6#ref-CR15" id="ref-link-section-d27611e1570">2004</a>), also known as cognitive workload, which represents the mental effort required to perform the tasks. It can be assessed by means of quantitative performance tests, physiological measures or subjective feedback collected through questionnaires (Moustafa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Moustafa K, Luz S, Longo L (2017) Assessment of mental workload: a comparison of machine learning methods and subjective assessment techniques. In: Communications in computer and information science, pp 30–50" href="/article/10.1007/s10055-019-00409-6#ref-CR61" id="ref-link-section-d27611e1573">2017</a>).</p><p>Several questionnaires can be found in the literature for the assessment of the perceived user experience (Assila et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Assila A, Marçal De Oliveira K, Ezzedine H (2016) Standardized usability questionnaires: features and quality focus. J Comput Sci Inf Technol (eJCSIT) 6:15–31" href="/article/10.1007/s10055-019-00409-6#ref-CR3" id="ref-link-section-d27611e1579">2016</a>; Lewis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Lewis JR (2018) Measuring perceived usability: the CSUQ, SUS, and UMUX. Int J Hum Comput Interact 34(12):1148–1156" href="/article/10.1007/s10055-019-00409-6#ref-CR50" id="ref-link-section-d27611e1582">2018</a>).</p><p>An important theoretical model for experiences in virtual environments is the Multi-criteria Assessment of Usability for Virtual Environments (MAUVE) (Stanney et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Stanney KM, Mollaghasemi M, Reeves L, Breaux R, Graeber DA (2003) Usability engineering of virtual environments (VEs): Identifying multiple criteria that drive effective VE system design. Int J Hum Comput Stud 58(4):447–481" href="/article/10.1007/s10055-019-00409-6#ref-CR81" id="ref-link-section-d27611e1589">2003</a>), where wayfinding, navigation, selection and manipulation characterize usability, while ease of interaction, user-initiated control, pictorial realism, length of exposure, social factors and system factors characterize presence.</p><p>Besides questionnaires, alternative approaches to assess the user experience are based on the analysis of electroencephalographic (EEG) signals gathered through some proper headsets worn by users (Invitto et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2015) Interactive entertainment, virtual motion training and brain ergonomy. In: 7th international conference on intelligent technologies for interactive entertainment (INTETAIN 2015), Torino, Italy, June 10–12, 2015, pp 88–94" href="/article/10.1007/s10055-019-00409-6#ref-CR45" id="ref-link-section-d27611e1595">2015</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2016) Haptic, virtual interaction and motor imagery: entertainment tools and psychophysiological testing. Sensors 16(3):394" href="/article/10.1007/s10055-019-00409-6#ref-CR46" id="ref-link-section-d27611e1598">2016</a>; Sun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Sun R, Wu YJ, Cai Q (2018) The effect of a virtual reality learning environment on learners’ spatial ability. Virtual Real. &#xA;                https://doi.org/10.1007/s10055-018-0355-2&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR83" id="ref-link-section-d27611e1601">2018</a>).</p><p>For our study, we chose two general-purpose questionnaires, UMUX and SUS, designed for usability evaluation and a third questionnaire, PQ, specifically tailored for assessing the user experience in virtual environments. Therefore, we prepared a single questionnaire by putting together the items of these three questionnaires described in detail in the following subsections.</p><p>Even though some experimental studies (Lewis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Lewis JR, Utesch BS, Maher DE (2015) Investigating the correspondence between UMUX-LITE and SUS scores. In: Lecture notes in computer science, pp 204–211" href="/article/10.1007/s10055-019-00409-6#ref-CR52" id="ref-link-section-d27611e1610">2015</a>) highlighted some correlations between UMUX and SUS, we decided to consider both the metrics to carry out a comparative analysis between two different interaction modalities.</p><p>We recruited 78 subjects to test our virtual reality application by wearing the HTC Vive headset: half of them used the Vive controller, while the other ones used the Myo armband. Before using the Myo armband, users were asked to carry out a guided calibration procedure through the software provided by the vendor: this is necessary to improve gesture detection according to the specific morphology of a user’s forearm. After the virtual reality experience, each user was requested to fill the questionnaire. For each user, we computed three scores, normalized on a scale ranging from 0 to 100, deriving from the UMUX, SUS and PQ questionnaires described in the following subsections.</p><h3 class="c-article__sub-heading" id="Sec12">UMUX</h3><p>The Usability Metric for User Experience (UMUX) (Finstad <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Finstad K (2010) The usability metric for user experience. Interact Comput 22(5):323–327" href="/article/10.1007/s10055-019-00409-6#ref-CR30" id="ref-link-section-d27611e1623">2010</a>) is based on a four-item questionnaire inspired by the ISO 9241-11 definition of usability.</p><p>Users are requested to express their opinion on each of the following items according to a 7-point Likert scale from “Strongly Disagree” to “Strongly Agree.”</p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">1.</span>
                  
                    <p>This system’s capabilities meet my requirements</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">2.</span>
                  
                    <p>Using this system is a frustrating experience</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">3.</span>
                  
                    <p>This system is easy to use</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">4.</span>
                  
                    <p>I have to spend too much time correcting things with this system</p>
                  
                </li>
              </ol><p>The scores of odd items are computed by subtracting 1 from the scale value chosen by the user. The scores of even items are computed by subtracting the scale value chosen by the user from 7. The sum of the item scores is divided by 24 and multiplied by 100 to obtain an overall score for each user expressed on a percentage scale.</p><h3 class="c-article__sub-heading" id="Sec13">SUS</h3><p>The System Usability Scale (SUS) (Brooke <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="&#xA;Brooke J (1996) SUS—a quick and dirty usability scale. In: Jordan PW, Thomas B, McClelland IL, Weerdmeester B (eds) Usability evaluation in industry. CRC Press, Boca Raton" href="/article/10.1007/s10055-019-00409-6#ref-CR13" id="ref-link-section-d27611e1685">1996</a>; Borsci et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Borsci S, Federici S, Lauriola M (2009) On the dimensionality of the system usability scale: a test of alternative measurement models. Cognit Process 10(3):193–197" href="/article/10.1007/s10055-019-00409-6#ref-CR10" id="ref-link-section-d27611e1688">2009</a>) consists of a 10 item questionnaire with five response options ranging from “Strongly Disagree” to “Strongly Agree.”</p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">1.</span>
                  
                    <p>I think that I would like to use this system frequently.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">2.</span>
                  
                    <p>I found the system unnecessarily complex.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">3.</span>
                  
                    <p>I thought the system was easy to use.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">4.</span>
                  
                    <p>I think that I would need the support of a technical person to be able to use this system.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">5.</span>
                  
                    <p>I found the various functions in this system were well integrated.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">6.</span>
                  
                    <p>I thought there was too much inconsistency in this system.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">7.</span>
                  
                    <p>I would imagine that most people would learn to use this system very quickly.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">8.</span>
                  
                    <p>I found the system very cumbersome to use.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">9.</span>
                  
                    <p>I felt very confident using the system.</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">10.</span>
                  
                    <p>I needed to learn a lot of things before I could get going with this system.</p>
                  
                </li>
              </ol><p>The questionnaire can be decomposed into two subscales (Lewis and Sauro <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer science, pp 94–103" href="/article/10.1007/s10055-019-00409-6#ref-CR51" id="ref-link-section-d27611e1805">2009</a>): <i>learnability</i>, which refers to the ability of quickly and independently learning how to use a system, and <i>usability</i> in a more strict sense.</p><p>The scores of odd items are computed by subtracting 1 from the scale value chosen by the user. The scores of even items are computed by subtracting the scale value chosen by the user from 5 (since for such items higher values denote worse opinions expressed by the users). The sum of the item scores is multiplied by 2.5 to obtain an overall score for each user expressed on a percentage scale.</p><h3 class="c-article__sub-heading" id="Sec14">PQ</h3><p>The Presence Questionnaire (PQ) (Witmer and Singer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Witmer BG, Singer MJ (1998) Measuring presence in virtual environments: a presence questionnaire. Presence Teleoper Virtual Environ 7(3):225–240" href="/article/10.1007/s10055-019-00409-6#ref-CR97" id="ref-link-section-d27611e1826">1998</a>) was designed with the aim of assessing the sense of <i>presence</i> in virtual environments. It consists of 24 items with seven response options. We discarded the items proposed for the assessment of audio and haptic interaction, since the current version of our virtual simulator does not include such elements, and considered only the first 19 items reported below.</p><ol class="u-list-style-none">
                <li>
                  <span class="u-custom-list-number">1.</span>
                  
                    <p>How much were you able to control events?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">2.</span>
                  
                    <p>How responsive was the environment to actions that you initiated (or performed)?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">3.</span>
                  
                    <p>How natural did your interactions with the environment seem?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">4.</span>
                  
                    <p>How much did the visual aspects of the environment involve you?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">5.</span>
                  
                    <p>How natural was the mechanism which controlled movement through the environment?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">6.</span>
                  
                    <p>How compelling was your sense of objects moving through space?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">7.</span>
                  
                    <p>How much did your experiences in the virtual environment seem consistent with your real-world experiences?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">8.</span>
                  
                    <p>Were you able to anticipate what would happen next in response to the actions that you performed?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">9.</span>
                  
                    <p>How completely were you able to actively survey or search the environment using vision?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">10.</span>
                  
                    <p>How compelling was your sense of moving around inside the virtual environment?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">11.</span>
                  
                    <p>How closely were you able to examine objects?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">12.</span>
                  
                    <p>How well could you examine objects from multiple viewpoints?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">13.</span>
                  
                    <p>How involved were you in the virtual environment experience?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">14.</span>
                  
                    <p>How much delay did you experience between your actions and expected outcomes?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">15.</span>
                  
                    <p>How quickly did you adjust to the virtual environment experience?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">16.</span>
                  
                    <p>How proficient in moving and interacting with the virtual environment did you feel at the end of the experience?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">17.</span>
                  
                    <p>How much did the visual display quality interfere or distract you from performing assigned tasks or required activities?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">18.</span>
                  
                    <p>How much did the control devices interfere with the performance of assigned tasks or with other activities?</p>
                  
                </li>
                <li>
                  <span class="u-custom-list-number">19.</span>
                  
                    <p>How well could you concentrate on the assigned tasks or required activities rather than on the mechanisms used to perform those tasks or activities?</p>
                  
                </li>
              </ol><p>The questionnaire can be decomposed into four factors (Witmer et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper Virtual Environ 14:298–312" href="/article/10.1007/s10055-019-00409-6#ref-CR98" id="ref-link-section-d27611e2047">2005</a>) representing <i>involvement</i> (items 1, 2, 3, 4, 5, 6, 7, 10 and 13), <i>interface quality</i> (items 17, 18 and 19), <i>adaptation/immersion</i> (items 8, 9, 14, 15, 16 and 19) and <i>visual fidelity</i> (items 11 and 12). In particular, the <i>involvement</i> factor is made up of <i>control</i> items (1 and 2), <i>natural</i> interaction items (3, 5 and 7) and <i>involvement</i> items in a more strict sense (4, 6, 10 and 13). However, based on the first study described in Witmer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper Virtual Environ 14:298–312" href="/article/10.1007/s10055-019-00409-6#ref-CR98" id="ref-link-section-d27611e2076">2005</a>), the analysis presented in the following section considers these subcategories as belonging to one global <i>involvement</i> factor.</p><p>The scores of items 14, 17 and 18 are computed by subtracting the scale value chosen by the user from 7 (since for such items higher values denote worse opinions expressed by the users). The scores of all the remaining items are computed by subtracting 1 from the scale value chosen by the user. The sum of the item scores is divided by 24 and multiplied by 100 to obtain an overall score for each user expressed on a percentage scale.</p></div></div></section><section aria-labelledby="Sec15"><div class="c-article-section" id="Sec15-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec15">Test results</h2><div class="c-article-section__content" id="Sec15-content"><h3 class="c-article__sub-heading" id="Sec16">Multivariate analysis of variance</h3><p>We started our analysis by studying how the three considered metrics (UMUX, SUS, PQ) are jointly influenced by the control device (Vive controller or Myo) chosen to interact with the 3D models.</p><p>We evaluated the presence of a significant effect of the used device on the combination of UMUX, SUS and PQ variables by means of a multivariate analysis of variance (MANOVA) (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Tests of MANOVA assumptions and <i>p</i> value result</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As a preliminary step, we checked the validity of MANOVA assumptions. We performed a Shapiro–Wilk test (Shapiro and Wilk <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1965" title="Shapiro SS, Wilk MB (1965) An analysis of variance test for normality (complete samples). Biometrika 52(3/4):591–611" href="/article/10.1007/s10055-019-00409-6#ref-CR76" id="ref-link-section-d27611e2223">1965</a>) to assess multivariate normality of the Vive and Myo datasets: since the obtained <i>p</i> values are greater than the 0.05 threshold, they reveal we can accept such hypothesis.</p><p>Then, we performed a Bartlett test (Bartlett <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1937" title="Bartlett MS (1937) Properties of sufficiency and statistical tests. Proc R Soc Lond Ser A Math Phys Sci 160(901):268–282" href="/article/10.1007/s10055-019-00409-6#ref-CR6" id="ref-link-section-d27611e2232">1937</a>) to assess the homogeneity of variances of the entire dataset: since the obtained <i>p</i> value is greater than the 0.05 threshold, it reveals we can accept such hypothesis.</p><p>Finally, we performed the MANOVA test with four statistics: Wilk, Hotelling–Lawley, Pillai and Roy. All the four statistics provided the same MANOVA <i>p</i> value, reported in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab1">1</a>: from this very small value, we can infer the three metrics have a significant difference between the two modalities.</p><h3 class="c-article__sub-heading" id="Sec17">Analysis of variance on UMUX, SUS and PQ</h3><p>In a second phase, we analyzed the influence of the control device on each single metric by means of a one-way ANOVA. The Shapiro–Wilk test reveals we can accept the normality of the overall residuals of the model. However, since the Bartlett test does not allow to accept the hypothesis about the homogeneity of variances for the UMUX variable, we performed also the Kruskal–Wallis rank sum test (Kruskal and Wallis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1952" title="Kruskal WH, Wallis WA (1952) Use of ranks in one-criterion variance analysis. J Am Stat Assoc 47(260):583–621" href="/article/10.1007/s10055-019-00409-6#ref-CR48" id="ref-link-section-d27611e2256">1952</a>): it is a nonparametric alternative to one-way ANOVA that can be used also when ANOVA assumptions are not met. The results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab2">2</a> suggest there is a significant difference on the three metrics, especially on UMUX, for which the Kruskal–Wallis <i>p</i> value is quite small.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 ANOVA and Kruskal–Wallis test results for each metric</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Then, we went deeper in our analysis by studying the influence of the control device on each component of the considered metrics.</p><p>Since the UMUX metric consists of only four questions, we considered each single question as a component to be analyzed with the Kruskal–Wallis test. The <i>p</i> value results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab3">3</a> suggest there is no significant difference on the first two UMUX items. On the contrary, there is a significant difference on the last two UMUX items. This confirms the impressions reported by users during the tests, telling the Myo armband seems less reactive than the Vive controller: however, according to the results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab3">3</a>, the use of the Myo device affected neither the satisfaction of users about the whole system (item <span class="mathjax-tex">\(I_{1}\)</span>) nor their experience (item <span class="mathjax-tex">\(I_{2}\)</span>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Kruskal–Wallis test results for each of the four items of the UMUX metric</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>On the contrary, as disclosed in Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec13">7.2</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec14">7.3</a>, for SUS and PQ metrics we considered the factor structures proposed in Lewis and Sauro (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer science, pp 94–103" href="/article/10.1007/s10055-019-00409-6#ref-CR51" id="ref-link-section-d27611e2948">2009</a>) and Witmer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper Virtual Environ 14:298–312" href="/article/10.1007/s10055-019-00409-6#ref-CR98" id="ref-link-section-d27611e2951">2005</a>), respectively. The former groups the SUS items into two subscales: <i>learnability</i>, which refers to the ability of quickly and independently learning how to use a system, and <i>usability</i> in a more strict sense. The latter groups the PQ items into four factors: <i>involvement</i>, <i>interface quality</i>, <i>adaptation/immersion</i> and <i>visual fidelity</i>. Based on such grouping criteria, we computed partial scores, which vary on a scale from 0 to 100, by using the proper normalization factors.</p><p>Despite the calibration phase the Myo users had to go through, the results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab4">4</a> suggest a significant difference mainly for the <i>usability</i> component of SUS rather than for the <i>learnability</i> one.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Kruskal–Wallis test results for each of the two SUS components</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab5">5</a> suggest a significant difference in PQ <i>involvement</i> and <i>adaptation/immersion</i> components: in particular, the latter seems the most influenced by the usability and the responsiveness of the control device, even tough control items are included in the <i>involvement</i> component.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Kruskal–Wallis test results for each of the four PQ components</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/5"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec18">Principal component analysis</h3><p>The aim of principal component analysis (PCA) is trying to group together more variables according to their variability. In this section, we compare the principal component analysis computed in the Vive controller and Myo armband scenarios to detect any significant difference or any variability pattern that seems specific for one of the two interaction modalities.</p><p>The first set of variables we analyzed with PCA in both the interaction scenarios is made up of the scores of the three metrics UMUX, SUS and PQ. We obtained three principal components PC1, PC2 and PC3, characterized by the data summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab6">6</a>.
The barplots in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig12">12</a> depict the variances of the detected components in the two scenarios. The biplots depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig13">13</a> provide a combined representation of two pieces of information (Gabriel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1971" title="Gabriel KR (1971) The biplot graphic display of matrices with application to principal component analysis. Biometrika 58(3):453–467" href="/article/10.1007/s10055-019-00409-6#ref-CR32" id="ref-link-section-d27611e3221">1971</a>). The left and bottom axes represent PC1 and PC2 components, respectively: in this reference system, dots represent the PCA scores of the samples. Furthermore, the top and right axes of the same chart identify another reference system, where loadings, which stand for the influence strength of each considered variable, are depicted as vectors.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Principal component analysis of UMUX, SUS and PQ metrics in the Vive controller and Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/6"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig12_HTML.png?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig12_HTML.png" alt="figure12" loading="lazy" width="685" height="292" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Barplot of the variances of the components for UMUX, SUS and PQ metrics in the Vive controller and the Myo armband scenarios</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig13_HTML.png?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig13_HTML.png" alt="figure13" loading="lazy" width="685" height="358" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Biplot for UMUX, SUS and PQ metrics in the Vive controller and the Myo armband scenarios</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>We can notice the first two components account for the 88–89% of the total variability: in particular, the barplot of the variances shows a clear dominance of the first component even on the second one. Therefore, we preserved only these two components and applied a varimax rotation (Kaiser <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1958" title="Kaiser HF (1958) The varimax criterion for analytic rotation in factor analysis. Psychometrika 23(3):187–200" href="/article/10.1007/s10055-019-00409-6#ref-CR47" id="ref-link-section-d27611e3507">1958</a>), which maximizes the sum of the variances of the squared loadings. We obtained the loadings in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab7">7</a>: for the Vive controller, the first component can be expressed as a combination of SUS and PQ, while the second component is related only to UMUX; for the Myo armband, the first component can be expressed as a combination of UMUX and PQ, while the second one mainly depends on SUS (since UMUX and PQ loadings are closer to 0 in this case). Moreover, in the Vive controller scenario, SUS and PQ loadings are quite similar: this is in agreement with the biplot in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig13">13</a>, where the SUS and the PQ vectors are very close together. For each principal component, we highlighted in bold the loadings with higher magnitude (i.e., &gt; 0.4): these groups of loadings refer to scores that vary together. For the Vive controller, we can point out that SUS and PQ scores grow together with a similar rate, since the loadings are quite similar. On the other hand, for the Myo armband an increase in the PQ score is coupled with an increase in the UMUX score, even though with a lower rate.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 Loadings of the first two principal components of UMUX, SUS and PQ metrics in the Vive controller and the Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/7"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Then, we performed a principal component analysis also on the factors composing each considered metric and obtain the data in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab8">8</a>. Since the UMUX metric consists of only four questions,
we considered each single question as a factor. The barplot of the variances and the biplot of the first two  principal components are depicted in Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig14">14</a> and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig15">15</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-8"><figure><figcaption class="c-article-table__figcaption"><b id="Tab8" data-test="table-caption">Table 8 Principal component analysis of UMUX items in the Vive controller and the Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/8"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig14_HTML.png?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig14_HTML.png" alt="figure14" loading="lazy" width="685" height="306" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Barplot of the variances of the components for UMUX items in the Vive controller and the Myo armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig15_HTML.png?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig15_HTML.png" alt="figure15" loading="lazy" width="685" height="357" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Biplot of the first two principal components for UMUX in the Vive controller and the Myo armband scenarios. <span class="mathjax-tex">\(I_{1}\)</span>: This system’s capabilities meet my requirements. <span class="mathjax-tex">\(I_{2}\)</span>: Using this system is a frustrating experience. <span class="mathjax-tex">\(I_{3}\)</span>: This system is easy to use. <span class="mathjax-tex">\(I_{4}\)</span>: I have to spend too much time correcting things with this system</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>Since the first three components account for 89% and 85% of the total variability in the Vive controller and Myo armband scenario, respectively, we discarded the last component and obtained the loadings in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab9">9</a> by applying a varimax rotation.</p><p>The different way the loadings of <span class="mathjax-tex">\(I_{1}\)</span>, <span class="mathjax-tex">\(I_{2}\)</span>, <span class="mathjax-tex">\(I_{3}\)</span> and <span class="mathjax-tex">\(I_{4}\)</span> are grouped into principal components suggests the way the four items influence each other varies between the two considered interaction modalities. For the Vive controller use case, we can point out that scores related to <span class="mathjax-tex">\(I_{1}\)</span> and <span class="mathjax-tex">\(I_{3}\)</span> items grow together with the same rate (since their loadings are almost the same): when users perceive the system meets their requirements more closely, they consider it easier to use too. In the same use case, <span class="mathjax-tex">\(I_{2}\)</span> and <span class="mathjax-tex">\(I_{4}\)</span> seem independent and have no influence on <span class="mathjax-tex">\(I_{1}\)</span> and <span class="mathjax-tex">\(I_{3}\)</span>. On the other hand, in the Myo armband use case, we can notice that when <span class="mathjax-tex">\(I_{3}\)</span> score increases, also <span class="mathjax-tex">\(I_{4}\)</span> score increases, even though at a lower rate: when users perceive the system is easier to use, they feel there are fewer things to be fixed (recalling that high scores for <span class="mathjax-tex">\(I_{4}\)</span> mean few things to be corrected). Moreover, in the same use case, the scores related to <span class="mathjax-tex">\(I_{2}\)</span> and <span class="mathjax-tex">\(I_{4}\)</span> decrease together: when users feel more frustrated during the virtual experience, they spend more time “to correct things with the system.” In this case, <span class="mathjax-tex">\(I_{1}\)</span> item seems the only one that varies independently from the other ones. A possible explanation for these differences is the poor frequency of usability issues in the Vive controller use case: perhaps, due to the higher responsiveness of the Vive controller, usability issues are so infrequent in such scenario to let the users concentrate on other aspects dealing with the general requirements met by the system; on the contrary, in the Myo armband scenario, the device usability seems the dominant factor catching the user attention, while the adherence to the user expectations (<span class="mathjax-tex">\(I_{1}\)</span>) remains confined to the least significant principal component PC3.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-9"><figure><figcaption class="c-article-table__figcaption"><b id="Tab9" data-test="table-caption">Table 9 Loadings of the first three principal components for UMUX in the Vive controller and the Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/9"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-10"><figure><figcaption class="c-article-table__figcaption"><b id="Tab10" data-test="table-caption">Table 10 Principal component analysis of SUS components in the Vive controller and the Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/10"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For SUS, we consider again the 2-factor structure described in Lewis and Sauro (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer science, pp 94–103" href="/article/10.1007/s10055-019-00409-6#ref-CR51" id="ref-link-section-d27611e5085">2009</a>) to perform the principal component analysis. The cumulative proportions in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab10">10</a> do not suggest a clear predominance of one of the two components, as confirmed also by the barplot of the variances in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig16">16</a>. The biplot of the first two principal components is depicted in Fig.  <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig17">17</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig16_HTML.png?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig16_HTML.png" alt="figure16" loading="lazy" width="685" height="309" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Barplot of the variances of the components for SUS factors in the Vive controller and the Myo armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig17_HTML.png?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig17_HTML.png" alt="figure17" loading="lazy" width="685" height="355" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Biplot of the first two principal components for SUS in the Vive controller and the Myo armband scenarios. <span class="mathjax-tex">\(S_{1}\)</span>: SUS learnability. <span class="mathjax-tex">\(S_{2}\)</span>: SUS usability</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For PQ, we considered again the 4-factors structure described in Witmer et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper Virtual Environ 14:298–312" href="/article/10.1007/s10055-019-00409-6#ref-CR98" id="ref-link-section-d27611e5179">2005</a>) to perform the principal component analysis. We obtained four principal components PC1, PC2, PC3 and PC4, described by the data summarized in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab11">11</a>, the barplot of the variances in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig18">18</a> and the biplot in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00409-6#Fig19">19</a>.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-11"><figure><figcaption class="c-article-table__figcaption"><b id="Tab11" data-test="table-caption">Table 11 Principal component analysis of PQ in the Vive controller and Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/11"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Since the first three components account for 90% and 92% of the total variability in the Vive controller and Myo armband scenario, respectively, we discarded the last component and obtained the loadings in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab12">12</a> by applying a varimax rotation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig18_HTML.png?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig18_HTML.png" alt="figure18" loading="lazy" width="685" height="306" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Barplot of the variances of the components for PQ factors in the Vive controller and the Myo armband</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig19_HTML.png?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00409-6/MediaObjects/10055_2019_409_Fig19_HTML.png" alt="figure19" loading="lazy" width="685" height="353" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Biplot of the first two principal components for PQ in the Vive controller and the Myo armband scenarios. <span class="mathjax-tex">\(P_{1}\)</span>: involvement. <span class="mathjax-tex">\(P_{2}\)</span>: interface quality. <span class="mathjax-tex">\(P_{3}\)</span>: adaptation/immersion. <span class="mathjax-tex">\(P_{4}\)</span>: visual fidelity</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00409-6/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-12"><figure><figcaption class="c-article-table__figcaption"><b id="Tab12" data-test="table-caption">Table 12 Loadings of the first three principal components of PQ in the Vive controller and Myo armband scenarios</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/tables/12"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab12">12</a> suggest significant differences between the two scenarios. In the Vive controller use case, the loadings with the most significant magnitude for principal component PC1 suggest that a higher <i>involvement</i> tends to be coupled with a higher <i>visual fidelity</i>. In the same use case, principal component PC2 represents any problem related to the interface quality. <i>Adaptation/immersion</i> and <i>visual fidelity</i>, which have a lesser influence, are correlated: when the former decreases, the latter decreases too.</p><p>In the Myo armband scenario, <i>interface quality</i> and <i>adaptation/immersion</i> become predominant over <i>involvement</i> in terms of influence on the total variability, since they switch from component PC2 to component PC1, which accounts for 63% of the total variability. In such scenario, <i>involvement</i> is confined to the last component PC3, and therefore, it becomes the lowest influence factor, while <i>visual fidelity</i> switches from PC3 to PC2. However, we should point out in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab11">11</a> the difference in terms of proportion of variance is almost negligible between PC2 and PC3 for both the interaction modalities: therefore, we could assume factors belonging to PC2 and PC3 components have almost the same influence.</p><p>In the Vive controller use case, <i>interface quality</i> has no influence on other factors, whereas, in the Myo armband use case, it has a significant influence on <i>adaptation/immersion;</i> moreover, <i>involvement</i> is the most influential factor during the use of the Vive controller, whereas <i>interface quality</i> and <i>adaptation/immersion</i> are predominant during the use of the Myo armband.</p></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Discussion</h2><div class="c-article-section__content" id="Sec19-content"><p>The results revealed that the device used for the interaction has a significant influence on the user experience. From the analysis of the UMUX components, we can infer that the chosen device influences the perceived ease of use, reactivity and stability of the system, but it does not affect the user satisfaction.</p><p>Moreover, we can notice important consequences on the sense of presence in the virtual environment, for which immersion and involvement are two necessary requirements. As explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00409-6#Sec3">3</a>, immersion relies on perception of self-inclusion and natural forms of interaction, while involvement is related to the user attention on the provided stimuli. A previous work (Shu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Shu Y, Huang YZ, Chang SH, Chen MY (2018) Do virtual reality head-mounted displays make a difference? A comparison of presence and self-efficacy between head-mounted displays and desktop computer-facilitated virtual environments. Virtual Real. &#xA;                https://doi.org/10.1007/s10055-018-0376-x&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR77" id="ref-link-section-d27611e5990">2018</a>) proved that, compared to desktop-based environments, head-mounted displays can enhance presence and immersion by providing all-around visual experiences and visual stimulations, even though users exhibit the same self-efficacy with the two devices. Moreover, spatial presence and mental immersion benefit from the physical movements of the users involved in the virtual experience. In this context, also the handheld controller bundled with a head-mounted display can enhance spatial presence and mental immersion by fostering more free movements, which partially recalls operations performed by hand in daily life. In the light of these findings, the research community could wonder whether a touchless interface, based on intuitive and natural free hand gestures, could further increase this sensation. Nevertheless, as highlighted by another work (Caggianese et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Caggianese G, Gallo L, Neroni P (2019) The vive controllers vs. leap motion for interactions in virtual environments: a comparative evaluation. In: smart innovation, systems and technologies, pp 24–33" href="/article/10.1007/s10055-019-00409-6#ref-CR14" id="ref-link-section-d27611e5993">2019</a>), manipulation tasks are perceived as more difficult when accomplished by means of touchless devices such as the Leap Motion controller.</p><p>In our analysis, we have discovered the main difference between the Vive handheld controller and the Myo armband is in terms of usability, whereas users perceive they spent almost the same effort in learning how to use the two devices (since results in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab4">4</a> suggest no significant difference between the two devices for the <i>SUS learnability</i> component). This means that, contrary to our initial hypothesis, Myo’s gestures and Vive’s controls have a similar level of intuitiveness. Of course, both the devices are still far from the ideal concept of guessability, which denotes an immediate proficiency in the interaction without any preliminary learning phase. However, a negative impact on the immersion level derives more from the unreliable behavior of the Myo device than from the naturalness of the implemented gestures.</p><p>In the Vive controller use case, the dominant aspects that characterize and differentiate the experience of each user are related to involvement and visual factors, whereas the device usability (which can be identified with the <i>interface quality</i>) assumes a secondary importance (since it has a minor influence on the total variability). In this scenario, we can hypothesize there are no evident usability issues that could interfere with the user involvement, which seems to be influenced only by the visual quality. Also <i>immersion</i> and <i>adaptation</i> are influenced by <i>visual fidelity</i>, but they have a minor importance than <i>involvement</i>.</p><p>On the contrary, in the Myo use case, users’ opinions mostly differ in terms of <i>immersion/adaptation</i> and <i>interface quality</i>, which become the main causes for statistical variability. In this scenario, <i>visual fidelity</i> has a minor importance, since users seem to pay more attention on Myo’s usability issues. This fact is in accordance with the higher importance, for the Myo scenario, of the <span class="mathjax-tex">\(I_{4}\)</span> UMUX item, which switches to the most significant principal component PC1 in the right side of Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab9">9</a>: of course, the effort for “correcting things with the system” diverts the user attention from the visual component of the virtual experience. We can suppose such parallelism between UMUX items and PQ factors is in accordance with the correlation between UMUX and PQ metrics we observed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00409-6#Tab7">7</a> for the Myo use case.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusions and future work</h2><div class="c-article-section__content" id="Sec20-content"><p>In this paper, we have presented a comparative study of the user experience in a virtual environment with two different interaction modalities: the former is based on the use of the Vive controller bundled with the HTC Vive headset, while the latter exploits the Myo armband touchless device. We asked users to fill in a questionnaire made up of two questionnaires about the system usability (UMUX and SUS) and a presence questionnaire (PQ) about the user experience in virtual environments.</p><p>The results of the statistical analysis confirmed our hypothesis about the influence of the interaction modality on the user experience: users’ higher familiarity with handheld game controllers increases the system usability in the Vive controller use case, with important consequences on the perceived sense of presence. The correlations among UMUX and PQ scores and the strength of the metric subcomponents are different between the two modalities: due to the poor frequency of control and usability issues in the Vive controller scenario, users are able to concentrate mainly on other aspects, dealing with the sense of presence; on the other hand, difficulties in using Myo have consequences on immersion and adaptation in the virtual environment.</p><p>In the SUS score, the interaction modality seems to have no influence on the learnability subcomponent, which represents the effort for learning how to use the system: only the usability in a strict sense appears to be significantly influenced.</p><p>In future work, we could include also sounds and haptic interaction in our virtual environment to conduct a more exhaustive analysis and study how these two components can enhance the user experience.</p><p>Furthermore, we could extend our study by considering other devices for touchless interaction. We could employ the Leap Motion controller, which exploits an infrared camera to detect movements of hands and fingers, or Microsoft Kinect, another camera-based device able to detect movements of arms and legs.</p><p>Moreover, we could conduct similar analysis for touchless interaction modalities based on simulating the act of grasping virtual objects. To this aim, we could employ Microsoft HoloLens (Microsoft <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="Microsoft (2019b) Microsoft Hololens. Retrieved 15 November 2019, from &#xA;                https://www.microsoft.com/en-IE/hololens&#xA;                &#xA;              &#xA;" href="/article/10.1007/s10055-019-00409-6#ref-CR59" id="ref-link-section-d27611e6089">2019b</a>), which would allow to transpose our studies also in a mixed reality environment, where virtual objects are well integrated in the real scene and can interact with real objects. In such scenario, the immersion concept needs to be revisited.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Anwar S, Sinha SK, Vivek S, Ashank V (2019) Hand gesture recognition: a survey. In: Lecture notes in electrica" /><p class="c-article-references__text" id="ref-CR1">Anwar S, Sinha SK, Vivek S, Ashank V (2019) Hand gesture recognition: a survey. In: Lecture notes in electrical engineering, pp 365–371</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Argelaguet, C. Andujar, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Argelaguet F, Andujar C (2013) A survey of 3D object selection techniques for virtual environments. Comput Gra" /><p class="c-article-references__text" id="ref-CR2">Argelaguet F, Andujar C (2013) A survey of 3D object selection techniques for virtual environments. Comput Graph (Pergamon) 37(3):121–136</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2012.12.003" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%203D%20object%20selection%20techniques%20for%20virtual%20environments&amp;journal=Comput%20Graph%20%28Pergamon%29&amp;volume=37&amp;issue=3&amp;pages=121-136&amp;publication_year=2013&amp;author=Argelaguet%2CF&amp;author=Andujar%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Assila, K. Marçal De Oliveira, H. Ezzedine, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Assila A, Marçal De Oliveira K, Ezzedine H (2016) Standardized usability questionnaires: features and quality " /><p class="c-article-references__text" id="ref-CR3">Assila A, Marçal De Oliveira K, Ezzedine H (2016) Standardized usability questionnaires: features and quality focus. J Comput Sci Inf Technol (eJCSIT) 6:15–31</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Standardized%20usability%20questionnaires%3A%20features%20and%20quality%20focus&amp;journal=J%20Comput%20Sci%20Inf%20Technol%20%28eJCSIT%29&amp;volume=6&amp;pages=15-31&amp;publication_year=2016&amp;author=Assila%2CA&amp;author=Mar%C3%A7al%20De%20Oliveira%2CK&amp;author=Ezzedine%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Bachmann, F. Weichert, G. Rinkenauer, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus " /><p class="c-article-references__text" id="ref-CR4">Bachmann D, Weichert F, Rinkenauer G (2018) Review of three-dimensional human–computer interaction with focus on the leap motion controller. Sensors 18:2194</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs18072194" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Review%20of%20three-dimensional%20human%E2%80%93computer%20interaction%20with%20focus%20on%20the%20leap%20motion%20controller&amp;journal=Sensors&amp;volume=18&amp;publication_year=2018&amp;author=Bachmann%2CD&amp;author=Weichert%2CF&amp;author=Rinkenauer%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bailey SK, Johnson CI, Sims VK (2019) Using natural gesture interactions leads to higher usability and presenc" /><p class="c-article-references__text" id="ref-CR5">Bailey SK, Johnson CI, Sims VK (2019) Using natural gesture interactions leads to higher usability and presence in a computer lesson. In: Advances in intelligent systems and computing, pp 663–671</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MS. Bartlett, " /><meta itemprop="datePublished" content="1937" /><meta itemprop="headline" content="Bartlett MS (1937) Properties of sufficiency and statistical tests. Proc R Soc Lond Ser A Math Phys Sci 160(90" /><p class="c-article-references__text" id="ref-CR6">Bartlett MS (1937) Properties of sufficiency and statistical tests. Proc R Soc Lond Ser A Math Phys Sci 160(901):268–282</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?63.1092.03" aria-label="View reference 6 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Properties%20of%20sufficiency%20and%20statistical%20tests&amp;journal=Proc%20R%20Soc%20Lond%20Ser%20A%20Math%20Phys%20Sci&amp;volume=160&amp;issue=901&amp;pages=268-282&amp;publication_year=1937&amp;author=Bartlett%2CMS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bhattacharyya A, Mazumder O, Chakravarty K, Chatterjee D, Sinha A, Gavas R (2018) Development of an interactiv" /><p class="c-article-references__text" id="ref-CR7">Bhattacharyya A, Mazumder O, Chakravarty K, Chatterjee D, Sinha A, Gavas R (2018) Development of an interactive gaming solution using MYO sensor for rehabilitation. In: 2018 international conference on advances in computing, communications and informatics, ICACCI 2018, pp 2127–2130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Borges M, Symington A, Coltin B, Smith T, Ventura R (2018) HTC Vive: analysis and accuracy improvement. In: IE" /><p class="c-article-references__text" id="ref-CR8">Borges M, Symington A, Coltin B, Smith T, Ventura R (2018) HTC Vive: analysis and accuracy improvement. In: IEEE international conference on intelligent robots and systems, pp 2610–2615</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Borrego, J. Latorre, M. Alcañiz, R. Llorens, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Borrego A, Latorre J, Alcañiz M, Llorens R (2018) Comparison of Oculus Rift and HTC Vive: feasibility for virt" /><p class="c-article-references__text" id="ref-CR9">Borrego A, Latorre J, Alcañiz M, Llorens R (2018) Comparison of Oculus Rift and HTC Vive: feasibility for virtual reality-based exploration, navigation, exergaming, and rehabilitation. Games Health J 7(3):151–156</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fg4h.2017.0114" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparison%20of%20Oculus%20Rift%20and%20HTC%20Vive%3A%20feasibility%20for%20virtual%20reality-based%20exploration%2C%20navigation%2C%20exergaming%2C%20and%20rehabilitation&amp;journal=Games%20Health%20J&amp;volume=7&amp;issue=3&amp;pages=151-156&amp;publication_year=2018&amp;author=Borrego%2CA&amp;author=Latorre%2CJ&amp;author=Alca%C3%B1iz%2CM&amp;author=Llorens%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Borsci, S. Federici, M. Lauriola, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Borsci S, Federici S, Lauriola M (2009) On the dimensionality of the system usability scale: a test of alterna" /><p class="c-article-references__text" id="ref-CR10">Borsci S, Federici S, Lauriola M (2009) On the dimensionality of the system usability scale: a test of alternative measurement models. Cognit Process 10(3):193–197</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10339-009-0268-9" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20the%20dimensionality%20of%20the%20system%20usability%20scale%3A%20a%20test%20of%20alternative%20measurement%20models&amp;journal=Cognit%20Process&amp;volume=10&amp;issue=3&amp;pages=193-197&amp;publication_year=2009&amp;author=Borsci%2CS&amp;author=Federici%2CS&amp;author=Lauriola%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, RP. McMahan, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43" /><p class="c-article-references__text" id="ref-CR11">Bowman DA, McMahan RP (2007) Virtual reality: how much immersion is enough? Computer 40(7):36–43</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2007.257" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20how%20much%20immersion%20is%20enough%3F&amp;journal=Computer&amp;volume=40&amp;issue=7&amp;pages=36-43&amp;publication_year=2007&amp;author=Bowman%2CDA&amp;author=McMahan%2CRP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DA. Bowman, S. Coquillart, B. Froehlich, M. Hirose, Y. Kitamura, K. Kiyokawa, W. Stuerzlinger, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Bowman DA, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interfac" /><p class="c-article-references__text" id="ref-CR12">Bowman DA, Coquillart S, Froehlich B, Hirose M, Kitamura Y, Kiyokawa K, Stuerzlinger W (2008) 3D user interfaces: new directions and perspectives. IEEE Comput Graph Appl 28(6):20–36</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2008.109" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20new%20directions%20and%20perspectives&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=28&amp;issue=6&amp;pages=20-36&amp;publication_year=2008&amp;author=Bowman%2CDA&amp;author=Coquillart%2CS&amp;author=Froehlich%2CB&amp;author=Hirose%2CM&amp;author=Kitamura%2CY&amp;author=Kiyokawa%2CK&amp;author=Stuerzlinger%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Brooke, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="&#xA;Brooke J (1996) SUS—a quick and dirty usability scale. In: Jordan PW, Thomas B, McClelland IL, Weerdmeester B" /><p class="c-article-references__text" id="ref-CR13">
Brooke J (1996) SUS—a quick and dirty usability scale. In: Jordan PW, Thomas B, McClelland IL, Weerdmeester B (eds) Usability evaluation in industry. CRC Press, Boca Raton</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Usability%20evaluation%20in%20industry&amp;publication_year=1996&amp;author=Brooke%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Caggianese G, Gallo L, Neroni P (2019) The vive controllers vs. leap motion for interactions in virtual enviro" /><p class="c-article-references__text" id="ref-CR14">Caggianese G, Gallo L, Neroni P (2019) The vive controllers vs. leap motion for interactions in virtual environments: a comparative evaluation. In: smart innovation, systems and technologies, pp 24–33</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cain B (2004) A review of the mental workload literature. NATO RTO-TR-HFM-121-Part-II" /><p class="c-article-references__text" id="ref-CR15">Cain B (2004) A review of the mental workload literature. NATO RTO-TR-HFM-121-Part-II</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="FM. Caputo, A. Giachetti, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Caputo FM, Giachetti A (2015) Evaluation of basic object manipulation modes for low-cost immersive virtual rea" /><p class="c-article-references__text" id="ref-CR16">Caputo FM, Giachetti A (2015) Evaluation of basic object manipulation modes for low-cost immersive virtual reality. In: Proceedings of the 11th biannual conference on Italian SIGCHI chapter. ACM, New York, pp 74–77</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20basic%20object%20manipulation%20modes%20for%20low-cost%20immersive%20virtual%20reality&amp;pages=74-77&amp;publication_year=2015&amp;author=Caputo%2CFM&amp;author=Giachetti%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen MY, Tung YC, Wu PJ, Hsu CY, Chyou S, Valstar A, Wang HY, Lin JW (2015) User-defined game input for smart " /><p class="c-article-references__text" id="ref-CR17">Chen MY, Tung YC, Wu PJ, Hsu CY, Chyou S, Valstar A, Wang HY, Lin JW (2015) User-defined game input for smart glasses in public space</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cook H, Nguyen QV, Simoff S, Trescak T, Preston D (2015) A close-range gesture interaction with Kinect. In: 20" /><p class="c-article-references__text" id="ref-CR18">Cook H, Nguyen QV, Simoff S, Trescak T, Preston D (2015) A close-range gesture interaction with Kinect. In: 2015 big data visual analytics, BDVA 2015, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Csapo AB, Nagy H, Kristjansson A, Wersenyi G (2017) Evaluation of human-Myo gesture control capabilities in co" /><p class="c-article-references__text" id="ref-CR19">Csapo AB, Nagy H, Kristjansson A, Wersenyi G (2017) Evaluation of human-Myo gesture control capabilities in continuous search and select operations. In: 7th IEEE international conference on cognitive infocommunications, CogInfoCom 2016</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Paolis LT (2016) A touchless gestural platform for the interaction with the patients data. In: XIV Mediterr" /><p class="c-article-references__text" id="ref-CR20">De Paolis LT (2016) A touchless gestural platform for the interaction with the patients data. In: XIV Mediterranean conference on medical and biological engineering and computing 2016 (MEDICON 2016), March 31st–April 2nd 2016, Paphos, Cyprus, IFMBE Proceedings. Springer, Berlin, pp 880–884</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Paolis LT (2018) Augmented visualization and touchless interaction with virtual organs. In: International c" /><p class="c-article-references__text" id="ref-CR21">De Paolis LT (2018) Augmented visualization and touchless interaction with virtual organs. In: International conference on bioinformatics and biomedical engineering (IWBBIO 2018) Granada, Spain, April 25–27, 2018. Lecture notes in bioinformatics, LNBI 10814. Springer, Berlin, pp 118–127</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LT. Paolis, V. Luca, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="De Paolis LT, De Luca V (2019) Augmented visualization with depth perception cues to improve the surgeon’s per" /><p class="c-article-references__text" id="ref-CR22">De Paolis LT, De Luca V (2019) Augmented visualization with depth perception cues to improve the surgeon’s performance in minimally invasive surgery. Med Biol Eng Comput 57(5):995–1013</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs11517-018-1929-6" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20visualization%20with%20depth%20perception%20cues%20to%20improve%20the%20surgeon%E2%80%99s%20performance%20in%20minimally%20invasive%20surgery&amp;journal=Med%20Biol%20Eng%20Comput&amp;volume=57&amp;issue=5&amp;pages=995-1013&amp;publication_year=2019&amp;author=Paolis%2CLT&amp;author=Luca%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LT. Paolis, A. Mauro, J. Raczkowsky, G. Aloisio, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="De Paolis LT, De Mauro A, Raczkowsky J, Aloisio G (2009) Virtual model of the human brain for neurosurgical si" /><p class="c-article-references__text" id="ref-CR23">De Paolis LT, De Mauro A, Raczkowsky J, Aloisio G (2009) Virtual model of the human brain for neurosurgical simulation. Stud Health Technol Inform 150:811–815</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20model%20of%20the%20human%20brain%20for%20neurosurgical%20simulation&amp;journal=Stud%20Health%20Technol%20Inform&amp;volume=150&amp;pages=811-815&amp;publication_year=2009&amp;author=Paolis%2CLT&amp;author=Mauro%2CA&amp;author=Raczkowsky%2CJ&amp;author=Aloisio%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="LT. Paolis, M. Pulimeno, G. Aloisio, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="De Paolis LT, Pulimeno M, Aloisio G (2010) Advanced visualization and interaction systems for surgical pre-ope" /><p class="c-article-references__text" id="ref-CR24">De Paolis LT, Pulimeno M, Aloisio G (2010) Advanced visualization and interaction systems for surgical pre-operative planning. J Comput Inf Technol 18(4):385–392</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2498%2Fcit.1001878" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advanced%20visualization%20and%20interaction%20systems%20for%20surgical%20pre-operative%20planning&amp;journal=J%20Comput%20Inf%20Technol&amp;volume=18&amp;issue=4&amp;pages=385-392&amp;publication_year=2010&amp;author=Paolis%2CLT&amp;author=Pulimeno%2CM&amp;author=Aloisio%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="De Paolis LT, De Luca V, Paladini GI (2019) Touchless navigation in a multimedia application: the effects perc" /><p class="c-article-references__text" id="ref-CR25">De Paolis LT, De Luca V, Paladini GI (2019) Touchless navigation in a multimedia application: the effects perceived in an educational context. In: Sixth international conference augmented and virtual reality, and computer graphics (AVR 2019), Santa Maria al Bagno, Italy, June 24–27, 2019. Lecture notes in computer science, LNCS 11614. Springer, Berlin, pp 348–367</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dong H, Figueroa N, El Saddik A (2016) An elicitation study on gesture attitudes and preferences towards an in" /><p class="c-article-references__text" id="ref-CR26">Dong H, Figueroa N, El Saddik A (2016) An elicitation study on gesture attitudes and preferences towards an interactive hand-gesture vocabulary</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Duvinage, T. Castermans, M. Petieau, T. Hoellinger, G. Cheron, T. Dutoit, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Duvinage M, Castermans T, Petieau M, Hoellinger T, Cheron G, Dutoit T (2013) Performance of the Emotiv Epoc he" /><p class="c-article-references__text" id="ref-CR27">Duvinage M, Castermans T, Petieau M, Hoellinger T, Cheron G, Dutoit T (2013) Performance of the Emotiv Epoc headset for P300-based applications. BioMed Eng Online 12(1):56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1186%2F1475-925X-12-56" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Performance%20of%20the%20Emotiv%20Epoc%20headset%20for%20P300-based%20applications&amp;journal=BioMed%20Eng%20Online&amp;volume=12&amp;issue=1&amp;publication_year=2013&amp;author=Duvinage%2CM&amp;author=Castermans%2CT&amp;author=Petieau%2CM&amp;author=Hoellinger%2CT&amp;author=Cheron%2CG&amp;author=Dutoit%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Emotiv (2019) Emotiv EPOC+. Retrieved 15 November 2019, from https://www.emotiv.com&#xA;" /><p class="c-article-references__text" id="ref-CR28">Emotiv (2019) Emotiv EPOC+. Retrieved 15 November 2019, from <a href="https://www.emotiv.com">https://www.emotiv.com</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Figueiredo, E. Rodrigues, J. Teixeira, V. Techrieb, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Figueiredo L, Rodrigues E, Teixeira J, Techrieb V (2018) A comparative evaluation of direct hand and wand inte" /><p class="c-article-references__text" id="ref-CR29">Figueiredo L, Rodrigues E, Teixeira J, Techrieb V (2018) A comparative evaluation of direct hand and wand interactions on consumer devices. Comput Graph (Pergamon) 77:108–121</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cag.2018.10.006" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20evaluation%20of%20direct%20hand%20and%20wand%20interactions%20on%20consumer%20devices&amp;journal=Comput%20Graph%20%28Pergamon%29&amp;volume=77&amp;pages=108-121&amp;publication_year=2018&amp;author=Figueiredo%2CL&amp;author=Rodrigues%2CE&amp;author=Teixeira%2CJ&amp;author=Techrieb%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Finstad, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Finstad K (2010) The usability metric for user experience. Interact Comput 22(5):323–327" /><p class="c-article-references__text" id="ref-CR30">Finstad K (2010) The usability metric for user experience. Interact Comput 22(5):323–327</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.intcom.2010.04.004" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20usability%20metric%20for%20user%20experience&amp;journal=Interact%20Comput&amp;volume=22&amp;issue=5&amp;pages=323-327&amp;publication_year=2010&amp;author=Finstad%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Frankenstein, S. Brüssow, F. Ruzzoli, C. Hölscher, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Frankenstein J, Brüssow S, Ruzzoli F, Hölscher C (2012) The language of landmarks: the role of background know" /><p class="c-article-references__text" id="ref-CR31">Frankenstein J, Brüssow S, Ruzzoli F, Hölscher C (2012) The language of landmarks: the role of background knowledge in indoor wayfinding. Cognit Process 13(1):165–170</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10339-012-0482-8" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20language%20of%20landmarks%3A%20the%20role%20of%20background%20knowledge%20in%20indoor%20wayfinding&amp;journal=Cognit%20Process&amp;volume=13&amp;issue=1&amp;pages=165-170&amp;publication_year=2012&amp;author=Frankenstein%2CJ&amp;author=Br%C3%BCssow%2CS&amp;author=Ruzzoli%2CF&amp;author=H%C3%B6lscher%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KR. Gabriel, " /><meta itemprop="datePublished" content="1971" /><meta itemprop="headline" content="Gabriel KR (1971) The biplot graphic display of matrices with application to principal component analysis. Bio" /><p class="c-article-references__text" id="ref-CR32">Gabriel KR (1971) The biplot graphic display of matrices with application to principal component analysis. Biometrika 58(3):453–467</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=312645" aria-label="View reference 32 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0228.62034" aria-label="View reference 32 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1093%2Fbiomet%2F58.3.453" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20biplot%20graphic%20display%20of%20matrices%20with%20application%20to%20principal%20component%20analysis&amp;journal=Biometrika&amp;volume=58&amp;issue=3&amp;pages=453-467&amp;publication_year=1971&amp;author=Gabriel%2CKR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gamer PC (2019) Valve index review. Retrieved 15 November 2019, from https://www.pcgamer.com/valve-index-revie" /><p class="c-article-references__text" id="ref-CR33">Gamer PC (2019) Valve index review. Retrieved 15 November 2019, from <a href="https://www.pcgamer.com/valve-index-review/">https://www.pcgamer.com/valve-index-review/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Garber, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Garber L (2013) Gestural technology: moving interfaces in a new direction. Computer 46(10):22–25" /><p class="c-article-references__text" id="ref-CR34">Garber L (2013) Gestural technology: moving interfaces in a new direction. Computer 46(10):22–25</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2013.352" aria-label="View reference 34">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 34 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Gestural%20technology%3A%20moving%20interfaces%20in%20a%20new%20direction&amp;journal=Computer&amp;volume=46&amp;issue=10&amp;pages=22-25&amp;publication_year=2013&amp;author=Garber%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grandhi SA, Joue G, Mittelberg I (2011) Understanding naturalness and intuitiveness in gesture production—insi" /><p class="c-article-references__text" id="ref-CR35">Grandhi SA, Joue G, Mittelberg I (2011) Understanding naturalness and intuitiveness in gesture production—insights for touchless gestural interfaces. In: Proceedings of the international conference on human factors in computing systems (CHI’11), New York, NY, USA, pp 821–824</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gusai E, Bassano C, Solari F, Chessa M (2017) Interaction in an immersive collaborative virtual reality enviro" /><p class="c-article-references__text" id="ref-CR36">Gusai E, Bassano C, Solari F, Chessa M (2017) Interaction in an immersive collaborative virtual reality environment: a comparison between Leap Motion and HTC controllers. In: Lecture notes in computer science, pp 290–300</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Guzsvinecz, V. Szucs, C. Sik-Lanyi, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Guzsvinecz T, Szucs V, Sik-Lanyi C (2019) Suitability of the Kinect sensor and Leap Motion controller—a litera" /><p class="c-article-references__text" id="ref-CR37">Guzsvinecz T, Szucs V, Sik-Lanyi C (2019) Suitability of the Kinect sensor and Leap Motion controller—a literature review. Sensors 19:1072</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs19051072" aria-label="View reference 37">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 37 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Suitability%20of%20the%20Kinect%20sensor%20and%20Leap%20Motion%20controller%E2%80%94a%20literature%20review&amp;journal=Sensors&amp;volume=19&amp;publication_year=2019&amp;author=Guzsvinecz%2CT&amp;author=Szucs%2CV&amp;author=Sik-Lanyi%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hauser N, Wade E (2018) Detecting reach to grasp activities using motion and muscle activation data. In: Proce" /><p class="c-article-references__text" id="ref-CR38">Hauser N, Wade E (2018) Detecting reach to grasp activities using motion and muscle activation data. In: Proceedings of the annual international conference of the IEEE engineering in medicine and biology society, EMBS, pp 3264–3267</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-thro" /><p class="c-article-references__text" id="ref-CR39">Hilliges O, Kim D, Izadi S, Weiss M, Wilson A (2012) HoloDesk: direct 3D interactions with a situated see-through display Otmar. In: Proceedings of the 2012 ACM annual conference on human factors in computing systems-CHI ’12, New York, NY, USA, pp 2421–2430</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;HTC (2019a) HTC Vive controller. Retrieved 15 November 2019, from https://www.vive.com/eu/accessory/controlle" /><p class="c-article-references__text" id="ref-CR40">
HTC (2019a) HTC Vive controller. Retrieved 15 November 2019, from <a href="https://www.vive.com/eu/accessory/controller/">https://www.vive.com/eu/accessory/controller/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="HTC (2019b) HTC Vive. Retrieved 15 November 2019, from https://www.vive.com/us/product/vive-virtual-reality-sy" /><p class="c-article-references__text" id="ref-CR41">HTC (2019b) HTC Vive. Retrieved 15 November 2019, from <a href="https://www.vive.com/us/product/vive-virtual-reality-system/">https://www.vive.com/us/product/vive-virtual-reality-system/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;HTC (2019c) VIVE wireless adapter. Retrieved 15 November 2019, from https://www.vive.com/us/wireless-adapter/" /><p class="c-article-references__text" id="ref-CR42">
HTC (2019c) VIVE wireless adapter. Retrieved 15 November 2019, from <a href="https://www.vive.com/us/wireless-adapter/">https://www.vive.com/us/wireless-adapter/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Hudson, S. Matson-Barkat, N. Pallamin, G. Jegou, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Hudson S, Matson-Barkat S, Pallamin N, Jegou G (2019) With or without you? Interaction and immersion in a virt" /><p class="c-article-references__text" id="ref-CR43">Hudson S, Matson-Barkat S, Pallamin N, Jegou G (2019) With or without you? Interaction and immersion in a virtual reality experience. J Bus Res 100:459–468</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jbusres.2018.10.062" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=With%20or%20without%20you%3F%20Interaction%20and%20immersion%20in%20a%20virtual%20reality%20experience&amp;journal=J%20Bus%20Res&amp;volume=100&amp;pages=459-468&amp;publication_year=2019&amp;author=Hudson%2CS&amp;author=Matson-Barkat%2CS&amp;author=Pallamin%2CN&amp;author=Jegou%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Indraccolo C, De Paolis LT (2017) Augmented reality and MYO for a touchless interaction with virtual organs. I" /><p class="c-article-references__text" id="ref-CR44">Indraccolo C, De Paolis LT (2017) Augmented reality and MYO for a touchless interaction with virtual organs. In: Fourth international conference augmented and virtual reality, and computer graphics (AVR 2017), Ugento, Italy, June 12–15, 2017. Lecture notes in computer science, LNCS 10325, pp 63–73</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2015) Interactive entertainment, virtual motion tr" /><p class="c-article-references__text" id="ref-CR45">Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2015) Interactive entertainment, virtual motion training and brain ergonomy. In: 7th international conference on intelligent technologies for interactive entertainment (INTETAIN 2015), Torino, Italy, June 10–12, 2015, pp 88–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Invitto, C. Faggiano, S. Sammarco, V. Luca, LT. Paolis, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2016) Haptic, virtual interaction and motor imager" /><p class="c-article-references__text" id="ref-CR46">Invitto S, Faggiano C, Sammarco S, De Luca V, De Paolis LT (2016) Haptic, virtual interaction and motor imagery: entertainment tools and psychophysiological testing. Sensors 16(3):394</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs16030394" aria-label="View reference 46">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 46 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Haptic%2C%20virtual%20interaction%20and%20motor%20imagery%3A%20entertainment%20tools%20and%20psychophysiological%20testing&amp;journal=Sensors&amp;volume=16&amp;issue=3&amp;publication_year=2016&amp;author=Invitto%2CS&amp;author=Faggiano%2CC&amp;author=Sammarco%2CS&amp;author=Luca%2CV&amp;author=Paolis%2CLT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="HF. Kaiser, " /><meta itemprop="datePublished" content="1958" /><meta itemprop="headline" content="Kaiser HF (1958) The varimax criterion for analytic rotation in factor analysis. Psychometrika 23(3):187–200" /><p class="c-article-references__text" id="ref-CR47">Kaiser HF (1958) The varimax criterion for analytic rotation in factor analysis. Psychometrika 23(3):187–200</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0095.33603" aria-label="View reference 47 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF02289233" aria-label="View reference 47">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20varimax%20criterion%20for%20analytic%20rotation%20in%20factor%20analysis&amp;journal=Psychometrika&amp;volume=23&amp;issue=3&amp;pages=187-200&amp;publication_year=1958&amp;author=Kaiser%2CHF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="WH. Kruskal, WA. Wallis, " /><meta itemprop="datePublished" content="1952" /><meta itemprop="headline" content="Kruskal WH, Wallis WA (1952) Use of ranks in one-criterion variance analysis. J Am Stat Assoc 47(260):583–621" /><p class="c-article-references__text" id="ref-CR48">Kruskal WH, Wallis WA (1952) Use of ranks in one-criterion variance analysis. J Am Stat Assoc 47(260):583–621</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0048.11703" aria-label="View reference 48 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F01621459.1952.10483441" aria-label="View reference 48">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Use%20of%20ranks%20in%20one-criterion%20variance%20analysis&amp;journal=J%20Am%20Stat%20Assoc&amp;volume=47&amp;issue=260&amp;pages=583-621&amp;publication_year=1952&amp;author=Kruskal%2CWH&amp;author=Wallis%2CWA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Lachat, H. Macher, T. Landes, P. Grussenmeyer, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Lachat E, Macher H, Landes T, Grussenmeyer P (2015) Assessment and calibration of a RGB-D camera (Kinect v2 Se" /><p class="c-article-references__text" id="ref-CR49">Lachat E, Macher H, Landes T, Grussenmeyer P (2015) Assessment and calibration of a RGB-D camera (Kinect v2 Sensor) towards a potential use for close-range 3D modeling. Remote Sens 7(10):13070–13097</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Frs71013070" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessment%20and%20calibration%20of%20a%20RGB-D%20camera%20%28Kinect%20v2%20Sensor%29%20towards%20a%20potential%20use%20for%20close-range%203D%20modeling&amp;journal=Remote%20Sens&amp;volume=7&amp;issue=10&amp;pages=13070-13097&amp;publication_year=2015&amp;author=Lachat%2CE&amp;author=Macher%2CH&amp;author=Landes%2CT&amp;author=Grussenmeyer%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JR. Lewis, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Lewis JR (2018) Measuring perceived usability: the CSUQ, SUS, and UMUX. Int J Hum Comput Interact 34(12):1148–" /><p class="c-article-references__text" id="ref-CR50">Lewis JR (2018) Measuring perceived usability: the CSUQ, SUS, and UMUX. Int J Hum Comput Interact 34(12):1148–1156</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10447318.2017.1418805" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20perceived%20usability%3A%20the%20CSUQ%2C%20SUS%2C%20and%20UMUX&amp;journal=Int%20J%20Hum%20Comput%20Interact&amp;volume=34&amp;issue=12&amp;pages=1148-1156&amp;publication_year=2018&amp;author=Lewis%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer sci" /><p class="c-article-references__text" id="ref-CR51">Lewis JR, Sauro J (2009) The factor structure of the system usability scale. In: Lecture notes in computer science, pp 94–103</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lewis JR, Utesch BS, Maher DE (2015) Investigating the correspondence between UMUX-LITE and SUS scores. In: Le" /><p class="c-article-references__text" id="ref-CR52">Lewis JR, Utesch BS, Maher DE (2015) Investigating the correspondence between UMUX-LITE and SUS scores. In: Lecture notes in computer science, pp 204–211</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Livatino, LT. Paolis, M. D’Agostino, A. Zocco, A. Agrimi, A. Santis, LV. Bruno, M. Lapresa, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Livatino S, De Paolis LT, D’Agostino M, Zocco A, Agrimi A, De Santis A, Bruno LV, Lapresa M (2015) Stereoscopi" /><p class="c-article-references__text" id="ref-CR53">Livatino S, De Paolis LT, D’Agostino M, Zocco A, Agrimi A, De Santis A, Bruno LV, Lapresa M (2015) Stereoscopic visualization and 3-D technologies in medical endoscopic teleoperation. IEEE Trans Ind Electron 62(1):525–535</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTIE.2014.2334675" aria-label="View reference 53">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 53 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Stereoscopic%20visualization%20and%203-D%20technologies%20in%20medical%20endoscopic%20teleoperation&amp;journal=IEEE%20Trans%20Ind%20Electron&amp;volume=62&amp;issue=1&amp;pages=525-535&amp;publication_year=2015&amp;author=Livatino%2CS&amp;author=Paolis%2CLT&amp;author=D%E2%80%99Agostino%2CM&amp;author=Zocco%2CA&amp;author=Agrimi%2CA&amp;author=Santis%2CA&amp;author=Bruno%2CLV&amp;author=Lapresa%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lucas JF, Kim JS, Bowman DA (2005) Resizing beyond widgets: object resizing techniques for immersive virtual e" /><p class="c-article-references__text" id="ref-CR54">Lucas JF, Kim JS, Bowman DA (2005) Resizing beyond widgets: object resizing techniques for immersive virtual environments. In: Proceedings of ACM CHI 2005 conference on human factors in computing systems, New York, NY, USA, pp 1601–1604</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lund BAM (2001) Measuring usability with the USE questionnaire. STC usability SIG newsletter" /><p class="c-article-references__text" id="ref-CR55">Lund BAM (2001) Measuring usability with the USE questionnaire. STC usability SIG newsletter</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="McMahan RP, Gorton D, Gresock J, McConnell W, Bowman DA (2007) Separating the effects of level of immersion an" /><p class="c-article-references__text" id="ref-CR56">McMahan RP, Gorton D, Gresock J, McConnell W, Bowman DA (2007) Separating the effects of level of immersion and 3D interaction techniques. In: Proceedings of the ACM symposium on virtual reality software and technology. ACM, New York, pp 108–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Méndez, J. Flores, E. Castelló, JR. Viqueira, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Méndez R, Flores J, Castelló E, Viqueira JR (2019) Natural interaction in virtual TV sets through the synergis" /><p class="c-article-references__text" id="ref-CR57">Méndez R, Flores J, Castelló E, Viqueira JR (2019) Natural interaction in virtual TV sets through the synergistic operation of low-cost sensors. Univ Access Inf Soc 18(1):17–29</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10209-017-0586-0" aria-label="View reference 57">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 57 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20interaction%20in%20virtual%20TV%20sets%20through%20the%20synergistic%20operation%20of%20low-cost%20sensors&amp;journal=Univ%20Access%20Inf%20Soc&amp;volume=18&amp;issue=1&amp;pages=17-29&amp;publication_year=2019&amp;author=M%C3%A9ndez%2CR&amp;author=Flores%2CJ&amp;author=Castell%C3%B3%2CE&amp;author=Viqueira%2CJR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Microsoft (2019a) Azure Kinect DK. Retrieved 15 November 2019, from https://azure.microsoft.com/en-us/services" /><p class="c-article-references__text" id="ref-CR58">Microsoft (2019a) Azure Kinect DK. Retrieved 15 November 2019, from <a href="https://azure.microsoft.com/en-us/services/kinect-dk/">https://azure.microsoft.com/en-us/services/kinect-dk/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Microsoft (2019b) Microsoft Hololens. Retrieved 15 November 2019, from https://www.microsoft.com/en-IE/hololen" /><p class="c-article-references__text" id="ref-CR59">Microsoft (2019b) Microsoft Hololens. Retrieved 15 November 2019, from <a href="https://www.microsoft.com/en-IE/hololens">https://www.microsoft.com/en-IE/hololens</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Motion Leap (2019) Leap Motion. Retrieved 15 November 2019, from https://www.leapmotion.com/&#xA;" /><p class="c-article-references__text" id="ref-CR60">Motion Leap (2019) Leap Motion. Retrieved 15 November 2019, from <a href="https://www.leapmotion.com/">https://www.leapmotion.com/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Moustafa K, Luz S, Longo L (2017) Assessment of mental workload: a comparison of machine learning methods and " /><p class="c-article-references__text" id="ref-CR61">Moustafa K, Luz S, Longo L (2017) Assessment of mental workload: a comparison of machine learning methods and subjective assessment techniques. In: Communications in computer and information science, pp 30–50</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DC. Niehorster, L. Li, M. Lappe, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Niehorster DC, Li L, Lappe M (2017) The accuracy and precision of position and orientation tracking in the HTC" /><p class="c-article-references__text" id="ref-CR62">Niehorster DC, Li L, Lappe M (2017) The accuracy and precision of position and orientation tracking in the HTC vive virtual reality system for scientific research. i-Perception 8(3):2041669517708205</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 62 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20accuracy%20and%20precision%20of%20position%20and%20orientation%20tracking%20in%20the%20HTC%20vive%20virtual%20reality%20system%20for%20scientific%20research&amp;journal=i-Perception&amp;volume=8&amp;issue=3&amp;publication_year=2017&amp;author=Niehorster%2CDC&amp;author=Li%2CL&amp;author=Lappe%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Nintendo (2019) Wii Remote. Retrieved 15 November 2019, from https://www.nintendo.com&#xA;" /><p class="c-article-references__text" id="ref-CR63">Nintendo (2019) Wii Remote. Retrieved 15 November 2019, from <a href="https://www.nintendo.com">https://www.nintendo.com</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oculus VR (2019a) Oculus Rift. Retrieved 15 November 2019, from https://www.oculus.com/&#xA;" /><p class="c-article-references__text" id="ref-CR64">Oculus VR (2019a) Oculus Rift. Retrieved 15 November 2019, from <a href="https://www.oculus.com/">https://www.oculus.com/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oculus VR (2019b) Oculus Touch. Retrieved 15 November 2019, from https://www.oculus.com/rift/accessories/&#xA;" /><p class="c-article-references__text" id="ref-CR65">Oculus VR (2019b) Oculus Touch. Retrieved 15 November 2019, from <a href="https://www.oculus.com/rift/accessories/">https://www.oculus.com/rift/accessories/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="YS. Pai, T. Dingler, K. Kunze, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Pai YS, Dingler T, Kunze K (2018) Assessing hands-free interactions for VR using eye gaze and electromyography" /><p class="c-article-references__text" id="ref-CR66">Pai YS, Dingler T, Kunze K (2018) Assessing hands-free interactions for VR using eye gaze and electromyography. Virtual Real 23:119–131</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-018-0371-2" aria-label="View reference 66">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 66 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Assessing%20hands-free%20interactions%20for%20VR%20using%20eye%20gaze%20and%20electromyography&amp;journal=Virtual%20Real&amp;volume=23&amp;pages=119-131&amp;publication_year=2018&amp;author=Pai%2CYS&amp;author=Dingler%2CT&amp;author=Kunze%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Polygon (2019) Oculus Go review. Retrieved 15 November 2019, from https://www.polygon.com/virtual-reality/2018" /><p class="c-article-references__text" id="ref-CR67">Polygon (2019) Oculus Go review. Retrieved 15 November 2019, from <a href="https://www.polygon.com/virtual-reality/2018/5/1/17284454/oculus-go-review">https://www.polygon.com/virtual-reality/2018/5/1/17284454/oculus-go-review</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="EJ. Rechy-Ramirez, A. Marin-Hernandez, HV. Rios-Figueroa, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Rechy-Ramirez EJ, Marin-Hernandez A, Rios-Figueroa HV (2018) Impact of commercial sensors in human computer in" /><p class="c-article-references__text" id="ref-CR68">Rechy-Ramirez EJ, Marin-Hernandez A, Rios-Figueroa HV (2018) Impact of commercial sensors in human computer interaction: a review. J Ambient Intell Humaniz Comput 9:1479–1496</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs12652-017-0568-3" aria-label="View reference 68">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 68 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Impact%20of%20commercial%20sensors%20in%20human%20computer%20interaction%3A%20a%20review&amp;journal=J%20Ambient%20Intell%20Humaniz%20Comput&amp;volume=9&amp;pages=1479-1496&amp;publication_year=2018&amp;author=Rechy-Ramirez%2CEJ&amp;author=Marin-Hernandez%2CA&amp;author=Rios-Figueroa%2CHV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Ruddle, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Ruddle R (2006) Review: 3D user interfaces: theory and practice. In: Bowman DA, Kruijff E, LaViola JJ Jr, Poup" /><p class="c-article-references__text" id="ref-CR69">Ruddle R (2006) Review: 3D user interfaces: theory and practice. In: Bowman DA, Kruijff E, LaViola JJ Jr, Poupyrev I (eds) Presence: teleoperators and virtual environments. Addison-Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 69 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Presence%3A%20teleoperators%20and%20virtual%20environments&amp;publication_year=2006&amp;author=Ruddle%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Samsung (2019) Samsung Gear VR. Retrieved 15 November 2019, from https://www.samsung.com/us/mobile/virtual-rea" /><p class="c-article-references__text" id="ref-CR70">Samsung (2019) Samsung Gear VR. Retrieved 15 November 2019, from <a href="https://www.samsung.com/us/mobile/virtual-reality/gear-vr/gear-vr-with-controller-sm-r324nzaaxar/">https://www.samsung.com/us/mobile/virtual-reality/gear-vr/gear-vr-with-controller-sm-r324nzaaxar/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="MEC. Santos, C. Sandor, H. Kato, G. Yamamoto, T. Taketomi, J. Polvi, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Santos MEC, Sandor C, Kato H, Yamamoto G, Taketomi T, Polvi J (2014) A usability scale for handheld augmented " /><p class="c-article-references__text" id="ref-CR71">Santos MEC, Sandor C, Kato H, Yamamoto G, Taketomi T, Polvi J (2014) A usability scale for handheld augmented reality. In: Proceedings of the 20th ACM symposium on virtual reality software and technology. ACM, New York, pp 167–176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 71 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20usability%20scale%20for%20handheld%20augmented%20reality&amp;pages=167-176&amp;publication_year=2014&amp;author=Santos%2CMEC&amp;author=Sandor%2CC&amp;author=Kato%2CH&amp;author=Yamamoto%2CG&amp;author=Taketomi%2CT&amp;author=Polvi%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MEC. Santos, J. Polvi, T. Taketomi, G. Yamamoto, C. Sandor, H. Kato, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Santos MEC, Polvi J, Taketomi T, Yamamoto G, Sandor C, Kato H (2015) Toward standard usability questionnaires " /><p class="c-article-references__text" id="ref-CR72">Santos MEC, Polvi J, Taketomi T, Yamamoto G, Sandor C, Kato H (2015) Toward standard usability questionnaires for handheld augmented reality. IEEE Comput Graph Appl 35(5):66–75</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2015.94" aria-label="View reference 72">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 72 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20standard%20usability%20questionnaires%20for%20handheld%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=35&amp;issue=5&amp;pages=66-75&amp;publication_year=2015&amp;author=Santos%2CMEC&amp;author=Polvi%2CJ&amp;author=Taketomi%2CT&amp;author=Yamamoto%2CG&amp;author=Sandor%2CC&amp;author=Kato%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Santos-Torres A, Zarraonandia T, Díaz P, Aedo I (2018) Exploring interaction mechanisms for map interfaces in " /><p class="c-article-references__text" id="ref-CR73">Santos-Torres A, Zarraonandia T, Díaz P, Aedo I (2018) Exploring interaction mechanisms for map interfaces in virtual reality environments. In: Proceedings of the XIX international conference on human computer interaction. ACM, New York, pp 1–7</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Sarbolandi, D. Lefloch, A. Kolb, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Sarbolandi H, Lefloch D, Kolb A (2015) Kinect range sensing: structured-light versus Time-of-Flight Kinect. Co" /><p class="c-article-references__text" id="ref-CR74">Sarbolandi H, Lefloch D, Kolb A (2015) Kinect range sensing: structured-light versus Time-of-Flight Kinect. Comput Vis Image Underst 139:1–20</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2015.05.006" aria-label="View reference 74">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 74 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Kinect%20range%20sensing%3A%20structured-light%20versus%20Time-of-Flight%20Kinect&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=139&amp;pages=1-20&amp;publication_year=2015&amp;author=Sarbolandi%2CH&amp;author=Lefloch%2CD&amp;author=Kolb%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sayin FS, Ozen S, Baspinar U (2018) Hand gesture recognition by using sEMG signals for human machine interacti" /><p class="c-article-references__text" id="ref-CR75">Sayin FS, Ozen S, Baspinar U (2018) Hand gesture recognition by using sEMG signals for human machine interaction applications. In: Signal processing—algorithms, architectures, arrangements, and applications conference proceedings, SPA, pp 27–30</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SS. Shapiro, MB. Wilk, " /><meta itemprop="datePublished" content="1965" /><meta itemprop="headline" content="Shapiro SS, Wilk MB (1965) An analysis of variance test for normality (complete samples). Biometrika 52(3/4):5" /><p class="c-article-references__text" id="ref-CR76">Shapiro SS, Wilk MB (1965) An analysis of variance test for normality (complete samples). Biometrika 52(3/4):591–611</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=205384" aria-label="View reference 76 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?0134.36501" aria-label="View reference 76 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.2307%2F2333709" aria-label="View reference 76">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 76 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20analysis%20of%20variance%20test%20for%20normality%20%28complete%20samples%29&amp;journal=Biometrika&amp;volume=52&amp;issue=3%2F4&amp;pages=591-611&amp;publication_year=1965&amp;author=Shapiro%2CSS&amp;author=Wilk%2CMB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Shu, YZ. Huang, SH. Chang, MY. Chen, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Shu Y, Huang YZ, Chang SH, Chen MY (2018) Do virtual reality head-mounted displays make a difference? A compar" /><p class="c-article-references__text" id="ref-CR77">Shu Y, Huang YZ, Chang SH, Chen MY (2018) Do virtual reality head-mounted displays make a difference? A comparison of presence and self-efficacy between head-mounted displays and desktop computer-facilitated virtual environments. Virtual Real. <a href="https://doi.org/10.1007/s10055-018-0376-x">https://doi.org/10.1007/s10055-018-0376-x</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-018-0376-x" aria-label="View reference 77">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 77 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Do%20virtual%20reality%20head-mounted%20displays%20make%20a%20difference%3F%20A%20comparison%20of%20presence%20and%20self-efficacy%20between%20head-mounted%20displays%20and%20desktop%20computer-facilitated%20virtual%20environments&amp;journal=Virtual%20Real&amp;doi=10.1007%2Fs10055-018-0376-x&amp;publication_year=2018&amp;author=Shu%2CY&amp;author=Huang%2CYZ&amp;author=Chang%2CSH&amp;author=Chen%2CMY">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Somrak, I. Humar, MS. Hossain, MF. Alhamid, MA. Hossain, J. Guna, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Somrak A, Humar I, Hossain MS, Alhamid MF, Hossain MA, Guna J (2019) Estimating VR sickness and user experienc" /><p class="c-article-references__text" id="ref-CR78">Somrak A, Humar I, Hossain MS, Alhamid MF, Hossain MA, Guna J (2019) Estimating VR sickness and user experience using different HMD technologies: an evaluation study. Future Gener Comput Syst 94:302–316</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.future.2018.11.041" aria-label="View reference 78">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 78 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimating%20VR%20sickness%20and%20user%20experience%20using%20different%20HMD%20technologies%3A%20an%20evaluation%20study&amp;journal=Future%20Gener%20Comput%20Syst&amp;volume=94&amp;pages=302-316&amp;publication_year=2019&amp;author=Somrak%2CA&amp;author=Humar%2CI&amp;author=Hossain%2CMS&amp;author=Alhamid%2CMF&amp;author=Hossain%2CMA&amp;author=Guna%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sony (2019) PlayStation Move. Retrieved15 November 2019,  from https://www.playstation.com/en-us/explore/acces" /><p class="c-article-references__text" id="ref-CR79">Sony (2019) PlayStation Move. Retrieved15 November 2019,  from <a href="https://www.playstation.com/en-us/explore/accessories/vr-accessories/playstation-move/">https://www.playstation.com/en-us/explore/accessories/vr-accessories/playstation-move/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KA. Spitzley, AR. Karduna, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Spitzley KA, Karduna AR (2019) Feasibility of using a fully immersive virtual reality system for kinematic dat" /><p class="c-article-references__text" id="ref-CR80">Spitzley KA, Karduna AR (2019) Feasibility of using a fully immersive virtual reality system for kinematic data collection. J Biomech 87:172–176</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.jbiomech.2019.02.015" aria-label="View reference 80">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 80 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Feasibility%20of%20using%20a%20fully%20immersive%20virtual%20reality%20system%20for%20kinematic%20data%20collection&amp;journal=J%20Biomech&amp;volume=87&amp;pages=172-176&amp;publication_year=2019&amp;author=Spitzley%2CKA&amp;author=Karduna%2CAR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KM. Stanney, M. Mollaghasemi, L. Reeves, R. Breaux, DA. Graeber, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Stanney KM, Mollaghasemi M, Reeves L, Breaux R, Graeber DA (2003) Usability engineering of virtual environment" /><p class="c-article-references__text" id="ref-CR81">Stanney KM, Mollaghasemi M, Reeves L, Breaux R, Graeber DA (2003) Usability engineering of virtual environments (VEs): Identifying multiple criteria that drive effective VE system design. Int J Hum Comput Stud 58(4):447–481</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS1071-5819%2803%2900015-6" aria-label="View reference 81">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 81 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Usability%20engineering%20of%20virtual%20environments%20%28VEs%29%3A%20Identifying%20multiple%20criteria%20that%20drive%20effective%20VE%20system%20design&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=58&amp;issue=4&amp;pages=447-481&amp;publication_year=2003&amp;author=Stanney%2CKM&amp;author=Mollaghasemi%2CM&amp;author=Reeves%2CL&amp;author=Breaux%2CR&amp;author=Graeber%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Steam (2019) SteamVR. Retrieved 15 November 2019, from https://steamcommunity.com/steamvr&#xA;" /><p class="c-article-references__text" id="ref-CR82">Steam (2019) SteamVR. Retrieved 15 November 2019, from <a href="https://steamcommunity.com/steamvr">https://steamcommunity.com/steamvr</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Sun, YJ. Wu, Q. Cai, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Sun R, Wu YJ, Cai Q (2018) The effect of a virtual reality learning environment on learners’ spatial ability. " /><p class="c-article-references__text" id="ref-CR83">Sun R, Wu YJ, Cai Q (2018) The effect of a virtual reality learning environment on learners’ spatial ability. Virtual Real. <a href="https://doi.org/10.1007/s10055-018-0355-2">https://doi.org/10.1007/s10055-018-0355-2</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-018-0355-2" aria-label="View reference 83">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 83 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20effect%20of%20a%20virtual%20reality%20learning%20environment%20on%20learners%E2%80%99%20spatial%20ability&amp;journal=Virtual%20Real&amp;doi=10.1007%2Fs10055-018-0355-2&amp;publication_year=2018&amp;author=Sun%2CR&amp;author=Wu%2CYJ&amp;author=Cai%2CQ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Sun, W. Hu, D. Xu, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Sun C, Hu W, Xu D (2019) Navigation modes, operation methods, observation scales and background options in UI " /><p class="c-article-references__text" id="ref-CR84">Sun C, Hu W, Xu D (2019) Navigation modes, operation methods, observation scales and background options in UI design for high learning performance in VR-based architectural applications. J Comput Des Eng 6(2):189–196</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 84 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Navigation%20modes%2C%20operation%20methods%2C%20observation%20scales%20and%20background%20options%20in%20UI%20design%20for%20high%20learning%20performance%20in%20VR-based%20architectural%20applications&amp;journal=J%20Comput%20Des%20Eng&amp;volume=6&amp;issue=2&amp;pages=189-196&amp;publication_year=2019&amp;author=Sun%2CC&amp;author=Hu%2CW&amp;author=Xu%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tcha-Tokey K, Loup-Escande E, Christmann O, Richir S (2017) Effects on user experience in an edutainment virtu" /><p class="c-article-references__text" id="ref-CR85">Tcha-Tokey K, Loup-Escande E, Christmann O, Richir S (2017) Effects on user experience in an edutainment virtual environment</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Thalmic Labs (2019) Myo armband. Retrieved 15 November 2019, from https://support.getmyo.com/hc/en-us/articles" /><p class="c-article-references__text" id="ref-CR87">Thalmic Labs (2019) Myo armband. Retrieved 15 November 2019, from <a href="https://support.getmyo.com/hc/en-us/articles/203398347-Getting-started-with-your-Myo-armband">https://support.getmyo.com/hc/en-us/articles/203398347-Getting-started-with-your-Myo-armband</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="T. Tullis, B. Albert, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Tullis T, Albert B (2013) Measuring the user experience: collecting, analyzing, and presenting usability metri" /><p class="c-article-references__text" id="ref-CR88">Tullis T, Albert B (2013) Measuring the user experience: collecting, analyzing, and presenting usability metrics, 2nd edn. Morgan Kaufmann, Los Altos</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 87 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20the%20user%20experience%3A%20collecting%2C%20analyzing%2C%20and%20presenting%20usability%20metrics&amp;publication_year=2013&amp;author=Tullis%2CT&amp;author=Albert%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Ullmer, H. Ishii, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Ullmer B, Ishii H (2010) Emerging frameworks for tangible user interfaces. IBM Syst J 39:915–931" /><p class="c-article-references__text" id="ref-CR89">Ullmer B, Ishii H (2010) Emerging frameworks for tangible user interfaces. IBM Syst J 39:915–931</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1147%2Fsj.393.0915" aria-label="View reference 88">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 88 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emerging%20frameworks%20for%20tangible%20user%20interfaces&amp;journal=IBM%20Syst%20J&amp;volume=39&amp;pages=915-931&amp;publication_year=2010&amp;author=Ullmer%2CB&amp;author=Ishii%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Uloziene, M. Totiliene, A. Paulauskas, T. Blažauskas, V. Marozas, D. Kaski, V. Ulozas, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Uloziene I, Totiliene M, Paulauskas A, Blažauskas T, Marozas V, Kaski D, Ulozas V (2017) Subjective visual ver" /><p class="c-article-references__text" id="ref-CR90">Uloziene I, Totiliene M, Paulauskas A, Blažauskas T, Marozas V, Kaski D, Ulozas V (2017) Subjective visual vertical assessment with mobile virtual reality system. Medicina (Lithuania) 53(6):394–402</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 89 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Subjective%20visual%20vertical%20assessment%20with%20mobile%20virtual%20reality%20system&amp;journal=Medicina%20%28Lithuania%29&amp;volume=53&amp;issue=6&amp;pages=394-402&amp;publication_year=2017&amp;author=Uloziene%2CI&amp;author=Totiliene%2CM&amp;author=Paulauskas%2CA&amp;author=Bla%C5%BEauskas%2CT&amp;author=Marozas%2CV&amp;author=Kaski%2CD&amp;author=Ulozas%2CV">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Unity Technologies (2019) Unity3D. Retrieved 15 November 2019, from https://unity3d.com&#xA;" /><p class="c-article-references__text" id="ref-CR86">Unity Technologies (2019) Unity3D. Retrieved 15 November 2019, from <a href="https://unity3d.com">https://unity3d.com</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="UploadVR (2019a) Oculus Quest review. Retrieved 15 November 2019, from https://uploadvr.com/oculus-quest-revie" /><p class="c-article-references__text" id="ref-CR91">UploadVR (2019a) Oculus Quest review. Retrieved 15 November 2019, from <a href="https://uploadvr.com/oculus-quest-review/">https://uploadvr.com/oculus-quest-review/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="UploadVR (2019b) OC6: Oculus Quest is getting camera-based finger tracking early next year. Retrieved 15 Novem" /><p class="c-article-references__text" id="ref-CR92">UploadVR (2019b) OC6: Oculus Quest is getting camera-based finger tracking early next year. Retrieved 15 November 2019, from <a href="https://uploadvr.com/oculus-quest-finger-tracking/">https://uploadvr.com/oculus-quest-finger-tracking/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="UploadVR (2019c) Oculus Rift S is official. Retrieved 15 November 2019, from https://uploadvr.com/oculus-rift-" /><p class="c-article-references__text" id="ref-CR93">UploadVR (2019c) Oculus Rift S is official. Retrieved 15 November 2019, from <a href="https://uploadvr.com/oculus-rift-s-official/">https://uploadvr.com/oculus-rift-s-official/</a>
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Vosinakis, P. Koutsabasis, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Vosinakis S, Koutsabasis P (2018) Evaluation of visual feedback techniques for virtual grasping with bare hand" /><p class="c-article-references__text" id="ref-CR94">Vosinakis S, Koutsabasis P (2018) Evaluation of visual feedback techniques for virtual grasping with bare hands using Leap Motion and Oculus Rift. Virtual Real 22(1):47–62</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-017-0313-4" aria-label="View reference 94">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 94 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20visual%20feedback%20techniques%20for%20virtual%20grasping%20with%20bare%20hands%20using%20Leap%20Motion%20and%20Oculus%20Rift&amp;journal=Virtual%20Real&amp;volume=22&amp;issue=1&amp;pages=47-62&amp;publication_year=2018&amp;author=Vosinakis%2CS&amp;author=Koutsabasis%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Vrellis I, Moutsioulis A, Mikropoulos TA (2014) Primary school students’ attitude towards gesture based intera" /><p class="c-article-references__text" id="ref-CR95">Vrellis I, Moutsioulis A, Mikropoulos TA (2014) Primary school students’ attitude towards gesture based interaction: a comparison between Microsoft Kinect and mouse. In: Proceedings—IEEE 14th international conference on advanced learning technologies, ICALT 2014, pp 678–682</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift® DK2 and Samsung Gear VR®. In: 2017 ASEE an" /><p class="c-article-references__text" id="ref-CR96">Webster R, Dues J (2017) System usability scale (SUS): Oculus Rift® DK2 and Samsung Gear VR®. In: 2017 ASEE annual conference &amp; exposition, ASEE conferences, Columbus, Ohio</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BG. Witmer, MJ. Singer, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Witmer BG, Singer MJ (1998) Measuring presence in virtual environments: a presence questionnaire. Presence Tel" /><p class="c-article-references__text" id="ref-CR97">Witmer BG, Singer MJ (1998) Measuring presence in virtual environments: a presence questionnaire. Presence Teleoper Virtual Environ 7(3):225–240</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474698565686" aria-label="View reference 97">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 97 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20presence%20in%20virtual%20environments%3A%20a%20presence%20questionnaire&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=7&amp;issue=3&amp;pages=225-240&amp;publication_year=1998&amp;author=Witmer%2CBG&amp;author=Singer%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BG. Witmer, CJ. Jerome, MJ. Singer, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper V" /><p class="c-article-references__text" id="ref-CR98">Witmer BG, Jerome CJ, Singer MJ (2005) The factor structure of the presence questionnaire. Presence Teleoper Virtual Environ 14:298–312</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474605323384654" aria-label="View reference 98">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 98 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20factor%20structure%20of%20the%20presence%20questionnaire&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=14&amp;pages=298-312&amp;publication_year=2005&amp;author=Witmer%2CBG&amp;author=Jerome%2CCJ&amp;author=Singer%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JO. Wobbrock, HH. Aung, B. Rothrock, BA. Myers, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="&#xA;Wobbrock JO, Aung HH, Rothrock B, Myers BA (2005) Maximizing the guessability of symbolic input. In: CHI ’05 " /><p class="c-article-references__text" id="ref-CR99">
Wobbrock JO, Aung HH, Rothrock B, Myers BA (2005) Maximizing the guessability of symbolic input. In: CHI ’05 extended abstracts on human factors in computing systems. ACM, New York, pp 1869–1872</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 99 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Maximizing%20the%20guessability%20of%20symbolic%20input&amp;pages=1869-1872&amp;publication_year=2005&amp;author=Wobbrock%2CJO&amp;author=Aung%2CHH&amp;author=Rothrock%2CB&amp;author=Myers%2CBA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Yu, R. Zhou, H. Wang, W. Zhao, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Yu M, Zhou R, Wang H, Zhao W (2019) An evaluation for VR glasses system user experience: the influence factors" /><p class="c-article-references__text" id="ref-CR100">Yu M, Zhou R, Wang H, Zhao W (2019) An evaluation for VR glasses system user experience: the influence factors of interactive operation and motion sickness. Appl Ergon 74:206–213</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.apergo.2018.08.012" aria-label="View reference 100">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 100 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20evaluation%20for%20VR%20glasses%20system%20user%20experience%3A%20the%20influence%20factors%20of%20interactive%20operation%20and%20motion%20sickness&amp;journal=Appl%20Ergon&amp;volume=74&amp;pages=206-213&amp;publication_year=2019&amp;author=Yu%2CM&amp;author=Zhou%2CR&amp;author=Wang%2CH&amp;author=Zhao%2CW">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00409-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Engineering for Innovation, University of Salento, Lecce, Italy</p><p class="c-article-author-affiliation__authors-list">Lucio Tommaso De Paolis &amp; Valerio De Luca</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Lucio_Tommaso-De_Paolis"><span class="c-article-authors-search__title u-h3 js-search-name">Lucio Tommaso De Paolis</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Lucio Tommaso+De Paolis&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Lucio Tommaso+De Paolis" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Lucio Tommaso+De Paolis%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Valerio-De_Luca"><span class="c-article-authors-search__title u-h3 js-search-name">Valerio De Luca</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Valerio+De Luca&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Valerio+De Luca" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Valerio+De Luca%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00409-6/email/correspondent/c1/new">Valerio De Luca</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=The%20impact%20of%20the%20input%20interface%20in%20a%20virtual%20environment%3A%20the%20Vive%20controller%20and%20the%20Myo%20armband&amp;author=Lucio%20Tommaso%20De%20Paolis%20et%20al&amp;contentID=10.1007%2Fs10055-019-00409-6&amp;publication=1359-4338&amp;publicationDate=2019-11-21&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00409-6" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00409-6" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">De Paolis, L.T., De Luca, V. The impact of the input interface in a virtual environment: the Vive controller and the Myo armband.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00409-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00409-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-07-18">18 July 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-11-07">07 November 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-11-21">21 November 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00409-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00409-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Touchless interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gesture</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User experience</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Usability</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Presence</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual environment</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00409-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=409;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

