<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A usability study of multimodal input in an augmented reality environm"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="
In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A usability study of multimodal input in an augmented reality environment"/>

    <meta name="dc.source" content="Virtual Reality 2013 17:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-09-21"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="
In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech input in a natural way using a multimodal fusion architecture. We describe the system architecture and present a study exploring the usability of the AR MMI compared with speech-only and 3D-hand-gesture-only interaction conditions. The interface was used in an AR application for selecting 3D virtual objects and changing their shape and color. For each interface condition, we measured task completion time, the number of user and system errors, and user satisfactions. We found that the MMI was more usable than the gesture-only interface conditions, and users felt that the MMI was more satisfying to use than the speech-only interface conditions; however, it was neither more effective nor more efficient than the speech-only interface. We discuss the implications of this research for designing AR MMI and outline directions for future work. The findings could also be used to help develop MMIs for a wider range of AR applications, for example, in AR navigation tasks, mobile AR interfaces, or AR game applications."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-09-21"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="293"/>

    <meta name="prism.endingPage" content="305"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0230-0"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0230-0"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0230-0.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0230-0"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A usability study of multimodal input in an augmented reality environment"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2013/11"/>

    <meta name="citation_online_date" content="2013/09/21"/>

    <meta name="citation_firstpage" content="293"/>

    <meta name="citation_lastpage" content="305"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0230-0"/>

    <meta name="DOI" content="10.1007/s10055-013-0230-0"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0230-0"/>

    <meta name="description" content="
In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that"/>

    <meta name="dc.creator" content="Minkyung Lee"/>

    <meta name="dc.creator" content="Mark Billinghurst"/>

    <meta name="dc.creator" content="Woonhyuk Baek"/>

    <meta name="dc.creator" content="Richard Green"/>

    <meta name="dc.creator" content="Woontack Woo"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Presence: Teleoperators Virtual Environ; citation_title=A survey of augmented reality; citation_author=RT Azuma; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=355-385; citation_id=CR1"/>

    <meta name="citation_reference" content="citation_journal_title=Softw Qual J; citation_title=Measuring usability as quality of use; citation_author=N Bevan; citation_volume=4; citation_publication_date=1995; citation_pages=111-150; citation_doi=10.1007/BF00402715; citation_id=CR2"/>

    <meta name="citation_reference" content="
Bolt RA (1980) Put-that-there: voice and gesture at the graphics interface. Proc Annu Conf Comput Graph Interact Tech 14(3):262&#8211;270"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Graph Image Process; citation_title=Distance transformations in digital images; citation_author=G Borgefors; citation_volume=34; citation_publication_date=1986; citation_pages=344-371; citation_doi=10.1016/S0734-189X(86)80047-0; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE TENCONO&#8217;00; citation_title=A Bayesian approach to skin color classification in YCbCr color space; citation_author=D Chai, A Bouzerdoum; citation_volume=2; citation_publication_date=2000; citation_pages=421-424; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE Int Conf Robot Automat; citation_title=Multimodal interface for a virtual reality based computer aided design system; citation_author=CP Chu, TH Dani, R Gadh; citation_volume=2; citation_publication_date=1997; citation_pages=1329-1334; citation_doi=10.1109/ROBOT.1997.614321; citation_id=CR6"/>

    <meta name="citation_reference" content="Cohen PR, Sullivan JW (1989) Synergistic user of direct manipulation and natural language. In: CHI&#39;89 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 227&#8211;233"/>

    <meta name="citation_reference" content="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for distributed applications. In: Proceedings of the fifth ACM international conference on multimedia. ACM Press, New York, pp 31&#8211;40"/>

    <meta name="citation_reference" content="Fels S, Hinton G (1995) Glove-TalkII: an adaptive gesture-to-formant interface. In: CHI&#39;95 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 456&#8211;463"/>

    <meta name="citation_reference" content="citation_journal_title=CHI Conf Proc; citation_title=Measuring usability: are effectiveness, efficiency, and satisfaction really correlated?; citation_author=E Fr&#248;kj&#230;r, M Hertzum, K Hornb&#230;k; citation_volume=2; citation_issue=1; citation_publication_date=2000; citation_pages=345-352; citation_id=CR10"/>

    <meta name="citation_reference" content="
Hartley R,  Zisserman A (2004) Multiple view geometry in computer vision. Cambridge University Press, Cambridge"/>

    <meta name="citation_reference" content="Hauptmann AG (1989) Speech and gestures for graphic image manipulation. CHI Conf Proc 241&#8211;245"/>

    <meta name="citation_reference" content="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI&#8217;04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53&#8211;60"/>

    <meta name="citation_reference" content="Irawati S, Green S, Billinghurst M, Duenser A, Ko H (2006a) Move the couch where? Developing an augmented reality multimodal interface. ICAT: 1&#8211;4"/>

    <meta name="citation_reference" content="Irawati S, Green S, Billinghurst M, Duenser A,  Ko H (2006b) An evaluation of an augmented reality multimodal interface using speech and paddle gestures. In: Advances in artificial reality and tele-existence, Lecture notes in computer science, vol 4282. pp 272&#8211;283"/>

    <meta name="citation_reference" content="LaViola Jr. JJ (1999) A multimodal interface framework for using hand gestures and speech in virtual environment applications. Gesture-Based Commun Hum Comp Interact 303&#8211;341"/>

    <meta name="citation_reference" content="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Cohen P, Feiner S (2003) Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. Proceedings of international conference on multimodal interfaces 12&#8211;19"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality (ISAR 2000). Munich, Germany, pp 111&#8211;119"/>

    <meta name="citation_reference" content="K&#246;lsch M, Turk M,  Tobias H (2004) Vision-based interfaces for mobility. Proc MobiQuitous&#8217;04 86&#8211;94"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Multimodal interaction with a wearable augmented reality system; citation_author=M K&#246;lsch, M Turk, H Tobias; citation_volume=26; citation_issue=3; citation_publication_date=2006; citation_pages=62-71; citation_doi=10.1109/MCG.2006.66; citation_id=CR21"/>

    <meta name="citation_reference" content="Koons DB, Sparrell CJ (1994) ICONIC: speech and depictive gestures at the human-machine interface. In: CHI&#39;94 Conference companion on human factors in computing systems. ACM, New York, pp 453&#8211;454"/>

    <meta name="citation_reference" content="Krum DM, Omotesto O, Ribarsky W, Starner T,  Hodges LF (2002) Speech and gesture control of a whole earth 3D visualization environment. Proc Jt Eurograph-IEEE TCVG Symp Vis 195&#8211;200"/>

    <meta name="citation_reference" content="citation_journal_title=AFRIGRAPH; citation_title=A gesture processing framework for multimodal interaction in virtual reality; citation_author=ME Latoschik; citation_volume=2001; citation_publication_date=2001; citation_pages=95-100; citation_doi=10.1145/513867.513888; citation_id=CR24"/>

    <meta name="citation_reference" content="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249&#8211;256"/>

    <meta name="citation_reference" content="Lucente M, Zwart GJ, George AD (1998) Visualization space: a testbed for deviceless multimodal user interface. Proc AAAI Spring Symp Intell Environ. AAAI TR SS-98-02"/>

    <meta name="citation_reference" content="Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimodal augmented reality system. Proc Int Symp Mix Augment Real 300&#8211;301"/>

    <meta name="citation_reference" content="Oviatt S, Coulson R,  Lunsford R (2004) When Do We Interact Multimodally? Cognitive load and multimodal communication patterns. Proc Int Conf Multimod Interfaces 129&#8211;136"/>

    <meta name="citation_reference" content="Point Grey Research Inc (2009) 
                    http://www.ptgrey.com/products/stereo.asp
                    
                  . Accessed 20 Nov 2009 [26]"/>

    <meta name="citation_reference" content="citation_journal_title=TOCHI; citation_title=Multimodal human discourse: gesture and speech; citation_author=F Quek, D McNeil, R Bryll, S Duncan, X Ma, C Kirbas, KE McCullough, R Ansari; citation_volume=9; citation_issue=3; citation_publication_date=2002; citation_pages=171-193; citation_doi=10.1145/568513.568514; citation_id=CR32"/>

    <meta name="citation_reference" content="Rauschert I, Agrawal P, Sharmar R, Fuhrmann S, Brewer I, MacEachren A, Wang H,  Cai G (2002) Designing a human-centered, multimodal GIS interface to support emergency management. Proc Geogrc Inf Syst 119&#8211;124"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=The limits of speech recognition; citation_author=B Shneiderman; citation_volume=43; citation_issue=9; citation_publication_date=2000; citation_pages=63-65; citation_doi=10.1145/348941.348990; citation_id=CR34"/>

    <meta name="citation_reference" content="Tse E, Greenberg S,  Shen C (2006) GSI DEMO: Multiuser gesture/speech interaction over digital tables by wrapping single user applications. Proc Int Conf Multimod Interfaces 76&#8211;83"/>

    <meta name="citation_reference" content="Weimer D, Genapathy SK (1989) A synthetic visual environment with hand gesturing and voice input. Proc Conf Hum Factors Comput Syst 235&#8211;240"/>

    <meta name="citation_reference" content="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 1330&#8211;1334"/>

    <meta name="citation_author" content="Minkyung Lee"/>

    <meta name="citation_author_email" content="ming2279@gmail.com"/>

    <meta name="citation_author_institution" content="Technology Strategy Office, R&amp;D Center, KT, Seoul, Korea"/>

    <meta name="citation_author" content="Mark Billinghurst"/>

    <meta name="citation_author_email" content="mark.billinghurst@hitlabnz.org"/>

    <meta name="citation_author_institution" content="The HIT Lab NZ, University of Canterbury, Christchurch, New Zealand"/>

    <meta name="citation_author" content="Woonhyuk Baek"/>

    <meta name="citation_author_email" content="wbaek@daumcorp.com"/>

    <meta name="citation_author_institution" content="Multimedia Research Team, Daum Communications, Jeju-do, Korea"/>

    <meta name="citation_author" content="Richard Green"/>

    <meta name="citation_author_email" content="richard.green@canterbury.ac.nz"/>

    <meta name="citation_author_institution" content="Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand"/>

    <meta name="citation_author" content="Woontack Woo"/>

    <meta name="citation_author_email" content="wwoo@kaist.ac.kr"/>

    <meta name="citation_author_institution" content="GSCT UVR Lab, KAIST, Daejeon, Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0230-0&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0230-0"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A usability study of multimodal input in an augmented reality environment"/>
        <meta property="og:description" content="In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech input in a natural way using a multimodal fusion architecture. We describe the system architecture and present a study exploring the usability of the AR MMI compared with speech-only and 3D-hand-gesture-only interaction conditions. The interface was used in an AR application for selecting 3D virtual objects and changing their shape and color. For each interface condition, we measured task completion time, the number of user and system errors, and user satisfactions. We found that the MMI was more usable than the gesture-only interface conditions, and users felt that the MMI was more satisfying to use than the speech-only interface conditions; however, it was neither more effective nor more efficient than the speech-only interface. We discuss the implications of this research for designing AR MMI and outline directions for future work. The findings could also be used to help develop MMIs for a wider range of AR applications, for example, in AR navigation tasks, mobile AR interfaces, or AR game applications."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A usability study of multimodal input in an augmented reality environment | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0230-0","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Multimodal interface, Augmented reality, Usability, Efficiency, Effectiveness, Satisfaction","kwrd":["Multimodal_interface","Augmented_reality","Usability","Efficiency","Effectiveness","Satisfaction"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0230-0","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0230-0","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=230;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0230-0">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A usability study of multimodal input in an augmented reality environment
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0230-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0230-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-09-21" itemprop="datePublished">21 September 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A usability study of multimodal input in an augmented reality environment</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Minkyung-Lee" data-author-popup="auth-Minkyung-Lee" data-corresp-id="c1">Minkyung Lee<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="KT" /><meta itemprop="address" content="grid.471201.1, 0000 0004 1798 5379, Technology Strategy Office, R&amp;D Center, KT,  17 Woomyeon-dong, SeoCho-gu, Seoul, 137-792, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Mark-Billinghurst" data-author-popup="auth-Mark-Billinghurst">Mark Billinghurst</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canterbury" /><meta itemprop="address" content="grid.21006.35, 0000000121791970, The HIT Lab NZ, University of Canterbury, Private Bag 4800, Ilam, Christchurch, 8140, New Zealand" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Woonhyuk-Baek" data-author-popup="auth-Woonhyuk-Baek">Woonhyuk Baek</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Daum Communications" /><meta itemprop="address" content="Multimedia Research Team, Daum Communications, 2181 Yeongpyeong-dong, Jeju-si, Jeju-do, 690-140, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Richard-Green" data-author-popup="auth-Richard-Green">Richard Green</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Canterbury" /><meta itemprop="address" content="grid.21006.35, 0000000121791970, Computer Science and Software Engineering, University of Canterbury, Private Bag 4800, Ilam, Christchurch, 8140, New Zealand" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Woontack-Woo" data-author-popup="auth-Woontack-Woo">Woontack Woo</a></span><sup class="u-js-hide"><a href="#Aff5">5</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="KAIST" /><meta itemprop="address" content="grid.37172.30, 0000000122920500, GSCT UVR Lab, KAIST, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">293</span>–<span itemprop="pageEnd">305</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1554 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">27 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">0 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0230-0/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>
In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech input in a natural way using a multimodal fusion architecture. We describe the system architecture and present a study exploring the usability of the AR MMI compared with speech-only and 3D-hand-gesture-only interaction conditions. The interface was used in an AR application for selecting 3D virtual objects and changing their shape and color. For each interface condition, we measured task completion time, the number of user and system errors, and user satisfactions. We found that the MMI was more usable than the gesture-only interface conditions, and users felt that the MMI was more satisfying to use than the speech-only interface conditions; however, it was neither more effective nor more efficient than the speech-only interface. We discuss the implications of this research for designing AR MMI and outline directions for future work. The findings could also be used to help develop MMIs for a wider range of AR applications, for example, in AR navigation tasks, mobile AR interfaces, or AR game applications.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Augmented reality (AR) is a technology that overlays computer-generated information onto the real world (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environ 6(4):355–385" href="/article/10.1007/s10055-013-0230-0#ref-CR1" id="ref-link-section-d46149e453">1997</a>). AR systems can provide users with information-enhanced environments that seamlessly connect real and virtual worlds. To achieve this, accurate tracking and registration methods are used for aligning real and virtual objects. In addition, natural interaction techniques for manipulating the AR content should also be provided.</p><p>In the field of human–computer interaction, previous research has shown that a multimodal interface (MMI) can provide very natural interaction by allowing a person to simultaneously use two or more input channels, for example, combining speech input with pen gestures to create an intuitive command-and-control application (Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for distributed applications. In: Proceedings of the fifth ACM international conference on multimedia. ACM Press, New York, pp 31–40" href="/article/10.1007/s10055-013-0230-0#ref-CR8" id="ref-link-section-d46149e459">1997</a>). This is because in human communication, gestures and speech are co-expressive; they arise from a single shared semantic source but are able to express different but complimentary information (Quek et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Quek F, McNeil D, Bryll R, Duncan S, Ma X, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discourse: gesture and speech. TOCHI 9(3):171–193" href="/article/10.1007/s10055-013-0230-0#ref-CR32" id="ref-link-section-d46149e462">2002</a>). In the past, MMIs have been used not only for 2D user interfaces but also in virtual reality (VR) applications for interacting with 3D virtual content (Chu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Chu CP, Dani TH, Gadh R (1997) Multimodal interface for a virtual reality based computer aided design system. Proc IEEE Int Conf Robot Automat 2:1329–1334" href="/article/10.1007/s10055-013-0230-0#ref-CR6" id="ref-link-section-d46149e465">1997</a>) or to navigate through virtual worlds (Krum et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Krum DM, Omotesto O, Ribarsky W, Starner T,  Hodges LF (2002) Speech and gesture control of a whole earth 3D visualization environment. Proc Jt Eurograph-IEEE TCVG Symp Vis 195–200" href="/article/10.1007/s10055-013-0230-0#ref-CR23" id="ref-link-section-d46149e468">2002</a>). Based on this work, MMIs should also be an ideal interaction technique for AR applications.</p><p>In our research, we have been exploring the usability of MMI for AR applications. We have built a simple AR application and a MMI which combines computer-vision-based natural hand gesture input and speech input with a multimodal fusion architecture that merges the two different input modalities in a natural way. This MMI system is tested in an AR application and compared with speech-only and 3D-hand-gesture-only conditions. This comparison is done in order to verify the usability of the MMI compared to unimodal interfaces.</p><p>There have been previous AR MMI applications using paddle input (Irawati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Irawati S, Green S, Billinghurst M, Duenser A,  Ko H (2006b) An evaluation of an augmented reality multimodal interface using speech and paddle gestures. In: Advances in artificial reality and tele-existence, Lecture notes in computer science, vol 4282. pp 272–283" href="/article/10.1007/s10055-013-0230-0#ref-CR16" id="ref-link-section-d46149e476">2006b</a>) or hand gestures with data gloves and speech (Olwal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimodal augmented reality system. Proc Int Symp Mix Augment Real 300–301" href="/article/10.1007/s10055-013-0230-0#ref-CR29" id="ref-link-section-d46149e479">2003</a>); however, these require additional input devices. Using a natural, free-hand gesture input with speech commands should provide users with a very intuitive way to interact in AR environments. For example, Kölsch et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kölsch M, Turk M, Tobias H (2006) Multimodal interaction with a wearable augmented reality system. IEEE Comput Graph Appl 26(3):62–71" href="/article/10.1007/s10055-013-0230-0#ref-CR21" id="ref-link-section-d46149e482">2006</a>) and Heidemann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53–60" href="/article/10.1007/s10055-013-0230-0#ref-CR14" id="ref-link-section-d46149e485">2004</a>) used bare hand interaction with speech input for their AR applications. However, Kölsch et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kölsch M, Turk M, Tobias H (2006) Multimodal interaction with a wearable augmented reality system. IEEE Comput Graph Appl 26(3):62–71" href="/article/10.1007/s10055-013-0230-0#ref-CR21" id="ref-link-section-d46149e488">2006</a>) did not conduct a user study to evaluate their MMI, and although Heidemann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53–60" href="/article/10.1007/s10055-013-0230-0#ref-CR14" id="ref-link-section-d46149e492">2004</a>) evaluated the effectiveness of their MMI, they did not explore the full usability of their system.</p><p>The focus of our research is on the usability of an MMI for AR interfaces. Usability is defined by Bevan (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bevan N (1995) Measuring usability as quality of use. Softw Qual J 4:111–150" href="/article/10.1007/s10055-013-0230-0#ref-CR2" id="ref-link-section-d46149e499">1995</a>) as “quality in use” and has three key aspects: effectiveness (accuracy and completeness), efficiency (use of time and resources), and satisfaction (preferences). It is important to account for all three aspects because a subset is often insufficient as an indicator of overall usability (Frøkjær et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Frøkjær E, Hertzum M, Hornbæk K (2000) Measuring usability: are effectiveness, efficiency, and satisfaction really correlated? CHI Conf Proc 2(1):345–352" href="/article/10.1007/s10055-013-0230-0#ref-CR10" id="ref-link-section-d46149e502">2000</a>). Thus, in our work we aim to evaluate the effectiveness, efficiency, and user satisfaction of people interacting with our AR multimodal interface.</p><p>MMIs allow users to interact naturally and intuitively because human-to-human interaction is multimodal. As a result, MMI should provide a more efficient and effective user experience than a unimodal interface. Following Bevan’s definition of usability, a MMI could provide a better quality of user experience, but may have other possible problems. For example, users may need to spend more time getting used to the multimodal interface because it includes several unimodal interface methods that must be learned. MMIs may also create greater mental workload because the user has to pay attention to pick an appropriate interface type for a given task. Additionally, Shneiderman’s research (Shneiderman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Shneiderman B (2000) The limits of speech recognition. Commun ACM 43(9):63–65" href="/article/10.1007/s10055-013-0230-0#ref-CR34" id="ref-link-section-d46149e508">2000</a>) on the limits of speech recognition showed that the human brain processes problem solving and speaking/listening transiently; however, it processes problem solving and physical activity in parallel. This means that using speech input in human–computer interaction can interfere significantly with other cognitive tasks. In our research we wanted to explore how users feel about the problematic aspects of the MMI, such as physical demands, mental demands, and frustrations, and others.</p><p>In the rest of the paper, we will first present earlier related work (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0230-0#Sec2">2</a>), and then we describe our system and experimental setup (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0230-0#Sec3">3</a>). Next, we present a pilot study to test the implemented AR interface and identify areas for improvement (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0230-0#Sec7">4</a>). We conduct a full user study with the improved AR MMI (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0230-0#Sec8">5</a>) and finally conclude the paper with directions for future research.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>The first research on multimodal interfaces with gesture and speech input dates back to the Bolt’s “Put-that-there” work (Bolt <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="&#xA;Bolt RA (1980) Put-that-there: voice and gesture at the graphics interface. Proc Annu Conf Comput Graph Interact Tech 14(3):262–270" href="/article/10.1007/s10055-013-0230-0#ref-CR3" id="ref-link-section-d46149e534">1980</a>). In this case a user was able to use pointing gestures and speech to manipulate 2D icons in a command-and-control application. Many multimodal interfaces after Bolt’s work were map- or screen-based 2D applications (Tse et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Tse E, Greenberg S,  Shen C (2006) GSI DEMO: Multiuser gesture/speech interaction over digital tables by wrapping single user applications. Proc Int Conf Multimod Interfaces 76–83" href="/article/10.1007/s10055-013-0230-0#ref-CR35" id="ref-link-section-d46149e537">2006</a>; Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for distributed applications. In: Proceedings of the fifth ACM international conference on multimedia. ACM Press, New York, pp 31–40" href="/article/10.1007/s10055-013-0230-0#ref-CR8" id="ref-link-section-d46149e540">1997</a>). For these systems, multimodal input with pen-based or touch screen interface and speech input was enough to support a gesture-based interface on a surface (2D). For example, QuickSet (Cohen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for distributed applications. In: Proceedings of the fifth ACM international conference on multimedia. ACM Press, New York, pp 31–40" href="/article/10.1007/s10055-013-0230-0#ref-CR8" id="ref-link-section-d46149e543">1997</a>) supported combined pen-based gesture and speech multimodal input in a military planning application.</p><p>Multimodal interfaces that combine speech and hand gesture input were found to be an intuitive way to interact with 2D and 3D graphics desktop applications. This is because a combination of natural language and direct manipulation can overcome the limitations of unimodal input (Cohen and Sullivan <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Cohen PR, Sullivan JW (1989) Synergistic user of direct manipulation and natural language. In: CHI'89 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 227–233" href="/article/10.1007/s10055-013-0230-0#ref-CR7" id="ref-link-section-d46149e549">1989</a>; Oviatt et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Oviatt S, Coulson R,  Lunsford R (2004) When Do We Interact Multimodally? Cognitive load and multimodal communication patterns. Proc Int Conf Multimod Interfaces 129–136" href="/article/10.1007/s10055-013-0230-0#ref-CR30" id="ref-link-section-d46149e552">2004</a>; Hauptmann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Hauptmann AG (1989) Speech and gestures for graphic image manipulation. CHI Conf Proc 241–245" href="/article/10.1007/s10055-013-0230-0#ref-CR13" id="ref-link-section-d46149e555">1989</a>). Speech and gesture have complementary attributes, and combining them provides a more natural way of interacting with applications than in traditional GUI interfaces.</p><p>The input techniques and evaluation methodologies that were developed through these studies were valuable. However, in AR applications, we need to have a gesture interface that supports natural 3D object manipulation and seamless interaction between the real and virtual elements of the AR system. Earlier researchers have used speech input with hand-tracking devices or DataGloves (Weimer and Genapathy <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Weimer D, Genapathy SK (1989) A synthetic visual environment with hand gesturing and voice input. Proc Conf Hum Factors Comput Syst 235–240" href="/article/10.1007/s10055-013-0230-0#ref-CR36" id="ref-link-section-d46149e561">1989</a>; Koons and Sparrell <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Koons DB, Sparrell CJ (1994) ICONIC: speech and depictive gestures at the human-machine interface. In: CHI'94 Conference companion on human factors in computing systems. ACM, New York, pp 453–454" href="/article/10.1007/s10055-013-0230-0#ref-CR22" id="ref-link-section-d46149e564">1994</a>; LaViola <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola Jr. JJ (1999) A multimodal interface framework for using hand gestures and speech in virtual environment applications. Gesture-Based Commun Hum Comp Interact 303–341" href="/article/10.1007/s10055-013-0230-0#ref-CR17" id="ref-link-section-d46149e567">1999</a>; Latoschik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. AFRIGRAPH 2001:95–100" href="/article/10.1007/s10055-013-0230-0#ref-CR24" id="ref-link-section-d46149e570">2001</a>; Olwal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimodal augmented reality system. Proc Int Symp Mix Augment Real 300–301" href="/article/10.1007/s10055-013-0230-0#ref-CR29" id="ref-link-section-d46149e573">2003</a>) to explore multimodal input in 3D graphics environments. For example, Koons and Sparrell (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Koons DB, Sparrell CJ (1994) ICONIC: speech and depictive gestures at the human-machine interface. In: CHI'94 Conference companion on human factors in computing systems. ACM, New York, pp 453–454" href="/article/10.1007/s10055-013-0230-0#ref-CR22" id="ref-link-section-d46149e577">1994</a>) combined two-handed DataGlove gestures with speech input to allow users to arrange 3D objects in a virtual scene. LaViola (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="LaViola Jr. JJ (1999) A multimodal interface framework for using hand gestures and speech in virtual environment applications. Gesture-Based Commun Hum Comp Interact 303–341" href="/article/10.1007/s10055-013-0230-0#ref-CR17" id="ref-link-section-d46149e580">1999</a>) also developed multimodal interfaces using 3D hand gestures and speech for virtual reality applications. Although this research provides a natural way for people to use their hands to interact with 3D virtual objects, they had to wear encumbering data gloves and a number of tracking devices. This reduces the naturalness of the hand gesture interfaces made.</p><p>More recently, computer-vision-based hand-tracking techniques have been used in systems such as “VisSpace” (Lucente et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Lucente M, Zwart GJ, George AD (1998) Visualization space: a testbed for deviceless multimodal user interface. Proc AAAI Spring Symp Intell Environ. AAAI TR SS-98-02" href="/article/10.1007/s10055-013-0230-0#ref-CR26" id="ref-link-section-d46149e586">1998</a>) to estimate where users were pointing (Rauschert et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Rauschert I, Agrawal P, Sharmar R, Fuhrmann S, Brewer I, MacEachren A, Wang H,  Cai G (2002) Designing a human-centered, multimodal GIS interface to support emergency management. Proc Geogrc Inf Syst 119–124" href="/article/10.1007/s10055-013-0230-0#ref-CR33" id="ref-link-section-d46149e589">2002</a>). This overcomes the disadvantages of using DataGloves for capturing hand gesture input; however, these systems did not support natural manipulation of 3D objects as they were only concerned with where users were pointing.</p><p>There has been relatively little research in AR multimodal interfaces. Heidemann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53–60" href="/article/10.1007/s10055-013-0230-0#ref-CR14" id="ref-link-section-d46149e596">2004</a>) implemented an AR MMI to acquire visual knowledge and retrieve memorized objects online. They used 3D natural hand gestures and speech input. The speech input was used to select interface menu options, and 3D pointing gestures were used to select the object to be memorized. However, the main interaction was navigating the menu in 2D, and speech was used to select the menu items that the user wanted, in the same way that a mouse does. Thus, their AR MMI did not involve any 3D object manipulation.</p><p>Kölsch et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kölsch M, Turk M, Tobias H (2006) Multimodal interaction with a wearable augmented reality system. IEEE Comput Graph Appl 26(3):62–71" href="/article/10.1007/s10055-013-0230-0#ref-CR21" id="ref-link-section-d46149e602">2006</a>) developed a multimodal information visualization system with 2D natural hand gesture, speech, and trackball input in a wearable AR environment. They used HandVu (Kölsch et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Kölsch M, Turk M,  Tobias H (2004) Vision-based interfaces for mobility. Proc MobiQuitous’04 86–94" href="/article/10.1007/s10055-013-0230-0#ref-CR20" id="ref-link-section-d46149e605">2004</a>) to recognize the users’ hand gestures in 2D from texture and color; however, their MMI could not be used to manipulate virtual objects in 3D spaces. In addition, they did not evaluate the system’s usability.</p><p>Olwal’s SenseShape (Olwal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimodal augmented reality system. Proc Int Symp Mix Augment Real 300–301" href="/article/10.1007/s10055-013-0230-0#ref-CR29" id="ref-link-section-d46149e611">2003</a>) and its later extended version by Kaiser et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Cohen P, Feiner S (2003) Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. Proceedings of international conference on multimodal interfaces 12–19" href="/article/10.1007/s10055-013-0230-0#ref-CR18" id="ref-link-section-d46149e614">2003</a>) were some of the first truly 3D AR multimodal interfaces. In this case users had to wear a data glove and trackers to give gesture commands to the system. However, the focus of their interface evaluation was on the system’s mutual disambiguation capabilities and not usability.</p><p>Irawati et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006a" title="Irawati S, Green S, Billinghurst M, Duenser A, Ko H (2006a) Move the couch where? Developing an augmented reality multimodal interface. ICAT: 1–4" href="/article/10.1007/s10055-013-0230-0#ref-CR15" id="ref-link-section-d46149e620">2006a</a>) built an AR MMI using speech and paddle gestures for the VOMAR application (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality (ISAR 2000). Munich, Germany, pp 111–119" href="/article/10.1007/s10055-013-0230-0#ref-CR19" id="ref-link-section-d46149e623">2000</a>). VOMAR allows users to create their own interior designs by arranging virtual furniture in empty rooms using marker-attached paddle gestures. Irawati et al. extended this by adding speech input and a semantic multimodal fusion framework. However, they relied on paddle-based input and not free-hand interaction.</p><p>Among the AR MMI examples, only two research projects conducted a formal user evaluation. Irawati et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006b" title="Irawati S, Green S, Billinghurst M, Duenser A,  Ko H (2006b) An evaluation of an augmented reality multimodal interface using speech and paddle gestures. In: Advances in artificial reality and tele-existence, Lecture notes in computer science, vol 4282. pp 272–283" href="/article/10.1007/s10055-013-0230-0#ref-CR16" id="ref-link-section-d46149e629">2006b</a>) evaluated the usability of their MMI across the three different interface conditions: paddle gestures-only, speech and static paddle, and speech and paddle gestures. They found that combined multimodal speech and paddle gesture input is more accurate than using unimodal input alone. However, the system could not provide a natural gesture interface for users, but required the use of a paddle with computer vision tracking patterns on it. Heidemann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53–60" href="/article/10.1007/s10055-013-0230-0#ref-CR14" id="ref-link-section-d46149e632">2004</a>) also evaluated their interface; however, their interface used only pointing gestures.</p><p>Thus, there have only been a few examples of AR MMIs, and none of them have used computer vision techniques for natural 3D hand interaction. There has also been very little evaluation of AR MMIs, especially exploring the usability of AR MMIs.</p><p>Our research is novel because it tests the usability of an AR MMI with 3D natural hand gesture and speech input. We are interested in how MMI could improve the efficiency and effectiveness of an AR interface and measure this by comparing MMI with speech-only and gesture-only input conditions. We also wanted to know how users felt using the AR MMI. There is no earlier research that uses computer vision to support natural hand input in a 3D AR environment for 3D object manipulation. Thus, we implemented AR MMI with 3D natural hand gesture and speech input. In this study, we include an initial pilot study with five users who are experienced with AR environments but have little experience with speech, gesture input, or multimodal interaction. After running the pilot study, we apply for feedback from the users to improve the application and develop a final system for testing. We then evaluate the improved MMI in a full user study with twenty users. This research will be useful for others trying to develop multimodal AR interfaces and will help lay the foundation for a significant amount of future work.</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">An augmented reality multimodal interface</h2><div class="c-article-section__content" id="Sec3-content"><p>In this section we describe the AR MMI we have developed that combines 3D stereo vision-based natural hand gesture input and speech input. We also explain the multimodal fusion architecture that merges the two different input modalities.</p><p>Our AR MMI system is made up of a number of components that are connected together. They include input components for capturing video, recognizing gesture and speech input, a fusion module for combining input modalities, an AR scene generation, and manager modules for generating the AR output and providing feedback to the user. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig1">1</a> shows the AR MMI components and how they are connected.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>The architecture of the AR MMI</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In an earlier Wizard of Oz study (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e674">2008</a>), we observed how people use natural gesture and speech input in an AR environment and how the users integrate and synchronize the two different modalities. We found that the same gestures had different meanings based on the context; that is, the meaning of a gesture is varied according to its corresponding speech command. We also found that users mostly issued gestures before the corresponding speech input, meaning that a gesture-triggered time window needs to be used to capture related commands. From the study, we found that people used three different types of gestures: (1) open hand, (2) closed hand, and (3) pointing. In the next section, we describe the computer vision techniques we have used to capture these free-hand gestures.</p><h3 class="c-article__sub-heading" id="Sec4">3D hand gesture interface</h3><p>To capture 3D hand gestures from a stereo video input, we developed a computer vision module involving five steps: (1) camera calibration (off-line), (2) skin-color segmentation, (3) fingertip detection, (4) fingertip estimation in 3D, and (5) gesture recognition. For camera input, we used a BumbleBee stereo camera (Point Grey Research Inc <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Point Grey Research Inc (2009) &#xA;                    http://www.ptgrey.com/products/stereo.asp&#xA;                    &#xA;                  . Accessed 20 Nov 2009 [26]" href="/article/10.1007/s10055-013-0230-0#ref-CR31" id="ref-link-section-d46149e684">2009</a>) which provides two 320 × 240 pixel images at 25 frames per second.</p><p>First of all, we need to have accurate camera calibration. We used Zhang’s calibration algorithm to find out the intrinsic and extrinsic parameters of the two cameras (Zhang <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 1330–1334" href="/article/10.1007/s10055-013-0230-0#ref-CR37" id="ref-link-section-d46149e690">2000</a>). The parameters from the camera calibration were not only used to reconstruct the fingertips in 3D but also to augment the virtual objects in the real environment.</p><p>To find the users’ hand in the camera image, we used a skin-color segmentation method. We used a real-time statistical model-based skin-color segmentation algorithm in our gesture interface module, specifically Chai and Bouzerdoum’s algorithm that is based on a Bayesian approach to skin-color classification in YCbCr color space (Chai and Bouzerdoum <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Chai D, Bouzerdoum A (2000) A Bayesian approach to skin color classification in YCbCr color space. Proc IEEE TENCONO’00 2:421–424" href="/article/10.1007/s10055-013-0230-0#ref-CR5" id="ref-link-section-d46149e696">2000</a>). To guarantee stable skin-color segmentation, we controlled the environment with a single-colored background. We estimate fingertip positions by (1) drawing the convex hull based on the segmented hand region, (2) applying a distance transform (Borgefors <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1986" title="Borgefors G (1986) Distance transformations in digital images. Comput Vis Graph Image Process 34:344–371" href="/article/10.1007/s10055-013-0230-0#ref-CR4" id="ref-link-section-d46149e699">1986</a>) to find out the center point of the hand (the furthest point would be the center of the hand), (3) removing the palm area to leave only segmented fingers, (4) finding the contour of each finger blob, (5) calculating the distance from points on each contour to the hand center, and (6) marking the furthest point on each finger blob as a fingertip. The algorithm we use is simple, and it works effectively with reduced computational complexity.</p><p>When the fingertip locations and camera calibration matrices are known, we can estimate the 3D position of the fingertips in real time. This is done by performing the triangulation that solves the linear equation generated from two corresponding fingertip points observed by each camera (Hartley and Zisserman <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="&#xA;Hartley R,  Zisserman A (2004) Multiple view geometry in computer vision. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-013-0230-0#ref-CR12" id="ref-link-section-d46149e705">2004</a>). We were able to track the user’s fingertip with accuracy from 4.5 up to 26.2 mm depending on the distance between the user’s hand and the cameras. This was enough to support our object manipulation tasks.</p><p>Based on our earlier Wizard of Oz study work (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e712">2008</a>), there are three gestures commonly used in AR MMI: (a) open hand, (b) closed hand, and (c) pointing. For the open hand gesture, a user stretches out their fingers from the palm. In our system this is used to select or to drop an object. The closed hand is recognized as a grabbing gesture and is used to grab a virtual object. The pointing gesture is used to identify where the user is pointing at in 3D space. It is easy to recognize these gestures by considering the number of fingertips visible: An open hand has five fingertips; a closed hand has 0 fingertips; and a pointing gesture has only one fingertip. The moving gesture is recognized from a continuous movement of the closed hand. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig2">2</a> shows the three hand gestures we implemented. In this way we have a simplified hand gesture recognition module without the need to use a sophisticated machine learning algorithm. Although this may limit the variety of gestures compared to the other recognition algorithms, such as Fels and Hinton (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Fels S, Hinton G (1995) Glove-TalkII: an adaptive gesture-to-formant interface. In: CHI'95 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 456–463" href="/article/10.1007/s10055-013-0230-0#ref-CR9" id="ref-link-section-d46149e718">1995</a>), it still provides a very natural way to interact with virtual objects.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Hand gestures interacting with a virtual object; <b>a</b> pointing gesture, <b>b</b> open hand gesture, and <b>c</b> close hand gesture</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>The gesture recognition result is described in a semantic form. We divided the gestures into two groups: (1) those that require two reference points for interaction and (2) those that require only a single point. Pointing and moving gestures are included in the first group, while closed and open hand gestures require a single point. We represent the gesture recognition result in a semantic form that includes arrival time, type of gesture, position in 3D, and function. Although we defined three different hand gestures, when a user interacts multimodally, the meaning of each gesture could vary according to the corresponding speech command. Thus, for each gesture input, we provide two possible semantic forms for the gesture to support higher interaction possibilities.</p><h3 class="c-article__sub-heading" id="Sec5">Speech interface</h3><p>For the speech input, we used the Microsoft Speech API 5.3 with the Microsoft Speech Recognizer 8.0. Initially, the recognition rate was about 60 %, which was not sufficient to use for our study. Thus, we trained the speech recognizer with a male speaker to improve the recognition accuracy. As a result, the recognition accuracy improved to nearly 90 %. The training was performed in a quiet room so we could ignore the influence of background noise. We predefined the type of speech commands in advance that would be integrated later with the gesture input. The supported speech commands are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0230-0#Tab1">1</a>. We also needed to create a set of speech commands used for the speech-only condition. The grabbing command was triggered when a user called the color or the shape of the augmented objects. The moving command was triggered when the users gave a direction command, and backward and forward commands were used to move an augmented object in Z-axis direction. To move the augmented object with speech, a user had to issue the direction commands repetitively.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Supported speech commands</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0230-0/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Speech recognition results are described in a unified semantic form like the gesture recognition results. The arrival time of the speech input is passed to the multimodal fusion module with the type of commands and target object ID.</p><h3 class="c-article__sub-heading" id="Sec6">Multimodal fusion architecture</h3><p>We designed and implemented a multimodal fusion architecture that generates a single system output from two different input modalities (gesture and speech). The system architecture we used for our MMI system is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The multimodal fusion architecture for the seamless AR</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>When a speech or gesture input arrives, the recognition modules for each input will recognize what the speech or gesture input means. Then the result is passed to the semantic representation module with its arrival time. The output from the semantic representation module is passed to the speech and gesture historians, respectively. A unimodal historian will store a speech or gesture input for 10 s in case future commands need to refer to the earlier inputs. In around 95 % of AR MMI commands, a gesture is issued up to one second earlier than the related speech input (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e940">2008</a>). So, we take the latest speech input from the speech historian and compare the time difference with the latest gesture input from the gesture historian. If the time difference is &gt;1 s, the input is considered as a unimodal one and passed to the system feedback module. If the time difference is &lt;1 s, we compare the type of speech and gesture commands. If the types of commands are not the same, the system looks for the second possible type of gesture command. This is because the meaning of the gesture is able to vary according to the corresponding speech input. If the types of each unimodal command are matched, we take it as a multimodal input. If the types of commands are the same, the speech and gesture pair is passed into the adaptive filter module. If the second trial of matching command type was not successful, the commands are directly tossed to the system feedback module as a unimodal input.</p><p>We have two types of filters in the adaptive filter module: One is for moving commands (dynamic filter), and another is for other commands, pointing and touching (static filter). The dynamic filter handles commands that are related to moving the virtual objects. In this case, the dynamic filter needs to have the starting point and the destination point as input. The static filter handles commands that are not related to the moving commands, and so we only need to have a single point. The role of the adaptive filter module is only to fill out the information needed by the result of the type comparison. If all necessary information is successfully filled out, the output of the adaptive filter module is passed to the system feedback module that changes the AR scene according to the outputs.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Pilot study</h2><div class="c-article-section__content" id="Sec7-content"><p>We ran a pilot study with five users to test the prototype MMI. A user repeated ten task trials in each of three different commands conditions: speech-only, gesture-only, and an MMI condition. Before the task trials, we let the users spend enough time to get used to each gesture and the experimental setup.</p><p>For each task, the subjects had to manipulate one sample virtual object at a time. They were supposed to change the shape or color of the sample virtual object to match a second target object shown on the screen. We showed the target as a transparent object with a different color and shape to the sample object. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig4">4</a> shows the initial AR view of the task at the starting position; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig4">4</a> (1) shows the sample object which users interacted with, (2) is the target object: users have to change the color and shape of the sample object to this, (3) is the shape change tool, and (4) is the color change tool.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Initial view of the original AR scene; (<i>1</i>) sample object to interact with; (<i>2</i>) <i>target blue sphere</i> representing target shape, color, and position; (<i>3</i>) shape selection bar; (<i>4</i>) color selection bar</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>After running the pilot study, we found several problems, particularly with the lack of depth cues in the object rendering. In addition, the large number of color and shape selection conditions made it difficult to remember the correct speech input. It was not easy to recognize which object was the target object and which was the sample object to interact with. Although there was a hand–eye offset from the display setup, users quickly got used to this and were easily able to interact with the virtual object.</p><p>This feedback was used to modify the AR scene and available speech commands. We added a virtual ground plane and solved the occlusion problem of the virtual object covering the user’s real hand by segmenting out the users’ hand from the background. To allow the users to easily distinguish the sample object from the target object, we put a torus under the sample object. As a result, we had the improved application interface as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>The modified experimental environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     </div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Experiment</h2><div class="c-article-section__content" id="Sec8-content"><h3 class="c-article__sub-heading" id="Sec9">Experimental design</h3><p>Our experiment was designed to evaluate the usability of the MMI in a tabletop AR environment with a monitor display. We measured the interface efficiency by using the task completion time, the interface effectiveness by capturing the accuracy of the system input, and the user satisfaction from post-condition questionnaires.</p><p>Our overall hypothesis is that users would find the AR multimodal interface to be more effective, efficient, and satisfying than using a unimodal interface:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>H1: MMI is more efficient than speech or gesture interface.</p>
                    </li>
                    <li>
                      <p>H2: MMI is more effective than speech or gesture interface.</p>
                    </li>
                    <li>
                      <p>H3: MMI is more satisfying than speech or gesture interface.</p>
                    </li>
                  </ul>
                        <p>To test these hypotheses, we used three different interface conditions: (1) a speech-only interface, (2) a gesture-only interface, and (3) a MMI. For the speech-only and gesture-only conditions, the subjects were asked to use only a single modality (either speech or gesture); however, in the MMI condition, they could use both of them in any way they wanted to.</p><p>There were twenty-five participants in the experiment, twenty-two male and three female, with ages from 25 to 40 years and an average age of 31 years. They were researchers who are familiar with AR applications but had little experience with speech interfaces, gesture interfaces, and MMI. All except one were right-handed. From the total subject pool, we used five participants in an initial pilot study and then twenty users in the full user study. A within-subject design was used so all participants experienced all conditions.</p><h3 class="c-article__sub-heading" id="Sec10">Experimental task</h3><p>The subjects were asked to perform ten task trials three times each using the three different conditions for a total of 90 trials (10 trails × 3 repetitions × 3 conditions). Each task trial involved using a particular interface to interact with a sample virtual object in an AR application. There were short questionnaires at the beginning, after each condition, and at the end of all the trials. In total, the experiment took approximately 45 min. For counterbalancing of possible order effects among the different interfaces, we used a 3 × 3 Latin squares design.</p><p>From our earlier work (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e1068">2008</a>), we found that the types of commands to interact with the augmented virtual object were typically limited to a small number. As a result, we used the following simplified tasks to test each condition:</p><p>1. Change the visual characteristics of the augmented virtual objects.</p><p>2. Point, grab, and move the augmented virtual objects.</p><p>It is not easy to set up evaluation tasks that cover all possible interactions. To manage this problem, we need to restrict the task to include the most frequently used or relevant functions. This may limit the applicability of results of the evaluation to only some AR applications, but is a good test of the prototype AR MMI. The typical user tasks are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0230-0#Tab2">2</a>. For each condition, there were 10 task trials and each trial involved doing each of commands listed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-013-0230-0#Tab2">2</a> to complete a task. Note that the command list is given to show what the users are supposed to do to complete a given task; the order of the commands issued was up to the users. When the target object was moved within .5 cm of the target position, the system recognized it as a successful completion of the movement task. Once a user completed the movement task, the system reset the AR scene and started another task automatically.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Command list to complete a task</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-013-0230-0/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>Video data of user interaction were collected from each of the trials. Using the video data, we compared the command recognition results with the actual multimodal commands issued to calculate a recognition error rate. In addition, we also observed the user and system errors while the subjects were interacting with the given objects. Finally, we noted the speech and gesture command patterns used, by observing user actions in the recorded video.</p><p>We set up the experimental environment as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig6">6</a>. We used a BumbleBee camera (Point Grey Research Inc <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Point Grey Research Inc (2009) &#xA;                    http://www.ptgrey.com/products/stereo.asp&#xA;                    &#xA;                  . Accessed 20 Nov 2009 [26]" href="/article/10.1007/s10055-013-0230-0#ref-CR31" id="ref-link-section-d46149e1138">2009</a>), which has two cameras on a rigid body, to get two synchronized video input streams (320 × 240 pixel resolution, 25 fps). The BumbleBee camera was placed on the side of the user to grab images of the user environment and track the user’s hand in 3D space. Subjects wore a headset with a noise-cancelling microphone for speech input. A 37-inch LCD screen was placed in front of them for viewing the AR scene. In between the users and the screen, a blue-colored board was placed to provide a reference point for the augmentation and a unique background color for better skin-color segmentation results. We chose the monitor-based experimental setup based on user preference from a previous study (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e1141">2008</a>); however, we recognize that the setup may affect the accuracy of the interaction because of the hand–eye offset; the user’s hand appears 90 degrees rotated when a user looks at the screen.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Experimental setup</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Result and analysis</h2><div class="c-article-section__content" id="Sec11-content"><p>We compared the three interface conditions using the usability factors of (1) efficiency, (2) effectiveness, and (3) satisfaction. The measured factors were (1) the task completion time, (2) the number of user and system errors, and (3) user satisfaction (questionnaire), respectively. The experiment analysis is based on data from twenty users of the redesigned interface. We used a one-way repeated-measures ANOVA to test the results for a significant difference and performed a post hoc pairwise comparison with the Bonferroni correction.</p><h3 class="c-article__sub-heading" id="Sec12">Task completion time</h3><p>We measured the time between when users started a task and when they finished the task with the given interface conditions. There was a significant difference in the task completion time across conditions (F (2,18) = 8.78, <i>p</i> &lt; .01). After post hoc pairwise comparisons, we found that the task completion time with the gesture interface was significantly different from the time with the speech interface (<i>p</i> &lt; .01) and the MMI (<i>p</i> &lt; .01). It took longer to complete the given tasks with the gesture interface (mean = 15.44, std = 4.47) than with the speech interface (mean = 12.38, mean = 3.15) and the MMI (mean = 11.78, std = 2.70). However, we did not find any significant differences in task completion time between the speech-only interface condition and the MMI condition.</p><h3 class="c-article__sub-heading" id="Sec13">User errors</h3><p>User errors, as well as system errors, were used to measure the effectiveness of the system. To measure the user accuracy, we observed the video of users interacting with the system and counted the number of errors made. The average number of user errors with speech input was .41 times per task, the average with gesture input was .50 times per task, and the average number of user errors with MMI was .42 times per task. However, we did not find a significant difference in the average number of errors across the different interface conditions (F (2, 18) = .73, <i>p</i> &lt; .50).</p><p>There were several different types of common errors. Most of the user errors with gestures occurred when users did not trigger the proper gesture. For example, when they grabbed an object, they had to put their hand on top of the object and close it. However, some users closed their hands first and then moved to the sample object to grab it. Most of the user errors with speech input occurred when they did not issue the proper speech commands. For example, although a user wanted to change the sample object to cube, he/she may say “cone” by mistake.</p><h3 class="c-article__sub-heading" id="Sec14">System errors</h3><p>System errors are dependent on the speech and gesture recognition accuracy, and the multimodal fusion accuracy. The accuracy of each interface component was measured using the following equation:</p><div id="Equa" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ A_{\text{Interface}} = \frac{{N_{{{\text{Accurate\_respond\_to\_the\_command}}}} }}{{N_{{{\text{total\_triggered\_command}}}} }} $$</span></div></div>
                        <p>The average accuracy of the speech interface was 94.1 %, and of the gesture interface was 85.5 %.</p><p>In a multimodal system, a combined speech and gesture command could fail because of errors in either the speech recognition or the gesture recognition, or both the speech and gesture recognition. Thus, if we assume that the two interfaces are independent, the combined accuracy should be expected to be no smaller than the accuracy of the speech input multiplied by the accuracy of the gesture input. In addition, the maximum accuracy of the MMI should not be larger than the accuracy of the single input modality that has the best accuracy. Thus, the accuracy of our MMI can be represented as follows:</p><div id="Equb" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$ A_{\text{speech}} \times A_{\text{gesture}} &lt; A_{\text{MMI}} &lt; Max\left( {A_{\text{speech}} ,A_{\text{gesture}} } \right) $$</span></div></div>
                        <p>When we combine two unimodal inputs without the multimodal fusion architecture, we would expect each of these sources of error to multiply to produce an accuracy of around 81 %, and the maximum expected MMI accuracy would be 94 %. We found experimentally that the accuracy of the MMI was 90 %, showing that the fusion module helped to increase the system accuracy by capturing the related speech and gesture input and compensating for error. However, when we consider the fact that MMI requires two unimodal inputs, the accuracy of the MMI may vary greatly according to the combination of two modalities.</p><h3 class="c-article__sub-heading" id="Sec15">Satisfaction</h3><p>We also collected user feedback to observe user satisfaction with each modality, and the MMI. The subjects answered questions on a Likert scale from 1 (very low) to 7 (very high). The users’ English fluency was 4.2 out of 7 (std = 1.24). Their average experiences with speech and gesture interfaces were 3.45 (std = 1.19) and 3.55 (std = 1.31), respectively. Their average experience with MMI was 3.05 (std = 1.28), and their experience with AR was 5.1 (std = 1.45). We used a one-way repeated-measures ANOVA with post hoc pairwise comparisons (with Bonferroni correction) to see how different types of interface conditions affected the user satisfaction.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Naturalness of the interfaces</h4><p>We asked users how natural it was to manipulate the given object. There was a significant difference in the perceived naturalness of the interface (F (2, 18) = 9.62, <i>p</i> &lt; .01). The naturalness of the gesture interface was rated as significantly different from that of the speech (<i>p</i> &lt; .01) and that of the MMI (<i>p</i> &lt; .01). In the post hoc tests, we found that the subjects felt that using the speech interface (mean = 5.60, std = 1.10) and the MMI (mean = 5.80, std = .83) was more natural than using the gesture-only interface (mean = 4.60, std = 1.14). However, we did not find any significant differences in the perceived naturalness of the interface between the speech interface and the MMI conditions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Ease of use of the interfaces</h4><p>We asked users how easy it was to change the color and shape of the virtual objects and to point to and move the objects. We found a significant difference in the ease of the changing color task (F (2, 18) = 11.68, <i>p</i> &lt; .01). Using speech or MMI was rated significantly easier than using gestures (<i>p</i> &lt; .01). The subjects felt that using the speech interface (mean = 5.90, std = 1.02) and the MMI (mean = 5.90, std = 1.02) to change the object color was easier than using the gesture interface (mean = 4.05, std = 1.32). However, we did not find any significant differences in the ease of changing the object color between the speech-only input and MMI conditions.</p><p>There was a significant difference in the ease of the changing shape task (F (2, 18) = 14.52, <i>p</i> &lt; .01). Using gesture to change the shape of the object was different from using speech (<i>p</i> &lt; .01) and the MMI (<i>p</i> &lt; .01). The subjects also indicated that using the speech input (mean = 5.90, std = 1.02) and the MMI (mean = 6.00, std = .97) was easier to change the shape of the object than using the gesture interface (mean = 4.00, std = 1.34). However, we did not find any significant differences in the ease of changing the object shape between the speech input and the MMI conditions.</p><p>We also found a significant difference in the ease of moving tasks (F (2, 18) = 7.54, <i>p</i> &lt; .01). The MMI was different from the gesture (<i>p</i> &lt; .03) and the speech (<i>p</i> &lt; .04). Users felt that the MMI (mean = 5.70, std = .98) was easier for moving objects than the gesture-only (mean = 4.70, std = 1.53) and the speech-only (mean = 4.75, std = 1.29) methods. However, there was no significant difference in the ease of the pointing between gesture- and speech-only interfaces.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Interface performance</h4><p>We found a significant difference in the efficiency, speed, and accuracy of the interfaces. Overall, users felt that the MMI was the most efficient, fastest, and accurate interface compared to the gesture-only condition. However, we did not find a significant difference between the MMI and speech-only interface conditions.</p><p>We asked the subjects how they felt about the usability of each interface using the characteristics of efficiency, effectiveness, and satisfaction. The efficiency of the MMI was significantly different from that of the gesture-only and the speech-only interfaces (F (2, 18) = 12.61, <i>p</i> &lt; .01). From the analysis, we found that users felt that the MMI (mean = 6.05, std = 1.05) was more efficient than the gesture-only interface (mean = 4.45, std = 1.28) and the speech-only interface (mean = 5.15, std = 1.14). For fast interactions, they indicated that the interaction with the MMI (mean = 6.15, std = .93) was quicker than with the gesture-only (mean = 4.40, std = 1.35) and the speech-only interfaces (mean = 5.05, std = 1.19). There was also a significant difference in the accuracy of the interaction (F (2, 18) = 9.03, <i>p</i> &lt; .01). Users felt they interacted more accurately with the MMI (mean = 5.60, std = 1.19) than with the gesture interface (mean = 3.95, std = 1.39). However, there was no significant difference in the perceived accuracy between MMI and speech interfaces.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec19">Physical and mental demands of interfaces</h4><p>We also asked the subjects how they felt about the physical demands, mental demands, and frustrations of each interface.</p><p>The physical demand of the speech-only interface was significantly different from that of the gesture-only interface and the MMI (F (2, 18) = 7.20, <i>p</i> &lt; .01). From the analysis, we found that users felt that the speech interface (mean = 3.15, std = 1.53) was less physically demanding than the gesture-only interface (mean = 4.75, std = 1.25) and the MMI (mean = 3.85, std = 1.69). We found no significant difference in physical demand between the MMI and gesture interface. There was no significant difference in the mental demand and frustrations of the interface.</p><h3 class="c-article__sub-heading" id="Sec20">Interviews</h3><p>After the experiments, we asked users to rank the preferred interface. In total, 70 % of users preferred the MMI over the unimodal interfaces, 25 % preferred the speech-only interface, and only 5 % preferred the gesture-only interface. We asked users why the MMI was their favorite. They said that they preferred it because they could use the two different modalities simultaneously or sequentially and, as a result, they could complete the task more quickly and efficiently. They also felt that the two different modalities compensated for each other and let them do the task more accurately. For example, speech was used to change the color of the sample object, and gesture was useful in changing the position of the object. They also mentioned that the MMI was a more intuitive way to complete the given tasks than the unimodal interfaces. The users said that they could feel that using the MMI became easier as they proceeded with the given tasks. They said they preferred the gesture input condition least as they found it physically demanding, and they had to search the control menu to find the target colors and shapes. The users also mentioned that it took longer and was less accurate than using the other interfaces to complete the given tasks.</p><h3 class="c-article__sub-heading" id="Sec21">Observations</h3><p>We counted the number of commands that were used to complete each task. We found a significant difference between the number of commands used in each interface condition (F (2,18) = 11.58, <i>p</i> &lt; .01). From the post hoc tests, we found that the subjects issued more commands with the gesture-only interface (mean = 6.14, std = .57) than with the speech-only interface (mean = 5.23, std = 1.08) or the MMI (mean = 4.93, std = 1.11). However, there was no significant difference in the number of commands between the speech-only interface and the MMI conditions.</p><p>We classified the commands into two types: (1) characteristic and (2) movement commands. The characteristic commands involve object characteristics such as the shape and color, while the movement commands involve physical motions such as grabbing (selecting), moving, and dropping. We divided the MMI commands into two input modalities: (1) gesture and (2) speech. As a result, a simultaneously or sequentially issued MMI command was not recognized as a single command; instead, the MMI command was considered as multiple combinations of gesture and speech commands.</p><p>There was no significant difference between the numbers of characteristic commands with the different types of interfaces. This is not surprising because users only need to issue two characteristic commands to complete a task no matter whether the user interacts with a speech interface or a gesture interface. However, we found a significant difference in the number of movement commands with each interface condition (F (2,18) = 3.82, <i>p</i> &lt; .04). The speech-only modality (mean = 28.50, std = 2.07) required fewer commands than the gesture-only modality (mean = 35.25, std = 1.42) for moving the objects (<i>p</i> &lt; .03). We found no significant difference between the number of speech and MMI commands or between the number of gesture and MMI commands for moving the objects.</p><p>We also measured the proportions of simultaneous and sequential multimodal commands during the study (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0230-0#Fig7">7</a>). Most of multimodal commands were issued sequentially. On average, only 20.7 % of MMI commands were issued simultaneously, and 79.3 % were sequential. There were four users who only issued sequential multimodal commands. Speech commands precede gesture commands in only 1.1 % of a simultaneously integrated MMI.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0230-0/MediaObjects/10055_2013_230_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Percentage of multimodal commands per user involving simultaneous and sequential integration of speech and gesture input</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0230-0/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>We also observed that the subjects’ reactions to the system errors differed with each interface condition. Usually, the subjects repeated the same commands with the unimodal interfaces when they had an error. Most of the subjects did the same when they had system errors while using the MMI condition. There were only three subjects that switched modalities when they had a system error while interacting with the MMI. For example, a user triggered a pointing gesture to change the shape of the sample object to cylinder and it did not work. The user then spoke to the system and said “<i>cylinder,</i>” rather than repeating the same pointing gesture.</p></div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Discussion</h2><div class="c-article-section__content" id="Sec22-content"><p>By comparing the task completion times for the three different interface conditions, we found that the MMI significantly reduced interaction time in completing the given tasks compared to the gesture-only interface. This is partly because using the speech input in the MMI for changing color or shape of the objects took less time than using gesture input. Previous research (Kaiser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Cohen P, Feiner S (2003) Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. Proceedings of international conference on multimodal interfaces 12–19" href="/article/10.1007/s10055-013-0230-0#ref-CR18" id="ref-link-section-d46149e1499">2003</a>) has found that speech input is helpful for descriptive commands, such as changing the color or shape of an object, and that gesture is useful for spatial commands, such as pointing or moving an object. The MMI takes the advantage of the complementary nature of the two input modalities, combining descriptive and spatial commands. Although the users had little experience with speech interfaces, gesture interfaces, and MMIs, we could see that the users found that speech commands were more useful for descriptive commands and gesture commands were more helpful for spatial commands. Additionally, in the MMI case, speech and gesture commands were mostly issued sequentially. As a result, we observed that the MMI was more efficient than the gesture interface for this AR application. However, we did not find any significant differences in the efficiency between the speech-only interface and the MMI. Users also mentioned that the MMI became easier to use over time than the other interface conditions; however, we found no significant difference in the number of user errors.</p><p>These findings also agreed with the user feedback that we received. The subjects preferred the MMI condition over the gesture-only interface. Although speech recognition produced slightly fewer errors, they felt that the MMI was overall more accurate than the speech-only input condition. This is because performing the tasks well typically required a combination of speech and gesture input.</p><h3 class="c-article__sub-heading" id="Sec23">Design guidelines</h3><p>Based on these findings from the user study, we propose some design guidelines for MMIs in AR environments. These include using gesture-triggered multimodal fusion, providing audiovisual feedback, and using a learning module in the multimodal fusion architecture.</p><p>From the pilot study, we learned that users wanted audiovisual feedback for each gesture and speech command. Users had visual feedback after triggering each command by looking at the changed shape or color of the sample object. However, they also wanted to have sound feedback, such as a “ding” or “beep” sound when the system changes the color or shape of the sample object. In addition, we needed to provide visual cues for better depth perception. A virtual plane was used as a background for the AR scene which users said was helpful to better perceive the 3D position of the augmented virtual objects.</p><p>In the full usability test, we observed that in only 1 % of the multimodal commands did the speech input precede the gesture input. We also found a similar pattern in (Lee and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256" href="/article/10.1007/s10055-013-0230-0#ref-CR25" id="ref-link-section-d46149e1515">2008</a>) where 94 % of gesture commands preceded speech input. Thus, in general, multimodal AR applications may need to have a gesture-triggered MMI fusion architecture.</p><p>We designed our multimodal fusion architecture by observing users’ behavior while they were interacting with the MMI. As a result, we found that the gesture meaning would vary depending on the corresponding speech command. From the user study, we found that the way a user made errors with multimodal interface condition was different from the speech- and gesture-only error patterns. We also found that the pattern of the user error was similar for each user. Based on this, we can assume that if we had a learning module in the multimodal fusion architecture, the accuracy of the multimodal integration would be improved.</p></div></div></section><section aria-labelledby="Sec24"><div class="c-article-section" id="Sec24-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec24">Conclusions</h2><div class="c-article-section__content" id="Sec24-content"><p>In this paper, we described a pilot user study and a full user study exploring the usability of AR MMI for object manipulation, compared with speech-only and 3D-hand-gesture-only conditions. After running the pilot study, we found several problems, particularly with the lack of depth cues in the object rendering. Additionally, the large number of color and shape selection conditions made it difficult to remember the correct speech input commands. This feedback from the users was used to modify the AR scene and available speech commands. In the full user study, we compared the usability factors of (1) efficiency, (2) effectiveness, and (3) satisfaction for each interface. The metrics used to compare between interface conditions were (1) task completion time, (2) user and system errors, and (3) satisfaction (questionnaire), respectively.</p><p>We found that the MMI condition produced shorter task completion times than gesture-only interface. However, we found no significant difference in task completion times between the speech-only interface and MMI condition. Based on the analysis of task completion time, we can conclude that the MMI was more efficient than the gesture interface.</p><p>We did not find significant differences in user errors among different interface conditions. Thus, we did not find that the MMI condition was more effective than the gesture- or speech-only conditions. However, we did observe that our multimodal fusion architecture was more accurate than using a simple combination of speech and gesture input error rates.</p><p>From the post-experiment questionnaire, we found that the subjects felt the MMI was more natural, easier, and more effective to use than the other two unimodal interfaces. The subject felt that the MMI condition was more efficient than either gesture-only or speech-only interfaces. They also felt the MMI was more effective than the other two unimodal interfaces. The users felt they could interact more accurately with the MMI than with the gesture-only interface; however, we did not find a significant difference between the MMI and speech interface conditions. These results imply that the MMI is more usable for AR applications than the gesture-only interface. Although users were more satisfied with the MMI than with the gesture and speech interfaces, we found no significant difference in efficiency and effectiveness between the MMI and speech-only conditions. From the subjects’ feedback, we also found that using the MMI is more physically demanding than using the speech interface but not than using the gesture interface. We did not find any significant differences in the mental demands and frustrations between the different interfaces.</p><p>In conclusion, we have found that for these tasks users felt that the MMI was more satisfying than the speech-only interface condition. However, the small number of variables used to explore the main effects of the MMI may limit the general applicability of the results. In the future we may need to consider other metrics, such as spatial accuracy in docking objects, the error rates of the multimodal fusion, and the percentage of the different command types issued in the MMI.</p><p>The experimental results may also be biased by the overwhelmingly male gender of users and affected by the hand–eye offset from the display setup. Users did not mention any difficulties in interacting with the AR setup where a hand–eye offset exists and seemed to get used to the offset easily; however, the results could differ from one with a handheld display or head-mounted display setup. In the future it would be good to evaluate how different display types could affect the usability of the AR MMI.</p><p>The simple tasks may also affect the usability of the interface, especially easiness of the tasks. In the future, we need to study task performance in a variety of AR environments with different display types and balanced user genders. In addition, the performance may be improved by adding a feedback channel to give the user information about the fusion result and showing if there are system failures occurring. We could also use this to build a learning module into the multimodal fusion architecture which would improve the accuracy of the MMI based on the users’ behavior. In this way the system would adapt to the users’ individual input style. Finally, we should explore the value of MMI in a wider range of AR applications in addition to object manipulation, for example, in an AR navigation task or AR game application.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RT. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environ 6(4):355–385" /><p class="c-article-references__text" id="ref-CR1">Azuma RT (1997) A survey of augmented reality. Presence: Teleoperators Virtual Environ 6(4):355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Presence%3A%20Teleoperators%20Virtual%20Environ&amp;volume=6&amp;issue=4&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CRT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Bevan, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Bevan N (1995) Measuring usability as quality of use. Softw Qual J 4:111–150" /><p class="c-article-references__text" id="ref-CR2">Bevan N (1995) Measuring usability as quality of use. Softw Qual J 4:111–150</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00402715" aria-label="View reference 2">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20usability%20as%20quality%20of%20use&amp;journal=Softw%20Qual%20J&amp;volume=4&amp;pages=111-150&amp;publication_year=1995&amp;author=Bevan%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;Bolt RA (1980) Put-that-there: voice and gesture at the graphics interface. Proc Annu Conf Comput Graph Inter" /><p class="c-article-references__text" id="ref-CR3">
Bolt RA (1980) Put-that-there: voice and gesture at the graphics interface. Proc Annu Conf Comput Graph Interact Tech 14(3):262–270</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Borgefors, " /><meta itemprop="datePublished" content="1986" /><meta itemprop="headline" content="Borgefors G (1986) Distance transformations in digital images. Comput Vis Graph Image Process 34:344–371" /><p class="c-article-references__text" id="ref-CR4">Borgefors G (1986) Distance transformations in digital images. Comput Vis Graph Image Process 34:344–371</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0734-189X%2886%2980047-0" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Distance%20transformations%20in%20digital%20images&amp;journal=Comput%20Vis%20Graph%20Image%20Process&amp;volume=34&amp;pages=344-371&amp;publication_year=1986&amp;author=Borgefors%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Chai, A. Bouzerdoum, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Chai D, Bouzerdoum A (2000) A Bayesian approach to skin color classification in YCbCr color space. Proc IEEE T" /><p class="c-article-references__text" id="ref-CR5">Chai D, Bouzerdoum A (2000) A Bayesian approach to skin color classification in YCbCr color space. Proc IEEE TENCONO’00 2:421–424</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20Bayesian%20approach%20to%20skin%20color%20classification%20in%20YCbCr%20color%20space&amp;journal=Proc%20IEEE%20TENCONO%E2%80%9900&amp;volume=2&amp;pages=421-424&amp;publication_year=2000&amp;author=Chai%2CD&amp;author=Bouzerdoum%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CP. Chu, TH. Dani, R. Gadh, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Chu CP, Dani TH, Gadh R (1997) Multimodal interface for a virtual reality based computer aided design system. " /><p class="c-article-references__text" id="ref-CR6">Chu CP, Dani TH, Gadh R (1997) Multimodal interface for a virtual reality based computer aided design system. Proc IEEE Int Conf Robot Automat 2:1329–1334</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FROBOT.1997.614321" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20interface%20for%20a%20virtual%20reality%20based%20computer%20aided%20design%20system&amp;journal=Proc%20IEEE%20Int%20Conf%20Robot%20Automat&amp;volume=2&amp;pages=1329-1334&amp;publication_year=1997&amp;author=Chu%2CCP&amp;author=Dani%2CTH&amp;author=Gadh%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen PR, Sullivan JW (1989) Synergistic user of direct manipulation and natural language. In: CHI'89 Proceedi" /><p class="c-article-references__text" id="ref-CR7">Cohen PR, Sullivan JW (1989) Synergistic user of direct manipulation and natural language. In: CHI'89 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 227–233</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal intera" /><p class="c-article-references__text" id="ref-CR8">Cohen PR, Johnston M, McGee D, Oviatt S, Pittman J, Smith I, Chen L, Clow J (1997) QuickSet: multimodal interaction for distributed applications. In: Proceedings of the fifth ACM international conference on multimedia. ACM Press, New York, pp 31–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fels S, Hinton G (1995) Glove-TalkII: an adaptive gesture-to-formant interface. In: CHI'95 Proceedings of the " /><p class="c-article-references__text" id="ref-CR9">Fels S, Hinton G (1995) Glove-TalkII: an adaptive gesture-to-formant interface. In: CHI'95 Proceedings of the SIGCHI conference on human factors in computing systems. ACM Press, New York, pp 456–463</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Frøkjær, M. Hertzum, K. Hornbæk, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Frøkjær E, Hertzum M, Hornbæk K (2000) Measuring usability: are effectiveness, efficiency, and satisfaction re" /><p class="c-article-references__text" id="ref-CR10">Frøkjær E, Hertzum M, Hornbæk K (2000) Measuring usability: are effectiveness, efficiency, and satisfaction really correlated? CHI Conf Proc 2(1):345–352</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20usability%3A%20are%20effectiveness%2C%20efficiency%2C%20and%20satisfaction%20really%20correlated%3F&amp;journal=CHI%20Conf%20Proc&amp;volume=2&amp;issue=1&amp;pages=345-352&amp;publication_year=2000&amp;author=Fr%C3%B8kj%C3%A6r%2CE&amp;author=Hertzum%2CM&amp;author=Hornb%C3%A6k%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="&#xA;Hartley R,  Zisserman A (2004) Multiple view geometry in computer vision. Cambridge University Press, Cambrid" /><p class="c-article-references__text" id="ref-CR12">
Hartley R,  Zisserman A (2004) Multiple view geometry in computer vision. Cambridge University Press, Cambridge</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hauptmann AG (1989) Speech and gestures for graphic image manipulation. CHI Conf Proc 241–245" /><p class="c-article-references__text" id="ref-CR13">Hauptmann AG (1989) Speech and gestures for graphic image manipulation. CHI Conf Proc 241–245</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Procee" /><p class="c-article-references__text" id="ref-CR14">Heidemann G, Bax I, Bekel H (2004) Multimodal interaction in an augmented reality scenario. In: ICMI’04 Proceedings of the 6th international conference on multimodal interfaces. ACM, New York, pp 53–60</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Irawati S, Green S, Billinghurst M, Duenser A, Ko H (2006a) Move the couch where? Developing an augmented real" /><p class="c-article-references__text" id="ref-CR15">Irawati S, Green S, Billinghurst M, Duenser A, Ko H (2006a) Move the couch where? Developing an augmented reality multimodal interface. ICAT: 1–4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Irawati S, Green S, Billinghurst M, Duenser A,  Ko H (2006b) An evaluation of an augmented reality multimodal " /><p class="c-article-references__text" id="ref-CR16">Irawati S, Green S, Billinghurst M, Duenser A,  Ko H (2006b) An evaluation of an augmented reality multimodal interface using speech and paddle gestures. In: Advances in artificial reality and tele-existence, Lecture notes in computer science, vol 4282. pp 272–283</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="LaViola Jr. JJ (1999) A multimodal interface framework for using hand gestures and speech in virtual environme" /><p class="c-article-references__text" id="ref-CR17">LaViola Jr. JJ (1999) A multimodal interface framework for using hand gestures and speech in virtual environment applications. Gesture-Based Commun Hum Comp Interact 303–341</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Cohen P, Feiner S (2003) Mutual disambiguation of 3D m" /><p class="c-article-references__text" id="ref-CR18">Kaiser E, Olwal A, McGee D, Benko H, Corradini A, Li X, Cohen P, Feiner S (2003) Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. Proceedings of international conference on multimodal interfaces 12–19</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top A" /><p class="c-article-references__text" id="ref-CR19">Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. In: Proceedings of the international symposium on augmented reality (ISAR 2000). Munich, Germany, pp 111–119</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kölsch M, Turk M,  Tobias H (2004) Vision-based interfaces for mobility. Proc MobiQuitous’04 86–94" /><p class="c-article-references__text" id="ref-CR20">Kölsch M, Turk M,  Tobias H (2004) Vision-based interfaces for mobility. Proc MobiQuitous’04 86–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Kölsch, M. Turk, H. Tobias, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Kölsch M, Turk M, Tobias H (2006) Multimodal interaction with a wearable augmented reality system. IEEE Comput" /><p class="c-article-references__text" id="ref-CR21">Kölsch M, Turk M, Tobias H (2006) Multimodal interaction with a wearable augmented reality system. IEEE Comput Graph Appl 26(3):62–71</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCG.2006.66" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20interaction%20with%20a%20wearable%20augmented%20reality%20system&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=26&amp;issue=3&amp;pages=62-71&amp;publication_year=2006&amp;author=K%C3%B6lsch%2CM&amp;author=Turk%2CM&amp;author=Tobias%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koons DB, Sparrell CJ (1994) ICONIC: speech and depictive gestures at the human-machine interface. In: CHI'94 " /><p class="c-article-references__text" id="ref-CR22">Koons DB, Sparrell CJ (1994) ICONIC: speech and depictive gestures at the human-machine interface. In: CHI'94 Conference companion on human factors in computing systems. ACM, New York, pp 453–454</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Krum DM, Omotesto O, Ribarsky W, Starner T,  Hodges LF (2002) Speech and gesture control of a whole earth 3D v" /><p class="c-article-references__text" id="ref-CR23">Krum DM, Omotesto O, Ribarsky W, Starner T,  Hodges LF (2002) Speech and gesture control of a whole earth 3D visualization environment. Proc Jt Eurograph-IEEE TCVG Symp Vis 195–200</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="ME. Latoschik, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. AFRIGRAPH 20" /><p class="c-article-references__text" id="ref-CR24">Latoschik ME (2001) A gesture processing framework for multimodal interaction in virtual reality. AFRIGRAPH 2001:95–100</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F513867.513888" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20gesture%20processing%20framework%20for%20multimodal%20interaction%20in%20virtual%20reality&amp;journal=AFRIGRAPH&amp;volume=2001&amp;pages=95-100&amp;publication_year=2001&amp;author=Latoschik%2CME">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Inter" /><p class="c-article-references__text" id="ref-CR25">Lee M, Billinghurst M (2008) A wizard of Oz study for an AR multimodal interface. Proc Int Conf Multimod Interfaces 249–256</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lucente M, Zwart GJ, George AD (1998) Visualization space: a testbed for deviceless multimodal user interface." /><p class="c-article-references__text" id="ref-CR26">Lucente M, Zwart GJ, George AD (1998) Visualization space: a testbed for deviceless multimodal user interface. Proc AAAI Spring Symp Intell Environ. AAAI TR SS-98-02</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimod" /><p class="c-article-references__text" id="ref-CR29">Olwal A, Benko H,  Feiner S (2003) Sense shapes: using statistical geometry for object selection in a multimodal augmented reality system. Proc Int Symp Mix Augment Real 300–301</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oviatt S, Coulson R,  Lunsford R (2004) When Do We Interact Multimodally? Cognitive load and multimodal commun" /><p class="c-article-references__text" id="ref-CR30">Oviatt S, Coulson R,  Lunsford R (2004) When Do We Interact Multimodally? Cognitive load and multimodal communication patterns. Proc Int Conf Multimod Interfaces 129–136</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Point Grey Research Inc (2009) http://www.ptgrey.com/products/stereo.asp. Accessed 20 Nov 2009 [26]" /><p class="c-article-references__text" id="ref-CR31">Point Grey Research Inc (2009) <a href="http://www.ptgrey.com/products/stereo.asp">http://www.ptgrey.com/products/stereo.asp</a>. Accessed 20 Nov 2009 [26]</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Quek, D. McNeil, R. Bryll, S. Duncan, X. Ma, C. Kirbas, KE. McCullough, R. Ansari, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Quek F, McNeil D, Bryll R, Duncan S, Ma X, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discourse" /><p class="c-article-references__text" id="ref-CR32">Quek F, McNeil D, Bryll R, Duncan S, Ma X, Kirbas C, McCullough KE, Ansari R (2002) Multimodal human discourse: gesture and speech. TOCHI 9(3):171–193</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F568513.568514" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20human%20discourse%3A%20gesture%20and%20speech&amp;journal=TOCHI&amp;volume=9&amp;issue=3&amp;pages=171-193&amp;publication_year=2002&amp;author=Quek%2CF&amp;author=McNeil%2CD&amp;author=Bryll%2CR&amp;author=Duncan%2CS&amp;author=Ma%2CX&amp;author=Kirbas%2CC&amp;author=McCullough%2CKE&amp;author=Ansari%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rauschert I, Agrawal P, Sharmar R, Fuhrmann S, Brewer I, MacEachren A, Wang H,  Cai G (2002) Designing a human" /><p class="c-article-references__text" id="ref-CR33">Rauschert I, Agrawal P, Sharmar R, Fuhrmann S, Brewer I, MacEachren A, Wang H,  Cai G (2002) Designing a human-centered, multimodal GIS interface to support emergency management. Proc Geogrc Inf Syst 119–124</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Shneiderman, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Shneiderman B (2000) The limits of speech recognition. Commun ACM 43(9):63–65" /><p class="c-article-references__text" id="ref-CR34">Shneiderman B (2000) The limits of speech recognition. Commun ACM 43(9):63–65</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F348941.348990" aria-label="View reference 31">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20limits%20of%20speech%20recognition&amp;journal=Commun%20ACM&amp;volume=43&amp;issue=9&amp;pages=63-65&amp;publication_year=2000&amp;author=Shneiderman%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Tse E, Greenberg S,  Shen C (2006) GSI DEMO: Multiuser gesture/speech interaction over digital tables by wrapp" /><p class="c-article-references__text" id="ref-CR35">Tse E, Greenberg S,  Shen C (2006) GSI DEMO: Multiuser gesture/speech interaction over digital tables by wrapping single user applications. Proc Int Conf Multimod Interfaces 76–83</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Weimer D, Genapathy SK (1989) A synthetic visual environment with hand gesturing and voice input. Proc Conf Hu" /><p class="c-article-references__text" id="ref-CR36">Weimer D, Genapathy SK (1989) A synthetic visual environment with hand gesturing and voice input. Proc Conf Hum Factors Comput Syst 235–240</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 1330–1334" /><p class="c-article-references__text" id="ref-CR37">Zhang Z (2000) A flexible new technique for camera calibration. IEEE Trans Pattern Anal Mach Intell 1330–1334</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0230-0-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by the DigiLog Miniature Augmented Reality Research Program funded by KAIST Research Foundation. It was supported by the Global Frontier R&amp;D Program on &lt;Human-centered Interaction for Coexistence&gt; funded by the National Research Foundation of Korea grant funded by the Korean Government (MSIP) (NRF-2010-0029751).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Technology Strategy Office, R&amp;D Center, KT,  17 Woomyeon-dong, SeoCho-gu, Seoul, 137-792, Korea</p><p class="c-article-author-affiliation__authors-list">Minkyung Lee</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">The HIT Lab NZ, University of Canterbury, Private Bag 4800, Ilam, Christchurch, 8140, New Zealand</p><p class="c-article-author-affiliation__authors-list">Mark Billinghurst</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Multimedia Research Team, Daum Communications, 2181 Yeongpyeong-dong, Jeju-si, Jeju-do, 690-140, Korea</p><p class="c-article-author-affiliation__authors-list">Woonhyuk Baek</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Computer Science and Software Engineering, University of Canterbury, Private Bag 4800, Ilam, Christchurch, 8140, New Zealand</p><p class="c-article-author-affiliation__authors-list">Richard Green</p></li><li id="Aff5"><p class="c-article-author-affiliation__address">GSCT UVR Lab, KAIST, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea</p><p class="c-article-author-affiliation__authors-list">Woontack Woo</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Minkyung-Lee"><span class="c-article-authors-search__title u-h3 js-search-name">Minkyung Lee</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Minkyung+Lee&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Minkyung+Lee" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Minkyung+Lee%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Mark-Billinghurst"><span class="c-article-authors-search__title u-h3 js-search-name">Mark Billinghurst</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Mark+Billinghurst&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Mark+Billinghurst" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Mark+Billinghurst%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Woonhyuk-Baek"><span class="c-article-authors-search__title u-h3 js-search-name">Woonhyuk Baek</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Woonhyuk+Baek&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Woonhyuk+Baek" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Woonhyuk+Baek%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Richard-Green"><span class="c-article-authors-search__title u-h3 js-search-name">Richard Green</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Richard+Green&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Richard+Green" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Richard+Green%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Woontack-Woo"><span class="c-article-authors-search__title u-h3 js-search-name">Woontack Woo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Woontack+Woo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Woontack+Woo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Woontack+Woo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0230-0/email/correspondent/c1/new">Minkyung Lee</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20usability%20study%20of%20multimodal%20input%20in%20an%20augmented%20reality%20environment&amp;author=Minkyung%20Lee%20et%20al&amp;contentID=10.1007%2Fs10055-013-0230-0&amp;publication=1359-4338&amp;publicationDate=2013-09-21&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Lee, M., Billinghurst, M., Baek, W. <i>et al.</i> A usability study of multimodal input in an augmented reality environment.
                    <i>Virtual Reality</i> <b>17, </b>293–305 (2013). https://doi.org/10.1007/s10055-013-0230-0</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0230-0.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-22">22 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-15">15 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-09-21">21 September 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-11">November 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0230-0" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0230-0</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Multimodal interface</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Usability</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Efficiency</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Effectiveness</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Satisfaction</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0230-0.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=230;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

