<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A two visual systems approach to understanding voice and gestural inte"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a neuroanatomical model of perception and..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/8/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A two visual systems approach to understanding voice and gestural interaction"/>

    <meta name="dc.source" content="Virtual Reality 2005 8:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2005-06-14"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a neuroanatomical model of perception and action known as the two visual systems hypothesis, we conducted a study with two controlled experiments to compare four different kinds of spatial interaction: (1) voice-based input, (2) pointing with a visual cursor, (3) pointing without a visual cursor, and (4) pointing with a time-lagged visual cursor. Consistent with the two visual systems hypothesis, we found that voice-based input and pointing with a cursor were less robust to a display illusion known as the induced Roelofs effect than pointing without a cursor or even pointing with a lagged cursor. The implications of these findings are discussed, with an emphasis on how the two visual systems model can be used to understand the basis for voice and gestural interactions that support spatial target selection in large screen and immersive environments."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2005-06-14"/>

    <meta name="prism.volume" content="8"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="231"/>

    <meta name="prism.endingPage" content="241"/>

    <meta name="prism.copyright" content="2005 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-005-0156-2"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-005-0156-2"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-005-0156-2.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-005-0156-2"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A two visual systems approach to understanding voice and gestural interaction"/>

    <meta name="citation_volume" content="8"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2005/09"/>

    <meta name="citation_online_date" content="2005/06/14"/>

    <meta name="citation_firstpage" content="231"/>

    <meta name="citation_lastpage" content="241"/>

    <meta name="citation_article_type" content="Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-005-0156-2"/>

    <meta name="DOI" content="10.1007/s10055-005-0156-2"/>

    <meta name="citation_doi" content="10.1007/s10055-005-0156-2"/>

    <meta name="description" content="It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a ne"/>

    <meta name="dc.creator" content="Barry A. Po"/>

    <meta name="dc.creator" content="Brian D. Fisher"/>

    <meta name="dc.creator" content="Kellogg S. Booth"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IFIP Cong; citation_title=The ultimate display; citation_author=IE Sutherland; citation_volume=65; citation_publication_date=1965; citation_pages=506-508; citation_id=CR1"/>

    <meta name="citation_reference" content="Bolt RA (1980) &#8220;Put-that-there&#8221;: voice and gesture at the graphics interface. In: Proceedings of the 7th international conference on computer graphics and interactive techniques (SIGGRAPH), pp 262&#8211;270"/>

    <meta name="citation_reference" content="citation_journal_title=Commun ACM; citation_title=Multimodal interfaces that process what comes naturally; citation_author=SL Oviatt, PR Cohen; citation_volume=43; citation_issue=3; citation_publication_date=2000; citation_pages=45-53; citation_id=CR3"/>

    <meta name="citation_reference" content="citation_journal_title=Psychol Forsch; citation_title=Two mechanisms of vision in primates; citation_author=CB Trevarthen; citation_volume=31; citation_publication_date=1968; citation_pages=299-337; citation_doi=10.1007/BF00422717; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=The visual brain in action. Oxford Psychology Series 27; citation_publication_date=1995; citation_id=CR5; citation_author=AD Milner; citation_author=MA Goodale; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Interaction of cognitive and sensorimotor maps of visual space; citation_author=B Bridgeman, S Peery, S Anand; citation_volume=59; citation_issue=3; citation_publication_date=1997; citation_pages=456-469; citation_id=CR6"/>

    <meta name="citation_reference" content="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22&#8211;38"/>

    <meta name="citation_reference" content="citation_journal_title=Archiv f&#252;r Augenheilkd; citation_title=Optische localisation (Optical localization); citation_author=C Roelofs; citation_volume=109; citation_publication_date=1935; citation_pages=395-415; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_journal_title=Science; citation_title=Two visual systems: brain mechanisms for localization and discrimination are dissociated by tectal and cortical lesions; citation_author=GE Schneider; citation_volume=163; citation_publication_date=1969; citation_pages=895-902; citation_id=CR9"/>

    <meta name="citation_reference" content="citation_title=Analysis of visual behaviour; citation_publication_date=1982; citation_id=CR10; citation_author=LG Ungerleider; citation_author=W Mishkin; citation_publisher=MIT Press"/>

    <meta name="citation_reference" content="citation_journal_title=Exp Psychol: Hum Percept Perform; citation_title=Relation between cognitive and motor-oriented systems of visual position perception; citation_author=B Bridgeman, S Lewis, G Heit, W Nagle; citation_volume=5; citation_publication_date=1979; citation_pages=692-700; citation_doi=10.1037//0096-1523.5.4.692; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=Segregation of cognitive and motor aspects of visual function using induced motion; citation_author=B Bridgeman, M Kirch, A Sperling; citation_volume=29; citation_issue=4; citation_publication_date=1981; citation_pages=336-342; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Ecol Psychol; citation_title=Information, perception, and action: what should ecological psychologists learn from Milner and Goodale (1995)?; citation_author=CF Michaels; citation_volume=12; citation_issue=3; citation_publication_date=2000; citation_pages=241-258; citation_doi=10.1207/S15326969ECO1203_4; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Percept Psychophys; citation_title=A Simon effect induced by motion and location: evidence for a direct linkage of cognitive and motor maps; citation_author=D Kerzel, B Hommel, H Bekkering; citation_volume=63; citation_issue=5; citation_publication_date=2001; citation_pages=862-874; citation_id=CR14"/>

    <meta name="citation_reference" content="Bridgeman B, Dassonville P, Bala J, Thiem P (2003) What is stored in the sensorimotor visual system: map or egocentric calibration?. In: Proceedings of the 3rd annual meeting of the vision sciences society, pp 10"/>

    <meta name="citation_reference" content="Po BA, Fisher BD, Booth KS (2004). Mouse and touchscreen selection in the upper and lower visual fields. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 359&#8211;366"/>

    <meta name="citation_reference" content="citation_title=Visual illusions: their causes, characteristics, and applications; citation_publication_date=1965; citation_id=CR17; citation_author=M Luckiesh; citation_publisher=Dover Publications"/>

    <meta name="citation_reference" content="Myers BA, Bhatnagar R, Nichols J, Peck CH, Kong D, Miller R, Long AC (2002) Interacting a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 33&#8211;40"/>

    <meta name="citation_reference" content="citation_journal_title=Theor Issues Ergon Sci; citation_title=The earth is spherical (p &lt; 0.05): alternative methods of statistical inference; citation_author=KJ Vicente, GL Torenvliet; citation_volume=1; citation_issue=3; citation_publication_date=2000; citation_pages=248-271; citation_doi=10.1080/14639220110037065; citation_id=CR19"/>

    <meta name="citation_author" content="Barry A. Po"/>

    <meta name="citation_author_email" content="po@cs.ubc.ca"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of British Columbia, Vancouver, Canada"/>

    <meta name="citation_author" content="Brian D. Fisher"/>

    <meta name="citation_author_email" content="fisher@cs.ubc.ca"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of British Columbia, Vancouver, Canada"/>

    <meta name="citation_author" content="Kellogg S. Booth"/>

    <meta name="citation_author_email" content="ksbooth@cs.ubc.ca"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of British Columbia, Vancouver, Canada"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-005-0156-2&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2005/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-005-0156-2"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A two visual systems approach to understanding voice and gestural interaction"/>
        <meta property="og:description" content="It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a neuroanatomical model of perception and action known as the two visual systems hypothesis, we conducted a study with two controlled experiments to compare four different kinds of spatial interaction: (1) voice-based input, (2) pointing with a visual cursor, (3) pointing without a visual cursor, and (4) pointing with a time-lagged visual cursor. Consistent with the two visual systems hypothesis, we found that voice-based input and pointing with a cursor were less robust to a display illusion known as the induced Roelofs effect than pointing without a cursor or even pointing with a lagged cursor. The implications of these findings are discussed, with an emphasis on how the two visual systems model can be used to understand the basis for voice and gestural interactions that support spatial target selection in large screen and immersive environments."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A two visual systems approach to understanding voice and gestural interaction | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-005-0156-2","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Two visual systems, Pointing, Cursors, Visual feedback, Voice input, Visual illusions","kwrd":["Two_visual_systems","Pointing","Cursors","Visual_feedback","Voice_input","Visual_illusions"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-005-0156-2","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-005-0156-2","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=156;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-005-0156-2">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A two visual systems approach to understanding voice and gestural interaction
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0156-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0156-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2005-06-14" itemprop="datePublished">14 June 2005</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A two visual systems approach to understanding voice and gestural interaction</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Barry_A_-Po" data-author-popup="auth-Barry_A_-Po">Barry A. Po</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of British Columbia" /><meta itemprop="address" content="grid.17091.3e, 0000000122889830, Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, BC, V6T 1Z4, Canada" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Brian_D_-Fisher" data-author-popup="auth-Brian_D_-Fisher">Brian D. Fisher</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of British Columbia" /><meta itemprop="address" content="grid.17091.3e, 0000000122889830, Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, BC, V6T 1Z4, Canada" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Kellogg_S_-Booth" data-author-popup="auth-Kellogg_S_-Booth" data-corresp-id="c1">Kellogg S. Booth<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of British Columbia" /><meta itemprop="address" content="grid.17091.3e, 0000000122889830, Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, BC, V6T 1Z4, Canada" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 8</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">231</span>–<span itemprop="pageEnd">241</span>(<span data-test="article-publication-year">2005</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">94 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-005-0156-2/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>It is important to consider the physiological and behavioral mechanisms that allow users to physically interact with virtual environments. Inspired by a neuroanatomical model of perception and action known as the two visual systems hypothesis, we conducted a study with two controlled experiments to compare four different kinds of spatial interaction: (1) voice-based input, (2) pointing with a visual cursor, (3) pointing without a visual cursor, and (4) pointing with a time-lagged visual cursor. Consistent with the two visual systems hypothesis, we found that voice-based input and pointing with a cursor were <i>less</i> robust to a display illusion known as the induced Roelofs effect than pointing without a cursor or even pointing with a lagged cursor. The implications of these findings are discussed, with an emphasis on how the two visual systems model can be used to understand the basis for voice and gestural interactions that support spatial target selection in large screen and immersive environments.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>An important part of virtual reality (VR) research is the development of voice and gestural interaction. Ivan Sutherland, who is credited with first envisioning VR-style technology, noted that the muscles of the hands and arms allowed for such dextrous movement that gesture was a “natural choice” for computer control [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Sutherland IE (1965) The ultimate display. Proc IFIP Cong 65:506–508" href="/article/10.1007/s10055-005-0156-2#ref-CR1" id="ref-link-section-d38116e348">1</a>]. This was later reinforced by Bolt, whose classic “Put-That-There” demonstration explored the multimodal convergence of voice and gesture for spatial information systems [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Bolt RA (1980) “Put-that-there”: voice and gesture at the graphics interface. In: Proceedings of the 7th international conference on computer graphics and interactive techniques (SIGGRAPH), pp 262–270" href="/article/10.1007/s10055-005-0156-2#ref-CR2" id="ref-link-section-d38116e351">2</a>]. Further implications of allowing people to use speech and gesture to interact in ways most natural to them have been discussed by Oviatt and Cohen [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Oviatt SL Cohen PR (2000) Multimodal interfaces that process what comes naturally. Commun ACM 43(3):45–53" href="/article/10.1007/s10055-005-0156-2#ref-CR3" id="ref-link-section-d38116e354">3</a>]. Today, there are many examples of VR interaction techniques that are based on voice, gesture or the combination of both. Our focus is on understanding voice and gestural interaction for specifying spatial information, or the locations on an information display at which objects of interest are or will be displayed. We refer to this as spatial <i>target selection</i>, emphasizing the task of specifying the location of a target.</p><p>There are many assumptions made about what elements should be present in an interaction technique to support reliable voice and gestural interaction for target selection. For example, the inclusion of graphical feedback in the form of visual cursors for pointing is typical throughout the history of VR. Other graphical elements in addition to cursors that have been used to support interaction techniques include frames, scales, and shadows (in 3D environments). These graphical elements are used because of our intuition about the value of visual feedback and visual context. Especially with large-screen VR environments, the only graphical elements present are those within the display itself. The lack of contextual graphical elements in the surrounding environment could mean that graphical elements in the display take on stronger and sometimes unanticipated roles. With a renewed interest in multimodal interaction techniques for virtual environments, it is important to characterize the types of graphical elements that are present in a display and to understand their impact on voice and gestural input because this could have serious implications for multimodal techniques that are developed in the future.</p><p>Our approach to understanding the impact of graphical elements starts with a knowledge of the basic physiological and cognitive factors that allow people to use voice and gesture in virtual environments. In cognitive psychology and neuroscience, advances have been made in understanding how the functional architecture of the human brain allows people to perceive relevant aspects of the surrounding world and how this leads them to interact with the physical world in a useful, meaningful way. In particular, a neuroanatomical model known as the <i>two visual systems hypothesis</i> provides a glimpse of the complex relationship between visual perception and human motor movement [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Trevarthen CB (1968) Two mechanisms of vision in primates. Psychol Forsch 31:299–337" href="/article/10.1007/s10055-005-0156-2#ref-CR4" id="ref-link-section-d38116e368">4</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22–38" href="/article/10.1007/s10055-005-0156-2#ref-CR7" id="ref-link-section-d38116e371">7</a>]. The hypothesis suggests that voice and gesture rely on different mental representations for the processing of visual information, and this could mean that user performance might differ in the presence or absence of particular graphical elements when spatial tasks are performed.</p><p>We can use the two visual systems model to predict how target selection through voice or gestural pointing can be influenced by the presence or absence of two types of graphical elements: visible cursors and frames. We report the results of a study with two controlled experiments designed to test our predictions by contrasting the performance of users under four different interaction conditions: (1) voice-based input, (2) pointing with a visual cursor, (3) pointing without a visual cursor, and (4) pointing with a lagged visual cursor. By having users interact in an immersive virtual environment in the presence of graphical frames, we expected a visual illusion known as the <i>induced Roelofs Effect</i> to occur with certain kinds of input [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Roelofs C (1935) Optische localisation (Optical localization). Archiv für Augenheilkd 109:395–415" href="/article/10.1007/s10055-005-0156-2#ref-CR8" id="ref-link-section-d38116e380">8</a>]. We found that pointing without a visual cursor and even pointing with a lagged visual cursor could outperform both voice-based input and pointing with reliable visual feedback under these conditions. Our findings are discussed in the context of how the two visual systems hypothesis can help build a better understanding of VR interactions that rely on voice and gesture.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Background and related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Before describing the experimental approach we have taken, we review the fundamentals of the two visual systems hypothesis, which arose from a number of studies described in more detail later in this section [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Trevarthen CB (1968) Two mechanisms of vision in primates. Psychol Forsch 31:299–337" href="/article/10.1007/s10055-005-0156-2#ref-CR4" id="ref-link-section-d38116e391">4</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469" href="/article/10.1007/s10055-005-0156-2#ref-CR6" id="ref-link-section-d38116e394">6</a>]. The hypothesis proposes that two distinct mental representations of visual space are simultaneously generated when visual information is transduced at the neuroanatomical level. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig1">1</a> outlines the functional division between these two visual representations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fmb1.tif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fmb1.tif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>A general overview of the two visual systems hypothesis. Two separate mental representations of visual space are generated when processing visual information. The ventral stream (<i>lower arrow</i>) is specialized for the identification of physical object properties and maintains a world-relative view of space, while the dorsal stream (<i>upper arrow</i>) is specialized for the guidance of visual-based motor movements and maintains an egocentric view of space</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The <i>ventral stream</i>, also known as the <i>cognitive stream</i> of visual processing, generates an allocentric, or world-relative view, of visually perceived objects in the surrounding environment. The <i>dorsal stream</i>, also known as the <i>sensorimotor stream</i> of visual processing, concurrently generates an egocentric or body-relative view of these same objects. These streams are believed to have evolved independently from the biological need to use visual information to accomplish different tasks. The ventral stream is primarily responsible for enabling active object identification and parsing complex visual scenes, including the visual perception of physical object properties such as color and shape (a “what” system) and the relative positions of objects in the scene. The dorsal stream is primarily responsible for enabling visually guided motor movements, especially for physical actions that take place within peripersonal space such as pointing, reaching, and grasping (a “how” system). It is less concerned with the relative positions of objects.</p><p>The world-relative view provided by the ventral stream is known to be susceptible to difficulties when dealing with egocentric judgments, sometimes manifested as “visual illusions.” In contrast, the egocentric view provided by the dorsal stream is known to be robust against such ambiguities. As the ventral stream was specialized to process the physical, but not spatial, characteristics of visual information (sometimes called “vision for perception”), it failed to evolve a robust mechanism for dealing with spatial ambiguities. Knowing what kinds of visual features influence the ventral and dorsal representations should be useful for predicting performance differences in target selection in VR applications between cognitive-based interactions, such as voice-based spatial target selection, versus motor-based interactions like pointing and other gestural techniques.</p><p>The original motivation for the two visual systems model came from Trevarthen’s examination of split-brain monkeys and Schneider’s work on modular retinal projections [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Trevarthen CB (1968) Two mechanisms of vision in primates. Psychol Forsch 31:299–337" href="/article/10.1007/s10055-005-0156-2#ref-CR4" id="ref-link-section-d38116e444">4</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Schneider GE (1969) Two visual systems: brain mechanisms for localization and discrimination are dissociated by tectal and cortical lesions. Science 163:895–902" href="/article/10.1007/s10055-005-0156-2#ref-CR9" id="ref-link-section-d38116e447">9</a>]. Later work by Ungerleider and Mishkin [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Ungerleider LG, Mishkin W (1982) Analysis of visual behaviour. MIT Press, Cambridge, MA" href="/article/10.1007/s10055-005-0156-2#ref-CR10" id="ref-link-section-d38116e450">10</a>] characterized the ventral and dorsal streams of visual processing as, respectively, the “what” and “where” representations of visual space. Milner and Goodale [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Milner AD, Goodale MA (1995) The visual brain in action. Oxford Psychology Series 27. Oxford University Press, New York" href="/article/10.1007/s10055-005-0156-2#ref-CR5" id="ref-link-section-d38116e453">5</a>] reported several experiments with a patient DF, who exhibited the phenomenon of blind sight, or the inability to report visual awareness while retaining the ability to physically interact with the visual world. Their studies further characterized the ventral and dorsal streams as “what” and “how” representations of visual space.</p><p>A large body of experimental evidence with healthy subjects supports the two visual systems model. Early work by Bridgeman et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Bridgeman B, Lewis S Heit, G, Nagle W (1979) Relation between cognitive and motor-oriented systems of visual position perception. Exp Psychol: Hum Percept Perform 5:692–700" href="/article/10.1007/s10055-005-0156-2#ref-CR11" id="ref-link-section-d38116e460">11</a>] showed that subjects were not aware of visual displacements timed to occur in the middle of a visual saccade, a phenomenon known as saccadic suppression. However, they also found that subjects were always able to accurately point at displaced targets, regardless of whether they were able to report the displacement. Later work by Bridgeman et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Bridgeman B, Kirch M, Sperling A (1981) Segregation of cognitive and motor aspects of visual function using induced motion. Percept Psychophys 29(4):336–342" href="/article/10.1007/s10055-005-0156-2#ref-CR12" id="ref-link-section-d38116e463">12</a>] used the visual illusion of apparent motion, or the appearance that visual targets are moving in the presence of a displaced background, and showed that motor movements toward these targets remained accurate despite cognitive reports of apparent target movement. A more recent study by Bridgeman et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469" href="/article/10.1007/s10055-005-0156-2#ref-CR6" id="ref-link-section-d38116e466">6</a>] used the induced Roelofs effect [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Roelofs C (1935) Optische localisation (Optical localization). Archiv für Augenheilkd 109:395–415" href="/article/10.1007/s10055-005-0156-2#ref-CR8" id="ref-link-section-d38116e469">8</a>] to study the two visual systems and found differences in cognitive versus sensorimotor forms of report for spatial target selection in a (real) physical environment.</p><p>There is still vigorous debate in cognitive psychology about the exact mechanisms that underlie the two visual systems hypothesis and how researchers should interpret related evidence [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Michaels CF (2000) Information, perception, and action: what should ecological psychologists learn from Milner and Goodale (1995)?. Ecol Psychol 12(3):241–258" href="/article/10.1007/s10055-005-0156-2#ref-CR13" id="ref-link-section-d38116e475">13</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Bridgeman B, Dassonville P, Bala J, Thiem P (2003) What is stored in the sensorimotor visual system: map or egocentric calibration?. In: Proceedings of the 3rd annual meeting of the vision sciences society, pp 10" href="/article/10.1007/s10055-005-0156-2#ref-CR15" id="ref-link-section-d38116e478">15</a>]. However, as a means of understanding the physiological basis for voice and gestural interaction, the two visual systems model holds considerable value. In the general domain of human–computer interaction (HCI), we have recently shown how the direct linkage between the two visual systems and the upper and lower visual fields of the human eye leads to a measurable impact on mouse and touchscreen selection performance on large graphical displays [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Po BA, Fisher BD, Booth KS (2004). Mouse and touchscreen selection in the upper and lower visual fields. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 359–366" href="/article/10.1007/s10055-005-0156-2#ref-CR16" id="ref-link-section-d38116e481">16</a>]. In the two experiments presented here, we show how voice and pointing interactions can be influenced by the two visual systems.</p><h3 class="c-article__sub-heading" id="Sec3">Visual illusions and the induced Roelofs effect</h3><p>Visual illusions are frequently used in experimental psychology to test the limits of the human visual experience but they are avoided for activities requiring visual perception in everyday life [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Luckiesh M (1965) Visual illusions: their causes, characteristics, and applications. Dover Publications, New York" href="/article/10.1007/s10055-005-0156-2#ref-CR17" id="ref-link-section-d38116e491">17</a>]. In VR, developers are often taught to consider the impact of visual illusions, although these are more often mentioned in the context of theoretical study rather than applied practice.</p><p>The two visual systems hypothesis provides an interesting context for studying visual illusions and their implications for VR. Recent psychophysical experiments testing the two visual systems hypothesis have involved the use of a visual illusion known as the induced Roelofs effect, which has graphical elements that are similar to the ones seen in many kinds of VR graphical display applications [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469" href="/article/10.1007/s10055-005-0156-2#ref-CR6" id="ref-link-section-d38116e497">6</a>–<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Roelofs C (1935) Optische localisation (Optical localization). Archiv für Augenheilkd 109:395–415" href="/article/10.1007/s10055-005-0156-2#ref-CR8" id="ref-link-section-d38116e500">8</a>]. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig2">2</a> illustrates the perceptual effects of this visual illusion. The induced Roelofs effect is best described as a systematic bias in the perceived location of targets presented within a surrounding rectangular frame. This rectangular frame is either symmetrically centered on the physical midline of a viewer or is asymmetrically offset by some distance to the left or right of the viewer. When the frame is centered, there is no perceptual bias; presented targets are consistently perceived in their correct locations. However, when the frame is offset to the left, targets within the frame are systematically perceived as being further to the right of the viewer than they really are. Likewise, when the frame is offset to the right, targets within the frame are systematically perceived as being further to the left of the viewer than they really are.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb2.tif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb2.tif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The induced Roelofs effect. When targets are surrounded by an offset rectangular frame, targets appear more to the <i>left</i> or <i>right</i> of center than they really are. In the figure, <i>solid circles</i> represent actual target positions while <i>dashed circles</i> represent perceived positions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The rectangular frame is similar to bordering elements, such as virtual window frames or the physical walls of a CAVE, which provide visual context in a graphical display. The targets within the frame are likewise similar to the icons and interactive elements such as buttons, menu items or other objects in a virtual world. Thus, the “basic” visual illusion of the induced Roelofs effect offers an opportunity to study the influence of asymmetric frames in VR and how they could be a source for unintentional errors of execution in item selection and target acquisition tasks in immersive virtual environments.</p><p>The two controlled experiments presented here use an instance of the induced Roelofs effect to understand how the two visual systems influence voice-based and gestural spatial target selection in an immersive virtual environment. Each experiment examines four different interaction techniques for specifying target location and measures the presence or absence of the systematic errors predicted by the induced Roelofs effect.</p><p>Although the graphical elements in the induced Roelofs effect are not a full VR application, they are representative components of many VR applications, as we have just noted. There are several advantages to using this basic stimulus over a more complex display. First, the simpler display removes many possible confounding display factors that could cause performance differences between the four interaction techniques. Second, using this kind of display allows us to design experiments with basic interaction tasks that are representative of those used in many VR systems. Third, using this kind of visual display allows us to understand how even the most basic of visual elements can impact user performance in VR applications.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Predictions of user performance</h2><div class="c-article-section__content" id="Sec4-content"><p>The classic predictions associated with the two visual systems hypothesis suggest that purely cognitive forms of spatial interaction are susceptible to the perceptual biases of visual illusions while purely motor forms of interaction are robust against these biases. The hypothesis states that these performance differences are a direct result of different mental representations of visual space. Based on this, we formulated the following hypotheses: </p><ul class="u-list-style-dash">
                  <li>
                    <p><i>Voice-based input</i> is an inherently cognitive form of interaction that solely depends on the ventral stream of visual perception because no direct, physical movement is required for the response. Thus, this kind of interaction will be most susceptible to perceptual ambiguities such as the induced Roelofs effect.</p>
                  </li>
                  <li>
                    <p><i>Pointing without visual feedback</i> (i.e. without any visible graphical cursor) is an inherently motor form of interaction that solely depends on the dorsal stream of visual perception because there is a direct, physical movement required for response and there is no reliable way to make cognitive corrections to initial pointing movements. Thus, this kind of interaction will be unaffected by perceptual ambiguities such as the induced Roelofs effect.</p>
                  </li>
                </ul><p> This type of prediction is central to most of the experimental work on the two visual systems in cognitive psychology. We extended these by making interpretations consistent with the two visual systems hypothesis to predict user performance with other interaction techniques. We formulated two other experimental hypotheses: </p><ul class="u-list-style-dash">
                  <li>
                    <p><i>Pointing with visual feedback</i> (i.e. with a visible graphical cursor) engages the ventral stream of visual perception, thereby making it dependent on the cognitive representation of visual space. Thus, the presence of visual feedback means that “closed loop” interactions will be susceptible to perceptual ambiguities such as the induced Roelofs effect.</p>
                  </li>
                  <li>
                    <p><i>Pointing with lagged visual feedback</i> (i.e. with a temporally delayed graphical cursor) could engage either the ventral or dorsal streams of visual perception based on the interaction strategy employed by the user. Users who disregard the visual feedback effectively make the pointing interaction an “open loop” interaction, like pointing without visual feedback. Users who depend on the visual feedback effectively make the pointing interaction a “closed loop” interaction, like pointing with visual feedback. Thus, the presence of lagged visual feedback could cause some users to be susceptible to perceptual ambiguities such as the induced Roelofs effect, while other users might not be affected.</p>
                  </li>
                </ul><p> These hypotheses may seem counterintuitive. They predict that voice-based interaction will be more susceptible to perceptual errors than will other kinds of physical interaction and they predict that the pointing performance will be poorer <i>with</i> visual feedback present than when it is absent. Moreover, our hypotheses predict that a lag in displaying the visual cursor might actually <i>improve</i> performance compared to a nonlagged visual cursor. In contrast to these predictions, most VR applications assume that reliable feedback in the form of a visual cursor is necessary for pointing; emerging multimodal techniques assume that voice-based techniques are not susceptible to errors induced by the presence of graphical frames; and it is almost universally assumed that the presence of lag in visual cursors is detrimental to performance.</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Experiment one</h2><div class="c-article-section__content" id="Sec5-content"><p>Our first experiment in the study was an initial test of our theoretical predictions. We devised a simple target-acquisition task where vocal localization and spatial gesture interaction were equally feasible methods of interaction. In this experiment, subjects completed four blocks of trials that required them to select presented targets from fixed positions. Each block of trials used a different interaction mechanism for selection. One block used voice-based input and three blocks used pointing under varying levels of visual feedback. The experimental method was described in an earlier report that focused on the individual differences between subjects [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22–38" href="/article/10.1007/s10055-005-0156-2#ref-CR7" id="ref-link-section-d38116e604">7</a>], but we summarize it again here for comparison with the second experiment.</p><h3 class="c-article__sub-heading" id="Sec6">Methods</h3><p>This experiment used a within-subjects experimental design with four distinct conditions. Every subject attended a single, individual session lasting between 60 and 90 min where they completed all four conditions. Each condition was characterized by the use of a specific interaction technique and consisted of one block of 54 trials. Every subject completed four blocks (one for each condition) for a total of 216 trials. The conditions were identified as follows: </p><ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p><i>Voice-based input</i>. A multiple-choice voice protocol was used for target selection. All subjects began their sessions with this interaction method for target selection. No physical pointing interactions occurred in this condition. The three remaining conditions used a continuous spatial pointing interaction for target selection. A hand-held pointer (i.e. a pointing stylus) was used. These conditions differed from one another by the kind of visual feedback provided during trials.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p><i>Pointing without visual feedback</i>. No tracking cursor was visible during this condition, meaning that subjects were effectively “blind” to their pointing movements during this block of trials.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p><i>Pointing with visual feedback</i>. A tracking cursor was visible during trial pointing. The cursor was a graphical cross hair similar to the kind of visual feedback often used in interactive desktop and VR environments.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">4.</span>
                      
                        <p><i>Pointing with lagged visual feedback</i>. A small temporally delayed tracking cross hair was present during pointing. This was done by adding a one-half second lag to the cursor used in the pointing with visual feedback condition. Our intention was to identify the potential influence of lag on the two visual systems rather than to simulate the response lag seen in typical virtual environments. All subjects finished their sessions with this condition.</p>
                      
                    </li>
                  </ol><p>Conditions (2) and (3) were counterbalanced such that half of the subjects started with one condition before the other while the other half were presented with the conditions in the reverse order. This was done to keep the number of required subjects to a minimum compared to the number required for a fully counterbalanced study. Based on the literature, we were confident that the voice-based condition would exhibit an effect, so we used it as a “baseline” condition. We kept the lagged cursor condition last because we were less confident about it and did not want to confound the comparison between the other two pointing conditions even though early pilot studies had suggested that our predictions were valid.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Subjects</h4><p>Fourteen subjects between the ages of 17 to 31 were recruited for this study. Seven of the participants were male and seven of the participants were female. All had normal or corrected-to-normal vision. Twelve were right-handed and two were left-handed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Apparatus</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig3">3</a> presents a photograph of the VR display setting used in this study. The environment consisted of a three-screen, wide-angle projection surface, though only the center surface was used for the study. The active display was forward-projected and it had physical dimensions of 275×215 cm. Subjects were seated at a distance of 250 cm to avoid projector occlusion effects. With the exception of illumination from the projector and display surface, all light sources were extinguished. During the experiment, an experimenter was always present to facilitate session progression. A PC workstation and custom software were used to render trials and record quantitative data.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb3.tif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb3.tif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The VR display used for both experiments. Subjects were centered and seated before a three-screen, wide-angle display. Only the center display was used in this study. Spatial interaction was provided by a Polhemus Fastrak with an attached stylus and head-tracker. Arms and hands were kept underneath a large wooden table at all times. During sessions, all ambient light was extinguished and an experimenter was always present</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>A large, wooden table was constructed and positioned directly in front of subjects to obscure viewing of their hands and arms during sessions. The table had dimensions of 120×95×80 cm (width, depth, and height). These table dimensions ensured that all subjects would have enough space to make free distal pointing movements. By requiring subjects to keep their limbs underneath the table throughout the experiment, so that they could not see where their hands were pointing, we were able to strictly control their perception of visual information to only that provided by the display. Subjects still had proprioceptive information about their physical pointing actions.</p><p>A Polhemus Fastrak was used for pointing in this experiment. Prior to each session, the Fastrak was calibrated using software and all sources of metallic interference were kept away from the transmitter and sensors. Subjects wore a head-tracker and a pair of active stereo glasses even though no stereoscopic imagery was presented to them during this study. These requirements were made to ensure compatibility with future studies that might involve binocular perception. A stylus pointer attachment for the Polhemus Fastrak was held in the dominant hand of subjects during the pointing conditions. The form factor of the stylus was similar to that of a laser pointer or tracking wand, making it an ideal interaction device for experimental purposes. With the head-tracker and the stylus operating simultaneously, the Fastrak achieved an update rate of 60 Hz.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Procedure</h4><p>Trials consisted of a 1-s presentation of a single, red circular target surrounded by a green rectangular frame on a black background (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig2">2</a>). The circular targets were 0.5° of visual angle in diameter and could appear in one of three locations, either centered on the subject, or offset to the left or right of center by 1.5° of visual angle. The rectangular frames were 21.0° in width by 9.0° in height, with a thickness of 1°. The frames were either centered on the screen relative to a projection of the participant’s midline, or they were offset to the left or right of center by 4.0°.</p><p>After 1 s, the target and frame vanished, leaving only the black background. Either immediately, or after a 4-s delay, subjects were instructed to indicate the position of the now-extinguished target using the interaction technique specified for the condition they were completing. This “extinguish and point” design was used to ensure a fair assessment of performance between all of the tested interaction conditions. If the target remained present on the display, the pointing without visual feedback condition would be at a severe disadvantage compared to the other interaction conditions and we might not be able to learn anything about the differences between interaction techniques relative to the two visual systems.</p><p>These trial parameters resulted in 18 trial types: three target positions, three frame positions, and two response delays. Each trial type was repeated three times, yielding a total of 54 trials per condition. Trials were randomized prior to presentation in each condition such that no two consecutive trials in a condition had the same target position and frame position.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Voice condition</h4><p>Voice-based input was simulated for this experiment in a Wizard-of-Oz fashion. This meant that subjects were told to indicate target positions by providing vocal commands to the display software. In reality, the experimenter monitored subject responses and manually entered the responses into the recording software by hand. Once participants were told to respond, they did so by providing a vocal command in the form of one of five possible choices: “Far Left,” “Left,” “Center,” “Right,” and “Far Right.” Each choice corresponded to the potentially perceived position of a given target position. Although there were only three possible target positions in the actual trial set, the induced Roelofs effect could have made it appear as though targets were at a greater eccentricity than they really were. In these instances, the “Far Left” and “Far Right” responses allowed subjects to respond appropriately to their perception of target positions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Pointing conditions</h4><p>Pointing interactions were accomplished with the use of the Polhemus Fastrak and attached stylus. Responses were made by aiming the stylus at the display like a laser pointer. Once participants were satisfied with where they were aiming, they maintained their aim for approximately 2 s until an audio confirmation was provided. This kind of “point-and-dwell” response was required to avoid the inaccurate responses due to the pen-drop phenomenon that is common with button presses on input devices not grounded on a surface [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Myers BA, Bhatnagar R, Nichols J, Peck CH, Kong D, Miller R, Long AC (2002) Interacting a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 33–40" href="/article/10.1007/s10055-005-0156-2#ref-CR18" id="ref-link-section-d38116e739">18</a>].</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">Training</h4><p>Subjects were provided with instructions at the beginning of the session and prior to the start of each experimental condition. Each block of 54 trials was preceded by a minimum of 15 practice trials where subjects were offered the opportunity to familiarize themselves with the response protocol for that particular condition. Practice trials were presented in roughly the same fashion as actual experimental trials with the exception that the rectangular frame remained fixed in a centered position.</p><h3 class="c-article__sub-heading" id="Sec13">Results</h3><p>Subject data were analyzed using statistical techniques that are consistent with those used in psychophysical analysis and other similar human factors experiments [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Po BA, Fisher BD, Booth KS (2004). Mouse and touchscreen selection in the upper and lower visual fields. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 359–366" href="/article/10.1007/s10055-005-0156-2#ref-CR16" id="ref-link-section-d38116e758">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Vicente KJ, Torenvliet GL (2000) The earth is spherical (p &lt; 0.05): alternative methods of statistical inference. Theor Issues Ergon Sci 1(3):248–271" href="/article/10.1007/s10055-005-0156-2#ref-CR19" id="ref-link-section-d38116e761">19</a>]. Our primary analysis consisted of a series of two-way analyses of variance (ANOVAs) with repeated measures over independent factors of target position and frame position for each subject, condition, and response delay. These two-way ANOVAs allowed us to test for the presence or absence of the induced Roelofs effect in each subject and each interaction technique, which in turn allowed us to verify our theoretical predictions. While we present a new aggregate analysis of our data here, further analysis in the context of individual differences is available in our earlier report [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22–38" href="/article/10.1007/s10055-005-0156-2#ref-CR7" id="ref-link-section-d38116e764">7</a>].</p><p>If a participant were biased by the induced Roelofs effect with a particular kind of interaction, it would be visible in the ANOVA as a statistically significant main effect of frame position with an accompanying main effect of target position, uncomplicated by higher-order interaction effects. In terms of our experimental predictions, we expected a high proportion of our subjects to show main effects of frame position in the voice-based input and pointing with visual feedback conditions. Fewer were expected to show such effects in pointing with lagged visual feedback. We expected few, if any, to show main effects of frame position in pointing without visual feedback.</p><p>We used a dependent measure of subject response that differed depending on the condition being evaluated. For the voice-based input condition, subject responses were provided categorically, as one of the five previously described vocal choices. For the pointing conditions, subject responses were provided in a continuous fashion, recorded as the position on the graphical display where a line projected along the major axis of the stylus would intersect the screen. Since there were only horizontal variations in target and frame positions across trials, only the horizontal or <i>x</i> axis of participant response was used in our analysis.</p><p>In the discussion that follows, we adopt standard practice in experimental psychophysics and only report the smallest <i>F</i> values for significant results and the largest <i>F</i> values for nonsignificant results. Across all conditions, all 14 subjects had statistically significant main effects of target position [ <i>F</i>(2, 18) &gt; 4.214, <i>P</i> &lt; 0.032]. This indicated that their responses were highly consistent and highly reliable and thus provided us with assurance that subjects were completing the verbal and pointing tasks as instructed.</p><p>With respect to main effects of frame position for each subject, we found significant differences in susceptibility depending on the interaction technique, indicating that some interaction techniques were indeed more susceptible to the induced Roelofs effect than others. The following results are sorted from “most susceptible” to “least susceptible.” </p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Voice-based input</i>: Ten of the 14 subjects had significant main effects of frame position [ <i>F</i>(2, 18) &gt; 4.460, <i>P</i> &lt; 0.027], indicating that a majority of them were biased by the induced Roelofs effect.</p>
                    </li>
                    <li>
                      <p><i>Pointing with visual feedback</i>: Eight of the 14 subjects had significant main effects of frame position [ <i>F</i>(2, 18) &gt; 3.850, <i>P</i> &lt; 0.05]. A chi-square test against voice-based input showed no significant difference in the number of subjects affected by the illusion [(1, <i>N</i> = 14) = 1.4, <i>P</i> = 0.237].</p>
                    </li>
                    <li>
                      <p><i>Pointing with lagged visual feedback</i>: Six of the 14 participants had main effects of frame position [ <i>F</i>(2, 18) &gt; 4.280, <i>P</i> &lt; 0.030]. A chi-square test against voice-based input confirmed that there was a significant difference in the number of subjects affected by the illusion [(1, <i>N</i> = 14) = 5.6, <i>P</i> = 0.018].</p>
                    </li>
                    <li>
                      <p><i>Pointing without visual feedback</i>: Only 4 of the 14 participants had main effects of frame position [ <i>F</i>(2, 18) &gt; 5.650, <i>P</i> &lt; 0.013], indicating that a majority of them were <i>not</i> biased by the induced Roelofs effect. A chi-square test against voice-based input confirmed that there was a significant difference in the number of subjects affected by the illusion [(1, <i>N</i> = 14) = 12.6, <i>P</i> &lt; 0.001].</p>
                    </li>
                  </ul><p> For the voice-based input condition, the measured effect size for those with main effects of frame position was exactly one target position, or 1.5° of visual angle. For the three pointing conditions, the measured effect size for those with corresponding main effects was approximately half of a target position, or 0.75° of visual angle. Interestingly, we found no consistent differences for any of the response delay analyses, which was a parameter included for compatibility with several two visual systems experiments reported previously in the cognitive psychology literature [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469" href="/article/10.1007/s10055-005-0156-2#ref-CR6" id="ref-link-section-d38116e874">6</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Bridgeman B, Kirch M, Sperling A (1981) Segregation of cognitive and motor aspects of visual function using induced motion. Percept Psychophys 29(4):336–342" href="/article/10.1007/s10055-005-0156-2#ref-CR12" id="ref-link-section-d38116e877">12</a>].</p><p>The subject data and analysis from this first experiment supported our theoretical predictions. Within the framework of the two visual systems hypothesis, we can interpret our results as evidence that each of these interaction techniques predominantly draws upon different representations of visual space. Voice-based input and pointing with visual feedback appear to draw from the world-relative ventral representation of space while pointing without visual feedback and pointing with lagged visual feedback are more likely to draw from the egocentric dorsal representation of space.</p></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Experiment two</h2><div class="c-article-section__content" id="Sec14-content"><p>Although our first experiment yielded interesting results, the possibility remained that this was not actually due to the two visual systems but to the presence of uncontrolled variables in the experimental design. First, the lack of full counterbalancing meant that subjects could have been influenced by order effects. Second, the use of discrete target positions meant that subjects could have resorted to perceptual memory to simply recall where targets were. Third, it was unclear why some subjects were still susceptible to the induced Roelofs effect with the supposedly dorsal-driven physical pointing interaction conditions. A fourth problem, which was a consequence of the choice of discrete targets, was that we could not have a uniform measure of target selection error across all four interaction techniques because the categorical responses for the voice-based input could not be reliably scaled to match those in the pointing conditions.</p><p>We decided that it was important to conduct a second experiment with more stringent controls to address the weaknesses that were present in the first experiment and to allow more direct comparison of voice-based input and pointing. Many of these changes were the result of suggestions or comments on our preliminary report of Experiment 1 [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22–38" href="/article/10.1007/s10055-005-0156-2#ref-CR7" id="ref-link-section-d38116e893">7</a>]. We also took this opportunity to simplify the experimental design. In this experiment, only immediate responses were made by subjects because there was no effect of response delay (immediate versus a 4-s delay) in Experiment 1.</p><h3 class="c-article__sub-heading" id="Sec15">Methods</h3><p>As in the first experiment, we used a completely within-subjects design where subjects were asked to complete four blocks of target selection trials (one voice-based and three pointing) in a single, individual session. However, this experiment included a full counterbalancing of experimental conditions, which subsequently led to the testing of a much larger group of subjects. Moreover, this experiment presented targets over a continuous range instead of having them appear at fixed positions as in the previous study, allowing us to make direct comparisons of target selection accuracy across all four conditions. In addition, the voice and pointing interaction protocols used in this experiment were substantially improved, which led to more rapid and more accurate measures of subject responses within individual trials.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Subjects</h4><p>Twenty-four new subjects (16 males and 8 females) participated in this experiment. Their ages ranged from 18 to 31 years. All were right-handed and had normal or corrected-to-normal vision.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Apparatus and procedure</h4><p>Similar to the first experiment, subjects were instructed to specify the location of targets surrounded by rectangular frames that were presented for exactly 1 s. The four kinds of interaction used in the first experiment were once again tested here. Most of the graphical display parameters remained the same, except that the circular targets were 1° of visual angle in diameter (double the size of the targets in Experiment 1) and could appear anywhere along an 8° horizontal range from the center of the display (up to 4° to the left or right of center).</p><p>In the voice-based input condition, subjects specified their perceived location of targets on a nine-point scale, with the verbal indication “one” being furthest to the left and the verbal indication “nine” being furthest to the right. The choice of a particular point along the scale was effectively a subject’s closest estimation of a target’s presented position. Subjects were told to use whole numbers in their responses and that fractional values would not be accepted even though targets could be at nonintegral positions.</p><p>During pointing trials, a passive stylus for the Polhemus Fastrak was held in the dominant (right-hand) of subjects while a standard mouse was held in the nondominant (left-hand) of subjects. The two-handed stylus and mouse setup allowed subjects to point at the display with one hand while pressing a mouse button to confirm their final pointing position. This was used as a much cleaner alternative to the “point-and-dwell” mechanism used in the first experiment.</p><p>Unlike the first experiment, where subjects either responded immediately or after a 4-s delay, subjects were instructed to always respond immediately after the presented target and frame had vanished. Trials were repeated 12 times for each of the three possible frame positions (nonoffset, offset left, and offset right) for a total of 48 trials per condition (4 blocks × 48 trials = 192 trials per subject). Trials were randomized in each condition so that targets were uniformly distributed across the 8° horizontal visual range.</p><h3 class="c-article__sub-heading" id="Sec18">Results</h3><p>Like the first experiment, the statistical analysis of subject data was primarily meant to identify the presence or absence of the induced Roelofs effect. However, the refined design of this second experiment also allowed us to better characterize the individual differences between subjects and interaction conditions. A global one-way ANOVA was performed on each of the four interaction conditions to analyze responses to manipulations of frame position across all subjects. This analysis was complemented by individual one-way ANOVAs for each subject in each of the four interaction conditions, consistent with the analysis used in the first experiment. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig4">4</a> provides a summary of the primary analysis, measuring the effect size of the induced Roelofs effect across the varying frame positions and different interaction types tested in this study. The statistics below are again sorted from “most susceptible” to “least susceptible.” </p><ul class="u-list-style-dash">
                    <li>
                      <p><i>Voice-based input</i>: We found a statistically significant main effect of frame position overall for voice-based responses [ <i>F</i>(2, 766) = 252.85, <i>P</i> &lt; 0.001]. Sixteen of the 24 subjects were individually found to have significant main effects of frame position [ <i>F</i>(2, 30) &gt; 3.35, <i>P</i> &lt; 0.049].</p>
                    </li>
                    <li>
                      <p><i>Pointing with visual feedback</i>: We found an overall significant main effect of frame position for pointing with a visual cursor [ <i>F</i>(2, 766) = 27.91, <i>P</i> &lt; 0.01]. Fourteen of the 24 subjects had individually significant main effects of frame position [ <i>F</i>(2, 30) &gt; 3.80, <i>P</i> &lt; 0.034].</p>
                    </li>
                    <li>
                      <p><i>Pointing with lagged visual feedback</i>: We found no overall significant main effect of frame position for pointing with a lagged cursor [ <i>F</i>(2, 766) = 2.26, <i>P</i> = 0.105]. None of the subjects had any significant main effects of frame position [ <i>F</i>(2, 30) &lt; 1.82, <i>P</i> &gt; 0.180].</p>
                    </li>
                    <li>
                      <p><i>Pointing without visual feedback</i>: We found no overall significant main effect of frame position when pointing without any cursor [ <i>F</i>(2, 766) = 1.29, <i>P</i> = 0.277]. None of the subjects had any significant main effects of frame position [ <i>F</i>(2, 30) &lt; 0.88, <i>P</i> &gt; 0.425].</p>
                    </li>
                  </ul><p> Our results in this experiment were consistent with our theoretical predictions and with the results found in the previous experiment. The additional controls added into this experiment give us confidence that our results were not just random variations but are evidence that the presence of certain kinds of graphical information can bias visual processing, and in turn user performance, in systematic, predictable ways. The use of continuous rather than discrete targets in all four conditions allowed us to directly compare performance across the four conditions by measuring effect size, as illustrated in Fig.<a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-005-0156-2#Fig4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb4.tif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/MediaObjects/s10055-005-0156-2fhb4.tif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Measured effect size of the induced Roelofs effect across the different interaction types and varying frame positions in Experiment 2. Effect size indicates the degree to which subject responses deviated from actual target positions. Negative effect sizes indicate response deviations to the <i>left</i> while positive effect sizes indicate response deviations to the <i>right</i>, measured in degrees of visual angle. The steep slopes associated with voice-based input and pointing with visual feedback indicate these interaction types were highly susceptible to the induced Roelofs effect. The corresponding horizontal slopes associated with pointing with lagged visual feedback and pointing without visual feedback indicate these interaction types were highly robust against the induced Roelofs effect</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-005-0156-2/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Discussion</h2><div class="c-article-section__content" id="Sec19-content"><p>The two experiments presented here suggest that the two visual systems hypothesis has relevance for VR and an understanding of voice and gestural interaction. By showing how user performance can be manipulated in this context, several lessons can be learned. </p><ul class="u-list-style-dash">
                  <li>
                    <p><i>The relationship between visual perception and motor action is important for VR</i>. Many virtual environments have display form factors so large that the only perceptual cues provided by the environment are the ones that are being provided by the rendered graphics. Unlike desktop settings, where there are numerous kinds of perceptual framing cues like the physical edges of a monitor, immersive displays must often contend with having fewer contextual cues, leading to an increased chance that perceptually ambiguous effects such as visual illusions could have a substantial impact on user performance in VR if elements in the display masquerade as frames.</p>
                  </li>
                  <li>
                    <p><i>Perceptual judgments are not necessarily the same as motor judgments</i>. The ability to judge object sizes and spatial location is extremely important for certain kinds of tasks in VR, including computer-aided design (CAD) and design reviews in engineering. For systems that are safety-critical, it is possible to guard against illusory biases by using interactions that draw upon the dorsal representation of visual space rather than the ventral representation.</p>
                  </li>
                  <li>
                    <p><i>Voice interaction is more reliant on cognition than is gestural interaction</i>. The two visual systems model tells us that voice relies on “vision for perception” while gesture relies on “vision for action.” This suggests that voice interaction might benefit from avoiding perceptual ambiguities in the physical structure of display information (i.e. ambiguities in visual cues such as color or texture) while gestural interaction might benefit from visual cues that lend themselves to motor manipulation.</p>
                  </li>
                  <li>
                    <p><i>Even basic graphical elements can have a profound impact on visually guided interactions</i>. Considering how simple graphical elements like rendered frames and visual cursors can bias user performance, seemingly “obvious” design choices such as the inclusion of a tracking cursor or the presence of contextual asymmetry should be carefully assessed. This could be especially important when graphical elements are placed in the context of a much more complex display and cognitive resources are limited. Our experimental results suggest that minimization of visual information could be used to learn how to make interaction less demanding for users.</p>
                  </li>
                </ul><p> In both experiments, participants’ data were analyzed completely within-subjects because we wanted a measure of the proportion of participants who were significantly affected by the induced Roelofs effect under the various methods of input. This is consistent with statistical practice in experimental psychophysics and with more recent innovations in statistical inference that downplay the role of simple null hypothesis tests as the only criterion for experimental success [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469" href="/article/10.1007/s10055-005-0156-2#ref-CR6" id="ref-link-section-d38116e1088">6</a>]. Vicente and Torenvliet [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Vicente KJ, Torenvliet GL (2000) The earth is spherical (p &lt; 0.05): alternative methods of statistical inference. Theor Issues Ergon Sci 1(3):248–271" href="/article/10.1007/s10055-005-0156-2#ref-CR19" id="ref-link-section-d38116e1091">19</a>] observe that averaging results across participants can be statistically misleading, yet this is often the only technique used to determine whether a particular phenomenon is practically significant. They further indicate that an analysis of participants individually, as was done here, is a robust alternative method of statistical inference that can yield more information about practical significance than a single null hypothesis test alone. We chose to expose our results in this fashion to offer readers more information with which to make their own judgments about whether the effects presented here are practically significant for their own applications.</p><p>With respect to the use of voice and gesture in VR, our results should not be interpreted to suggest that one kind of interaction is uniformly better than another. Rather, these experiments should suggest to researchers and developers that challenging one’s intuition about the traditional use of visual information is especially important for VR systems that intend to succeed in real-world applications. The ability to combine voice and gesture into a virtual environment has advantages in many situations, but it is important to understand the elements that make these interactions useful and it is equally important to understand how their use changes the way visual items are perceived and processed to make decisions about presented information on graphical displays.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Conclusion</h2><div class="c-article-section__content" id="Sec20-content"><p>We have presented two controlled experiments that compared user performance with four different interaction techniques in a virtual environment. Our results characterized how the presence or absence of certain visual cues, such as tracking cursors and asymmetric frames, can influence voice and gestural interaction for spatial target selection. The induced Roelofs effect is only one of a broad class of visual illusions that may behave similarly; it serves as a test case. Our results demonstrate that a two visual systems approach to interaction can help researchers better understand the relationship between basic graphical elements and their impact on multimodal interaction in VR applications. The same may be true for other visual illusions, suggesting a number of avenues for future research.</p></div></div></section>
                        
                    

                    <section aria-labelledby="abbreviations"><div class="c-article-section" id="abbreviations-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="abbreviations">Abbreviations</h2><div class="c-article-section__content" id="abbreviations-content"><dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>VR:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Virtual reality</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>HCI:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Human–computer interaction</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>ANOVA:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Analysis of variance</p>
                  </dd><dt class="c-abbreviation_list__term"><dfn>CAD:</dfn></dt><dd class="c-abbreviation_list__description">
                    <p>Computer-aided design</p>
                  </dd></dl></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="IE. Sutherland, " /><meta itemprop="datePublished" content="1965" /><meta itemprop="headline" content="Sutherland IE (1965) The ultimate display. Proc IFIP Cong 65:506–508" /><span class="c-article-references__counter">1.</span><p class="c-article-references__text" id="ref-CR1">Sutherland IE (1965) The ultimate display. Proc IFIP Cong 65:506–508</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20ultimate%20display&amp;journal=Proc%20IFIP%20Cong&amp;volume=65&amp;pages=506-508&amp;publication_year=1965&amp;author=Sutherland%2CIE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bolt RA (1980) “Put-that-there”: voice and gesture at the graphics interface. In: Proceedings of the 7th inter" /><span class="c-article-references__counter">2.</span><p class="c-article-references__text" id="ref-CR2">Bolt RA (1980) “Put-that-there”: voice and gesture at the graphics interface. In: Proceedings of the 7th international conference on computer graphics and interactive techniques (SIGGRAPH), pp 262–270</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SL. Oviatt, PR. Cohen, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Oviatt SL Cohen PR (2000) Multimodal interfaces that process what comes naturally. Commun ACM 43(3):45–53" /><span class="c-article-references__counter">3.</span><p class="c-article-references__text" id="ref-CR3">Oviatt SL Cohen PR (2000) Multimodal interfaces that process what comes naturally. Commun ACM 43(3):45–53</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multimodal%20interfaces%20that%20process%20what%20comes%20naturally&amp;journal=Commun%20ACM&amp;volume=43&amp;issue=3&amp;pages=45-53&amp;publication_year=2000&amp;author=Oviatt%2CSL&amp;author=Cohen%2CPR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CB. Trevarthen, " /><meta itemprop="datePublished" content="1968" /><meta itemprop="headline" content="Trevarthen CB (1968) Two mechanisms of vision in primates. Psychol Forsch 31:299–337" /><span class="c-article-references__counter">4.</span><p class="c-article-references__text" id="ref-CR4">Trevarthen CB (1968) Two mechanisms of vision in primates. Psychol Forsch 31:299–337</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00422717" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3ACCaD1c%252FksVw%253D" aria-label="View reference 4 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4973634" aria-label="View reference 4 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two%20mechanisms%20of%20vision%20in%20primates&amp;journal=Psychol%20Forsch&amp;volume=31&amp;pages=299-337&amp;publication_year=1968&amp;author=Trevarthen%2CCB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="AD. Milner, MA. Goodale, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Milner AD, Goodale MA (1995) The visual brain in action. Oxford Psychology Series 27. Oxford University Press," /><span class="c-article-references__counter">5.</span><p class="c-article-references__text" id="ref-CR5">Milner AD, Goodale MA (1995) The visual brain in action. Oxford Psychology Series 27. Oxford University Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20visual%20brain%20in%20action.%20Oxford%20Psychology%20Series%2027&amp;publication_year=1995&amp;author=Milner%2CAD&amp;author=Goodale%2CMA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Bridgeman, S. Peery, S. Anand, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept P" /><span class="c-article-references__counter">6.</span><p class="c-article-references__text" id="ref-CR6">Bridgeman B, Peery S, Anand S (1997) Interaction of cognitive and sensorimotor maps of visual space. Percept Psychophys 59(3):456–469</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3AByiB1c3gt1w%253D" aria-label="View reference 6 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=9136275" aria-label="View reference 6 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interaction%20of%20cognitive%20and%20sensorimotor%20maps%20of%20visual%20space&amp;journal=Percept%20Psychophys&amp;volume=59&amp;issue=3&amp;pages=456-469&amp;publication_year=1997&amp;author=Bridgeman%2CB&amp;author=Peery%2CS&amp;author=Anand%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display" /><span class="c-article-references__counter">7.</span><p class="c-article-references__text" id="ref-CR7">Po BA, Fisher BD, Booth KS (2003) Pointing and visual feedback for spatial interaction in large-screen display environments. In: Proceedings of the 3rd international symposium on smart graphics, pp 22–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Roelofs, " /><meta itemprop="datePublished" content="1935" /><meta itemprop="headline" content="Roelofs C (1935) Optische localisation (Optical localization). Archiv für Augenheilkd 109:395–415" /><span class="c-article-references__counter">8.</span><p class="c-article-references__text" id="ref-CR8">Roelofs C (1935) Optische localisation (Optical localization). Archiv für Augenheilkd 109:395–415</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optische%20localisation%20%28Optical%20localization%29&amp;journal=Archiv%20f%C3%BCr%20Augenheilkd&amp;volume=109&amp;pages=395-415&amp;publication_year=1935&amp;author=Roelofs%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GE. Schneider, " /><meta itemprop="datePublished" content="1969" /><meta itemprop="headline" content="Schneider GE (1969) Two visual systems: brain mechanisms for localization and discrimination are dissociated b" /><span class="c-article-references__counter">9.</span><p class="c-article-references__text" id="ref-CR9">Schneider GE (1969) Two visual systems: brain mechanisms for localization and discrimination are dissociated by tectal and cortical lesions. Science 163:895–902</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3ACCaC3czlsVU%253D" aria-label="View reference 9 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=5763873" aria-label="View reference 9 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Two%20visual%20systems%3A%20brain%20mechanisms%20for%20localization%20and%20discrimination%20are%20dissociated%20by%20tectal%20and%20cortical%20lesions&amp;journal=Science&amp;volume=163&amp;pages=895-902&amp;publication_year=1969&amp;author=Schneider%2CGE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="LG. Ungerleider, W. Mishkin, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Ungerleider LG, Mishkin W (1982) Analysis of visual behaviour. MIT Press, Cambridge, MA" /><span class="c-article-references__counter">10.</span><p class="c-article-references__text" id="ref-CR10">Ungerleider LG, Mishkin W (1982) Analysis of visual behaviour. MIT Press, Cambridge, MA</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20visual%20behaviour&amp;publication_year=1982&amp;author=Ungerleider%2CLG&amp;author=Mishkin%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Bridgeman, S. Lewis, G. Heit, W. Nagle, " /><meta itemprop="datePublished" content="1979" /><meta itemprop="headline" content="Bridgeman B, Lewis S Heit, G, Nagle W (1979) Relation between cognitive and motor-oriented systems of visual p" /><span class="c-article-references__counter">11.</span><p class="c-article-references__text" id="ref-CR11">Bridgeman B, Lewis S Heit, G, Nagle W (1979) Relation between cognitive and motor-oriented systems of visual position perception. Exp Psychol: Hum Percept Perform 5:692–700</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1037%2F%2F0096-1523.5.4.692" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3ABi%252BC3MnptlI%253D" aria-label="View reference 11 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Relation%20between%20cognitive%20and%20motor-oriented%20systems%20of%20visual%20position%20perception&amp;journal=Exp%20Psychol%3A%20Hum%20Percept%20Perform&amp;volume=5&amp;pages=692-700&amp;publication_year=1979&amp;author=Bridgeman%2CB&amp;author=Lewis%2CS&amp;author=Heit%2CG&amp;author=Nagle%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="B. Bridgeman, M. Kirch, A. Sperling, " /><meta itemprop="datePublished" content="1981" /><meta itemprop="headline" content="Bridgeman B, Kirch M, Sperling A (1981) Segregation of cognitive and motor aspects of visual function using in" /><span class="c-article-references__counter">12.</span><p class="c-article-references__text" id="ref-CR12">Bridgeman B, Kirch M, Sperling A (1981) Segregation of cognitive and motor aspects of visual function using induced motion. Percept Psychophys 29(4):336–342</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=7279556" aria-label="View reference 12 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Segregation%20of%20cognitive%20and%20motor%20aspects%20of%20visual%20function%20using%20induced%20motion&amp;journal=Percept%20Psychophys&amp;volume=29&amp;issue=4&amp;pages=336-342&amp;publication_year=1981&amp;author=Bridgeman%2CB&amp;author=Kirch%2CM&amp;author=Sperling%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CF. Michaels, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Michaels CF (2000) Information, perception, and action: what should ecological psychologists learn from Milner" /><span class="c-article-references__counter">13.</span><p class="c-article-references__text" id="ref-CR13">Michaels CF (2000) Information, perception, and action: what should ecological psychologists learn from Milner and Goodale (1995)?. Ecol Psychol 12(3):241–258</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1207%2FS15326969ECO1203_4" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%2C%20perception%2C%20and%20action%3A%20what%20should%20ecological%20psychologists%20learn%20from%20Milner%20and%20Goodale%20%281995%29%3F&amp;journal=Ecol%20Psychol&amp;volume=12&amp;issue=3&amp;pages=241-258&amp;publication_year=2000&amp;author=Michaels%2CCF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Kerzel, B. Hommel, H. Bekkering, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Kerzel D, Hommel B, Bekkering H (2001) A Simon effect induced by motion and location: evidence for a direct li" /><span class="c-article-references__counter">14.</span><p class="c-article-references__text" id="ref-CR14">Kerzel D, Hommel B, Bekkering H (2001) A Simon effect induced by motion and location: evidence for a direct linkage of cognitive and motor maps. Percept Psychophys 63(5):862–874</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="/articles/cas-redirect/1%3ASTN%3A280%3ADC%252BD3MvnvVejsw%253D%253D" aria-label="View reference 14 on CAS">CAS</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11521852" aria-label="View reference 14 on PubMed" rel="nofollow">PubMed</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20Simon%20effect%20induced%20by%20motion%20and%20location%3A%20evidence%20for%20a%20direct%20linkage%20of%20cognitive%20and%20motor%20maps&amp;journal=Percept%20Psychophys&amp;volume=63&amp;issue=5&amp;pages=862-874&amp;publication_year=2001&amp;author=Kerzel%2CD&amp;author=Hommel%2CB&amp;author=Bekkering%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bridgeman B, Dassonville P, Bala J, Thiem P (2003) What is stored in the sensorimotor visual system: map or eg" /><span class="c-article-references__counter">15.</span><p class="c-article-references__text" id="ref-CR15">Bridgeman B, Dassonville P, Bala J, Thiem P (2003) What is stored in the sensorimotor visual system: map or egocentric calibration?. In: Proceedings of the 3rd annual meeting of the vision sciences society, pp 10</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Po BA, Fisher BD, Booth KS (2004). Mouse and touchscreen selection in the upper and lower visual fields. In: P" /><span class="c-article-references__counter">16.</span><p class="c-article-references__text" id="ref-CR16">Po BA, Fisher BD, Booth KS (2004). Mouse and touchscreen selection in the upper and lower visual fields. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 359–366</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Luckiesh, " /><meta itemprop="datePublished" content="1965" /><meta itemprop="headline" content="Luckiesh M (1965) Visual illusions: their causes, characteristics, and applications. Dover Publications, New Y" /><span class="c-article-references__counter">17.</span><p class="c-article-references__text" id="ref-CR17">Luckiesh M (1965) Visual illusions: their causes, characteristics, and applications. Dover Publications, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20illusions%3A%20their%20causes%2C%20characteristics%2C%20and%20applications&amp;publication_year=1965&amp;author=Luckiesh%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Myers BA, Bhatnagar R, Nichols J, Peck CH, Kong D, Miller R, Long AC (2002) Interacting a distance: measuring " /><span class="c-article-references__counter">18.</span><p class="c-article-references__text" id="ref-CR18">Myers BA, Bhatnagar R, Nichols J, Peck CH, Kong D, Miller R, Long AC (2002) Interacting a distance: measuring the performance of laser pointers and other devices. In: Proceedings of the ACM conference on human factors in computing (CHI), pp 33–40</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="KJ. Vicente, GL. Torenvliet, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Vicente KJ, Torenvliet GL (2000) The earth is spherical (p &lt; 0.05): alternative methods of statistical inferen" /><span class="c-article-references__counter">19.</span><p class="c-article-references__text" id="ref-CR19">Vicente KJ, Torenvliet GL (2000) The earth is spherical (<i>p</i> &lt; 0.05): alternative methods of statistical inference. Theor Issues Ergon Sci 1(3):248–271</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F14639220110037065" aria-label="View reference 19">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20earth%20is%20spherical%20%28p%20%3C%200.05%29%3A%20alternative%20methods%20of%20statistical%20inference&amp;journal=Theor%20Issues%20Ergon%20Sci&amp;volume=1&amp;issue=3&amp;pages=248-271&amp;publication_year=2000&amp;author=Vicente%2CKJ&amp;author=Torenvliet%2CGL">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-005-0156-2-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was funded by the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, BC, V6T 1Z4, Canada</p><p class="c-article-author-affiliation__authors-list">Barry A. Po, Brian D. Fisher &amp; Kellogg S. Booth</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Barry_A_-Po"><span class="c-article-authors-search__title u-h3 js-search-name">Barry A. Po</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Barry A.+Po&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Barry A.+Po" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Barry A.+Po%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Brian_D_-Fisher"><span class="c-article-authors-search__title u-h3 js-search-name">Brian D. Fisher</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Brian D.+Fisher&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Brian D.+Fisher" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Brian D.+Fisher%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Kellogg_S_-Booth"><span class="c-article-authors-search__title u-h3 js-search-name">Kellogg S. Booth</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Kellogg S.+Booth&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Kellogg S.+Booth" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Kellogg S.+Booth%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-005-0156-2/email/correspondent/c1/new">Kellogg S. Booth</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20two%20visual%20systems%20approach%20to%20understanding%20voice%20and%20gestural%20interaction&amp;author=Barry%20A.%20Po%20et%20al&amp;contentID=10.1007%2Fs10055-005-0156-2&amp;publication=1359-4338&amp;publicationDate=2005-06-14&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Po, B.A., Fisher, B.D. &amp; Booth, K.S. A two visual systems approach to understanding voice and gestural interaction.
                    <i>Virtual Reality</i> <b>8, </b>231–241 (2005). https://doi.org/10.1007/s10055-005-0156-2</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-005-0156-2.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-06-14">14 June 2005</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2005-09">September 2005</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-005-0156-2" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-005-0156-2</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Two visual systems</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Pointing</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Cursors</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual feedback</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Voice input</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual illusions</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-005-0156-2.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=156;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

