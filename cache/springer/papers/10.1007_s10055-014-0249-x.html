<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Aerial full spherical HDR imaging and display"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/18/4.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Aerial full spherical HDR imaging and display"/>

    <meta name="dc.source" content="Virtual Reality 2014 18:4"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2014-08-27"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2014 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic superimposition of virtual objects using image-based lighting. We propose a complete and practical system to acquire full spherical HDR images from the sky, using two omnidirectional cameras mounted above and below an unmanned aircraft. The HDR images are generated by combining multiple omnidirectional images captured with different exposures controlled automatically. Our system consists of methods for image completion, alignment, and color correction, as well as a novel approach for automatic exposure control, which selects optimal exposure so as to avoid banding artifacts. Experimental results indicated that our system generated better spherical images compared to an ordinary spherical image completion system in terms of naturalness and accuracy. In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed system. The experiment demonstrated HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses of HDR images."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2014-08-27"/>

    <meta name="prism.volume" content="18"/>

    <meta name="prism.number" content="4"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="255"/>

    <meta name="prism.endingPage" content="269"/>

    <meta name="prism.copyright" content="2014 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-014-0249-x"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-014-0249-x"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-014-0249-x.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-014-0249-x"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Aerial full spherical HDR imaging and display"/>

    <meta name="citation_volume" content="18"/>

    <meta name="citation_issue" content="4"/>

    <meta name="citation_publication_date" content="2014/11"/>

    <meta name="citation_online_date" content="2014/08/27"/>

    <meta name="citation_firstpage" content="255"/>

    <meta name="citation_lastpage" content="269"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-014-0249-x"/>

    <meta name="DOI" content="10.1007/s10055-014-0249-x"/>

    <meta name="citation_doi" content="10.1007/s10055-014-0249-x"/>

    <meta name="description" content="This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama "/>

    <meta name="dc.creator" content="Fumio Okura"/>

    <meta name="dc.creator" content="Masayuki Kanbara"/>

    <meta name="dc.creator" content="Naokazu Yokoya"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Mag; citation_title=Google street view: capturing the world at street level; citation_author=D Anguelov, C Dulong, D Filip, C Frueh, S Lafon, R Lyon, A Ogale, L Vincent, J Weaver; citation_volume=43; citation_issue=6; citation_publication_date=2010; citation_pages=32-38; citation_doi=10.1109/MC.2010.170; citation_id=CR1"/>

    <meta name="citation_reference" content="Chen SE (1995) Quicktime VR: an image-based approach to virtual environment navigation. In: Proceedings of ACM SIGGRAPH&#8217;95, Los Angeles, CA, pp 29&#8211;38"/>

    <meta name="citation_reference" content="Debevec P (1998) Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In: Proceedings of ACM SIGGRAPH&#8217;98, Orlando, FL, pp 189&#8211;198"/>

    <meta name="citation_reference" content="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of ACM SIGGRAPH&#8217;96, New Orleans, LA, pp 11&#8211;20"/>

    <meta name="citation_reference" content="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH&#8217;97. Los Angeles, CA, pp 369&#8211;378"/>

    <meta name="citation_reference" content="Goodnight N, Wang R, Woolley C, Humphreys G (2003) Interactive time-dependent tone mapping using programmable graphics hardware. In: Proceedings of 14th Eurographics Workshop on Rendering. Belgium, Leuven, pp 26&#8211;37"/>

    <meta name="citation_reference" content="citation_journal_title=J Field Robot; citation_title=Supporting wilderness search and rescue using a camera-equipped mini UAV; citation_author=MA Goodrich, BS Morse, D Gerhardt, JL Cooper, M Quigley, JA Adams, C Humphrey; citation_volume=25; citation_issue=1; citation_publication_date=2008; citation_pages=89-110; citation_doi=10.1002/rob.20226; citation_id=CR7"/>

    <meta name="citation_reference" content="Grossberg MD, Nayar SK (2003) High dynamic range from multiple images: which exposures to combine? In: Proceedings of IEEE workshop on color and photometric methods in computer vision (CPMCV), Nice, France, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Hasinoff SW, Durand F, Freeman WT (2010) Noise-optimal capture for high dynamic range photography. In: Proceedings of 23rd IEEE conference computer vision pattern recognition (CVPR&#8217;10), San Francisco, CA, pp 553&#8211;560"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Electron Agric; citation_title=Imaging from an unmanned aerial vehicle: agricultural surveillance and decision support; citation_author=S Herwitz, L Johnson, S Dunagan, R Higgins, D Sullivan, J Zheng, B Lobitz, J Leung, B Gallmeyer, M Aoyagi, R Slye, J Brass; citation_volume=44; citation_publication_date=2004; citation_pages=49-61; citation_doi=10.1016/j.compag.2004.02.006; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Sol Energy; citation_title=Models of sky radiance distribution and sky luminance distribution; citation_author=N Igawa, Y Koga, T Matsuzawa, H Nakamura; citation_volume=77; citation_issue=2; citation_publication_date=2004; citation_pages=137-157; citation_doi=10.1016/j.solener.2004.04.016; citation_id=CR11"/>

    <meta name="citation_reference" content="Ikeda S, Sato T, Yokoya N, (2003) High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system. In: Proceedings of 2003 IEEE international conference multisensor fusion integration intelligent system (MFI&#8217;03), Tokyo, Japan, pp 155&#8211;160"/>

    <meta name="citation_reference" content="citation_journal_title=(Proc. ACM SIGGRAPH&#8217;03); citation_title=High dynamic range video. ACM Trans Graph; citation_author=SB Kang, M Uyttendaele, S Winder, R Szeliski; citation_volume=22; citation_issue=3; citation_publication_date=2003; citation_pages=319-325; citation_doi=10.1145/882262.882270; citation_id=CR13"/>

    <meta name="citation_reference" content="Kawai N, Sato T, Yokoya N (2009) Image inpainting considering brightness change and spatial locality of textures and its evaluation. In: Proceedings of third Pacific-Rim symposium image and video technology (PSIVT&#8217;09), Tokyo, Japan, pp 271&#8211;282"/>

    <meta name="citation_reference" content="citation_journal_title=IPSJ Trans Comput Vis Appl; citation_title=Video completion for generating omnidirectional video without invisible areas; citation_author=N Kawai, K Machikita, T Sato, N Yokoya; citation_volume=2; citation_publication_date=2010; citation_pages=200-213; citation_id=CR15"/>

    <meta name="citation_reference" content="Kitaura M, Okura F, Kanbara M, Yokoya N (2012) Tone mapping for HDR images with dimidiate luminance and spatial distributions of bright and dark regions. In: Proceedings of SPIE, vol 8292, Burlingame, CA, ArticleNo 829205"/>

    <meta name="citation_reference" content="Kopf J, Chen B, Szeliski R, Cohen M (2010) Street slide: browsing street level imagery. CM Trans Graph (Proc. ACM SIGGRAPH&#8217;10) 29(4), Article No 96"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Multimed; citation_title=Special issue on immersive telepresence; citation_author=S Moezzi; citation_volume=4; citation_issue=1; citation_publication_date=1997; citation_pages=17-56; citation_doi=10.1109/MMUL.1997.580996; citation_id=CR18"/>

    <meta name="citation_reference" content="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH&#8217;11 Posters, Vancouver, BC, Article No 78"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Telepresence by real-time view-dependent image generation from omnidirectional video streams; citation_author=Y Onoe, K Yamazawa, H Takemura, N Yokoya; citation_volume=71; citation_issue=2; citation_publication_date=1998; citation_pages=154-165; citation_doi=10.1006/cviu.1998.0705; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=J Graph Tools; citation_title=Parameter estimation for photographic tone reproduction; citation_author=E Reinhard; citation_volume=7; citation_issue=1; citation_publication_date=2002; citation_pages=45-51; citation_doi=10.1080/10867651.2002.10487554; citation_id=CR21"/>

    <meta name="citation_reference" content="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH&#8217;02) 21(3):267&#8211;276"/>

    <meta name="citation_reference" content="citation_title=High dynamic range imaging: Acquisition, display, and image-based lighting; citation_publication_date=2010; citation_id=CR23; citation_author=E Reinhard; citation_author=W Heidrich; citation_author=P Debevec; citation_author=S Pattanaik; citation_author=G Ward; citation_author=K Myszkowski; citation_publisher=Morgan Kaufmann"/>

    <meta name="citation_reference" content="Rother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive foreground extraction using iterated graph cuts. ACM Trans Graph (Proc. ACM SIGGRAPH&#8217;04) 23(3):309&#8211;314"/>

    <meta name="citation_reference" content="Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system. In: Proceedings of eighth European conference computer vision (ECCV&#8217;04), Prague, Czech Republic 2:326&#8211;340"/>

    <meta name="citation_reference" content="Shi J, Tomasi C, (1994) Good features to track. In: Proceedings of 1994 IEEE conference computer vision pattern recognition (CVPR&#8217;94), Seattle, WA, pp 593&#8211;600"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Circuits Syst for Video Technol; citation_title=Survey of image-based representations and compression techniques; citation_author=H Shum, S Kang, S Chan; citation_volume=13; citation_issue=11; citation_publication_date=2003; citation_pages=1020-1037; citation_doi=10.1109/TCSVT.2003.817360; citation_id=CR27"/>

    <meta name="citation_reference" content="Stumpfel J, Tchou C, Jones A, Hawkins T, Wenger A, Debevec P (2004) Direct HDR capture of the sun and sky. In: Proceedings of third international conference computer graph, virtual reality, visualization and interaction in Africa (AFRIGRAPH&#8217;04), Stellenbosch, South Africa, pp 145&#8211;149"/>

    <meta name="citation_reference" content="citation_journal_title=J Graph Tools; citation_title=Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures; citation_author=G Ward; citation_volume=8; citation_issue=2; citation_publication_date=2003; citation_pages=17-30; citation_doi=10.1080/10867651.2003.10487583; citation_id=CR29"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=Mosaicing new views: the crossed-slits projection; citation_author=A Zomet, D Feldman, S Peleg, D Weinshall; citation_volume=25; citation_issue=6; citation_publication_date=2003; citation_pages=741-754; citation_doi=10.1109/TPAMI.2003.1201823; citation_id=CR30"/>

    <meta name="citation_author" content="Fumio Okura"/>

    <meta name="citation_author_email" content="fumio-o@is.naist.jp"/>

    <meta name="citation_author_institution" content="Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Masayuki Kanbara"/>

    <meta name="citation_author_email" content="kanbara@is.naist.jp"/>

    <meta name="citation_author_institution" content="Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_author" content="Naokazu Yokoya"/>

    <meta name="citation_author_email" content="yokoya@is.naist.jp"/>

    <meta name="citation_author_institution" content="Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Ikoma, Japan"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-014-0249-x&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2014/11/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-014-0249-x"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Aerial full spherical HDR imaging and display"/>
        <meta property="og:description" content="This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic superimposition of virtual objects using image-based lighting. We propose a complete and practical system to acquire full spherical HDR images from the sky, using two omnidirectional cameras mounted above and below an unmanned aircraft. The HDR images are generated by combining multiple omnidirectional images captured with different exposures controlled automatically. Our system consists of methods for image completion, alignment, and color correction, as well as a novel approach for automatic exposure control, which selects optimal exposure so as to avoid banding artifacts. Experimental results indicated that our system generated better spherical images compared to an ordinary spherical image completion system in terms of naturalness and accuracy. In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed system. The experiment demonstrated HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses of HDR images."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Aerial full spherical HDR imaging and display | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-014-0249-x","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Omnidirectional camera, High dynamic range image, Immersive panorama, Image-based lighting, Tone-mapping","kwrd":["Omnidirectional_camera","High_dynamic_range_image","Immersive_panorama","Image-based_lighting","Tone-mapping"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-014-0249-x","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-014-0249-x","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=249;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-014-0249-x">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Aerial full spherical HDR imaging and display
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0249-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0249-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2014-08-27" itemprop="datePublished">27 August 2014</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Aerial full spherical HDR imaging and display</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Fumio-Okura" data-author-popup="auth-Fumio-Okura" data-corresp-id="c1">Fumio Okura<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Takayama-cho, Ikoma, Nara, 8916-5, Japan" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Masayuki-Kanbara" data-author-popup="auth-Masayuki-Kanbara">Masayuki Kanbara</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Takayama-cho, Ikoma, Nara, 8916-5, Japan" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Naokazu-Yokoya" data-author-popup="auth-Naokazu-Yokoya">Naokazu Yokoya</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Nara Institute of Science and Technology (NAIST)" /><meta itemprop="address" content="grid.260493.a, 0000000092272257, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Takayama-cho, Ikoma, Nara, 8916-5, Japan" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 18</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">255</span>–<span itemprop="pageEnd">269</span>(<span data-test="article-publication-year">2014</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">330 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">3 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-014-0249-x/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic superimposition of virtual objects using image-based lighting. We propose a complete and practical system to acquire full spherical HDR images from the sky, using two omnidirectional cameras mounted above and below an unmanned aircraft. The HDR images are generated by combining multiple omnidirectional images captured with different exposures controlled automatically. Our system consists of methods for image completion, alignment, and color correction, as well as a novel approach for automatic exposure control, which selects optimal exposure so as to avoid banding artifacts. Experimental results indicated that our system generated better spherical images compared to an ordinary spherical image completion system in terms of naturalness and accuracy. In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed system. The experiment demonstrated HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses of HDR images.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>This paper describes a framework for acquiring full spherical high dynamic range (HDR) aerial images. Whereas traditional studies of image-based lighting (IBL) (Debevec <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Debevec P (1998) Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In: Proceedings of ACM SIGGRAPH’98, Orlando, FL, pp 189–198" href="/article/10.1007/s10055-014-0249-x#ref-CR3" id="ref-link-section-d37164e322">1998</a>) have focused on methods for acquiring spherical HDR images as light probes (Reinhard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Reinhard E, Heidrich W, Debevec P, Pattanaik S, Ward G, Myszkowski K (2010) High dynamic range imaging: acquisition, display, and image-based lighting. Morgan Kaufmann, San Francisco" href="/article/10.1007/s10055-014-0249-x#ref-CR23" id="ref-link-section-d37164e325">2010</a>), in recent years, improvements in imaging technology have made acquisition much easier, allowing capture of fully immersive 60 °C panoramas. The first popular application of this type of immersive imaging was Apple’s QuickTime VR (Chen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Chen SE (1995) Quicktime VR: an image-based approach to virtual environment navigation. In: Proceedings of ACM SIGGRAPH’95, Los Angeles, CA, pp 29–38" href="/article/10.1007/s10055-014-0249-x#ref-CR2" id="ref-link-section-d37164e328">1995</a>), which synthesized spherical images from images taken at different rotations around an approximate centerpoint. Today, immersive panorama applications can capture spherical images/videos using omnidirectional cameras and convert them to view direction-dependent perspective images in real time. This technique has become crucial to popular, Web-based walk-through applications such as Google Street View (Anguelov et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Anguelov D, Dulong C, Filip D, Frueh C, Lafon S, Lyon R, Ogale A, Vincent L, Weaver J (2010) Google street view: capturing the world at street level. IEEE Comput Mag 43(6):32–38" href="/article/10.1007/s10055-014-0249-x#ref-CR1" id="ref-link-section-d37164e331">2010</a>), as well as emerging immersive systems that use head-mounted displays (HMDs) and head trackers (Onoe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Onoe Y, Yamazawa K, Takemura H, Yokoya N (1998) Telepresence by real-time view-dependent image generation from omnidirectional video streams. Comput Vis Image Underst 71(2):154–165" href="/article/10.1007/s10055-014-0249-x#ref-CR20" id="ref-link-section-d37164e334">1998</a>). While most of these systems use eight-bit low dynamic range (LDR) images, it is clear that HDR imaging can significantly improve the fidelity of scenes with high dynamic range, including most outdoor scenes. Recently, work on augmented reality (AR) visualization of cultural heritage (Okura et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78" href="/article/10.1007/s10055-014-0249-x#ref-CR19" id="ref-link-section-d37164e338">2011</a>) has provided users with panoramic aerial views overlaid with historical computer graphics (CG) models (e.g., of lost architectural structures) rendered using image-based lighting (IBL) techniques. The success of these systems in conveying realistic historical context offers a glimpse of the larger potential for HDR panoramic imaging.</p><p>This paper proposes a novel method for acquiring spherical HDR images from the sky. These images are intended for use in immersive and/or augmented panoramas supporting user fly-through. Our imaging framework also can be used to generate input images for more advancing display methods, such as multi-perspective panoramas (Zomet et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Zomet A, Feldman D, Peleg S, Weinshall D (2003) Mosaicing new views: the crossed-slits projection. IEEE Trans Pattern Anal Mach Intell 25(6):741–754" href="/article/10.1007/s10055-014-0249-x#ref-CR30" id="ref-link-section-d37164e344">2003</a>; Kopf et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kopf J, Chen B, Szeliski R, Cohen M (2010) Street slide: browsing street level imagery. CM Trans Graph (Proc. ACM SIGGRAPH’10) 29(4), Article No 96" href="/article/10.1007/s10055-014-0249-x#ref-CR17" id="ref-link-section-d37164e347">2010</a>) and free-viewpoint image generation using image-based rendering (Debevec et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of ACM SIGGRAPH’96, New Orleans, LA, pp 11–20" href="/article/10.1007/s10055-014-0249-x#ref-CR4" id="ref-link-section-d37164e350">1996</a>; Shum et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Shum H, Kang S, Chan S (2003) Survey of image-based representations and compression techniques. IEEE Trans Circuits Syst Video Technol 13(11):1020–1037" href="/article/10.1007/s10055-014-0249-x#ref-CR27" id="ref-link-section-d37164e353">2003</a>). In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed framework. Through the experiment, we have investigated the advantages and features of HDR imaging for immersive panoramas. The contribution of our study is summarized as follows:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>We propose a complete and practical system to capture full spherical aerial HDR images, which includes a novel approach for automatic exposure control.</p>
                  </li>
                  <li>
                    <p>An investigation of display methods demonstrates HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses (i.e., photography and IBL).</p>
                  </li>
                </ul><p>The remainder of this paper is organized as follows. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec2">2</a> describes the difficulties inherent in capturing spherical HDR images from aerial platforms. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec5">3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec17">4</a> present our system for acquiring full spherical HDR images using an aerial vehicle and omnidirectional cameras, as well as some experimental results for practical imaging contexts. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec22">5</a> presents and evaluates methods for displaying aerial spherical HDR images in immersive panorama applications. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec32">6</a> provides concluding remarks.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Problems in acquiring aerial spherical HDR images</h2><div class="c-article-section__content" id="Sec2-content"><p>The proposed aerial imaging system resolves two problems that commonly occur when spherical images are acquired using omnidirectional cameras: (1) <i>the occurrence of missing areas</i>, and (2) <i>deficiency in dynamic range</i>. In this section, we briefly look at each of these problems, along with related work by other researchers, and present our basic approach to solution.</p><h3 class="c-article__sub-heading" id="Sec3">Occurrence of missing areas</h3><p>Missing areas, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>a, b, appear in all single-capture spherical images. They are caused by limitations in the field of view of the omnidirectional camera and/or occlusion of the field of view by hardware. In the case of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>, the primary occlusion is the aircraft on which the camera is mounted. Such areas subtract from the immersive value of the image and disqualify it from use in IBL rendering systems.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Full spherical HDR image generation using two omnidirectional cameras. <b>a</b> Full spherical image. <b>b</b> Image captured beneath aircraft</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Existing approaches to filling in such missing areas can be classified into two categories—namely, filling without observed intensities and filling with observed intensities:</p><ul class="u-list-style-bullet">
                    <li>
                      <p>
                        <i>Filling without observed intensities. </i>Okura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78" href="/article/10.1007/s10055-014-0249-x#ref-CR19" id="ref-link-section-d37164e449">2011</a>) filled missing areas in the upper hemisphere of spherical videos with the aid of a statistical “sky model” of various weather conditions, whereas Kawai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kawai N, Sato T, Yokoya N (2009) Image inpainting considering brightness change and spatial locality of textures and its evaluation. In: Proceedings of third Pacific-Rim symposium image and video technology (PSIVT’09), Tokyo, Japan, pp 271–282" href="/article/10.1007/s10055-014-0249-x#ref-CR14" id="ref-link-section-d37164e452">2009</a>) filled them from the image itself, using inpainting techniques.</p>
                    </li>
                    <li>
                      <p>
                        <i>Filling with observed intensities. </i> In a study by Kawai et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Kawai N, Machikita K, Sato T, Yokoya N (2010) Video completion for generating omnidirectional video without invisible areas. IPSJ Trans Comput Vis Appl 2:200–213" href="/article/10.1007/s10055-014-0249-x#ref-CR15" id="ref-link-section-d37164e464">2010</a>), missing areas in the ground portion of spherical video sequences were filled by estimating the intensities of the missing area based on the intensities of images of the same location taken from other viewpoints.</p>
                    </li>
                  </ul><p>The objective of the former of these categories is merely to patch over missing areas in an acceptable manner. The results may differ greatly in intensity from the real objects at the time of capture, or virtual objects rendered using IBL. Our proposed approach falls in the latter category, in which the real intensities are observed. Specifically, we use two omnidirectional cameras, mounted above and below an unmanned aircraft, to capture the entire scene, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>c.</p><h3 class="c-article__sub-heading" id="Sec4">Deficiency in dynamic range</h3><p>Proper IBL rendering requires unsaturated spherical images (Debevec and Malik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378" href="/article/10.1007/s10055-014-0249-x#ref-CR5" id="ref-link-section-d37164e481">1997</a>). Unfortunately, the dynamic range of many daytime outdoor scenes is too high to capture using standard eight-bit cameras, as the sun is approximately <span class="mathjax-tex">\(2^{17}\)</span> times brighter than darker areas of sky or cloud cover (Stumpfel et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Stumpfel J, Tchou C, Jones A, Hawkins T, Wenger A, Debevec P (2004) Direct HDR capture of the sun and sky. In: Proceedings of third international conference computer graph, virtual reality, visualization and interaction in Africa (AFRIGRAPH’04), Stellenbosch, South Africa, pp 145–149" href="/article/10.1007/s10055-014-0249-x#ref-CR28" id="ref-link-section-d37164e508">2004</a>). An HDR imaging method using LDR images taken at multiple exposure values (multi-exposure images), originally proposed by Debevec and Malik (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378" href="/article/10.1007/s10055-014-0249-x#ref-CR5" id="ref-link-section-d37164e511">1997</a>), is now widely used to capture scenes below saturation using commodity camera hardware. We employ this technique to generate our spherical HDR images.</p><p>When multi-exposure images are captured in motion, misalignment will naturally occur. Although solutions to this problem have been proposed (e.g., Ward <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ward G (2003) Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures. J Graph Tools 8(2):17–30" href="/article/10.1007/s10055-014-0249-x#ref-CR29" id="ref-link-section-d37164e517">2003</a>), large misalignments remain difficult to fix. For this reason, it is best to use fewer multi-exposure images to generate HDR images and to ensure that camera velocity and frame rate remain constant.</p><p>In generating HDR images from a small number of multi-exposure images, particular care must be taken to quantize the luminance. When there are high-luminance objects like the sun within the field of view, exposure times must be short. This, in turn, leaves fewer gradations for low-luminance objects, which tend to cover most of the view area. To adjust for this discrepancy, we employ an automatic exposure control method to accommodate illumination changes while capturing an image sequence. Specifically, exposure times are adapted to the luminance distribution of the scene, based on an HDR histogram generated from multi-exposure images.</p><p>An omnidirectional camera mounted atop the aircraft mainly captures sun and sky, which are too bright to capture without saturation even at the shortest exposure time. To address this problem, neutral density (ND) filters are attached to the camera to overcome, as was done in related work by Stumpfel et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Stumpfel J, Tchou C, Jones A, Hawkins T, Wenger A, Debevec P (2004) Direct HDR capture of the sun and sky. In: Proceedings of third international conference computer graph, virtual reality, visualization and interaction in Africa (AFRIGRAPH’04), Stellenbosch, South Africa, pp 145–149" href="/article/10.1007/s10055-014-0249-x#ref-CR28" id="ref-link-section-d37164e525">2004</a>).</p></div></div></section><section aria-labelledby="Sec5"><div class="c-article-section" id="Sec5-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec5">Acquisition of full spherical HDR images</h2><div class="c-article-section__content" id="Sec5-content"><h3 class="c-article__sub-heading" id="Sec6">Overview</h3><p>Our proposed system generates full spherical HDR images from multi-exposure images captured with two omnidirectional cameras mounted above and below an unmanned aircraft. As depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig2">2</a>, the procedure is divided into three stages:</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Overview of aerial full spherical HDR imaging</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <ol class="u-list-style-none">
                    <li>
                      <span class="u-custom-list-number">1.</span>
                      
                        <p>
                          <i>Multi-exposure aerial image capture. </i> Multi-exposure images are captured using two omnidirectional cameras mounted above and below an unmanned aircraft. Since the camera above the vehicle mainly captures the sun and sky, it has ND filters attached. Exposure times are controlled automatically so as to capture high- and low-luminance images effectively.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">2.</span>
                      
                        <p>
                          <i>HDR image generation from multi-exposure images. </i> HDR spherical images are generated from the multi-exposure images captured above and below the aircraft. Large misalignments are corrected by estimating changes in camera orientation over the sequence of captured images.</p>
                      
                    </li>
                    <li>
                      <span class="u-custom-list-number">3.</span>
                      
                        <p>
                          <i>Composition of HDR images captured by the two cameras. </i> Full spherical HDR images are synthesized from the HDR images generated in stage two. Because the precise geometric relationship between images from above and below the aircraft is difficult to fix (due to the slight deformability of the unmanned aircraft), we align the images by estimating the relative rotation from one camera to the other for every frame. We also correct chromatic changes that may occur in images captured under ND filtering.</p>
                      
                    </li>
                  </ol>
                <h3 class="c-article__sub-heading" id="Sec7">Multi-exposure aerial image capture</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec8">Configuration of the aerial imaging system</h4><p>We used a 12-m remote-controlled unmanned airship (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig3">3</a>) as our aerial imaging platform. The aircraft was equipped with a differential GPS (Hitachi Zosen Corp., P4-GPS) for acquiring the locations at which images were captured. Two omnidirectional multi-camera systems (Point Grey Research Ladybug2, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0249-x#Tab1">1</a>) were mounted above and below the aircraft. The cameras were connected to a laptop PC for time-stamped storage. ND filters (Fujifilm Corp., ND 2.0) that block all but 1 % of visible light were attached to the camera above the vehicle. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>a, b show panoramic images after removal of limb darkening and geometric transformation (Ikeda et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ikeda S, Sato T, Yokoya N, (2003) High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system. In: Proceedings of 2003 IEEE international conference multisensor fusion integration intelligent system (MFI’03), Tokyo, Japan, pp 155–160" href="/article/10.1007/s10055-014-0249-x#ref-CR12" id="ref-link-section-d37164e628">2003</a>). Note that the amount of limb darkening differs between images captured with and without ND filters.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Unmanned airship and omnidirectional cameras. Two cameras are <i>mounted above</i> and <i>below</i> the aircraft. ND filters are attached to the camera above. A GPS antenna is also <i>mounted atop</i> the aircraft</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Ladybug2 specifications</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0249-x/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Although our imaging framework does not require a specific kind of aircraft for mounting the camera, unmanned airships, which were used in our imaging equipment, are beneficial to the imaging for aerial immersive panoramas. They have recently attracted attention for low-height aerial imaging (Herwitz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Herwitz S, Johnson L, Dunagan S, Higgins R, Sullivan D, Zheng J, Lobitz B, Leung J, Gallmeyer B, Aoyagi M, Slye R, Brass J (2004) Imaging from an unmanned aerial vehicle: agricultural surveillance and decision support. Comput Electron Agric 44:49–61" href="/article/10.1007/s10055-014-0249-x#ref-CR10" id="ref-link-section-d37164e783">2004</a>; Goodrich et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Goodrich MA, Morse BS, Gerhardt D, Cooper JL, Quigley M, Adams JA, Humphrey C (2008) Supporting wilderness search and rescue using a camera-equipped mini UAV. J Field Robot 25(1):89–110" href="/article/10.1007/s10055-014-0249-x#ref-CR7" id="ref-link-section-d37164e786">2008</a>), which can acquire higher resolution images. In addition, low-speed vehicles, including airships, are beneficial to multi-exposure imaging because small misalignments occur on the images. However, owing to the deformability of the body of the aircraft, it is difficult to fix the cameras on it; thus, we overcome a problem due to the deformability during the composition of two HDR images.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec9">Automatic exposure control</h4><p>The exposure times used to capture multi-exposure images are automatically determined from the intensity of previously captured images. Many still cameras provide an auto-bracketing function for capturing multi-exposure images. They determine a proper exposure value with standard auto-exposure controls and then use fixed multiples of this as the remaining exposure values (Kang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kang SB, Uyttendaele M, Winder S, Szeliski R (2003) High dynamic range video. ACM Trans Graph. (Proc. ACM SIGGRAPH’03) 22(3):319–325" href="/article/10.1007/s10055-014-0249-x#ref-CR13" id="ref-link-section-d37164e797">2003</a>). Grossberg and Nayar (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Grossberg MD, Nayar SK (2003) High dynamic range from multiple images: which exposures to combine? In: Proceedings of IEEE workshop on color and photometric methods in computer vision (CPMCV), Nice, France, pp 1–8" href="/article/10.1007/s10055-014-0249-x#ref-CR8" id="ref-link-section-d37164e800">2003</a>) proposed a method of determining unfixed exposure sets from the dynamic range of the scene. Our approach, unlike existing approaches, utilizes rich information of the scene to determine a more appropriate exposure set; that is, apart from the dynamic range, our approach also uses an HDR histogram of the scene, which is acquired by the last multi-exposure image set to reduce the side-effects of large quantization steps on human vision, such as banding (i.e., posterization) resulting from too few multi-exposure images. Note that our approach optimizes the set of exposure times, whereas other modern studies have sought to minimize signal-to-noise ratio (SNR) (Hasinoff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Hasinoff SW, Durand F, Freeman WT (2010) Noise-optimal capture for high dynamic range photography. In: Proceedings of 23rd IEEE conference computer vision pattern recognition (CVPR’10), San Francisco, CA, pp 553–560" href="/article/10.1007/s10055-014-0249-x#ref-CR9" id="ref-link-section-d37164e803">2010</a>). Such minimization could also be integrated into our approach, as a means of improving the appearance of HDR images, particularly those captured in dark environments.</p><p>New exposure times <span class="mathjax-tex">\({s_{{\rm new}_{1}}}, \ldots , {s_{{\rm new}_n}}\)</span> are determined by multi-exposure images captured using old exposures <span class="mathjax-tex">\({s_{{\rm old}_{1}}},\ldots , {s_{{\rm old}_{n}}}\)</span>. In practice, we set <span class="mathjax-tex">\(n=4\)</span> as the number of exposures in our experiment to match the number of registers available in the Ladybug2 cameras for storing exposure times.</p><p>The shortest exposure time <span class="mathjax-tex">\({s_{{\rm new}_{1}}}\)</span> is the at which the scene captured using <span class="mathjax-tex">\({s_{{\rm old}_{1}}}\)</span> can be captured without saturation, as follows:</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {s_{{\rm new}_{1}}}=\left\{ \begin{array}{ll} 0.5 {s_{{\rm old}_{1}}} &amp; \quad (L_{1} = {{\rm MAX}\_{\rm IN}})\\ \displaystyle \frac{{{\rm MAX}\_{\rm IN}}+\theta _{{\rm sh}}}{2 L_{1}}{s_{{\rm old}_{1}}} &amp; \quad (L_{1} \le \theta _{{\rm sh}}) \\ {s_{{\rm old}_{1}}} &amp; \quad (\hbox {otherwise}), \\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>where <span class="mathjax-tex">\(L_{1}\)</span> denotes the maximum intensity in the image captured using <span class="mathjax-tex">\({s_{{\rm old}_{1}}}\)</span>, and <span class="mathjax-tex">\({\text{MAX\_IN}}\)</span> denotes the maximum representable intensity of the LDR image (<span class="mathjax-tex">\(\text {MAX}\_{{\rm IN}}=255\)</span> for eight-bit images). Further, <span class="mathjax-tex">\(\theta _{{\rm sh}}\)</span> is a threshold that limits <span class="mathjax-tex">\(s_{{\rm new}_{1}}\)</span> to <span class="mathjax-tex">\(\theta _{{\rm sh}}&lt;L_{1}&lt;{\rm MAX}\_{{\rm IN}}\)</span> when the response curve is regarded as linear.</p><p>When only a small number of multi-exposure images are acquired, side-effects of large quantization steps, such as banding, can appear in the generated HDR image. Other exposure times <span class="mathjax-tex">\({s_{{\rm new}_{2}},\ldots , s_{{\rm new}_{n}}}\)</span> are therefore determined to reduce such effects. To estimate exposure times that have minimum quantization steps, an HDR histogram is calculated from the captured images, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig4">4</a>. <span class="mathjax-tex">\(H_{i}\)</span> is the maximum intensity captured using the <span class="mathjax-tex">\(i\)</span>-th shortest <span class="mathjax-tex">\((i\ge 2)\)</span> exposure time <span class="mathjax-tex">\({s_{{\rm new}_{i}}}\)</span> in the HDR histogram. Number of pixels <span class="mathjax-tex">\(M_i\)</span> that belongs to histogram bins between <span class="mathjax-tex">\(H_{i+1}\)</span> and <span class="mathjax-tex">\(H_{i}\)</span> (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig4">4</a>) is computed using <span class="mathjax-tex">\(H_{i}\)</span> as </p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$M_{i} = \sum\limits_{{k = {\mathcal{H}}}}^{{H_{i} }} {j_{k} } ,$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <span class="mathjax-tex">\(j_k\)</span> is the number of pixels whose intensity is <span class="mathjax-tex">\(k\)</span>, and <span class="mathjax-tex">\({\mathcal{H}}\)</span> is defined as</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} {\mathcal{H}}=\left\{ \begin{array}{ll} 0 &amp; \quad (i=n)\\ H_{i+1} &amp; \quad (\text {otherwise}). \\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>The quantization step <span class="mathjax-tex">\(\Delta _i\)</span> corresponding to <span class="mathjax-tex">\({s_{{\rm new}_{i}}}\)</span> on the HDR histogram can be calculated as follows:</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \Delta _i = \frac{H_{i}}{{\rm MAX}\_{\rm IN}}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Finally, <span class="mathjax-tex">\({s_{{\rm new}_{i}}} (i\ge 2)\)</span> is determined by varying <span class="mathjax-tex">\({s_{{\rm new}_{i}}}\)</span> to minimize the energy function <span class="mathjax-tex">\(E_{{\rm s}}\)</span>, where <span class="mathjax-tex">\(E_{{\rm s}}\)</span> is the sum of the products of the quantization step <span class="mathjax-tex">\(\Delta _i\)</span> and the number of pixels <span class="mathjax-tex">\(M_i\)</span> corresponding to the exposure time <span class="mathjax-tex">\({s_{{\rm new}_{i}}}\)</span>:</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} E_{{\rm s}} = \sum _i (M_i \Delta _i). \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>
                    <span class="mathjax-tex">\(E_{{\rm s}}\)</span> is a nonlinear function that has multiple local minima. We apply a simple coarse-to-fine search technique to find a reasonable solution.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Illustrative example of HDR histogram for <span class="mathjax-tex">\(n=4\)</span>, where <span class="mathjax-tex">\(\Delta _i\)</span> denotes quantization steps and <span class="mathjax-tex">\(H_{i}\)</span> is the maximum intensity that can be captured using exposure time <span class="mathjax-tex">\(s_{{\rm new}_{i}}\)</span>. Further, <span class="mathjax-tex">\(M_3\)</span> is the number of pixels in the filled area of the histogram, with the other <span class="mathjax-tex">\(M_i\)</span> determined in a similar fashion</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>The above process for determining exposure values is repeated every several seconds to allow for changes in the real-world lighting environment. Note that <span class="mathjax-tex">\({s_{{\rm new}_{1}}}\)</span> may not converge within a single cycle and that several cycles are often needed.</p><h3 class="c-article__sub-heading" id="Sec10">HDR image generation from multi-exposure images</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec11">Alignment of multi-exposure images</h4><p>Misalignments occur among multi-exposure images due to changes in the position and orientation of camera while capturing images at different exposure times. This causes blurring in the resulting HDR images, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>HDR images with and without multi-exposure image alignment. <b>a</b> Without alignment. <b>b</b> With alignment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>For an imaging aircraft moving at 5 m/s and rotating 30 °C, the misalignment angles among multi-exposure images are calculated as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0249-x#Tab2">2</a>. In cases where the camera captures the scene from a great distance, the amount of misalignment is predominantly affected by changes in the orientation of the camera. We correct such misalignments by estimating the changes in camera orientation from the captured image sequence. Note that this correction method does not take position change into consideration and will therefore ignore misalignments due to fast-moving objects captured at short distances. Although our approach is quite simple compared with other alignment techniques that consider a position change (e.g., using a median threshold bitmap Ward <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Ward G (2003) Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures. J Graph Tools 8(2):17–30" href="/article/10.1007/s10055-014-0249-x#ref-CR29" id="ref-link-section-d37164e2678">2003</a>), it is expected to be robust to misalignment due to orientation change. Fortunately, such existing multi-exposure alignment methods can simply be combined with our approach when significant camera motion occurs.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Amount of misalignment due to changes in position and orientation of the camera: one cycle represents the time taken to capture a single multi-exposure image set (0.25 s)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0249-x/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig6">6</a> depicts the steps of our multi-exposure images alignment process. In the first step, two images with the same exposure are selected and the camera rotation between them is estimated, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig6">6</a>a. Among the multi-exposure images, those having the fewest saturated pixels (intensity of 255) and underexposed pixels (intensity of less than 16 in our implementation) are selected. The corresponding points between the selected images are determined by the Kanade–Lucas–Tomasi (KLT) tracker (Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C, (1994) Good features to track. In: Proceedings of 1994 IEEE conference computer vision pattern recognition (CVPR’94), Seattle, WA, pp 593–600" href="/article/10.1007/s10055-014-0249-x#ref-CR26" id="ref-link-section-d37164e2795">1994</a>) and are projected onto a unit sphere. <span class="mathjax-tex">\({\mathbf{p}}_m\)</span> and <span class="mathjax-tex">\({\mathbf{q}}_m\)</span>, respectively, denote the projected corresponding points. The parameters for rotation <span class="mathjax-tex">\({\mathbf{Rc}}_i\)</span> between the two images are estimated by nonlinearly minimizing the energy function <span class="mathjax-tex">\(E_l\)</span>, defined as the sum of squared Euclidean distances between projected corresponding points <span class="mathjax-tex">\(|{\mathbf{p}}_m, {\mathbf{q}_m}|\)</span>:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} E_l = \sum\limits_{m} |{\mathbf{p}}_m, {\mathbf{q}}_m|^2. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>Note that RANSAC is used to reduce errors due to mismatches.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Process for aligning multi-exposure images. <b>a</b> Estimating rotation between images that have the same exposure. <b>b</b> Calculating rotation between neighboring images using spherical linear interpolation. <b>c </b>Calculating rotation to reference image from other images.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Next, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig6">6</a>b, rotation <span class="mathjax-tex">\({\mathbf{Rf}}_{(i,j)}\)</span> between every two adjacent images are calculated by interpolating <span class="mathjax-tex">\({\mathbf{Rc}}_i\)</span> using spherical linear interpolation.</p><p>Rotations to an arbitrary reference image from neighboring images are then calculated from <span class="mathjax-tex">\({\mathbf{Rf}}_{(i,j)}\)</span>, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig6">6</a>c. Multi-exposure images are aligned to the reference image by transformation using the obtained rotation parameters. To generate aligned, multi-exposure images for an entire video sequence, <span class="mathjax-tex">\({\mathbf{Rf}}_{(i,j)}\)</span> must first be calculated for the entire sequence, after which each frame in the sequence can be treated as the reference image.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec12">HDR image generation</h4><p>We use the HDR imaging method for multi-exposure images based on (Debevec and Malik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378" href="/article/10.1007/s10055-014-0249-x#ref-CR5" id="ref-link-section-d37164e3221">1997</a>), to compose the intensity of our LDR images. An ordinary method (Debevec and Malik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378" href="/article/10.1007/s10055-014-0249-x#ref-CR5" id="ref-link-section-d37164e3224">1997</a>) is modified to consider light attenuation due to the ND filters attached to the camera on top of the aircraft. If the response curve is linear, or linearized in advance, intensities in the HDR image <span class="mathjax-tex">\(I_h\)</span> can be calculated using the LDR intensity <span class="mathjax-tex">\(I_l\)</span> and the exposure time <span class="mathjax-tex">\(t\)</span>[s] of the LDR image:</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I_h = \left\{ \begin{array}{ll} \gamma \frac{I_l}{t} &amp;{\rm (without\ ND\ filters),}\\ \gamma \frac{I_l}{\eta t} &amp;{\rm (with\ ND\ filters).}\\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>The nominal light transmittance <span class="mathjax-tex">\(\eta\)</span> of the ND filter is newly employed in addition to the existent HDR composition. Although the scale factor <span class="mathjax-tex">\(\gamma\)</span> is meaningless when only relative pixel intensities are needed, it can be determined and used to calculate absolute radiance values <span class="mathjax-tex">\({\rm [W/sr/m^2]}\)</span>. The HDR intensities calculated from each set of multi-exposure images are composed in accordance with the method proposed by (Debevec and Malik <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378" href="/article/10.1007/s10055-014-0249-x#ref-CR5" id="ref-link-section-d37164e3546">1997</a>).</p><h3 class="c-article__sub-heading" id="Sec13">Composition of HDR images captured by two cameras</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Alignment between the two cameras</h4><p>The HDR images captured above and below the aircraft are aligned by estimating the relative rotation between the cameras. To estimate the parameters of this rotation, missing areas in the images are manually masked in advance, as shown on the left side of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig7">7</a>. Note that only one mask image is required for an entire sequence captured by a given camera, as the missing area for omnidirectional images does not change significantly. The area marked by white pixels on the right side of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig7">7</a> indicates the overlap area where the scene has been captured by both cameras without occlusions. This area is defined as the conjunction of the negation of missing areas in the paired images.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Masks used for alignment of images from the two cameras. <i>Top left</i> black pixels indicate the missing area from the top of the aircraft. <i>Bottom left</i> black pixels indicate the <i>missing area</i> from the <i>bottom</i> of the aircraft. <i>Right</i> white pixels indicate the area of overlap</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Corresponding points are detected by applying the KLT tracker (Shi and Tomasi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Shi J, Tomasi C, (1994) Good features to track. In: Proceedings of 1994 IEEE conference computer vision pattern recognition (CVPR’94), Seattle, WA, pp 593–600" href="/article/10.1007/s10055-014-0249-x#ref-CR26" id="ref-link-section-d37164e3605">1994</a>) to feature points in the overlap area of the image captured from the bottom of the aircraft. The parameters for rotation between the cameras from top to bottom are determined in the way described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec11">3.3.1</a>.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Correction of chromatic change due to ND filters</h4><p>Although ND filters are designed to transmit all wavelengths of light equally, they do cause a certain amount of chromatic changes. The ND filters used in our experiment transmit less red light, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig8">8</a>. Such changes are corrected by estimating linear intensity transformation parameters from intensities in the overlap areas of images from the two cameras. The RGB values <span class="mathjax-tex">\((R_{{\rm top}}(\mathbf{x}), G_{top}(\mathbf{x}), B_{top}(\mathbf{x}))\)</span> for a pixel <span class="mathjax-tex">\(\mathbf{x}\)</span> in the image captured from above the aircraft are converted to corrected pixel values <span class="mathjax-tex">\((R^{\prime }_{top}({\mathbf{x}}), G^{\prime }_{{\rm top}}({\mathbf{x}}), B^{\prime }_{{\rm top}}({\mathbf{x}}))\)</span> using the following linear transformations:</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} R^{\prime }_{{\rm top}}({\mathbf{x}})&amp;= \beta _r R_{{\rm top}}({\mathbf{x}}), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div>
                    <div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} G^{\prime }_{{\rm top}}({\mathbf{x}})&amp;= \beta _g G_{{\rm top}}({\mathbf{x}}), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div>
                    <div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} B^{\prime }_{{\rm top}}({\mathbf{x}})&amp;= \beta _b B_{{\rm top}}({\mathbf{x}}), \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>where <span class="mathjax-tex">\(\beta _r\)</span> is estimated by</p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \beta _r = \frac{\Sigma _{\mathbf{x}\in A}\frac{R_{{\rm bot}}}{(}\mathbf{x})}{R_{{\rm top}}(\mathbf{x})}{N_{A}}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>where <span class="mathjax-tex">\((R_{{\rm bot}}(\mathbf{x}), G_{{\rm bot}}(\mathbf{x}), B_{{\rm bot}}(\mathbf{x}))\)</span> denote the RGB values of the corresponding pixel <span class="mathjax-tex">\(\mathbf{x}\)</span> in the image captured from the bottom of the aircraft, <span class="mathjax-tex">\(A\)</span> denotes the overlap area, and <span class="mathjax-tex">\(N_{A}\)</span> indicates the number of pixels belonging to <span class="mathjax-tex">\(A\)</span>. <span class="mathjax-tex">\(\beta_g\)</span> and <span class="mathjax-tex">\(\beta_b\)</span> are estimated in a parallel manner.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Example of chromatic change due to ND filter. A pair of HDR images converted to the same exposure is shown in close-up, at region where the two images overlap. <b>a</b> Without ND filter (<i>image captured from bottom of airship</i>). <i>Average intensity</i> (R,G,B) = (111.5, 108.3, 119.7). <b>b</b> With ND filter (<i>image captured from top of airship</i>). Average intensity (R, G, B) = (95.1, 103.5, 111.9)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Combination of corrected HDR images</h4><p>Full spherical HDR images are generated by combining paired HDR images that have been realigned and chromatically corrected, as described above. The intensities in the overlap area of the full spherical image are determined by alpha blending the two images. From the intensities <span class="mathjax-tex">\(I_{{\rm top}}\)</span> and <span class="mathjax-tex">\(I_{{\rm bot}}\)</span> of a pixel in the overlap areas of the two images, the intensity <span class="mathjax-tex">\(I_{\rm full}\)</span> of the corresponding pixel of the full spherical image is calculated using</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I_{{\rm full}} = \alpha I_{{\rm bot}} + (1-\alpha )I_{{\rm top}}, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where <span class="mathjax-tex">\(\alpha\)</span> varies linearly between zero on the upper boundary of the overlap area and unity on the lower boundary.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Results and discussions of generating full spherical HDR image and video</h2><div class="c-article-section__content" id="Sec17-content"><h3 class="c-article__sub-heading" id="Sec18">Generation of HDR image</h3><p>To confirm that the HDR image generated using our proposed method reflects the real environment with reasonable fidelity, we conducted an experiment in which we generated a full spherical HDR image from still images captured using an unmanned aircraft. The aircraft was flown at 3 m/s at a height of 130 m while capturing the multi-exposure images. <span class="mathjax-tex">\({{\rm MAX}}\_{{\rm IN}}=255\)</span> and <span class="mathjax-tex">\(\theta _{{\rm sh}}=192\)</span> were used in this experimental environment.</p><p>The captured multi-exposure images and corresponding exposure times are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig9">9</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>a, b show HDR images from the top and bottom of the aircraft, respectively. The full spherical HDR image composed using Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>a, b is shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig1">1</a>c. Note that the two realigned and chromatically corrected images were successfully combined, leaving no obvious artifacts. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig10">10</a> shows the full spherical images for exposures equal to those in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig9">9</a>e, h, respectively. From this, we can confirm that the full spherical HDR image satisfactorily combined the captured multi-exposure images. The full spherical HDR image can also be visualized by typical tone-mapping methods such as the one in Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e4823">2002</a>), as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig11">11</a>. Such tone-mapped images are suitable for immersive panoramas, which provide users a sense of looking around a location, and seeing textures of various radiances.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Captured multi-exposure images and corresponding exposure time. <b>a</b>
                          <i>Top</i> 0.1 ms (with ND filters). <b>b</b>
                          <i>Top</i> 4.1 ms (with ND filters). <b>c</b>
                          <i>Top</i> 14.4 ms (with ND filters). <b>d</b>
                          <i>Top</i> 43.3 ms (with ND filters). <b>e</b>
                          <i>Bottom</i> 0.1 ms. <b>f</b>
                          <i>Bottom</i> 0.4 ms. <b>g</b>
                          <i>Bottom</i> 1.1 ms. <b>h</b>
                          <i>Bottom</i> 2.4 ms</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Full spherical HDR images with cropped intensity and exposures equal to those of the multi-exposure images. <b>a</b> Exposure for Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig9">9</a>e. <b>b</b> Exposure for Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig9">9</a>h</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Full spherical image tone-mapped in accordance with the method by Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e4939">2002</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Full spherical tone-mapped images generated from video frames. The images were aligned using estimated relative camera orientation, based on Sato et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system. In: Proceedings of eighth European conference computer vision (ECCV’04), Prague, Czech Republic 2:326–340" href="/article/10.1007/s10055-014-0249-x#ref-CR25" id="ref-link-section-d37164e4962">2004</a>). <b>a</b> First frame. <b>b</b> 100th frame. <b>c</b> 200th frame. <b>d</b> 300th frame. <b>e</b> 400th frame. <b>f</b> 500th frame</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec19">Generation of HDR video sequence</h3><p>To generate full spherical HDR videos for flying through applications, we applied our approach to a video sequence consisting of 500 frames. The aircraft was flown at a speed of 5–8 m/s and a height of 130 m while capturing the multi-exposure images. The frame rate of the generated HDR video was 16 fps, the same as that of the source multi-exposure video. Processing time was approximately 10 s per frame on a desktop PC with Core i7-2600 CPU (3.40 GHz, 4 Cores). The position and orientation of the camera beneath the aircraft were estimated using the structure-from-motion technique for omnidirectional cameras (Sato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system. In: Proceedings of eighth European conference computer vision (ECCV’04), Prague, Czech Republic 2:326–340" href="/article/10.1007/s10055-014-0249-x#ref-CR25" id="ref-link-section-d37164e5001">2004</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig12">12</a> shows samples of the full spherical frames. Note that the position and orientation of the video frames were successfully estimated. This information can be used for geometric registration between real and virtual objects in <i>augmented</i> immersive panoramas.</p><h3 class="c-article__sub-heading" id="Sec20">Evaluation of missing area completion using sky radiance model [Okura et al. (2011)]</h3><p>A promising method of spherical image completion is to fit a model of sky radiance to images acquired by a groundward camera and forgo use of a skyward camera altogether, as described in Okura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78" href="/article/10.1007/s10055-014-0249-x#ref-CR19" id="ref-link-section-d37164e5018">2011</a>). To demonstrate the effectiveness of this approach for IBL, we generated and completed a full spherical image using the All-Sky-Model Igawa et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Igawa N, Koga Y, Matsuzawa T, Nakamura H (2004) Models of sky radiance distribution and sky luminance distribution. Sol Energy 77(2):137–157" href="/article/10.1007/s10055-014-0249-x#ref-CR11" id="ref-link-section-d37164e5021">2004</a>), which provides radiance parameters for common weather conditions. Note that the optimal parameters of the sky model were estimated from the intensity of the captured image using a method described in Okura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78" href="/article/10.1007/s10055-014-0249-x#ref-CR19" id="ref-link-section-d37164e5024">2011</a>).</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a> shows the same frame as Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig12">12</a>b, but with the missing area completed using the sky model. Note that, from a subjective viewpoint, the estimated <i>smooth</i> textures in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a> may seem unnatural and decrease the immersive value of the image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Completion of missing area based on Okura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78" href="/article/10.1007/s10055-014-0249-x#ref-CR19" id="ref-link-section-d37164e5052">2011</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>To investigate the effects when using full spherical images for IBL, we compared the estimated intensity shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a> with the acquired intensity. Note that the average intensity of the sky estimated using the sky model was quite different from the intensity captured by the camera, (<span class="mathjax-tex">\(1:0.218\)</span>), as illustrated by Figs. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig12">12</a>b and <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a>. When Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a> is used for IBL, virtual objects with Lambertian surfaces are rendered <span class="mathjax-tex">\(0.218\)</span> times brighter than their captured intensity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig12">12</a>b). Furthermore, in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig12">12</a>b, the highest intensity (which affects the appearance of cast shadows) was ten times larger than in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig13">13</a>. These results indicate that the proposed approach, which employs an additional skyward camera for capturing the actual intensity of the missing area, produces significantly better IBL rendering than current approaches using model-based sky area completion.</p><h3 class="c-article__sub-heading" id="Sec21">Discussion of auto-exposure control</h3><p>We compared the auto-exposure control method in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec9">3.2.2</a> with ordinary controls for HDR imaging. Most such controls, including that used by Kang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kang SB, Uyttendaele M, Winder S, Szeliski R (2003) High dynamic range video. ACM Trans Graph. (Proc. ACM SIGGRAPH’03) 22(3):319–325" href="/article/10.1007/s10055-014-0249-x#ref-CR13" id="ref-link-section-d37164e5148">2003</a>), first determine the base exposure automatically and then calculate the remaining exposure values at fixed stops from the base exposure. So as not to saturate the scene, we used the shortest exposure time <span class="mathjax-tex">\(s_{c_1}\)</span>[ms] as our base exposure. The other exposure times <span class="mathjax-tex">\(s_{c_2}, \ldots , s_{c_4}\)</span> were then calculated for every two stops, meaning that <span class="mathjax-tex">\(s_{c_{i}}\)</span> was set <span class="mathjax-tex">\(2^2=4\)</span> times longer than <span class="mathjax-tex">\(s_{c_{i-1}}\)</span> (i.e., fixed multiples). Note that advancing two stops for multi-exposure imaging is the default commonly used in commercial auto-bracketing cameras.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0249-x#Tab3">3</a> shows the calculated maximum intensity and quantization steps for each approach. The maximum intensity of each multi-exposure image is a relative value to the maximum intensity in the scene, defined as 1,000. The quantization steps of the acquired multi-exposure images (eight-bit) were converted to HDR intensity by dividing the maximum intensity by 255, in the same way as <span class="mathjax-tex">\(\Delta\)</span> in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec9">3.2.2</a>. A smaller value is better here, as it helps prevent visual artifacts. A comparison of longer exposure times indicates that the quantization step of the conventional method was 6.7 times larger than that of our proposed approach. This difference can particularly affect areas of low intensity, such as most ground features. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a> shows close-ups of a generated HDR image of the same frame shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig11">11</a>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>c is the artificially re-quantized picture generated from this image using the exposure times of the conventional approach <span class="mathjax-tex">\(s_{c_i}(1\le i \le 4)\)</span>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>b, d show the gradient images of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>a, c, respectively. Note that banding appears in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>d, mainly in low-intensity areas. This indicates that our proposed approach can prevent visual artifacts produced using the conventional approach.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig14_HTML.jpg" alt="figure14" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Visualization of negative effect of large quantization steps. <b>a</b> Close-up of a generated HDR image. <b>b</b> Gradient of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>a by Sobel filter. <b>c</b> Image re-quantized using fixed multiple exposures. <b>d</b> Gradient of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig14">14</a>c by Sobel filter</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Quantization steps of the HDR image using the approach described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec9">3.2.2</a> and the conventional approach based on fixed multiples of the shortest exposure time</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0249-x/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                </div></div></section><section aria-labelledby="Sec22"><div class="c-article-section" id="Sec22-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec22">Displaying full spherical HDR image for immersive panoramas</h2><div class="c-article-section__content" id="Sec22-content"><p>In this section, we evaluate methods for displaying the full spherical HDR images generated in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec5">3</a> based on subjective review of an immersive panorama application. In most uses of HDR images, they are converted using parameters determined from the image as a whole, such as maximum and logarithmic mean intensity. Immersive panorama applications, unlike ordinary applications, generally crop and convert a spherical image to a perspective image so that the entire image is not displayed at once. Thus, it is important to investigate tone-mapping approaches suitable for such applications.</p><p>We conducted a subjective evaluation of four display methods using an immersive system comprising an head-mounted display (HMD) and a head-tracking device (Onoe et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Onoe Y, Yamazawa K, Takemura H, Yokoya N (1998) Telepresence by real-time view-dependent image generation from omnidirectional video streams. Comput Vis Image Underst 71(2):154–165" href="/article/10.1007/s10055-014-0249-x#ref-CR20" id="ref-link-section-d37164e5691">1998</a>). With this system, a spherical image is converted to a perspective image in real-time based on the user’s view direction, as detected by an electro-magnetic sensor (Fastrak, Polhemus) mounted on the HMD (HMZ-T1, Sony Corp.). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig15">15</a> depicts the system used. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig15_HTML.jpg?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig15_HTML.jpg" alt="figure15" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Immersive system used in the experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>The four display methods outlined below were evaluated. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig16">16</a> illustrates ideal tone curves for these methods.</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>LDR-like representation: This is a luminance transformation method that uses a linear tone curve, such as that shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig16">16</a>a, to generate an LDR image with the same appearance as one generated by traditional LDR imaging.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>Reinhard’s tone-mapping Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e5745">2002</a>): This uses maximum and log-mean intensity to tone-map the HDR image. It is one of the most popular tone-mapping methods.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>Region-wise tone-mapping: Aerial spherical images are divided into regions of bright sky and dark ground. Using the method of Kitaura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kitaura M, Okura F, Kanbara M, Yokoya N (2012) Tone mapping for HDR images with dimidiate luminance and spatial distributions of bright and dark regions. In: Proceedings of SPIE, vol 8292, Burlingame, CA, ArticleNo 829205" href="/article/10.1007/s10055-014-0249-x#ref-CR16" id="ref-link-section-d37164e5759">2012</a>), these regions are then tone-mapping independently, using different parameters.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>View direction-dependent tone-mapping: An input HDR image is tone-mapped based on the intensity of the field of view of a perspective image converted by the immersive system based on the user’s view direction. The intensity of a position in the scene varies according to this view direction.</p>
                    
                  </li>
                </ol>
                <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig16_HTML.gif?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig16_HTML.gif" alt="figure16" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Illustrative tone curves for four display methods</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <h3 class="c-article__sub-heading" id="Sec23">Display methods for aerial spherical HDR images</h3><p>Let us now look at the implementation of each display method and the results of subjective review.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec24">LDR-like representation</h4><p>Linear intensity transformation generates an image that looks the same as any generated by traditional LDR imaging. The tone curve is defined as</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I_l&amp;= \left\{ \begin{array}{ll} 1.0 &amp; \quad (I_h &gt; 1.0) \\ 2^{v}I_h &amp; \quad (\text {otherwise}), \\ \end{array} \right. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>where <span class="mathjax-tex">\(I_h\)</span> is the intensity in the HDR image, <span class="mathjax-tex">\(I_l\)</span> denotes the intensity of the LDR image normalized to the range <span class="mathjax-tex">\([0, 1]\)</span>, and <span class="mathjax-tex">\(v\)</span> is the exposure value of the generated LDR image. We determined <span class="mathjax-tex">\(v\)</span> so as to reduce overexposed and underexposed pixels in the generated LDR image. Underexposure was defined as <span class="mathjax-tex">\(I_l &lt; \frac{1}{16}\)</span> and overexposure as <span class="mathjax-tex">\(I_l = 1.0\)</span> for this experiment. Note that, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig17">17</a>, many overexposed and underexposed pixels remained in the resulting image, which was converted from the HDR image captured in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec17">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig17_HTML.jpg?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig17_HTML.jpg" alt="figure17" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Image generated by LDR-like representation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Reinhard’s tone-mapping</h4><p>The tone-mapping method proposed by Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e6160">2002</a>) is composed of two steps. In the first step, the intensity <span class="mathjax-tex">\(I_h\)</span> of an HDR image is transformed to LDR intensity <span class="mathjax-tex">\(I_l\)</span> by a function involving the log-mean <span class="mathjax-tex">\(\bar{I_h}\)</span>:</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I_{{\rm s}}&amp;= \frac{\alpha }{\bar{I_h}}I_h, \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div>
                    <div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} I_l&amp;= \frac{I_{{\rm s}}}{\left( 1+\frac{I_{{\rm s}}}{I^2_{{\rm max}}} \right) }{1+I_s}. \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>Note that <span class="mathjax-tex">\(I_{{\rm s}}\)</span> is a scaled intensity using a scale factor <span class="mathjax-tex">\(\alpha\)</span> and <span class="mathjax-tex">\(I_{{\rm max}}\)</span> denotes the maximum intensity scaled by Eq. (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0249-x#Equ14">14</a>). The second step, so-called dodging-and-burning, is applied to areas of low and high intensity to improve their visibility (detailed in Reinhard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e6503">2002</a>). Parameter <span class="mathjax-tex">\(\alpha\)</span> is automatically estimated by Reinhard (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E (2002) Parameter estimation for photographic tone reproduction. J Graph Tools 7(1):45–51" href="/article/10.1007/s10055-014-0249-x#ref-CR21" id="ref-link-section-d37164e6527">2002</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig18">18</a> shows the result of tone-mapping the same frame that used in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig17">17</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig18_HTML.jpg?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig18_HTML.jpg" alt="figure18" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Result using Reinhard’s tone-mapping method Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e6546">2002</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec26">Region-wise tone-mapping</h4><p>Dividing images into sky and ground regions, we applied the approach proposed by Kitaura et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kitaura M, Okura F, Kanbara M, Yokoya N (2012) Tone mapping for HDR images with dimidiate luminance and spatial distributions of bright and dark regions. In: Proceedings of SPIE, vol 8292, Burlingame, CA, ArticleNo 829205" href="/article/10.1007/s10055-014-0249-x#ref-CR16" id="ref-link-section-d37164e6566">2012</a>) to tone-map these regions using separate parameters. To fully automate this process, we specialized it for aerial spherical imaging. Specifically, we used a priori knowledge of aerial images to support automatic GrabCut segmentation in place of user input of seed pixels. We assumed that the camera pose was estimated by structure-from-motion (Sato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system. In: Proceedings of eighth European conference computer vision (ECCV’04), Prague, Czech Republic 2:326–340" href="/article/10.1007/s10055-014-0249-x#ref-CR25" id="ref-link-section-d37164e6569">2004</a>) in the same fashion as in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec19">4.2</a> and that the captured HDR image was aligned to fit the horizon in the scene to the horizontal scan line of the image. Under these assumptions, the seed pixels for the dark (ground) region can be taken from the lower half of the image, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig19">19</a>. The distribution of seed pixels for the bright (sky) region depends on the landscape of the scene. We used the upper third of the image as seed pixels for the bright region, resulting in successful separate of sky from ground, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig20">20</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-19"><figure><figcaption><b id="Fig19" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 19</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/19" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig19_HTML.gif?as=webp"></source><img aria-describedby="figure-19-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig19_HTML.gif" alt="figure19" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-19-desc"><p>Seed pixels for GrabCut Rother et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive foreground extraction using iterated graph cuts. ACM Trans Graph (Proc. ACM SIGGRAPH’04) 23(3):309–314" href="/article/10.1007/s10055-014-0249-x#ref-CR24" id="ref-link-section-d37164e6592">2004</a>): <span class="mathjax-tex">\(y\)</span> denotes the vertical component of the image coordinates; <span class="mathjax-tex">\(y_{\rm max}\)</span> is the height of the image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/19" data-track-dest="link:Figure19 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-20"><figure><figcaption><b id="Fig20" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 20</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/20" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig20_HTML.jpg?as=webp"></source><img aria-describedby="figure-20-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig20_HTML.jpg" alt="figure20" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-20-desc"><p>Regions determined by GrabCut Rother et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Rother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive foreground extraction using iterated graph cuts. ACM Trans Graph (Proc. ACM SIGGRAPH’04) 23(3):309–314" href="/article/10.1007/s10055-014-0249-x#ref-CR24" id="ref-link-section-d37164e6658">2004</a>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/20" data-track-dest="link:Figure20 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Region-wise tone-mapped images such as that shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig21">21</a> are then produced by applying Reinhard’s tone-mapping (Reinhard et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e6676">2002</a>) to each region independently. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-21"><figure><figcaption><b id="Fig21" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 21</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/21" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig21_HTML.jpg?as=webp"></source><img aria-describedby="figure-21-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig21_HTML.jpg" alt="figure21" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-21-desc"><p>Result of region-wise tone-mapping</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/21" data-track-dest="link:Figure21 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec27">View direction-dependent tone-mapping</h4><p>For immersive panorama applications in particular, it is worth investigating tone-mapping methods whose tone curves vary according to the user’s view direction. We applied a new approach similar to the tone-mapping for HDR video Kang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kang SB, Uyttendaele M, Winder S, Szeliski R (2003) High dynamic range video. ACM Trans Graph. (Proc. ACM SIGGRAPH’03) 22(3):319–325" href="/article/10.1007/s10055-014-0249-x#ref-CR13" id="ref-link-section-d37164e6706">2003</a>). The approach consists of two processes:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Offline process: Pre-calculation of log-mean and maximum intensity.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Online process: Tone-mapping using pre-calculated parameters.</p>
                        
                      </li>
                    </ol><p>Log-mean and maximum intensity were calculated from perspective images generated for each view direction (in this study, we calculated parameters for every degree of latitude and longitude). Equations (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0249-x#Equ14">14</a>) and (<a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-014-0249-x#Equ15">15</a>) were then applied for tone-mapping in real time using the calculated parameters and <span class="mathjax-tex">\(\alpha = 0.18\)</span>, which is the default value used in Reinhard et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276" href="/article/10.1007/s10055-014-0249-x#ref-CR22" id="ref-link-section-d37164e6766">2002</a>). Note that although this experiment used an offline process, it is possible to implement a fully real-time process by leveraging graphics processing unit (GPU)-based computation, as in other real-time tone-mapping proposals Goodnight et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Goodnight N, Wang R, Woolley C, Humphreys G (2003) Interactive time-dependent tone mapping using programmable graphics hardware. In: Proceedings of 14th Eurographics Workshop on Rendering. Belgium, Leuven, pp 26–37" href="/article/10.1007/s10055-014-0249-x#ref-CR6" id="ref-link-section-d37164e6769">2003</a>). The results, seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig22">22</a>, show how the intensity at a given position in the scene can vary according to the user’s view direction.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-22"><figure><figcaption><b id="Fig22" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 22</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/22" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig22_HTML.gif?as=webp"></source><img aria-describedby="figure-22-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig22_HTML.gif" alt="figure22" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-22-desc"><p>Examples of view direction-dependent tone-mapping: the intensity of the same position in the scene varies according to the view direction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/22" data-track-dest="link:Figure22 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <h3 class="c-article__sub-heading" id="Sec28">Subjective evaluation using the immersive system</h3><p>To investigate the features of the above four display methods for aerial spherical HDR images, we conducted a subjective evaluation using an immersive system. Examinees used an HMD to view scenes generated by the four methods and responded to each of the following questions with a rating of one (worst) to five (best): <dl class="c-abbreviation_list"><dt class="c-abbreviation_list__term"><dfn>Q1::</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>Were the scene and textures easily recognizable? (Visibility, Vis)</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>Q2::</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>Did the tone, brightness, and appearance of textures appear natural? (Naturalness, Nat)</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>Q3::</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>Did you get the feeling that you were actually flying through the sky? (Immersion, Imm)</p>
                      </dd><dt class="c-abbreviation_list__term"><dfn>Q4::</dfn></dt><dd class="c-abbreviation_list__description">
                        <p>Were you satisfied with the display method as a whole? (Total, Tot)</p>
                      </dd></dl> Note that the questions were originally given in Japanese. The examinees consisted of ten people in their twenties or thirties, among whom eight examinees were male, and all of them had normal color vision. Although some of them were familiar with the concept of virtual reality and image processing, the aim of the experiment was kept secret during the experiment. No specific tasks were given to the examinees, except for watching the scenes. Further, the examinees had the option to change their view direction. The order in which the four display methods were presented was randomized, and examinees were allowed to switch and watch the display mode among the representations as many times as they wished, even while answering questions. The examinees were also allowed to comment freely after answering the questionnaire.</p><p>The results of the evaluation are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig23">23</a> (in detail, see Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-014-0249-x#Tab4">4</a>). The figure also indicates the pairs that had significant difference (<span class="mathjax-tex">\(p\,&lt;\,0.05\)</span>), as calculated using the Tukey’s test. In total, Reinhard’s method and the view direction-dependent method had higher ratings than the LDR-like method. Region-wise tone-mapping was significantly lower than the other three, especially with regard to naturalness.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-23"><figure><figcaption><b id="Fig23" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 23</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/23" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig23_HTML.gif?as=webp"></source><img aria-describedby="figure-23-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-014-0249-x/MediaObjects/10055_2014_249_Fig23_HTML.gif" alt="figure23" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-23-desc"><p>Results of subjective experiment involving the four display methods by ten examinees. It also indicates the pairs that had significant differences (<span class="mathjax-tex">\(p\,&lt;\,0.05\)</span>) calculated using the Tukey’s test, in which the type-I error rate and the degrees of freedom for each sample were set as <span class="mathjax-tex">\(0.05\)</span> and <span class="mathjax-tex">\(10*4-4 = 36\)</span>, respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-014-0249-x/figures/23" data-track-dest="link:Figure23 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                  <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Specific results of the subjective experiment</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-014-0249-x/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <h3 class="c-article__sub-heading" id="Sec29">Discussion of display methods</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec30">Effects of HDR imaging for immersive panorama</h4><p>For Q4 in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec28">5.2</a>, with the exception of region-wise tone-mapping, nonlinear tone-mapping methods had significantly higher ratings than LDR-like representation methods (as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-014-0249-x#Fig23">23</a>). We can therefore confirm that HDR imaging and appropriate display methods are superior for immersive panoramic applications, as well as for traditional uses of HDR images such as conventional photography and IBL. This superiority is most noticeable in the reduction of overexposed and underexposed pixels. Furthermore, since immersion is one of the chief goals of virtual reality Moezzi (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Moezzi S (1997) Special issue on immersive telepresence. IEEE Multimed 4(1):17–56" href="/article/10.1007/s10055-014-0249-x#ref-CR18" id="ref-link-section-d37164e7186">1997</a>), it is noteworthy that HDR imaging does not decrease the immersion despite the use of a nonlinear intensity transformation in the tone-mapping, which can be regarded as unnatural.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec31">The best HDR image display method for immersive panorama</h4><p>What is the best display method for HDR immersive panorama applications? Although region-wise tone-mapping had a higher visibility rating (along with other HDR tone-mapping methods), significantly lower ratings were obtained for the other questions. Free comments from the examinees suggested that the most critical factor in determining the naturalness was the darkness of the sky relative to the intensity of the ground.</p><p>There are no significant differences between Reinhard’s tone-mapping method and viewing direction-dependent tone-mapping. Free commentary from some examinees indicated that view direction-dependent tone-mapping improved visibility over Reinhard’s method, yet produced unnatural variations in intensity as view direction changed in real time. Although both Reinhard’s tone-mapping method and view direction-dependent tone-mapping were thought to be promising representations for immersive display, the parameters related to human factors, such as acceptable rates of intensity variation, should be investigated to guide further development. If it can overcome this perceived deficiency, view direction-dependent tone-mapping may become a more popular display method.</p></div></div></section><section aria-labelledby="Sec32"><div class="c-article-section" id="Sec32-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec32">Conclusion</h2><div class="c-article-section__content" id="Sec32-content"><p>This paper has focused on both the acquisition and display of aerial full spherical HDR images. The resulting HDR images can be used in immersive panoramic viewers, IBL, and other applications that use spherical images. The multi-exposure images used to generate these HDR images are captured by a pair of omnidirectional cameras mounted above and below an aircraft, and can even be used to generate full spherical HDR video, with position and orientation information available for superimposed virtual objects. We also investigated tone-mapping methods of full spherical HDR images using an immersive panorama system using an HMD. It was confirmed that the HDR imaging is well suited to such immersive applications, and the results also show that a tone-mapping method that depends on the user’s viewing direction as well as an ordinary tone-mapping yields good results with respect to critical human factors.</p></div></div></section><section aria-labelledby="Sec33"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33">Future work</h2><div class="c-article-section__content" id="Sec33-content"><p>Although we have developed a complete system for acquiring aerial full spherical HDR images, there are some issues that should be investigated/developed further.</p><p>Our system employs a new exposure control approach for multi-exposure imaging, which selects exposures such that the quantization steps become small. This approach can be applied for general multi-exposure imaging in principle. To utilize our approach for general situations, the performance should be compared with more sophisticated exposure control approaches, in addition to a simple method using fixed multiple exposures as in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-014-0249-x#Sec21">4.4</a>.</p><p>With the recent growth of hardware technology, some CCD/CMOS sensors can directly acquire HDR (e.g., 12- or 15-bit) scenes. However, it is not enough to capture a whole dynamic range of outdoor scenes, as well as a whole direction of the scenes in a single shot, in which the dynamic range only of the sky can be up to approximately <span class="mathjax-tex">\(1:2^{17}\)</span> (may become larger for a whole scene including the ground). However, such HDR imaging sensors would be a good substitute for using multi-exposure imaging in the near future.</p><p>It is a good future direction to investigate/develop more state-of-the-art display methods for HDR immersive panorama. The experiment, which was a comparison among some relatively old tone-mapping methods, showed that there remains room for improvement with regard to the view direction-dependent approach. Uses for Web browser-based immersive panorama systems, similar to Google Street View, can also be taken into account in addition to using an HMD.</p><p>We also intend to use full spherical HDR images for photorealistic superimposition of virtual objects, based on camera position and orientation information. Such applications are promising for mixed-reality-based visualization such as cultural heritages.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon, R. Lyon, A. Ogale, L. Vincent, J. Weaver, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Anguelov D, Dulong C, Filip D, Frueh C, Lafon S, Lyon R, Ogale A, Vincent L, Weaver J (2010) Google street vie" /><p class="c-article-references__text" id="ref-CR1">Anguelov D, Dulong C, Filip D, Frueh C, Lafon S, Lyon R, Ogale A, Vincent L, Weaver J (2010) Google street view: capturing the world at street level. IEEE Comput Mag 43(6):32–38</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMC.2010.170" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Google%20street%20view%3A%20capturing%20the%20world%20at%20street%20level&amp;journal=IEEE%20Comput%20Mag&amp;volume=43&amp;issue=6&amp;pages=32-38&amp;publication_year=2010&amp;author=Anguelov%2CD&amp;author=Dulong%2CC&amp;author=Filip%2CD&amp;author=Frueh%2CC&amp;author=Lafon%2CS&amp;author=Lyon%2CR&amp;author=Ogale%2CA&amp;author=Vincent%2CL&amp;author=Weaver%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Chen SE (1995) Quicktime VR: an image-based approach to virtual environment navigation. In: Proceedings of ACM" /><p class="c-article-references__text" id="ref-CR2">Chen SE (1995) Quicktime VR: an image-based approach to virtual environment navigation. In: Proceedings of ACM SIGGRAPH’95, Los Angeles, CA, pp 29–38</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec P (1998) Rendering synthetic objects into real scenes: bridging traditional and image-based graphics w" /><p class="c-article-references__text" id="ref-CR3">Debevec P (1998) Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In: Proceedings of ACM SIGGRAPH’98, Orlando, FL, pp 189–198</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- a" /><p class="c-article-references__text" id="ref-CR4">Debevec P, Taylor C, Malik J (1996) Modeling and rendering architecture from photographs: a hybrid geometry- and image-based approach. In: Proceedings of ACM SIGGRAPH’96, New Orleans, LA, pp 11–20</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of AC" /><p class="c-article-references__text" id="ref-CR5">Debevec PE, Malik J (1997) Recovering high dynamic range radiance maps from photographs. In: Proceedings of ACM SIGGRAPH’97. Los Angeles, CA, pp 369–378</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Goodnight N, Wang R, Woolley C, Humphreys G (2003) Interactive time-dependent tone mapping using programmable " /><p class="c-article-references__text" id="ref-CR6">Goodnight N, Wang R, Woolley C, Humphreys G (2003) Interactive time-dependent tone mapping using programmable graphics hardware. In: Proceedings of 14th Eurographics Workshop on Rendering. Belgium, Leuven, pp 26–37</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="MA. Goodrich, BS. Morse, D. Gerhardt, JL. Cooper, M. Quigley, JA. Adams, C. Humphrey, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Goodrich MA, Morse BS, Gerhardt D, Cooper JL, Quigley M, Adams JA, Humphrey C (2008) Supporting wilderness sea" /><p class="c-article-references__text" id="ref-CR7">Goodrich MA, Morse BS, Gerhardt D, Cooper JL, Quigley M, Adams JA, Humphrey C (2008) Supporting wilderness search and rescue using a camera-equipped mini UAV. J Field Robot 25(1):89–110</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Frob.20226" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Supporting%20wilderness%20search%20and%20rescue%20using%20a%20camera-equipped%20mini%20UAV&amp;journal=J%20Field%20Robot&amp;volume=25&amp;issue=1&amp;pages=89-110&amp;publication_year=2008&amp;author=Goodrich%2CMA&amp;author=Morse%2CBS&amp;author=Gerhardt%2CD&amp;author=Cooper%2CJL&amp;author=Quigley%2CM&amp;author=Adams%2CJA&amp;author=Humphrey%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grossberg MD, Nayar SK (2003) High dynamic range from multiple images: which exposures to combine? In: Proceed" /><p class="c-article-references__text" id="ref-CR8">Grossberg MD, Nayar SK (2003) High dynamic range from multiple images: which exposures to combine? In: Proceedings of IEEE workshop on color and photometric methods in computer vision (CPMCV), Nice, France, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hasinoff SW, Durand F, Freeman WT (2010) Noise-optimal capture for high dynamic range photography. In: Proceed" /><p class="c-article-references__text" id="ref-CR9">Hasinoff SW, Durand F, Freeman WT (2010) Noise-optimal capture for high dynamic range photography. In: Proceedings of 23rd IEEE conference computer vision pattern recognition (CVPR’10), San Francisco, CA, pp 553–560</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Herwitz, L. Johnson, S. Dunagan, R. Higgins, D. Sullivan, J. Zheng, B. Lobitz, J. Leung, B. Gallmeyer, M. Aoyagi, R. Slye, J. Brass, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Herwitz S, Johnson L, Dunagan S, Higgins R, Sullivan D, Zheng J, Lobitz B, Leung J, Gallmeyer B, Aoyagi M, Sly" /><p class="c-article-references__text" id="ref-CR10">Herwitz S, Johnson L, Dunagan S, Higgins R, Sullivan D, Zheng J, Lobitz B, Leung J, Gallmeyer B, Aoyagi M, Slye R, Brass J (2004) Imaging from an unmanned aerial vehicle: agricultural surveillance and decision support. Comput Electron Agric 44:49–61</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.compag.2004.02.006" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Imaging%20from%20an%20unmanned%20aerial%20vehicle%3A%20agricultural%20surveillance%20and%20decision%20support&amp;journal=Comput%20Electron%20Agric&amp;volume=44&amp;pages=49-61&amp;publication_year=2004&amp;author=Herwitz%2CS&amp;author=Johnson%2CL&amp;author=Dunagan%2CS&amp;author=Higgins%2CR&amp;author=Sullivan%2CD&amp;author=Zheng%2CJ&amp;author=Lobitz%2CB&amp;author=Leung%2CJ&amp;author=Gallmeyer%2CB&amp;author=Aoyagi%2CM&amp;author=Slye%2CR&amp;author=Brass%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Igawa, Y. Koga, T. Matsuzawa, H. Nakamura, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Igawa N, Koga Y, Matsuzawa T, Nakamura H (2004) Models of sky radiance distribution and sky luminance distribu" /><p class="c-article-references__text" id="ref-CR11">Igawa N, Koga Y, Matsuzawa T, Nakamura H (2004) Models of sky radiance distribution and sky luminance distribution. Sol Energy 77(2):137–157</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.solener.2004.04.016" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Models%20of%20sky%20radiance%20distribution%20and%20sky%20luminance%20distribution&amp;journal=Sol%20Energy&amp;volume=77&amp;issue=2&amp;pages=137-157&amp;publication_year=2004&amp;author=Igawa%2CN&amp;author=Koga%2CY&amp;author=Matsuzawa%2CT&amp;author=Nakamura%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ikeda S, Sato T, Yokoya N, (2003) High-resolution panoramic movie generation from video streams acquired by an" /><p class="c-article-references__text" id="ref-CR12">Ikeda S, Sato T, Yokoya N, (2003) High-resolution panoramic movie generation from video streams acquired by an omnidirectional multi-camera system. In: Proceedings of 2003 IEEE international conference multisensor fusion integration intelligent system (MFI’03), Tokyo, Japan, pp 155–160</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="SB. Kang, M. Uyttendaele, S. Winder, R. Szeliski, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Kang SB, Uyttendaele M, Winder S, Szeliski R (2003) High dynamic range video. ACM Trans Graph. (Proc. ACM SIGG" /><p class="c-article-references__text" id="ref-CR13">Kang SB, Uyttendaele M, Winder S, Szeliski R (2003) High dynamic range video. ACM Trans Graph. (Proc. ACM SIGGRAPH’03) 22(3):319–325</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1145%2F882262.882270" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=High%20dynamic%20range%20video.%20ACM%20Trans%20Graph&amp;journal=%28Proc.%20ACM%20SIGGRAPH%E2%80%9903%29&amp;volume=22&amp;issue=3&amp;pages=319-325&amp;publication_year=2003&amp;author=Kang%2CSB&amp;author=Uyttendaele%2CM&amp;author=Winder%2CS&amp;author=Szeliski%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kawai N, Sato T, Yokoya N (2009) Image inpainting considering brightness change and spatial locality of textur" /><p class="c-article-references__text" id="ref-CR14">Kawai N, Sato T, Yokoya N (2009) Image inpainting considering brightness change and spatial locality of textures and its evaluation. In: Proceedings of third Pacific-Rim symposium image and video technology (PSIVT’09), Tokyo, Japan, pp 271–282</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="N. Kawai, K. Machikita, T. Sato, N. Yokoya, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Kawai N, Machikita K, Sato T, Yokoya N (2010) Video completion for generating omnidirectional video without in" /><p class="c-article-references__text" id="ref-CR15">Kawai N, Machikita K, Sato T, Yokoya N (2010) Video completion for generating omnidirectional video without invisible areas. IPSJ Trans Comput Vis Appl 2:200–213</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Video%20completion%20for%20generating%20omnidirectional%20video%20without%20invisible%20areas&amp;journal=IPSJ%20Trans%20Comput%20Vis%20Appl&amp;volume=2&amp;pages=200-213&amp;publication_year=2010&amp;author=Kawai%2CN&amp;author=Machikita%2CK&amp;author=Sato%2CT&amp;author=Yokoya%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kitaura M, Okura F, Kanbara M, Yokoya N (2012) Tone mapping for HDR images with dimidiate luminance and spatia" /><p class="c-article-references__text" id="ref-CR16">Kitaura M, Okura F, Kanbara M, Yokoya N (2012) Tone mapping for HDR images with dimidiate luminance and spatial distributions of bright and dark regions. In: Proceedings of SPIE, vol 8292, Burlingame, CA, ArticleNo 829205</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kopf J, Chen B, Szeliski R, Cohen M (2010) Street slide: browsing street level imagery. CM Trans Graph (Proc. " /><p class="c-article-references__text" id="ref-CR17">Kopf J, Chen B, Szeliski R, Cohen M (2010) Street slide: browsing street level imagery. CM Trans Graph (Proc. ACM SIGGRAPH’10) 29(4), Article No 96</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Moezzi, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Moezzi S (1997) Special issue on immersive telepresence. IEEE Multimed 4(1):17–56" /><p class="c-article-references__text" id="ref-CR18">Moezzi S (1997) Special issue on immersive telepresence. IEEE Multimed 4(1):17–56</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMMUL.1997.580996" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Special%20issue%20on%20immersive%20telepresence&amp;journal=IEEE%20Multimed&amp;volume=4&amp;issue=1&amp;pages=17-56&amp;publication_year=1997&amp;author=Moezzi%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidir" /><p class="c-article-references__text" id="ref-CR19">Okura F, Kanbara M, Yokoya N (2011) Fly-through Heijo palace site: augmented telepresence using aerial omnidirectional videos. In: Proceedings of ACM SIGGRAPH’11 Posters, Vancouver, BC, Article No 78</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Onoe, K. Yamazawa, H. Takemura, N. Yokoya, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Onoe Y, Yamazawa K, Takemura H, Yokoya N (1998) Telepresence by real-time view-dependent image generation from" /><p class="c-article-references__text" id="ref-CR20">Onoe Y, Yamazawa K, Takemura H, Yokoya N (1998) Telepresence by real-time view-dependent image generation from omnidirectional video streams. Comput Vis Image Underst 71(2):154–165</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1006%2Fcviu.1998.0705" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Telepresence%20by%20real-time%20view-dependent%20image%20generation%20from%20omnidirectional%20video%20streams&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=71&amp;issue=2&amp;pages=154-165&amp;publication_year=1998&amp;author=Onoe%2CY&amp;author=Yamazawa%2CK&amp;author=Takemura%2CH&amp;author=Yokoya%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Reinhard, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Reinhard E (2002) Parameter estimation for photographic tone reproduction. J Graph Tools 7(1):45–51" /><p class="c-article-references__text" id="ref-CR21">Reinhard E (2002) Parameter estimation for photographic tone reproduction. J Graph Tools 7(1):45–51</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1888287" aria-label="View reference 21 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10867651.2002.10487554" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.emis.de/MATH-item?1022.68735" aria-label="View reference 21 on MATH">MATH</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Parameter%20estimation%20for%20photographic%20tone%20reproduction&amp;journal=J%20Graph%20Tools&amp;volume=7&amp;issue=1&amp;pages=45-51&amp;publication_year=2002&amp;author=Reinhard%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans" /><p class="c-article-references__text" id="ref-CR22">Reinhard E, Stark M, Shirley P, Ferwerda J (2002) Photographic tone reproduction for digital images. ACM Trans Graph (Proc. ACM SIGGRAPH’02) 21(3):267–276</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="E. Reinhard, W. Heidrich, P. Debevec, S. Pattanaik, G. Ward, K. Myszkowski, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Reinhard E, Heidrich W, Debevec P, Pattanaik S, Ward G, Myszkowski K (2010) High dynamic range imaging: acquis" /><p class="c-article-references__text" id="ref-CR23">Reinhard E, Heidrich W, Debevec P, Pattanaik S, Ward G, Myszkowski K (2010) High dynamic range imaging: acquisition, display, and image-based lighting. Morgan Kaufmann, San Francisco</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=High%20dynamic%20range%20imaging%3A%20Acquisition%2C%20display%2C%20and%20image-based%20lighting&amp;publication_year=2010&amp;author=Reinhard%2CE&amp;author=Heidrich%2CW&amp;author=Debevec%2CP&amp;author=Pattanaik%2CS&amp;author=Ward%2CG&amp;author=Myszkowski%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Rother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive foreground extraction using iterated graph cuts. A" /><p class="c-article-references__text" id="ref-CR24">Rother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive foreground extraction using iterated graph cuts. ACM Trans Graph (Proc. ACM SIGGRAPH’04) 23(3):309–314</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by" /><p class="c-article-references__text" id="ref-CR25">Sato T, Ikeda S, Yokoya N (2004) Extrinsic camera parameter recovery from multiple image sequences captured by an omni-directional multi-camera system. In: Proceedings of eighth European conference computer vision (ECCV’04), Prague, Czech Republic 2:326–340</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shi J, Tomasi C, (1994) Good features to track. In: Proceedings of 1994 IEEE conference computer vision patter" /><p class="c-article-references__text" id="ref-CR26">Shi J, Tomasi C, (1994) Good features to track. In: Proceedings of 1994 IEEE conference computer vision pattern recognition (CVPR’94), Seattle, WA, pp 593–600</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Shum, S. Kang, S. Chan, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Shum H, Kang S, Chan S (2003) Survey of image-based representations and compression techniques. IEEE Trans Cir" /><p class="c-article-references__text" id="ref-CR27">Shum H, Kang S, Chan S (2003) Survey of image-based representations and compression techniques. IEEE Trans Circuits Syst Video Technol 13(11):1020–1037</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTCSVT.2003.817360" aria-label="View reference 27">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Survey%20of%20image-based%20representations%20and%20compression%20techniques&amp;journal=IEEE%20Trans%20Circuits%20Syst%20for%20Video%20Technol&amp;volume=13&amp;issue=11&amp;pages=1020-1037&amp;publication_year=2003&amp;author=Shum%2CH&amp;author=Kang%2CS&amp;author=Chan%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stumpfel J, Tchou C, Jones A, Hawkins T, Wenger A, Debevec P (2004) Direct HDR capture of the sun and sky. In:" /><p class="c-article-references__text" id="ref-CR28">Stumpfel J, Tchou C, Jones A, Hawkins T, Wenger A, Debevec P (2004) Direct HDR capture of the sun and sky. In: Proceedings of third international conference computer graph, virtual reality, visualization and interaction in Africa (AFRIGRAPH’04), Stellenbosch, South Africa, pp 145–149</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Ward, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Ward G (2003) Fast, robust image registration for compositing high dynamic range photographs from hand-held ex" /><p class="c-article-references__text" id="ref-CR29">Ward G (2003) Fast, robust image registration for compositing high dynamic range photographs from hand-held exposures. J Graph Tools 8(2):17–30</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1080%2F10867651.2003.10487583" aria-label="View reference 29">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fast%2C%20robust%20image%20registration%20for%20compositing%20high%20dynamic%20range%20photographs%20from%20hand-held%20exposures&amp;journal=J%20Graph%20Tools&amp;volume=8&amp;issue=2&amp;pages=17-30&amp;publication_year=2003&amp;author=Ward%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Zomet, D. Feldman, S. Peleg, D. Weinshall, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Zomet A, Feldman D, Peleg S, Weinshall D (2003) Mosaicing new views: the crossed-slits projection. IEEE Trans " /><p class="c-article-references__text" id="ref-CR30">Zomet A, Feldman D, Peleg S, Weinshall D (2003) Mosaicing new views: the crossed-slits projection. IEEE Trans Pattern Anal Mach Intell 25(6):741–754</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2003.1201823" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mosaicing%20new%20views%3A%20the%20crossed-slits%20projection&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=25&amp;issue=6&amp;pages=741-754&amp;publication_year=2003&amp;author=Zomet%2CA&amp;author=Feldman%2CD&amp;author=Peleg%2CS&amp;author=Weinshall%2CD">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-014-0249-x-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research was supported by the Japan Society for the Promotion of Science (JSPS) Grant-in-Aid for Scientific Research (A), No. 23240024, Grant-in-Aid for JSPS Fellows No. 25-7448, and by the “Ambient Intelligence” project funded by Ministry of Education, Culture, Sports, Science and Technology (MEXT).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), Takayama-cho, Ikoma, Nara, 8916-5, Japan</p><p class="c-article-author-affiliation__authors-list">Fumio Okura, Masayuki Kanbara &amp; Naokazu Yokoya</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Fumio-Okura"><span class="c-article-authors-search__title u-h3 js-search-name">Fumio Okura</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Fumio+Okura&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Fumio+Okura" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Fumio+Okura%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Masayuki-Kanbara"><span class="c-article-authors-search__title u-h3 js-search-name">Masayuki Kanbara</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Masayuki+Kanbara&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Masayuki+Kanbara" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Masayuki+Kanbara%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Naokazu-Yokoya"><span class="c-article-authors-search__title u-h3 js-search-name">Naokazu Yokoya</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Naokazu+Yokoya&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Naokazu+Yokoya" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Naokazu+Yokoya%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-014-0249-x/email/correspondent/c1/new">Fumio Okura</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Aerial%20full%20spherical%20HDR%20imaging%20and%20display&amp;author=Fumio%20Okura%20et%20al&amp;contentID=10.1007%2Fs10055-014-0249-x&amp;publication=1359-4338&amp;publicationDate=2014-08-27&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-014-0249-x" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-014-0249-x" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Okura, F., Kanbara, M. &amp; Yokoya, N. Aerial full spherical HDR imaging and display.
                    <i>Virtual Reality</i> <b>18, </b>255–269 (2014). https://doi.org/10.1007/s10055-014-0249-x</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-014-0249-x.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-06-17">17 June 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08-07">07 August 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-08-27">27 August 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-11">November 2014</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-014-0249-x" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-014-0249-x</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Omnidirectional camera</span></li><li class="c-article-subject-list__subject"><span itemprop="about">High dynamic range image</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Immersive panorama</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Image-based lighting</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tone-mapping</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-014-0249-x.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=249;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

