<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Virtual environments for creative work in collaborative music-making"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. The development of virtual spaces for..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/10/2.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Virtual environments for creative work in collaborative music-making"/>

    <meta name="dc.source" content="Virtual Reality 2006 10:2"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2006-09-07"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. The development of virtual spaces for collaboration necessitates more clearly specified theorizing about the nature of physical copresence in music-making: how the available communicative cues are likely to affect the nature of visually mediated rehearsal and performance. Pilot data for a project carried out at the New School for Social Research demonstrate some important factors relevant to designing remote spaces for musical collaboration, and suggest that virtual environments for musical collaboration could actually enhance the feeling of being together that creative musical expression requires."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2006-09-07"/>

    <meta name="prism.volume" content="10"/>

    <meta name="prism.number" content="2"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="85"/>

    <meta name="prism.endingPage" content="94"/>

    <meta name="prism.copyright" content="2006 Springer-Verlag London Limited"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-006-0049-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-006-0049-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-006-0049-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-006-0049-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer-Verlag"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Virtual environments for creative work in collaborative music-making"/>

    <meta name="citation_volume" content="10"/>

    <meta name="citation_issue" content="2"/>

    <meta name="citation_publication_date" content="2006/10"/>

    <meta name="citation_online_date" content="2006/09/07"/>

    <meta name="citation_firstpage" content="85"/>

    <meta name="citation_lastpage" content="94"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-006-0049-z"/>

    <meta name="DOI" content="10.1007/s10055-006-0049-z"/>

    <meta name="citation_doi" content="10.1007/s10055-006-0049-z"/>

    <meta name="description" content="Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. "/>

    <meta name="dc.creator" content="Michael F. Schober"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Leonardo Music J; citation_title=Displaced soundscapes: a survey of network systems for music and sonic art creation; citation_author=A Barbosa; citation_volume=1; citation_issue=13; citation_publication_date=2003; citation_pages=53-59; citation_doi=10.1162/096112104322750791; citation_id=CR1"/>

    <meta name="citation_reference" content="Blank M, Davidson J (2006) &#8220;Nod when you&#8217;re ready&#8221;: an investigation into the gestures used by co-performers during rehearsal and performance. Paper presented at the Second International Conference on Music and Gesture, Manchester"/>

    <meta name="citation_reference" content="citation_title=Social influence within immersive virtual environments; citation_inbook_title=The social life of avatars: presence and interaction in shared virtual environments; citation_publication_date=2002; citation_pages=127-145; citation_id=CR3; citation_author=J Blascovich; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_title=Using language; citation_publication_date=1996; citation_id=CR4; citation_author=HH Clark; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_title=Flow: the psychology of happiness; citation_publication_date=1992; citation_id=CR5; citation_author=M Csikszentmihalyi; citation_publisher=Random House Limited"/>

    <meta name="citation_reference" content="citation_title=Optimal experience: psychological studies of flow in consciousness; citation_publication_date=1988; citation_id=CR6; citation_author=M Csikszentmihalyi; citation_author=I Csikszentmihalyi; citation_publisher=Cambridge University Press"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Commun Magazine; citation_title=Network-centric music performance: practice and experiments; citation_author=X Gu, M Dick, Z Kurtisi, U Noyer, L Wolf; citation_volume=43; citation_issue=6; citation_publication_date=2005; citation_pages=86-93; citation_doi=10.1109/MCOM.2005.1452835; citation_id=CR7"/>

    <meta name="citation_reference" content="Himberg T (2006) The role of visual and auditory communication channels in musical interaction. Paper presented at the Second International Conference on Music and Gesture, Manchester"/>

    <meta name="citation_reference" content="Jung B, Hwang J, Kim GJ, Lee E, Kim H (2000) Incorporating copresence in distributed virtual music environment. In: Proceedings of ACM symposium on virtual reality systems and technology (Seoul, Korea, October 22&#8211;25, 2000), VRST &#8216;00. ACM Press, New York, pp 206&#8211;211"/>

    <meta name="citation_reference" content="Konstantas D, Orlarey Y, Gibbs S, Carbonel O (1997) Distributed musical rehearsal. In: Proceedings of the international computer music conference (ICMC 97). The International Computer Music Association, San Francisco, pp 279&#8211;282"/>

    <meta name="citation_reference" content="Konstantas D, Gibbs S, Orlarey Y, Carbonel O (1998) Design and implementation of an ATM based distributed musical rehearsal studio. In: Proceedings of ECMAST&#8217;98, 3rd European conference on multimedia applications, services and techniques, Germany"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE MultiMedia; citation_title=The distributed musical rehearsal environment; citation_author=D Konstantas, Y Orlarey, O Carbonel, S Gibbs; citation_volume=6; citation_issue=3; citation_publication_date=1999; citation_pages=54-64; citation_doi=10.1109/93.790611; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Soc Res; citation_title=Making music together: a study in social relationship; citation_author=A Sch&#252;tz; citation_volume=18; citation_publication_date=1951; citation_pages=76-97; citation_id=CR13"/>

    <meta name="citation_reference" content="Slater M, Howell J, Steed A, Pertaub D, Garau M (2000) Acting in virtual reality. In: Churchill E, Reddy M (eds) Proceedings of the third international conference on collaborative virtual environments (San Francisco, CA). CVE &#8217;00, ACM Press, New York, pp 103&#8211;110"/>

    <meta name="citation_author" content="Michael F. Schober"/>

    <meta name="citation_author_email" content="schober@newschool.edu"/>

    <meta name="citation_author_institution" content="Department of Psychology, F330, New School for Social Research, New York, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-006-0049-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2006/10/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-006-0049-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Virtual environments for creative work in collaborative music-making"/>
        <meta property="og:description" content="Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. The development of virtual spaces for collaboration necessitates more clearly specified theorizing about the nature of physical copresence in music-making: how the available communicative cues are likely to affect the nature of visually mediated rehearsal and performance. Pilot data for a project carried out at the New School for Social Research demonstrate some important factors relevant to designing remote spaces for musical collaboration, and suggest that virtual environments for musical collaboration could actually enhance the feeling of being together that creative musical expression requires."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Virtual environments for creative work in collaborative music-making | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-006-0049-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual Environment, Virtual Space, Musical Score, Collaborative Performance, Visual Mediation","kwrd":["Virtual_Environment","Virtual_Space","Musical_Score","Collaborative_Performance","Visual_Mediation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-006-0049-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-006-0049-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=49;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-006-0049-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Virtual environments for creative work in collaborative music-making
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0049-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0049-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2006-09-07" itemprop="datePublished">07 September 2006</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Virtual environments for creative work in collaborative music-making</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Michael_F_-Schober" data-author-popup="auth-Michael_F_-Schober" data-corresp-id="c1">Michael F. Schober<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="New School for Social Research" /><meta itemprop="address" content="grid.264933.9, 0000000405239547, Department of Psychology, F330, New School for Social Research, 65 Fifth Avenue, New York, NY, 10003, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 10</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">85</span>–<span itemprop="pageEnd">94</span>(<span data-test="article-publication-year">2006</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">296 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-006-0049-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Virtual environments are beginning to allow musicians to perform collaboratively in real time at a distance, coordinating on timing and conceptualization. The development of virtual spaces for collaboration necessitates more clearly specified theorizing about the nature of physical copresence in music-making: how the available communicative cues are likely to affect the nature of visually mediated rehearsal and performance. Pilot data for a project carried out at the New School for Social Research demonstrate some important factors relevant to designing remote spaces for musical collaboration, and suggest that virtual environments for musical collaboration could actually enhance the feeling of being together that creative musical expression requires.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Music-making is a quintessentially creative activity, both in rehearsal and in performance. In rehearsal, the performer creates an interpretation of a piece, improvising upon some starting point—in Western classical and popular music, from a musical score (within a range of acceptable variability), and in jazz, from an underlying harmonic structure. In performance, creativity manifests itself in how the performance moves and breathes in that particular setting and moment. Collaborative music-making, where two or more performers rehearse and perform together, involves the same kinds of creativity in rehearsal and performance, but adds a large set of additional complexities. In rehearsal, collaborative musicians engage in a creative process of coming up with a joint interpretation or set of potential improvisations; in performance, they initiate and react to their partner’s variations in pacing, flow, and mood, co-constructing the particular performance dynamically. This is true even when musical partners are following a score. No two performances are ever alike, and it is creative deviation that makes a performance sing.</p><p>Musical collaborators’ feelings of copresence—of breathing together, of being able to anticipate each other’s moves, of feeling both independent and like one being—are essential for this creative process to occur. And yet in ordinary musical settings there can be multiple impediments to this copresence. Physical constraints can get in the way; if partners cannot see each other, because of their placement in an ensemble or because their instruments are blocking the view, feelings of copresence can be diminished. Partners’ level of knowledge about what the other is experiencing, or attentional limitations on their own performance (e.g., needing to fixate on a score), can make it hard or impossible to coordinate creatively.</p><p>Virtual reality allows us to imagine new forms of copresence that have the potential to enhance creativity in rehearsal and in performance. Freed from the constraints of the physical world, one could imagine being surrounded by one’s cellist partner’s bowing, seeing a visualization of the trumpet soloist’s improvisation, or feeling one’s singer partner’s breathing in a way that is physically impossible in real world settings. Enhanced access to cues could be useful both in rehearsal and performance.</p><p>At present, this sort of technology has not been fully developed. Broadband technology is only beginning to allow sufficient bandwidth to allow fully two-way communication with imperceptible lag (see, e.g., Barbosa <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Barbosa A (2003) Displaced soundscapes: a survey of network systems for music and sonic art creation. Leonardo Music J 1(13):53–59" href="/article/10.1007/s10055-006-0049-z#ref-CR1" id="ref-link-section-d55768e296">2003</a>; Gu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Gu X, Dick M, Kurtisi Z, Noyer U, Wolf L (2005) Network-centric music performance: practice and experiments. IEEE Commun Magazine 43(6):86–93" href="/article/10.1007/s10055-006-0049-z#ref-CR7" id="ref-link-section-d55768e299">2005</a>; Jung et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Jung B, Hwang J, Kim GJ, Lee E, Kim H (2000) Incorporating copresence in distributed virtual music environment. In: Proceedings of ACM symposium on virtual reality systems and technology (Seoul, Korea, October 22–25, 2000), VRST ‘00. ACM Press, New York, pp 206–211" href="/article/10.1007/s10055-006-0049-z#ref-CR9" id="ref-link-section-d55768e302">2000</a>; Konstantas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Konstantas D, Gibbs S, Orlarey Y, Carbonel O (1998) Design and implementation of an ATM based distributed musical rehearsal studio. In: Proceedings of ECMAST’98, 3rd European conference on multimedia applications, services and techniques, Germany" href="/article/10.1007/s10055-006-0049-z#ref-CR11" id="ref-link-section-d55768e305">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Konstantas D, Orlarey Y, Carbonel O, Gibbs S (1999) The distributed musical rehearsal environment. IEEE MultiMedia 6(3):54–64" href="/article/10.1007/s10055-006-0049-z#ref-CR12" id="ref-link-section-d55768e308">1999</a>, for discussion of technical specifications for remote musical coordination). Beyond this, there is a vast amount of additional knowledge needed to understand what kinds of visualizations or sensory representations will actually enhance creative rehearsal and performance, and for which kinds of performers and musical styles. Systematic research on the visual and auditory cues that musicians use to coordinate in rehearsal and performance is underway (e.g., Blank and Davidson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Blank M, Davidson J (2006) “Nod when you’re ready”: an investigation into the gestures used by co-performers during rehearsal and performance. Paper presented at the Second International Conference on Music and Gesture, Manchester" href="/article/10.1007/s10055-006-0049-z#ref-CR2" id="ref-link-section-d55768e312">2006</a>; Himberg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Himberg T (2006) The role of visual and auditory communication channels in musical interaction. Paper presented at the Second International Conference on Music and Gesture, Manchester" href="/article/10.1007/s10055-006-0049-z#ref-CR8" id="ref-link-section-d55768e315">2006</a>), but at present far too little is still known. We have nothing like a systematic database or taxonomy of different kinds of musical coordination which would allow us to precisely specify how, for example, the cues that orchestra members use in coordinating with conductors and each other differ from the cues in rock bands, jazz ensembles, or string quartets.</p><p>This paper describes the beginnings of a line of research that will form the building blocks for understanding virtual environments for creative work in musical collaboration, allowing us to better understand which aspects of physical copresence are worth enhancing in a virtual environment and which are not. The work starts by looking at low-level forms of virtuality that are technically feasible today: remote video and remote audio connections.</p><p>Obviously these are only the tip of the iceberg as far as what virtual environments will allow, but understanding what happens here is an essential first step. The virtual space that is created even with remote video is—at least in the minds of the musicians using it—already at a substantial remove from the true physical copresence that musicians expect. Understanding what happens when visual cues are presented to musicians in a non-physically-copresent way is the first step towards far more elaborate virtual musical spaces.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Remote copresence in chamber music</h2><div class="c-article-section__content" id="Sec2-content"><p>In recent years musicians have created rehearsals and performances where two or more participants are separated in space and time, sometimes even across continents (e.g., Konstantas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Konstantas D, Orlarey Y, Gibbs S, Carbonel O (1997) Distributed musical rehearsal. In: Proceedings of the international computer music conference (ICMC 97). The International Computer Music Association, San Francisco, pp 279–282" href="/article/10.1007/s10055-006-0049-z#ref-CR10" id="ref-link-section-d55768e331">1997</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Konstantas D, Gibbs S, Orlarey Y, Carbonel O (1998) Design and implementation of an ATM based distributed musical rehearsal studio. In: Proceedings of ECMAST’98, 3rd European conference on multimedia applications, services and techniques, Germany" href="/article/10.1007/s10055-006-0049-z#ref-CR11" id="ref-link-section-d55768e334">1998</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Konstantas D, Orlarey Y, Carbonel O, Gibbs S (1999) The distributed musical rehearsal environment. IEEE MultiMedia 6(3):54–64" href="/article/10.1007/s10055-006-0049-z#ref-CR12" id="ref-link-section-d55768e337">1999</a>; Jung et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Jung B, Hwang J, Kim GJ, Lee E, Kim H (2000) Incorporating copresence in distributed virtual music environment. In: Proceedings of ACM symposium on virtual reality systems and technology (Seoul, Korea, October 22–25, 2000), VRST ‘00. ACM Press, New York, pp 206–211" href="/article/10.1007/s10055-006-0049-z#ref-CR9" id="ref-link-section-d55768e340">2000</a>; see Barbosa <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Barbosa A (2003) Displaced soundscapes: a survey of network systems for music and sonic art creation. Leonardo Music J 1(13):53–59" href="/article/10.1007/s10055-006-0049-z#ref-CR1" id="ref-link-section-d55768e343">2003</a>, for discussion of system requirements for different kinds of collaborative musical tasks). Thus far bandwidth limitations remain such that the performances are not fully two-way in the way that live performances with physically copresent musicians are; typically a second party must follow a first party’s lead because of notable signal delay. But such limitations are on their way to being reduced or eliminated so that musicians in different physical environments will be able to collaborate with no perceptible time delay. Consider, for example, the October 2002 Interactive Cross-Continental Jazz performance between Montreal, Canada and Palo Alto, CA, where the delay was at times reduced to 50 ms; this reportedly felt to the musicians “almost like being on the same stage” (see <a href="http://www.ultravideo.mcgill.edu/overview/">http://www.ultravideo.mcgill.edu/overview/</a>).</p><p>The stage is thus set for the beginnings of far more elaborate virtual environments for collaborative musical performance. The sine qua non is that any such environment must allow signals to be passed so quickly that true two-way reactivity can occur. In order to build such environments, however, we will need to theorize the nature of musical collaboration far more clearly.</p><h3 class="c-article__sub-heading" id="Sec3">Levels of coordination</h3><p>A key part of traditional chamber and jazz collaborative performance is that the performers must coordinate the timing and expressivity of their performances at the millisecond level. Simply coordinating competently enough that there is nothing obviously wrong—no mistimed attacks, no notes held longer by one party than the other, no obvious differences of opinion about what the right tempo should be or how exactly a phrase should slow down or speed up—is no mean feat, as skilled soloists who attempt to collaborate for the first time quickly discover. Musicians following a score have a script to follow that dictates a good deal about what they should do, sometimes including precise metronomic tempo markings and expressive timing cues. Musicians improvising without a score, say on the basis of a jazz chord progression, have a less detailed and more implicit “script” to follow. But with or without a score, how exactly the script gets enacted by particular performers varies across performers, and even within a performer on different occasions. This is why musicians rehearse a piece again and again before deeming the performance ready for the world. Coordination isn’t an all or none phenomenon—performers can be more or less tightly coordinated on timing, dynamics, expressive features, and conceptualizations.</p><p>Beyond this, collaborative musicians strive to achieve a fuller sense of copresence or mind-meld, such that the coordination and give-and-take feels effortless and organic, and the pair or group operates as one entity. To use Shütz’s (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1951" title="Schütz A (1951) Making music together: a study in social relationship. Soc Res 18:76–97" href="/article/10.1007/s10055-006-0049-z#ref-CR13" id="ref-link-section-d55768e365">1951</a>) characterization, they strive to achieve a “mutual tuning-in relationship by which the ‘I’ and the ‘Thou’ are experienced by both participants as a ‘We’ in vivid presence.” Some performers are better at this than others, and some combinations of performers are better at this than others. Although the vocabulary for talking about this is impoverished, and not every musician feels comfortable discussing the issues, an almost mystical discourse has arisen among musicians who do talk about the shared mental space they work at creating. They talk of getting into the magic zone or getting into “chamber space,” and they share their peak experiences, stumbling for words, as if describing food or sex. The give-and-take of high-level collaborative performance, where different partners “drive” (take the initiative) at different points, has the quality of transcendent play.</p><p>Of course, such a high-level feeling of copresence is not unique to collaborative musicians. Participants in other joint actions—pairs of ice dancers, actors performing a scene, tennis doubles partners—have related notions of being “in the zone.” They also, in various ways, can distinguish between carrying out a joint activity competently enough that it works and entering a kind of shared space in which they achieve what feels like real mind-meld.</p><p>Little in current scholarly theorizing quite gets at distinguishing joint action at this very high level from ordinary joint action. The work on “flow” (Csikszentmihalyi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Csikszentmihalyi M (1992) Flow: the psychology of happiness. Random House Limited, London" href="/article/10.1007/s10055-006-0049-z#ref-CR5" id="ref-link-section-d55768e373">1992</a>; Csikszentmihalyi and Csikszentmihalyi <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Csikszentmihalyi M, Csikszentmihalyi I (1988) Optimal experience: psychological studies of flow in consciousness. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-006-0049-z#ref-CR6" id="ref-link-section-d55768e376">1988</a>) focuses on how individuals feel a sense of connectedness with and transcendence in the tasks they engage in, but does not focus on pairs’ coordination. Theorizing on joint action and copresence (e.g., Blascovich <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Blascovich J (2002) Social influence within immersive virtual environments. In: Schroeder R (ed) The social life of avatars: presence and interaction in shared virtual environments. Springer, Berlin Heidelberg New York, pp 127–145" href="/article/10.1007/s10055-006-0049-z#ref-CR3" id="ref-link-section-d55768e379">2002</a>; Clark <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Clark HH (1996) Using language. Cambridge University Press, Cambridge" href="/article/10.1007/s10055-006-0049-z#ref-CR4" id="ref-link-section-d55768e382">1996</a>) does not distinguish between competent and masterful coordination. In any case, far more needs to be understood from a scientific perspective about the basics of musical coordination, let alone the most transcendent levels.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Cues in collaborative musical performance</h2><div class="c-article-section__content" id="Sec4-content"><p>Predicting how musicians will coordinate in virtual environments, and what sorts of spaces will afford what sorts of coordination, requires greater understanding of the cues that musicians pass back and forth when they are physically copresent. Although the music that performers create is acoustic, visual cues are also available, and these cues have been proposed to be all-important (Schütz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1951" title="Schütz A (1951) Making music together: a study in social relationship. Soc Res 18:76–97" href="/article/10.1007/s10055-006-0049-z#ref-CR13" id="ref-link-section-d55768e394">1951</a>):</p><blockquote class="c-blockquote"><div class="c-blockquote__body">
                  <p>“...making music together occurs in a true face-to-face relationship—inasmuch as the participants are sharing not only a section of time but also a sector of space. The other’s facial expressions, his gestures in handling his instrument, in short all the activities of performing, gear into the outer world and can be grasped by the partner in immediacy. Even if performed without communicative intent, these activities are interpreted by him as indications of what the other is going to do and therefore as suggestions or even commands for his own behavior. Any chamber musician knows how disturbing an arrangement that prevents the coperformers from seeing each other can be” (p. 91).</p>
                </div></blockquote><p>Collaborative musicians are indeed trained to watch for each other’s signals—to breathe together, literally and metaphorically. But just how crucial are visual cues for competent or masterful performance? Just how crucial is actual physical copresence, as opposed to merely being able to see the other’s visual cues (for example, via remote video)? Just how harmed will coperformers be by not being able to see each other at all?</p><p>From musicians’ perspective, visual cues ought to be particularly useful only in some circumstances. One could imagine that audio cues could partially or fully substitute—for example, a singer’s inbreath may be just as audible as visible. Visual cues won’t be particularly useful if the musicians are unfamiliar enough with the music that their attention is fully engrossed in reading a score, keeping track of where they are in the chord progression, or mastering their (solo) technique. Visual cues might also not be particularly important for a collaborating pair who have performed together so often that they can predict each other’s stylistic moves from prior knowledge, or who have rehearsed a particular piece so well that the precise timing is prestored.</p><p>The section that follows lays out cues that are available to musicians when they can see each other in the same physical space, and explores how these cues will vary in video-mediated and audio-mediated settings.</p><h3 class="c-article__sub-heading" id="Sec5">Cues for physically copresent musicians</h3><p>As collaborative cues will vary for particular settings and styles of music, let us consider as an initial test case a classical chamber setting with two collaborating musicians, a pianist and a singer. (This can form the basis for comparisons with other settings and styles. It also has the useful feature that performance accuracy can be assessed in relation to the musical score). In the typical physical setup, the singer stands in the bend of the grand piano facing outwards towards an audience. In rehearsal, the singer will often have a music stand with a score on it; in performance, the singer may perform from memory. Collaborative pianists most often perform with a score.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">Auditory cues</h4><p>The most obvious auditory cues available to physically copresent performers are the sounds each instrument makes. Beyond the obvious cues of producing the correct pitches in the correct rhythm, each partner presents additional audio signals for coordination based both on the musical score and their interpretation: rhythmic and dynamic impulses that signal how the other partner should join in. How a singer or pianist creatively “moves” a phrase can clearly convey their performance intentions and conceptualization.</p><p>Other audio cues include the words that performers use in rehearsal to help them coordinate. For example, performers can use language to decide where in the piece to begin and to discuss tempo, dynamics, or other expressive features; they can use language to discuss their creative and interpretive visions of the piece. Other potential auditory cues include sounds of breath intake and sounds of the pages of scores turning. The quality of musical sound being produced by the partner can also be informative; for example, a pianist might hear that a singer is running out of breath and thus might end a sequence a bit more quickly.</p><p>Note that the acoustic quality of the audio available to each performer is rather different. Each performer hears the other’s instrument from an external (audience) rather than internal (performer’s) perspective, with the attending directionality and volume.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Visual cues</h4><h5 class="c-article__sub-heading" id="Sec8">Jointly attended artifacts</h5><p>During rehearsal, both performers will usually have a score for the music they are working on, and so there will be periods of time that both performers are looking at the musical score rather than at each other. The scores may or may not be identical; both parties may or may not see exactly what the other sees, and page turns may or may not be in the same places. (With other pairings, it is more frequent that solo instruments have just their own part, and the pianist may have a score that shows both parts).</p><p>The score forms a point of joint attentional focus—even if it is not the same physical object—that is relatively unaffected by the physical setting or which aspects of the partner are visible. Nonetheless, attention to the score can differentially affect how singers and pianist see each other, because singers and pianists do not have identical views of each other. The typical physical arrangement for a pianist and singer allows the pianist to see the singer peripherally while looking forward at the score, or to focus on the singer with a simple glance upward. For singers looking at their score, the pianist is usually noticeably off to the side, and so looking at the pianist peripherally is less straightforward.</p><h5 class="c-article__sub-heading" id="Sec9">Visual cues from the partner</h5><p>The precise visual cues vary depending on the precise physical arrangement of the performers. Most frequently there will be a number of gaze cues, some of which will be intended as communicative and some of which are simply symptoms of the partner’s processing. Partners can tend to see if the other is looking at the score, at their hands, at the audience, or at the other partner; occasionally they may look at each other simultaneously.</p><p>Body movements (e.g., “conducting” arm movements, large breath intakes), nods, and looks can be informative about one or the other partner’s readiness to begin and their conception of the piece. When both parties need to begin music simultaneously after silence, in-breaths that signal the start of a phrase may be essential for the attacks to be coordinated. Facial and bodily affective expression may also be informative about a partner’s performing intentions (fierceness, calm, joy, sorrow expressed for audience effect; a bodily lean that conveys a rhythmic or phrasal impulse), and other kinds of looks might be informative about a partner’s having trouble due to nerves or fatigue.</p><h3 class="c-article__sub-heading" id="Sec10">Visual cues for virtual performers</h3><p>Would all of the cues available to physically copresent performers be sufficiently available if the performers are video-mediated, or otherwise visually represented in a virtual space? Are there physiological cues that depend on being in the same space? This depends on just how much information is cut off through visual mediation. If the visual representations degrade the images of each other that performers see (e.g., shrinking them too far, reducing clarity, leaving some bodily or facial movements invisible), this could sufficiently reduce the sense of shared presence and thus hinder creative musical coordination. If the visual representation actually omits the partner cues that are potentially essential for coordination—shared gaze, physical movement, facial expression—creative collaboration could be harmed. Of course, this only matters if what is degraded cannot be compensated for with other (auditory) cues.</p><p>If visual mediation simply <i>feels</i> unnatural, this could also distract enough from musicianship that musical coordination will be disturbed. Presumably, experience in virtual spaces of this sort could reduce the sense of unnaturalness.</p><h3 class="c-article__sub-heading" id="Sec11">Cues for audio-mediated performers</h3><p>Performers with no access to visual cues of any sort must rely on audio cues instead. The extent to which this degrades performance should depend on the extent to which audio cues can replace visual cues.</p><p>For example, having to rely only on audio cues might be especially disturbing at particular stages of rehearsal or performance. Experienced performers who can decenter enough from their solo performance to be able to benefit from visual cues might suffer more from having to rely only on audio cues; inexperienced performers who are still focusing on the musical score might not mind so much. Also, independent of the technical (vocal or pianistic) difficulty for each partner, some pieces are harder to coordinate than others, with more complicated timing, attacks and tempos. Performances of such pieces should suffer more if only auditory cues are available. Finally, pairs of performers may become so used to each other’s general style of performance or so well rehearsed on a specific piece that the visual cues are no longer paramount; they may be able to rely solely on sensitive listening to audio cues.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Pilot study</h2><div class="c-article-section__content" id="Sec12-content"><p>A project at the New School for Social Research has been starting to explore musicians’ sense of copresence in simulated remote spaces of the future, where bandwidth and signal delay are not at issue. The methods of the project could also be used to explore the effects of various signal delays on the success of collaborative performance.</p><p>In the project, two relatively soundproof rooms are connected via video and audio cabling. A camera and microphones in each space pick up video and audio signals and project them onto a large monitor and high quality speakers in the other room. In the first iteration of the project, each room contained a grand piano. In the pianist’s room, the camera and monitor were placed in the crook of the piano exactly where a singer would ordinarily stand, with the monitor placed so that the pianist seated at the keyboard could see it. In the singer’s room, the camera and monitor were placed where the pianist would ordinarily sit, so that the singer standing in the crook of the piano could see just those parts of the pianist that she would ordinarily see. Through numerous iterative attempts it became clear that the camera and monitor needed to be closely aligned, so that when one party looked directly at the monitor the other party would experience their partner looking directly at them; even very small misalignments proved extremely distracting (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0049-z#Fig1">1</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Singer’s and pianist’s video-mediated views of each other (minus time stamp)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
              <p>With this setup, we could now compare the experience of rehearsing and performing in the same physical space (full physical copresence) with video-mediated copresence (video-and-audio copresence without physical copresence). We could also compare this with audio-only copresence by keeping the same audio setup (microphones and speakers) but removing the cameras and monitors. Again, this is many steps removed from the potential kinds of virtual environments we intend to explore; it is simply a first step on the path toward more elaborate musical CVEs.</p><p>This setup allows for a number of different measures of the sense of presence. First, one can ask the performers to report about their experience. More intriguingly, one can assess the success of their collaboration on several dimensions. On the basis of the musical score, one can measure the accuracy of a pair’s timing, at the millisecond level, with sound editing software: the extent to which notes that are supposed to start and end simultaneously actually do start and end simultaneously. One can assess the accuracy of the pitches produced. One can also solicit ratings (from the performers, from other trained musicians, and from a naïve audience) of the quality of the pair’s conceptual coordination: the extent to which they conceived of phrases the same way.</p><p>Here I report on pilot data from two pairs of chamber performers who were strangers to each other before the test sessions. Both pairs were asked to rehearse and perform an unfamiliar art song, John Duke’s (1967) “The White Dress.” The piece was chosen not only because it was likely to be unfamiliar, but also because the vocal and piano lines are not of great technical difficulty. Nonetheless, the piece has its tricky parts; the meter varies between 3, 4, and 5 beats per measure, and the intervallic and harmonic changes are not necessarily predictable on an initial sight reading.</p><p>Both pairs were first given 10 min of solo rehearsal time in separate rooms (each room had a piano in it). They then had 10 min of rehearsal time in the same room (full physical copresence). Then they had a sequence of three “performances” (full run-throughs of the piece): first in the same room (full physical copresence), then in separate rooms but video- and audio-mediated, then in separate rooms with only audio mediation. (Obviously in a full-scale study some pairs would rehearse under video- or audio-mediation, and the ordering of performances would be counterbalanced). All rehearsals and performances were videotaped for subsequent analysis.</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Results</h2><div class="c-article-section__content" id="Sec13-content"><p>The pilot results from the two pairs provide a set of intriguing observations about visual and auditory cues in rehearsal and performance. Overall, Pair A (Female singer, Male pianist) had a great deal of trouble sight reading the music and coordinating at all; Pair B (Male singer, Female pianist) started at a much higher level and were able to give passable performances by the end of the session.</p><h3 class="c-article__sub-heading" id="Sec14">Rehearsals</h3><p>During the joint rehearsal periods in the same room, both pairs ended up moving much closer to each other, with the singer standing facing the piano to the pianist’s right. Apparently neither pair felt sufficiently copresent when they were placed in the typical performance setup (singer in crook of piano).</p><p>Pair A’s rehearsal proceeded with great difficulty, with a great many starts and stops. The singer in particular had trouble finding the pitches and rhythms; the pianist sometimes forged ahead in hopes the singer would keep up, but this led to frequent breakdowns. At times the pianist stopped playing his part and simply played the singer’s pitches in order to help her along. Both partners fixated steadfastly on their own scores, with only very occasional direct glances at each other during moments of breakdown (never during music-making).</p><p>Close attention to one early breakdown sequence is informative about how the pianist and singer managed to coordinate on where they were and when they would start. Here is a verbal and action transcript of the relevant moments for the singer (S) and pianist (P):</p><div class="c-article-table-container"><div class="c-article-table-border c-table-scroll-wrapper"><div class="c-table-scroll-wrapper__content" data-component-scroll-wrapper=""><table class="data last-table"><tbody><tr><td class="u-text-left ">
                            20:39
                          </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            [Fails to enter at <i>I shall</i> and stops singing]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            “We’re changing – changing three and four” [explains that metrical confusion caused breakdown]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            P:
                          </td><td class="u-text-left ">
                            [Plays singer’s pitch]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            “Oh”
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            [steps back]
                          </td></tr><tr><td class="u-text-left ">
                            20:53
                          </td><td class="u-text-left ">
                            P:
                          </td><td class="u-text-left ">
                            Says something unintelligible [looking at S’s face 2 seconds]
                          </td></tr><tr><td class="u-text-left ">
                            20:55
                          </td><td class="u-text-left ">
                            S
                          </td><td class="u-text-left ">
                            “<i>motion</i>?” [proposes text/pitch as starting place]
                          </td></tr><tr><td class="u-text-left ">
                            20:58
                          </td><td class="u-text-left ">
                            S
                          </td><td class="u-text-left ">
                            [glances overtly at P’s score]
                          </td></tr><tr><td class="u-text-left ">
                            20:58
                          </td><td class="u-text-left ">
                            P:
                          </td><td class="u-text-left ">
                            “*Yeah*” [overlapping speech]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            “*Same-*” [overlapping speech]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            P:
                          </td><td class="u-text-left ">
                            “Oh yeah. The same” [points at his score to agree to proposed starting place]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            “<i>Motionless</i>?”
                          </td></tr><tr><td class="u-text-left ">
                            21:04
                          </td><td class="u-text-left ">
                            P: 
                          </td><td class="u-text-left ">
                            [begins playing]
                          </td></tr><tr><td class="u-text-left "> </td><td class="u-text-left ">
                            S:
                          </td><td class="u-text-left ">
                            [glances at P’s hand 3 times in 6 seconds, perhaps to verify where he is]
                          </td></tr></tbody></table></div></div></div><p>A few things are notable in this sequence. First, agreeing on where to start in again takes some negotiation; the singer’s proposal, although it turns out to be accepted, requires several turns of talk before the pianist starts again, and the singer must repeat “<i>Motionless</i>?” before she is convinced that they have agreed on where to start. Second, in addition to their verbal negotiation, both parties engage in visible behaviors as they coordinate on where to start: the singer glances at the pianist’s score, and the pianist points at his score as if to verify that they are in the same place. But neither of these visible behaviors is accompanied by the partner’s looking at them. Even if the partners can see each other’s visible cues peripherally, there is no evidence of any uptake by the partner. There is never any mutual gaze during this sequence; in fact, there is virtually no mutual gaze during the entire rehearsal (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0049-z#Fig2">2</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig2_HTML.jpg?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig2_HTML.jpg" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Pair A focus on their scores and do not look at each other</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>It is as if the singer and pianist both are making visible gestures <i>available</i> to the other, but they are not checking at all whether their partner is noticing their gestures. This suggests that technical physical copresence does not automatically lead to full visual (or psychological) copresence, and that visual cues can go unnoticed. It also suggests that Pair A would likely not take advantage of visual cues in a virtual visually mediated setting since they neglected to use them to coordinate in the face to face setting.</p><p>Pair B’s joint rehearsal is notable for other reasons. The pair made it through two full run-throughs of the song with far fewer stops and starts than Pair A. From the start, the singer took charge of the rehearsal, saying “Why don’t I just start singing and you follow me, right?” He also set the tempo through much of the rehearsal by conducting with the arm that was not holding his score. As in Pair A, the singer in Pair B moved closer to the pianist, just behind her and to her right. It is unclear how much of the singer’s conducting the pianist could see through her peripheral vision; evidently the singer become dissatisfied with how much the pianist could see, because by the end of the first run-through the singer had moved into the pianist’s field of vision and was conducting in her face (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0049-z#Fig3">3</a>).
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The singer in Pair B conducts in the pianist’s field of vision</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>By this time, the singer had abandoned his own score so that the pianist and singer could jointly focus on a single score (the pianist’s). This suggests just how important shared focus of attention—a jointly attended artifact—can be to the sense of copresence. Note that the videomediated condition, as set up in this experiment, would not allow this flexibility of positioning. To the extent that this pair depended on these sorts of visual cues, they should have more trouble in a virtual space that does not allow them.</p><p>Despite the singer’s visual intrusions into the pianist’s field of vision, the pianist remained focused on her score. The only times she ever looked at the singer were when they interrupted the music-making to talk. For example, the pianist looked directly at the singer when she proposed before the second run-through that the piece ought to be a bit slower than they had taken it the first time through. In the context of all the non-looking, this is quite notable; it makes sense that it should happen at a moment where the pianist presents a potentially confrontational opinion and asserts some control over the rehearsal situation.</p><p>The singer continued to conduct throughout the second run-through, most notably during a slowdown (ritardando) section, which both parties need to coordinate. The singer was dissatisfied, and even counted out loud during the ritardando to make clear how he would like it to go (note the redundancy of the visual conducting cues and the auditory counting cues; clearly one could substitute for the other if necessary). There was eventually explicit discussion about the ritardando at the end of the piece after the pair failed to coordinate on it; again, note that such explicit discussion does not depend on visual cues and could happen in a purely audio setup.</p><p>What is curious about the singer’s conducting is that he continues to do it even after he moves to a position behind the pianist where she certainly cannot see it. This suggests that he conducts in part for himself rather than simply to control the pianist; nonetheless, it is clear that sometimes the conducting is also intended to be communicative.</p><h3 class="c-article__sub-heading" id="Sec15">Performances</h3><p>Pair A’s performances are unimpressive; they are unable to get through the piece without stopping, and without the pianist’s abandoning his part to play the singer’s vocal line.</p><p>Pair B’s performances are good enough that they can be evaluated for both pitch accuracy and rhythmic accuracy. The metric used here was the percentage of measures (out of 102 for the pianist, 75 for the singer) that contained no errors of commission or omission.</p><p>As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0049-z#Fig4">4</a> shows, the singer in Pair B was far more accurate than the pianist in general; this is unsurprising as the pianist had far more pitches to manage. Over the course of the three performances, the pianist improved notably between the first (physically copresent) and second (videomediated). The very fact of this improvement despite the video mediation suggests that there are practice effects that are independent of the visual cues that are being passed. Given that the pianist never looked at the singer during the performances, it makes sense that the reduction and absence of visual cues would do little to affect her performance.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Percentage of measures with no pitch errors (out of 102 for pianist, 75 for singer)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>The findings for rhythmic errors are slightly more complicated. As Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-006-0049-z#Fig5">5</a> shows, video mediation did seem to bring a slight decrement in rhythmic accuracy compared to full physical copresence. As the video mediation took away the possibility of conducting (which continued during the physically copresent performance), the reduction in accuracy may have to do with the loss of the conducting cue. Nonetheless, with additional practice—and despite the loss of additional visual cues—rhythmic accuracy improved in the audio-mediated condition.
</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-006-0049-z/MediaObjects/10055_2006_49_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Percentage of measures with no major rhythmic errors (out of 102 for pianist, 75 for singer)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-006-0049-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                <p>Informal self-reports from the participants suggest that they did not find either video or audio mediation particularly troubling, and they were intrigued at the prospect of remote rehearsal and performance. They were quite aware of the coordination troubles they had had, but did not seem to feel that the remote settings affected the quality of their coordination one way or another.</p></div></div></section><section aria-labelledby="Sec16"><div class="c-article-section" id="Sec16-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec16">Discussion</h2><div class="c-article-section__content" id="Sec16-content"><p>Obviously, a large sample of performers in different genres of music would have to be tested in order to be able to make generalizable claims about the broader effects of even the simple kinds of visual and audio mediation studied here on collaborative music rehearsal and performance. (Such studies are in the works). And much more work would need to be done on different kinds of visual representation of musicians in more elaborate virtual environments to understand what kinds of information about one’s partner is particularly effective for facilitating creative collaboration. But these pilot data, as well as informal tests with experienced chamber musicians during the setup of the rooms, allow some plausible conclusions.</p><p>First, the video-and-audio situation can instantly feel remarkably like the fully copresent situation. At one level this should not be surprising; many of the same sets of visual and auditory cues—intakes of breath, body leans, eyebrow raises—are, after all, available. But one might expect some delay in feeling comfortable with the remote situation, and this is not what our participants reported. This is encouraging news for imagining more elaborate virtual environments for coordinated music-making.</p><p>Nonetheless, there may be decrements in performance—as in Pair B’s rhythmic coordination—resulting from the lack of real physical copresence that are independent of the performers’ level of comfort. This should vary depending on how much coperformers rely on visual cues of each other (as opposed to focusing on a score) as they rehearse and perform. But note that even Pair B’s decrements were not large, and on another dimension of coordination (pitch accuracy) Pair B <i>improved</i> across performances (a practice effect) despite visual mediation. Of course, we cannot know if they would have improved more if they had been fully copresent; that is what a larger study could show. But given how rarely the pianist in this pair looked at the singer at all, any effects of mediation are likely to be small.</p><p>All in all these findings suggest that the sense of remote copresence is established through measurable sensory cues (here, visual and audio), and that manipulating these cues can affect how copresence is achieved. Disentangling all the factors that make a difference—not only for competent but also for masterful coordination—will require a good deal of further research. One might imagine, for example, that the effects of mediation on either rehearsal or performance can be affected by</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Individual technical mastery and skill level—performers who are less attentionally tied to their scores should have more resources available for making use of visual cues, and thus be more likely to be affected by their absence. On the other hand, highly skilled performers may be more able to substitute audio for video cues without decrement.</p>
                  </li>
                  <li>
                    <p>A pair’s familiarity with each other and experience performing a particular piece could make visual cues less necessary.</p>
                  </li>
                  <li>
                    <p>Individual personality differences could make visual cues more or less important to a musician, which would thus affect who will be influenced most by the loss of visual cues. As Blascovich (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Blascovich J (2002) Social influence within immersive virtual environments. In: Schroeder R (ed) The social life of avatars: presence and interaction in shared virtual environments. Springer, Berlin Heidelberg New York, pp 127–145" href="/article/10.1007/s10055-006-0049-z#ref-CR3" id="ref-link-section-d55768e944">2002</a>) proposed, there are individual differences in susceptibility to social presence effects that should correspond with differences in levels of empathy and suggestibility.</p>
                  </li>
                </ul><p>In the longer run, it will be important to understand how findings from this kind of study will scale up to far more elaborate virtual environments. Do representations of one’s musical partner need to look human? Would it be more useful—for training purposes in rehearsal, or even in performance—to see a representation of one’s partner’s breathing, heart rate, or galvanic skin response than to see the partner’s face? to see a visual representation of one’s partner’s emotions that does not rely on the partner’s face? to see information about one’s partner amidst the musical score that one is reading rather than having to switch attentional focus back and forth between one’s score and one’s partner’s cues? To what extent would it be helpful for each performer to control or manipulate how their avatars are displayed for their partners (as suggested in the remote acting rehearsals in Slater et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Slater M, Howell J, Steed A, Pertaub D, Garau M (2000) Acting in virtual reality. In: Churchill E, Reddy M (eds) Proceedings of the third international conference on collaborative virtual environments (San Francisco, CA). CVE ’00, ACM Press, New York, pp 103–110" href="/article/10.1007/s10055-006-0049-z#ref-CR14" id="ref-link-section-d55768e950">2000</a>)? Might it be helpful for musical partners to use VR technology to augment reality even in physically copresent situations—say, representing each other’s breathing? Which real-time VR representations of aspects of musical partner’s performance can already be implemented with today’s networks, given that they require less bandwidth than full-scale video networking?</p><p>The set of factors that are likely to be relevant for answering these kinds of questions is large. I propose that simulated experimental setups that do not require enormous amounts of development effort to get off the ground, like the one used in this pilot study, will allow careful manipulations that can help provide guidelines for the development of interfaces for virtual musical environments that promote both competent and masterful creative coordination at a distance. At the same time, studies like these have the added benefit that they can help clarify our understanding of the nature of musical coordination more generally—how pairs of people come to feel that “we-ness” that makes their performances take flight.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Barbosa, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Barbosa A (2003) Displaced soundscapes: a survey of network systems for music and sonic art creation. Leonardo" /><p class="c-article-references__text" id="ref-CR1">Barbosa A (2003) Displaced soundscapes: a survey of network systems for music and sonic art creation. Leonardo Music J 1(13):53–59</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F096112104322750791" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Displaced%20soundscapes%3A%20a%20survey%20of%20network%20systems%20for%20music%20and%20sonic%20art%20creation&amp;journal=Leonardo%20Music%20J&amp;volume=1&amp;issue=13&amp;pages=53-59&amp;publication_year=2003&amp;author=Barbosa%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Blank M, Davidson J (2006) “Nod when you’re ready”: an investigation into the gestures used by co-performers d" /><p class="c-article-references__text" id="ref-CR2">Blank M, Davidson J (2006) “Nod when you’re ready”: an investigation into the gestures used by co-performers during rehearsal and performance. Paper presented at the Second International Conference on Music and Gesture, Manchester</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Blascovich, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Blascovich J (2002) Social influence within immersive virtual environments. In: Schroeder R (ed) The social li" /><p class="c-article-references__text" id="ref-CR3">Blascovich J (2002) Social influence within immersive virtual environments. In: Schroeder R (ed) The social life of avatars: presence and interaction in shared virtual environments. Springer, Berlin Heidelberg New York, pp 127–145</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20social%20life%20of%20avatars%3A%20presence%20and%20interaction%20in%20shared%20virtual%20environments&amp;pages=127-145&amp;publication_year=2002&amp;author=Blascovich%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="HH. Clark, " /><meta itemprop="datePublished" content="1996" /><meta itemprop="headline" content="Clark HH (1996) Using language. Cambridge University Press, Cambridge" /><p class="c-article-references__text" id="ref-CR4">Clark HH (1996) Using language. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Using%20language&amp;publication_year=1996&amp;author=Clark%2CHH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Csikszentmihalyi, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Csikszentmihalyi M (1992) Flow: the psychology of happiness. Random House Limited, London" /><p class="c-article-references__text" id="ref-CR5">Csikszentmihalyi M (1992) Flow: the psychology of happiness. Random House Limited, London</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Flow%3A%20the%20psychology%20of%20happiness&amp;publication_year=1992&amp;author=Csikszentmihalyi%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Csikszentmihalyi, I. Csikszentmihalyi, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Csikszentmihalyi M, Csikszentmihalyi I (1988) Optimal experience: psychological studies of flow in consciousne" /><p class="c-article-references__text" id="ref-CR6">Csikszentmihalyi M, Csikszentmihalyi I (1988) Optimal experience: psychological studies of flow in consciousness. Cambridge University Press, Cambridge</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Optimal%20experience%3A%20psychological%20studies%20of%20flow%20in%20consciousness&amp;publication_year=1988&amp;author=Csikszentmihalyi%2CM&amp;author=Csikszentmihalyi%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="X. Gu, M. Dick, Z. Kurtisi, U. Noyer, L. Wolf, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Gu X, Dick M, Kurtisi Z, Noyer U, Wolf L (2005) Network-centric music performance: practice and experiments. I" /><p class="c-article-references__text" id="ref-CR7">Gu X, Dick M, Kurtisi Z, Noyer U, Wolf L (2005) Network-centric music performance: practice and experiments. IEEE Commun Magazine 43(6):86–93</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FMCOM.2005.1452835" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Network-centric%20music%20performance%3A%20practice%20and%20experiments&amp;journal=IEEE%20Commun%20Magazine&amp;volume=43&amp;issue=6&amp;pages=86-93&amp;publication_year=2005&amp;author=Gu%2CX&amp;author=Dick%2CM&amp;author=Kurtisi%2CZ&amp;author=Noyer%2CU&amp;author=Wolf%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Himberg T (2006) The role of visual and auditory communication channels in musical interaction. Paper presente" /><p class="c-article-references__text" id="ref-CR8">Himberg T (2006) The role of visual and auditory communication channels in musical interaction. Paper presented at the Second International Conference on Music and Gesture, Manchester</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jung B, Hwang J, Kim GJ, Lee E, Kim H (2000) Incorporating copresence in distributed virtual music environment" /><p class="c-article-references__text" id="ref-CR9">Jung B, Hwang J, Kim GJ, Lee E, Kim H (2000) Incorporating copresence in distributed virtual music environment. In: Proceedings of ACM symposium on virtual reality systems and technology (Seoul, Korea, October 22–25, 2000), VRST ‘00. ACM Press, New York, pp 206–211</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Konstantas D, Orlarey Y, Gibbs S, Carbonel O (1997) Distributed musical rehearsal. In: Proceedings of the inte" /><p class="c-article-references__text" id="ref-CR10">Konstantas D, Orlarey Y, Gibbs S, Carbonel O (1997) Distributed musical rehearsal. In: Proceedings of the international computer music conference (ICMC 97). The International Computer Music Association, San Francisco, pp 279–282</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Konstantas D, Gibbs S, Orlarey Y, Carbonel O (1998) Design and implementation of an ATM based distributed musi" /><p class="c-article-references__text" id="ref-CR11">Konstantas D, Gibbs S, Orlarey Y, Carbonel O (1998) Design and implementation of an ATM based distributed musical rehearsal studio. In: Proceedings of ECMAST’98, 3<sup>rd</sup> European conference on multimedia applications, services and techniques, Germany</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Konstantas, Y. Orlarey, O. Carbonel, S. Gibbs, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Konstantas D, Orlarey Y, Carbonel O, Gibbs S (1999) The distributed musical rehearsal environment. IEEE MultiM" /><p class="c-article-references__text" id="ref-CR12">Konstantas D, Orlarey Y, Carbonel O, Gibbs S (1999) The distributed musical rehearsal environment. IEEE MultiMedia 6(3):54–64</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F93.790611" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20distributed%20musical%20rehearsal%20environment&amp;journal=IEEE%20MultiMedia&amp;volume=6&amp;issue=3&amp;pages=54-64&amp;publication_year=1999&amp;author=Konstantas%2CD&amp;author=Orlarey%2CY&amp;author=Carbonel%2CO&amp;author=Gibbs%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Schütz, " /><meta itemprop="datePublished" content="1951" /><meta itemprop="headline" content="Schütz A (1951) Making music together: a study in social relationship. Soc Res 18:76–97" /><p class="c-article-references__text" id="ref-CR13">Schütz A (1951) Making music together: a study in social relationship. Soc Res 18:76–97</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Making%20music%20together%3A%20a%20study%20in%20social%20relationship&amp;journal=Soc%20Res&amp;volume=18&amp;pages=76-97&amp;publication_year=1951&amp;author=Sch%C3%BCtz%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Slater M, Howell J, Steed A, Pertaub D, Garau M (2000) Acting in virtual reality. In: Churchill E, Reddy M (ed" /><p class="c-article-references__text" id="ref-CR14">Slater M, Howell J, Steed A, Pertaub D, Garau M (2000) Acting in virtual reality. In: Churchill E, Reddy M (eds) Proceedings of the third international conference on collaborative virtual environments (San Francisco, CA). <i>CVE ’00</i>, ACM Press, New York, pp 103–110</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-006-0049-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work received partial support from NSF grant nos. IIS-0081550 and SES-0551294. Special thanks to Kamala Sankaram and Allen Jones for assistance with collecting and digitizing the pilot data, to Martin Mueller at the New School’s Jazz &amp; Contemporary Music program for access to rehearsal rooms, and to the reviewers for useful comments.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Psychology, F330, New School for Social Research, 65 Fifth Avenue, New York, NY, 10003, USA</p><p class="c-article-author-affiliation__authors-list">Michael F. Schober</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Michael_F_-Schober"><span class="c-article-authors-search__title u-h3 js-search-name">Michael F. Schober</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Michael F.+Schober&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Michael F.+Schober" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Michael F.+Schober%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-006-0049-z/email/correspondent/c1/new">Michael F. Schober</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Virtual%20environments%20for%20creative%20work%20in%20collaborative%20music-making&amp;author=Michael%20F.%20Schober&amp;contentID=10.1007%2Fs10055-006-0049-z&amp;publication=1359-4338&amp;publicationDate=2006-09-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Schober, M.F. Virtual environments for creative work in collaborative music-making.
                    <i>Virtual Reality</i> <b>10, </b>85–94 (2006). https://doi.org/10.1007/s10055-006-0049-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-006-0049-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-02-24">24 February 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-05-12">12 May 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-09-07">07 September 2006</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2006-10">October 2006</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-006-0049-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-006-0049-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual Environment</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Virtual Space</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Musical Score</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Collaborative Performance</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Visual Mediation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-006-0049-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=49;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

