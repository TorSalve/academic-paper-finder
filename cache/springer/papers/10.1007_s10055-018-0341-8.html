<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Detecting rigid links between sensors for automatic sensor space align"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality applications. However, calibration of..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/23/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Detecting rigid links between sensors for automatic sensor space alignment in virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2018 23:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2018-03-31"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality applications. However, calibration of multiple sensor technologies is non-trivial and at a minimum will require significant, and likely regular, user actioned calibration procedures. To enable ambient sensor calibration, we present techniques for automatically identifying relations between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems for body tracking. The techniques allow for subsequent automatic alignment of the sensor systems. Two techniques are presented, analysed in simulation for performance under varying noise and latency conditions, and are applied to two case studies. The first study identified sensors tracked by a gold standard rigid body tracker with one of six rigid bodies tracked by the first generation Kinect sensor with each sensor identified correctly in at least 76% of estimates. The second case study was an interactive version of the system that can detect a change in sensor configuration in 1&#8211;2&#160;s and only requires movements of less than 15&#160;cm or 
                  
                    
                  
                  $$90^\circ$$
                  
                    
                  
                . Our methods represent a key step in creating highly accessible multi-device 3D virtual environments."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2018-03-31"/>

    <meta name="prism.volume" content="23"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="71"/>

    <meta name="prism.endingPage" content="84"/>

    <meta name="prism.copyright" content="2018 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-018-0341-8"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-018-0341-8"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-018-0341-8.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-018-0341-8"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Detecting rigid links between sensors for automatic sensor space alignment in virtual environments"/>

    <meta name="citation_volume" content="23"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2019/03"/>

    <meta name="citation_online_date" content="2018/03/31"/>

    <meta name="citation_firstpage" content="71"/>

    <meta name="citation_lastpage" content="84"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-018-0341-8"/>

    <meta name="DOI" content="10.1007/s10055-018-0341-8"/>

    <meta name="citation_doi" content="10.1007/s10055-018-0341-8"/>

    <meta name="description" content="Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality app"/>

    <meta name="dc.creator" content="Jake Fountain"/>

    <meta name="dc.creator" content="Shamus P. Smith"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bahle G, Lukowicz P, Kunze K, Kise K (2013) I see you: how to improve wearable activity recognition by leveraging information from environmental cameras. In: 2013 IEEE international conference on pervasive computing and communications workshops (PERCOM workshops), pp 409&#8211;412"/>

    <meta name="citation_reference" content="Banos O, Calatroni A, Damas M, Pomares H, Rojas I, Sagha H, del R&#160;Milln J, Tr&#246;ster G, Chavarriaga R, Roggen D (2012) Kinect= IMU? Learning MIMO signal mappings to automatically translate activity recognition systems across sensor modalities. In: 2012 16th international symposium on wearable computers (ISWC). IEEE, pp 92&#8211;99"/>

    <meta name="citation_reference" content="Calatroni A, Roggen D, Tr&#246;ster G (2010) A methodology to use unknown new sensors for activity recognition by leveraging sporadic interactions with primitive sensors and behavioral assumptions. Eidgen&#246;ssische Technische Hochschule Z&#252;rich, D-ITET, Institut f&#252;r Elektronik"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Ubiquitous Comput; citation_title=Unsupervised adaptation for acceleration-based activity recognition: robustness to sensor displacement and rotation; citation_author=R Chavarriaga, H Bayati, JdR Milln; citation_volume=17; citation_issue=3; citation_publication_date=2013; citation_pages=479-490; citation_doi=10.1007/s00779-011-0493-y; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Human-Comput Stud; citation_title=Understanding the impact of multimodal interaction using gaze informed mid-air gesture control in 3D virtual objects manipulation; citation_author=S Deng, N Jiang, J Chang, S Guo, JJ Zhang; citation_volume=105; citation_issue=Supplement C; citation_publication_date=2017; citation_pages=68-80; citation_doi=10.1016/j.ijhcs.2017.04.002; citation_id=CR5"/>

    <meta name="citation_reference" content="Destelle F, Ahmadi A, O&#8217;Connor N, Moran K, Chatzitofis A, Zarpalas D, Daras P (2014) Low-cost accurate skeleton tracking based on fusion of kinect and wearable inertial sensors. In: 2014 Proceedings of the 22nd European signal processing conference (EUSIPCO), pp 371&#8211;375"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Simultaneous robot-world and hand-eye calibration; citation_author=F Dornaika, R Horaud; citation_volume=14; citation_publication_date=1998; citation_pages=617-622; citation_doi=10.1109/70.704233; citation_id=CR7"/>

    <meta name="citation_reference" content="Forster K, Roggen D, Tr&#246;ster G (2009) Unsupervised classifier self-calibration through repeated context occurences: is there robustness against sensor displacement to gain? In: International symposium on wearable computers (ISWC), pp 77&#8211;84"/>

    <meta name="citation_reference" content="Fountain J, Smith SP (2016) Automatic identification of rigidly linked 6DoF sensors. In: IEEE virtual reality 2016. IEEE, pp 175&#8211;176"/>

    <meta name="citation_reference" content="Fountain J, Smith SP (2017) Real-time ambient fusion of commodity tracking systems for virtual reality. In: International conference on artificial reality and telexistence and eurographics symposium on virtual environments (ICAT-EGVE 2017). Eurographics Association, pp 1&#8211;8"/>

    <meta name="citation_reference" content="Gottschalk S, Hughes JF (1993) Autocalibration for virtual environment tracking hardware. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, NY, USA, SIGGRAPH &#8217;93, pp 65&#8211;72"/>

    <meta name="citation_reference" content="Kunze K, Lukowicz P (2008) Dealing with sensor displacement in motion-based onbody activity recognition systems. In: Proceedings of the 10th international conference on ubiquitous computing. ACM, pp 20&#8211;29"/>

    <meta name="citation_reference" content="Kunze K, Lukowicz P, Junker H, Trster G (2005) Where am I: recognizing on-body positions of wearable sensors. In: Strang T, Linnhoff-Popien C (eds) Location- and context-awareness, no. 3479 in Lecture notes in computer science. Springer, Berlin, pp 264&#8211;275"/>

    <meta name="citation_reference" content="Kunze K, Lukowicz P, Partridge K, Begole B (2009) Which way am I facing: inferring horizontal device orientation from an accelerometer signal. In: International symposium on wearable computers, 2009. ISWC &#8217;09, pp 149&#8211;150"/>

    <meta name="citation_reference" content="citation_title=3D user interfaces: theory and practice; citation_publication_date=2017; citation_id=CR15; citation_author=JJ LaViola; citation_author=E Kruijff; citation_author=RP McMahan; citation_author=DA Bowman; citation_author=I Poupyrev; citation_publisher=Addison-Wesley"/>

    <meta name="citation_reference" content="citation_title=&#8220;Are You with Me?&#8221;&#8212;Using accelerometers to determine if two devices are carried by the same person; citation_inbook_title=Pervasive computing; citation_publication_date=2004; citation_pages=33-50; citation_id=CR16; citation_author=J Lester; citation_author=B Hannaford; citation_author=G Borriello; citation_publisher=Springer"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Phys Sci; citation_title=Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product; citation_author=A Li, L Wang, D Wu; citation_volume=5; citation_issue=10; citation_publication_date=2010; citation_pages=1530-1536; citation_id=CR17"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Subjective evaluation of a semi-automatic optical see-through head-mounted display calibration technique; citation_author=K Moser, Y Itoh, K Oshima, J Swan, G Klinker, C Sandor; citation_volume=21; citation_issue=4; citation_publication_date=2015; citation_pages=491-500; citation_doi=10.1109/TVCG.2015.2391856; citation_id=CR18"/>

    <meta name="citation_reference" content="Pearl T (2012) Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion. Master&#8217;s thesis, Vienna University of Technology"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Vis Comput Graph; citation_title=Corneal-imaging calibration for optical see-through head-mounted displays; citation_author=A Plopski, Y Itoh, C Nitschke, K Kiyokawa, G Klinker, H Takemura; citation_volume=21; citation_issue=4; citation_publication_date=2015; citation_pages=481-490; citation_doi=10.1109/TVCG.2015.2391857; citation_id=CR20"/>

    <meta name="citation_reference" content="Sanderson C (2010) Armadillo: An open source C++ linear algebra library for fast prototyping and computationally intensive experiments. Technical Report, NICTA, Australia"/>

    <meta name="citation_reference" content="Schapansky K (2014) Jester: a device abstraction and data fusion API for skeletal tracking. Master&#8217;s Thesis, California Polytechnic State University"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Vis Image Underst; citation_title=Comparing two sets of corresponding six degree of freedom data; citation_author=M Shah; citation_volume=115; citation_issue=10; citation_publication_date=2011; citation_pages=1355-1362; citation_doi=10.1016/j.cviu.2011.05.007; citation_id=CR23"/>

    <meta name="citation_reference" content="citation_journal_title=J Mech Robot; citation_title=Solving the robot-world/hand-eye calibration problem using the Kronecker product; citation_author=M Shah; citation_volume=5; citation_issue=3; citation_publication_date=2013; citation_pages=031,007-031,007; citation_doi=10.1115/1.4024473; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot Autom; citation_title=Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB; citation_author=H Zhuang, ZS Roth, R Sudhakar; citation_volume=10; citation_issue=4; citation_publication_date=1994; citation_pages=549-554; citation_doi=10.1109/70.313105; citation_id=CR25"/>

    <meta name="citation_author" content="Jake Fountain"/>

    <meta name="citation_author_email" content="Jake.Fountain@uon.edu.au"/>

    <meta name="citation_author_institution" content="School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, Australia"/>

    <meta name="citation_author" content="Shamus P. Smith"/>

    <meta name="citation_author_email" content="Shamus.Smith@newcastle.edu.au"/>

    <meta name="citation_author_institution" content="School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, Australia"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-018-0341-8&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2019/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-018-0341-8"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Detecting rigid links between sensors for automatic sensor space alignment in virtual environments"/>
        <meta property="og:description" content="Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality applications. However, calibration of multiple sensor technologies is non-trivial and at a minimum will require significant, and likely regular, user actioned calibration procedures. To enable ambient sensor calibration, we present techniques for automatically identifying relations between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems for body tracking. The techniques allow for subsequent automatic alignment of the sensor systems. Two techniques are presented, analysed in simulation for performance under varying noise and latency conditions, and are applied to two case studies. The first study identified sensors tracked by a gold standard rigid body tracker with one of six rigid bodies tracked by the first generation Kinect sensor with each sensor identified correctly in at least 76% of estimates. The second case study was an interactive version of the system that can detect a change in sensor configuration in 1–2&amp;nbsp;s and only requires movements of less than 15&amp;nbsp;cm or $$90^\circ$$ 90 ∘ . Our methods represent a key step in creating highly accessible multi-device 3D virtual environments."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Detecting rigid links between sensors for automatic sensor space alignment in virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-018-0341-8","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Tracking, Input devices, Calibration, Usability, Sensors","kwrd":["Tracking","Input_devices","Calibration","Usability","Sensors"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-018-0341-8","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-018-0341-8","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=341;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-018-0341-8">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Detecting rigid links between sensors for automatic sensor space alignment in virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0341-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0341-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2018-03-31" itemprop="datePublished">31 March 2018</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Detecting rigid links between sensors for automatic sensor space alignment in virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jake-Fountain" data-author-popup="auth-Jake-Fountain">Jake Fountain</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of Newcastle" /><meta itemprop="address" content="0000 0000 8831 109X, grid.266842.c, School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, New South Wales, 2308, Australia" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shamus_P_-Smith" data-author-popup="auth-Shamus_P_-Smith" data-corresp-id="c1">Shamus P. Smith<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0001-9135-1356"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0001-9135-1356</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="The University of Newcastle" /><meta itemprop="address" content="0000 0000 8831 109X, grid.266842.c, School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, New South Wales, 2308, Australia" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 23</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">71</span>–<span itemprop="pageEnd">84</span>(<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">232 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-018-0341-8/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality applications. However, calibration of multiple sensor technologies is non-trivial and at a minimum will require significant, and likely regular, user actioned calibration procedures. To enable ambient sensor calibration, we present techniques for automatically identifying relations between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems for body tracking. The techniques allow for subsequent automatic alignment of the sensor systems. Two techniques are presented, analysed in simulation for performance under varying noise and latency conditions, and are applied to two case studies. The first study identified sensors tracked by a gold standard rigid body tracker with one of six rigid bodies tracked by the first generation Kinect sensor with each sensor identified correctly in at least 76% of estimates. The second case study was an interactive version of the system that can detect a change in sensor configuration in 1–2 s and only requires movements of less than 15 cm or <span class="mathjax-tex">\(90^\circ\)</span>. Our methods represent a key step in creating highly accessible multi-device 3D virtual environments.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Tracking technologies, and the associated sensor systems, are a core component for the realisation of interactive virtual environments. 3D user interfaces often require information about a user’s physical position, orientation or motion in 3D space (LaViola Jr et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="LaViola JJ Jr, Kruijff E, McMahan RP, Bowman DA, Poupyrev I (2017) 3D user interfaces: theory and practice, 2nd edn. Addison-Wesley, Boston" href="/article/10.1007/s10055-018-0341-8#ref-CR15" id="ref-link-section-d14777e355">2017</a>, p. 213). Modern virtual reality technologies are based around head tracking, typically embedded in head-mounted displays, and body or limb tracking via depth cameras and/or the use of 3D mice, i.e., the combination of spatial tracking with physical device components. An examples of the former includes the Microsoft Kinect, for skeleton tracking, and examples of the later include the Oculus Touch and HTC Vive wand-based technologies. Although the affordability and accessibility of such technologies are increasing, individual tracking systems can have range and accuracy limitations (Deng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Deng S, Jiang N, Chang J, Guo S, Zhang JJ (2017) Understanding the impact of multimodal interaction using gaze informed mid-air gesture control in 3D virtual objects manipulation. Int J Human-Comput Stud 105(Supplement C):68–80" href="/article/10.1007/s10055-018-0341-8#ref-CR5" id="ref-link-section-d14777e358">2017</a>). To improve interaction in virtual environments it is often desirable to have extended tracking range or improved tracking accuracy by combining multiple sensor systems which ordinarily do not communicate. This can provide a larger tracking space to support spatial interaction techniques and typically reduces the cost of the virtual environment, when compared to single-unit large-volume tracking solutions. Combining sensor systems can also eliminate the disadvantages of component sensor systems (LaViola Jr et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="LaViola JJ Jr, Kruijff E, McMahan RP, Bowman DA, Poupyrev I (2017) 3D user interfaces: theory and practice, 2nd edn. Addison-Wesley, Boston" href="/article/10.1007/s10055-018-0341-8#ref-CR15" id="ref-link-section-d14777e361">2017</a>, p. 212); for example, combining optical and inertial trackers maintains high frequency tracking while eliminating the drift inherent in inertial tracking. However, to combine sensor system information, the sensor spaces must first be aligned. Manual alignment is time consuming, error prone and depends heavily on user skill. An alternative solution is automatic alignment that aims to minimise user engagement with calibration and setup while improving the overall results (Gottschalk and Hughes <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Gottschalk S, Hughes JF (1993) Autocalibration for virtual environment tracking hardware. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, NY, USA, SIGGRAPH ’93, pp 65–72" href="/article/10.1007/s10055-018-0341-8#ref-CR11" id="ref-link-section-d14777e364">1993</a>; Fountain and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Fountain J, Smith SP (2017) Real-time ambient fusion of commodity tracking systems for virtual reality. In: International conference on artificial reality and telexistence and eurographics symposium on virtual environments (ICAT-EGVE 2017). Eurographics Association, pp 1–8" href="/article/10.1007/s10055-018-0341-8#ref-CR10" id="ref-link-section-d14777e367">2017</a>).</p><p>There are many effective methods for automatically aligning multiple 3D sensor systems, provided sensor dependencies are known. For example, two rigidly linked 6DoF (position and rotation) or 3DoF (rotation only) sensors from different sensor systems can be aligned using techniques including linear and nonlinear optimisation approaches (Dornaika and Horaud <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–622" href="/article/10.1007/s10055-018-0341-8#ref-CR7" id="ref-link-section-d14777e373">1998</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product. Int J Phys Sci 5(10):1530–1536" href="/article/10.1007/s10055-018-0341-8#ref-CR17" id="ref-link-section-d14777e376">2010</a>; Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e379">2013</a>; Zhuang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554" href="/article/10.1007/s10055-018-0341-8#ref-CR25" id="ref-link-section-d14777e382">1994</a>). However, if multiple unlabelled sensors are present, a user is required to configure relations between sensors. This paper details the use of sensor streams alone to automatically deduce the dependencies amongst a collection of unidentified 6DoF sensors. The aim is to allow for user disengaged automatic alignment of multiple sensor systems (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig1">1</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig1_HTML.gif?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig1_HTML.gif" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Sensor matching and alignment concept</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The calibration techniques in (Dornaika and Horaud <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–622" href="/article/10.1007/s10055-018-0341-8#ref-CR7" id="ref-link-section-d14777e409">1998</a>; Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product. Int J Phys Sci 5(10):1530–1536" href="/article/10.1007/s10055-018-0341-8#ref-CR17" id="ref-link-section-d14777e412">2010</a>; Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e415">2013</a>; Zhuang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554" href="/article/10.1007/s10055-018-0341-8#ref-CR25" id="ref-link-section-d14777e418">1994</a>) were originally developed for industrial robots. Here, we have explored their use in sensor identification and calibration for body tracking to increase the usability of multiple sensor systems in 3D virtual environments. The techniques we have developed apply to numerous existing and upcoming commercial devices, including the Oculus Constellation system,<sup><a href="#Fn1"><span class="u-visually-hidden">Footnote </span>1</a></sup> the Valve Lighthouse system,<sup><a href="#Fn2"><span class="u-visually-hidden">Footnote </span>2</a></sup> the Perception Neuron skeletal tracking system<sup><a href="#Fn3"><span class="u-visually-hidden">Footnote </span>3</a></sup> and many others.</p><p>The contributions of this work include two methods for identifying rigid links between sensors from separate systems (the invariant functional (IF) method and the calibration error (CE) method), a simulation analysis of performance, and two case studies demonstrating application of the methods.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><p>Automatic calibration is becoming increasingly important for body tracking in virtual environments because of the high precision required to deliver a realistic experience in tracked environments. Calibration is also important because of the natural variation between individual perceptions and physiology (Moser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Moser K, Itoh Y, Oshima K, Swan J, Klinker G, Sandor C (2015) Subjective evaluation of a semi-automatic optical see-through head-mounted display calibration technique. IEEE Trans Vis Comput Graph 21(4):491–500" href="/article/10.1007/s10055-018-0341-8#ref-CR18" id="ref-link-section-d14777e472">2015</a>; Plopski et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Plopski A, Itoh Y, Nitschke C, Kiyokawa K, Klinker G, Takemura H (2015) Corneal-imaging calibration for optical see-through head-mounted displays. IEEE Trans Vis Comput Graph 21(4):481–490" href="/article/10.1007/s10055-018-0341-8#ref-CR20" id="ref-link-section-d14777e475">2015</a>). Plopski et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Plopski A, Itoh Y, Nitschke C, Kiyokawa K, Klinker G, Takemura H (2015) Corneal-imaging calibration for optical see-through head-mounted displays. IEEE Trans Vis Comput Graph 21(4):481–490" href="/article/10.1007/s10055-018-0341-8#ref-CR20" id="ref-link-section-d14777e478">2015</a>) presents a method for automatic calibration of see-through displays for aligning augmented reality content, identifying the need for continuous automatic calibration.</p><p>Significant work has been done to improve the ease of use and robustness of wearable continuous activity monitoring systems. It has been demonstrated that it is possible to identify the categorical on-body position (Kunze et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Kunze K, Lukowicz P, Junker H, Trster G (2005) Where am I: recognizing on-body positions of wearable sensors. In: Strang T, Linnhoff-Popien C (eds) Location- and context-awareness, no. 3479 in Lecture notes in computer science. Springer, Berlin, pp 264–275" href="/article/10.1007/s10055-018-0341-8#ref-CR13" id="ref-link-section-d14777e484">2005</a>) and yaw orientation (Kunze et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Kunze K, Lukowicz P, Partridge K, Begole B (2009) Which way am I facing: inferring horizontal device orientation from an accelerometer signal. In: International symposium on wearable computers, 2009. ISWC ’09, pp 149–150" href="/article/10.1007/s10055-018-0341-8#ref-CR14" id="ref-link-section-d14777e487">2009</a>) of an inertial measurement unit (IMU) during walking. Detection of walk cycle characteristics in sensor measurements allows the desired information to be deduced based on models of typical walking patterns. Lester et al. demonstrated that it is possible to determine whether or not two sensors are attached to the same person based on frequency domain coherence function analysis of the two sensor data streams (Lester et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lester J, Hannaford B, Borriello G (2004) “Are You with Me?”—Using accelerometers to determine if two devices are carried by the same person. In: Ferscha A, Mattern F (eds) Pervasive computing. Springer, Berlin, pp 33–50" href="/article/10.1007/s10055-018-0341-8#ref-CR16" id="ref-link-section-d14777e490">2004</a>). By exploiting existing sensors and state space models (in this case, models of human behaviour), Calatroni et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Calatroni A, Roggen D, Tröster G (2010) A methodology to use unknown new sensors for activity recognition by leveraging sporadic interactions with primitive sensors and behavioral assumptions. Eidgenössische Technische Hochschule Zürich, D-ITET, Institut für Elektronik" href="/article/10.1007/s10055-018-0341-8#ref-CR3" id="ref-link-section-d14777e493">2010</a>) demonstrated improved integration of new sensors into existing networks based on sporadic user actions.</p><p>
Bahle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Bahle G, Lukowicz P, Kunze K, Kise K (2013) I see you: how to improve wearable activity recognition by leveraging information from environmental cameras. In: 2013 IEEE international conference on pervasive computing and communications workshops (PERCOM workshops), pp 409–412" href="/article/10.1007/s10055-018-0341-8#ref-CR1" id="ref-link-section-d14777e499">2013</a>) used dynamic time warping and time series approaches to localise on-body inertial sensors based on skeletal data extracted from an ambient depth camera. Bahle et al. aimed to use such methods to allow wearable devices to self correct for errors given anonymous public data from skeletal tracking cameras in public spaces. The angle which each limb made with the gravity vector was the key time series used to compare the sensors with the skeletal data. Our work provides an alternative method which uses all six degrees of freedom, but assumes a higher level of skeletal tracking fidelity.</p><p>
Banos et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Banos O, Calatroni A, Damas M, Pomares H, Rojas I, Sagha H, del R Milln J, Tröster G, Chavarriaga R, Roggen D (2012) Kinect= IMU? Learning MIMO signal mappings to automatically translate activity recognition systems across sensor modalities. In: 2012 16th international symposium on wearable computers (ISWC). IEEE, pp 92–99" href="/article/10.1007/s10055-018-0341-8#ref-CR2" id="ref-link-section-d14777e505">2012</a>) demonstrated a learning system for automatic modality translation from optical skeleton data to acceleration data for several upper torso IMUs. The approach performed transfer learning with least squares optimisation of a linear multi-input-multi-output (MIMO) map. The resulting MIMO map translates a time series of skeletal data to the corresponding accelerometer data and vice versa. The method is a manual user-sampled calibration algorithm and requires prior sensor location identification. We extend the work by Banos et al. by investigating their suggestion of using calibration error to automatically localise on-body sensors.</p><p>Fault detection and correction of on-body sensors is also a popular research topic. Kunze and Lukowicz (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Kunze K, Lukowicz P (2008) Dealing with sensor displacement in motion-based onbody activity recognition systems. In: Proceedings of the 10th international conference on ubiquitous computing. ACM, pp 20–29" href="/article/10.1007/s10055-018-0341-8#ref-CR12" id="ref-link-section-d14777e512">2008</a>) present heuristics for determining when an accelerometer is displaced by a small amount within its attached limb. Forster et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Forster K, Roggen D, Tröster G (2009) Unsupervised classifier self-calibration through repeated context occurences: is there robustness against sensor displacement to gain? In: International symposium on wearable computers (ISWC), pp 77–84" href="/article/10.1007/s10055-018-0341-8#ref-CR8" id="ref-link-section-d14777e515">2009</a>) demonstrate recalibration for detecting and compensating on-body inertial sensor rotations and displacements in a highly redundant sensor network. Chavarriaga et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Chavarriaga R, Bayati H, Milln JdR (2013) Unsupervised adaptation for acceleration-based activity recognition: robustness to sensor displacement and rotation. Pers Ubiquitous Comput 17(3):479–490" href="/article/10.1007/s10055-018-0341-8#ref-CR4" id="ref-link-section-d14777e518">2013</a>) extend the method developed by Forster et al. with an unsupervised online expectation-maximisation algorithm.</p><p>In the work presented here we focus on identification of sensors for body-scale skeletal tracking. Destelle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Destelle F, Ahmadi A, O’Connor N, Moran K, Chatzitofis A, Zarpalas D, Daras P (2014) Low-cost accurate skeleton tracking based on fusion of kinect and wearable inertial sensors. In: 2014 Proceedings of the 22nd European signal processing conference (EUSIPCO), pp 371–375" href="/article/10.1007/s10055-018-0341-8#ref-CR6" id="ref-link-section-d14777e524">2014</a>) used a procedure for fusing inertial data with optical data from the Microsoft Kinect<sup><a href="#Fn4"><span class="u-visually-hidden">Footnote </span>4</a></sup> to produce a hybrid skeletal tracking solution with accuracy rivalling that of expensive gold standard optical tracking solutions. However, part of the fusion process involves meticulous alignment of each IMU reference frame with one another and alignment of the IMUs with respect to the Kinect reference frame.</p><p>Jester is an open source human skeletal sensor fusion layer for virtual environments (Schapansky <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Schapansky K (2014) Jester: a device abstraction and data fusion API for skeletal tracking. Master’s Thesis, California Polytechnic State University" href="/article/10.1007/s10055-018-0341-8#ref-CR22" id="ref-link-section-d14777e543">2014</a>). Jester defines a middle-ware architecture for abstracting the hardware and software layers in a virtual environment, while also providing support for fusion of sensors with either Kalman or double exponential filters. However, Jester requires explicit configuration of the sensor locations by the user. To complement this system, we define a process for automatically determining sensor correspondences for multiple 3DoF and 6DoF sensor systems.</p><h3 class="c-article__sub-heading" id="Sec3">6DoF sensor calibration</h3><p>This section reviews the relation between two rigidly linked 6DoF sensors. Each sensor is assumed to measure a 3D pose, including both rotation and position, relative to some fixed coordinate frame. However, the coordinate frames of each sensor need not be the same but are assumed to differ by a constant rigid transform. Given two sensors which are rigidly linked and from separate sensor systems, the two unknown variables that determine the dependence relation are the fixed transformation between the sensor coordinate frames and the fixed transformation between the sensors themselves.</p><p>Let <span class="mathjax-tex">\(\underline{\text {S}}\)</span> and <span class="mathjax-tex">\(\underline{\text {Q}}\)</span> be two coordinate systems in <span class="mathjax-tex">\(\mathbb {R}^3\)</span> corresponding to two 6DoF sensor reference frames, each with a single sensor (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig2">2</a>). Let a sample for each of the two sensors be observed for the time steps <span class="mathjax-tex">\(t = 1,\ldots ,N_s\)</span> for some <span class="mathjax-tex">\(N_s \in \mathbb {N}\)</span>. For each <i>t</i>, the sensors each define coordinate systems <span class="mathjax-tex">\(S_t\)</span> and <span class="mathjax-tex">\(Q_t\)</span>, respectively. Sensor measurements yield the <span class="mathjax-tex">\(4\times 4\)</span> homogeneous matrices <span class="mathjax-tex">\(\mathbf {A}_t : S_t \rightarrow \underline{\text {S}}\)</span> and <span class="mathjax-tex">\({\mathbf {B}}_t : Q_t \rightarrow \underline{\text {Q}}\)</span> for each <i>t</i>. If the two sensors are rigidly linked, then there are two constant homogeneous matrices <span class="mathjax-tex">\(\mathbf {Y}: \underline{\text {Q}} \rightarrow \underline{\text {S}}\)</span> and <span class="mathjax-tex">\(\mathbf {X}: Q_t \rightarrow S_t\)</span> which satisfy</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf {A}_t \mathbf {X} = \mathbf {Y} \mathbf {B}_t \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>for every <span class="mathjax-tex">\(t = 1,\ldots , N_s\)</span>. The <i>robot-world and hand-eye calibration problem</i> is that of determining the transforms <span class="mathjax-tex">\(\mathbf {X}\)</span> and <span class="mathjax-tex">\(\mathbf {Y}\)</span> such that the sum squared error</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{t = 1}^{N_s} \Vert {\mathbf {A}_t \mathbf {X} - \mathbf {Y} \mathbf {B}_t \Vert }^2 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>is minimised.</p><p>
Zhuang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554" href="/article/10.1007/s10055-018-0341-8#ref-CR25" id="ref-link-section-d14777e1212">1994</a>) were the first to describe a linear approach to approximating the solution to the robot-world and hand-eye calibration problem. Zhuang et al. derived Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0341-8#Equ1">1</a> in the context of calibrating tool-flange and base-world transforms for industrial robots. Their technique computes the least squares solutions to two equations derived from the quaternion properties of Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0341-8#Equ1">1</a>. Firstly, the quaternions corresponding to <span class="mathjax-tex">\(\mathbf {X}\)</span> and <span class="mathjax-tex">\(\mathbf {Y}\)</span> are computed by solving a linear least squares equation. Secondly, the translation components of <span class="mathjax-tex">\(\mathbf {X}\)</span> and <span class="mathjax-tex">\(\mathbf {Y}\)</span> are computed using the result for their quaternion components in another linear least squares step. The technique generalises to 3DoF orientation sensors by only performing the first stage of the calculation.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>The relation between two 6DoF sensors has the characteristic equation given by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0341-8#Equ1">1</a>. <span class="mathjax-tex">\(\underline{\text {S}}\)</span> and <span class="mathjax-tex">\(\underline{\text {Q}}\)</span> are the sensor reference frames. <span class="mathjax-tex">\(S_t\)</span> and <span class="mathjax-tex">\(Q_t\)</span> are the coordinate systems corresponding to the 6DoF pose of each sensor at time step <i>t</i>. <span class="mathjax-tex">\(\mathbf {Y}\)</span> and <span class="mathjax-tex">\(\mathbf {X}\)</span> are the rigid transforms linking the sensor reference frames and the sensor poses, respectively. <span class="mathjax-tex">\(\mathbf {A}_t\)</span> and <span class="mathjax-tex">\(\mathbf {B}_t\)</span> are the 6DoF pose transforms of the two sensors at time step <i>t</i></p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Additional techniques have been developed by Dornaika and Horaud (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–622" href="/article/10.1007/s10055-018-0341-8#ref-CR7" id="ref-link-section-d14777e1518">1998</a>) with a closed form method and a nonlinear optimisation approach. The nonlinear method was demonstrated to be more robust than both the linear method proposed by Zhuang et al. and the closed form method. The linear and closed form methods were shown to have similar stability. Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product. Int J Phys Sci 5(10):1530–1536" href="/article/10.1007/s10055-018-0341-8#ref-CR17" id="ref-link-section-d14777e1521">2010</a>) proposed two additional techniques using dual quaternions and the Kronecker product, respectively. Shah (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e1524">2013</a>) proposed using the Kronecker product to determine the rotation components of <span class="mathjax-tex">\(\mathbf {X}\)</span> and <span class="mathjax-tex">\(\mathbf {Y}\)</span>. Each algorithm requires <span class="mathjax-tex">\(N_s \ge 3\)</span> samples of corresponding matrix pairs <span class="mathjax-tex">\(\mathbf {A}_t\)</span> and <span class="mathjax-tex">\(\mathbf {B}_t\)</span>. The algorithms also trivially cover the 3DoF case, usually by omitting the second part of the computation.</p><p>Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec4">3</a> begins our contribution, including development of two methods for identifying rigid links: the invariant functional (IF) method and the calibration error (CE) method. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec8">4</a> describes an analysis performed on a set of simulated data and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec9">5</a> describes a real world case study where rigidly linked sensors are matched between two real sensor systems offline. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec10">6</a> details an interactive implementation of the system.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Matching of sensors</h2><div class="c-article-section__content" id="Sec4-content"><p>Previous work on matching 6DoF time series assumed that <span class="mathjax-tex">\(\mathbf {Y}\)</span> is known (Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Shah M (2011) Comparing two sets of corresponding six degree of freedom data. Comput Vis Image Underst 115(10):1355–1362" href="/article/10.1007/s10055-018-0341-8#ref-CR23" id="ref-link-section-d14777e1694">2011</a>). No studies approaching the problem without knowledge of both <span class="mathjax-tex">\(\mathbf {Y}\)</span> and <span class="mathjax-tex">\(\mathbf {X}\)</span> are known by the authors. This section expands our previous work (Fountain and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Fountain J, Smith SP (2016) Automatic identification of rigidly linked 6DoF sensors. In: IEEE virtual reality 2016. IEEE, pp 175–176" href="/article/10.1007/s10055-018-0341-8#ref-CR9" id="ref-link-section-d14777e1737">2016</a>) in more detail before extending it.</p><p>Let two sensor systems <span class="mathjax-tex">\(\underline{\text {S}}\)</span> and <span class="mathjax-tex">\(\underline{\text {Q}}\)</span> each measure a collection of 6DoF sensors <span class="mathjax-tex">\(\{S^k\}_{k = 1, \ldots , K}\)</span> and <span class="mathjax-tex">\(\{Q^l\}_{l = 1, \ldots , L}\)</span>. The sensors <span class="mathjax-tex">\(S^k\)</span> and <span class="mathjax-tex">\(Q^l\)</span> measure the <span class="mathjax-tex">\(4 \times 4\)</span> homogeneous matrices <span class="mathjax-tex">\(\mathbf {A}^k_t\)</span> and <span class="mathjax-tex">\(\mathbf {B}^l_t\)</span> for each time step <i>t</i> such that <span class="mathjax-tex">\(\mathbf {A}^k_t:S^k_t \rightarrow \underline{\text {S}}\)</span> and <span class="mathjax-tex">\(\mathbf {B}^l_t:Q^l_t \rightarrow \underline{\text {Q}}\)</span>. Whenever sensor <span class="mathjax-tex">\(S^k\)</span> is rigidly linked to sensor <span class="mathjax-tex">\(Q^l\)</span>, we have the relation</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf {A}^k_t \mathbf {X}_{k,l} = \mathbf {Y} \mathbf {B}^l_t \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>where <span class="mathjax-tex">\(\mathbf {X}_{k,l}:Q^l \rightarrow S^k\)</span> is the rigid link transform between the sensors and <span class="mathjax-tex">\(\mathbf {Y}:\underline{\text {Q}} \rightarrow \underline{\text {S}}\)</span> is the transform between the sensor spaces.<sup><a href="#Fn5"><span class="u-visually-hidden">Footnote </span>5</a></sup> The aim is to automatically identify, or <i>match</i>, such rigidly linked sensors in an automatic, unprompted user-sampled calibration procedure.</p><h3 class="c-article__sub-heading" id="Sec5">Invariant functional (IF) method</h3><p>In this section we present a sensor matching method based on the intuition that some characteristics of the systems will remain invariant, i.e., unchanged, under the transformation from one sensor space to another. For example, two rigidly linked sensors will rotate through the same angle over a given time window (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig3">3</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>The IF Method involves measuring the angle through which each sensor rotates over a sliding window of time, <span class="mathjax-tex">\(N_s\)</span> steps into the past. The angle <span class="mathjax-tex">\(\theta _R\)</span> is then compared as a time series between to other candidate sensors. In this synthetic example it is easy to see that A and B measurements are associated, but C is not</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Given two 6DoF sensor streams <span class="mathjax-tex">\(\mathbf {A}_i\)</span> and <span class="mathjax-tex">\(\mathbf {B}_i\)</span> for <span class="mathjax-tex">\(i = 1,\ldots ,t\)</span>, define the <i>time series matrices</i><span class="mathjax-tex">\(\mathbf {A}_{1:t}\)</span> and <span class="mathjax-tex">\(\mathbf {B}_{1:t}\)</span> as the <span class="mathjax-tex">\(4 \times 4t\)</span> matrices given by concatenation of the transforms</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf {A}_{1:t} = [ \mathbf {A}_1 \ldots \mathbf {A}_t ] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>and similar for <span class="mathjax-tex">\(\mathbf {B}_{1:t}\)</span>. Define an <i>invariant functional under rigid linkage</i> to be any <span class="mathjax-tex">\(\phi : M_{4,4t}(\mathbb {R}) \rightarrow \mathbb {R}\)</span> such that, for any homogeneous <span class="mathjax-tex">\(4 \times 4\)</span> matrices <span class="mathjax-tex">\(\mathbf {Y}\)</span> and <span class="mathjax-tex">\(\mathbf {X}\)</span>,</p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \mathbf {B}_i = \mathbf {Y}^{-1} \mathbf {A}_i \mathbf {X}, \forall i \in \{1,\ldots ,t\}\Rightarrow \phi (\mathbf {A}_{1:t}) = \phi (\mathbf {B}_{1:t}) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p>If we can find a <span class="mathjax-tex">\(\phi\)</span> satisfying this property, then identifying a relation between the high dimensional time series <span class="mathjax-tex">\(\mathbf {A}_{1:t}\)</span> and <span class="mathjax-tex">\(\mathbf {B}_{1:t}\)</span> is reduced to the problem of discarding any hypotheses satisfying <span class="mathjax-tex">\(\phi (\mathbf {A}_{1:t}) \ne \phi (\mathbf {B}_{1:t})\)</span>. This is simply a comparison of two one-dimensional time series.</p><p>One such <span class="mathjax-tex">\(\phi\)</span> for the simplified case of <span class="mathjax-tex">\(\mathbf {A}_t\)</span> and <span class="mathjax-tex">\(\mathbf {B}_t\)</span> representing 3x3 rotation matrices is that of the angle between the earliest and latest sample:</p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \phi _R(\mathbf {Z}_{1:t}) := \angle (\mathbf {Z}_t^{-1} \mathbf {Z}_1) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><p>where <span class="mathjax-tex">\(\angle (\mathbf {Z})\)</span> is the total angle through which <span class="mathjax-tex">\(\mathbf {Z} \in SO(3)\)</span> rotates,<sup><a href="#Fn6"><span class="u-visually-hidden">Footnote </span>6</a></sup> and is given by</p><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \angle (\mathbf {Z}) := \max _{x\in S^2} \left\{ |\cos ^{-1} \left( \left\langle x, \mathbf {Z}x \right\rangle \right) |\right\} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p>
Zhuang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554" href="/article/10.1007/s10055-018-0341-8#ref-CR25" id="ref-link-section-d14777e3597">1994</a>) provide a proof that this functional is invariant.</p><p>To measure the likeness of two candidate sensor streams with indices <i>k</i> and <i>l</i>, the <i>likelihood score</i> was calculated for the invariant functional (IF) matching method according to</p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s_t(k,l) = \exp \left( {-| \phi _R(\mathbf {A}^k_{1:t}) - \phi _R(\mathbf {B}^l_{1:t}) |_\theta ^2} \right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p>where</p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} |\alpha |_\theta := \min \{|\alpha |,2\pi -|\alpha |\} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (9)
                </div></div><p>accounts for the periodicity of <span class="mathjax-tex">\(\phi _R\)</span>. This score estimates the likelihood that the two streams are rigidly linked. The angle function <span class="mathjax-tex">\(\angle (\cdot )\)</span> was computed from the quaternion corresponding to the argument matrix.</p><h3 class="c-article__sub-heading" id="Sec6">Calibration error (CE) method</h3><p>An alternative method of matching sensor streams is through computation of the transforms <span class="mathjax-tex">\(\mathbf {Y}\)</span> and <span class="mathjax-tex">\(\mathbf {X}_{k,l}\)</span> for each possible sensor pairing, followed by comparison of each pairing based on the error given by<sup><a href="#Fn7"><span class="u-visually-hidden">Footnote </span>7</a></sup></p><div id="Equ10" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{t = 1}^{N_s} \left\| ( \mathbf {A}_t\mathbf {X})^{-1}\mathbf {Y}\mathbf {B}_t \right\| _{SE(3)}^2 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (10)
                </div></div><p>where<sup><a href="#Fn8"><span class="u-visually-hidden">Footnote </span>8</a></sup></p><div id="Equ11" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \left\| \begin{bmatrix} \mathbf {R}&amp;T \\ 0&amp;1 \end{bmatrix} \right\| _{SE(3)} := |\angle (\mathbf {R}) + ||{T}|||\end{aligned}$$</span></div><div class="c-article-equation__number">
                    (11)
                </div></div><p>and <span class="mathjax-tex">\(\angle (\cdot )\)</span> is defined by Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0341-8#Equ7">7</a> and computed from the quaternion corresponding to <span class="mathjax-tex">\(\mathbf {R}\)</span>.</p><p>A collection of queues were used to collect the data for the calibration procedure; one for each sensor pair hypothesis (<i>k</i>, <i>l</i>). The queues had maximum length <span class="mathjax-tex">\(N_s \in \mathbb {N}\)</span> , with the oldest samples discarded whenever capacity was exceeded. The sample pair <span class="mathjax-tex">\((A_t^k,B_t^l)\)</span> was inserted into queue (<i>k</i>, <i>l</i>) only when at least one of the following was satisfied</p><div id="Equ16" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned}&amp;\left\| \left( \mathbf {B}^l_{t} \right) ^{-1} \mathbf {B}^l_{t-1}\right\| _{SE(3)}&gt; \delta _s \\&amp;\left\| \left( \mathbf {A}^k_{t} \right) ^{-1} \mathbf {A}^k_{t-1}\right\| _{SE(3)} &gt; \delta _s \end{aligned}$$</span></div></div><p>where <span class="mathjax-tex">\(\delta _s\)</span> is a fixed sample threshold. When all of the queues reached maximum capacity, the robot-world and hand-eye calibration (from Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e4661">2013</a>) was performed <i>KL</i> times to produce a <span class="mathjax-tex">\(\mathbf {Y}_{k,l}\)</span> and <span class="mathjax-tex">\(\mathbf {X}_{k,l}\)</span> for each <i>k</i> and <i>l</i>. The likelihood score for the calibration error (CE) method was then computed as</p><div id="Equ12" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} s_t(k,l) = \exp \left[ - \left( \frac{1}{N_s} \sum _{t=1}^{N_s} \left\| (\mathbf {A}^k_t \mathbf {X}_{k,l})^{-1}\mathbf {Y}_{k,l}\mathbf {B}^l_t\right\| _{SE(3)} \right) ^2 \right] \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (12)
                </div></div><p>where the matrix norm <span class="mathjax-tex">\(\left\| \cdot \right\| _{SE(3)}\)</span> is defined in Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-018-0341-8#Equ11">11</a>. Pairings of sensors which are rigidly linked will have a likelihood score close to 1 (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig4">4</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig4_HTML.gif?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig4_HTML.gif" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>The CE method involves performing the calibration proposed by Shah (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e4997">2013</a>) between every pair of candidate sensors. The error is then an indicator of how likely it is that the sensors are linked—high error suggests no correlation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec7">Match selection and score filtering</h3><p>Each of the above computations produces a set of likelihood scores <span class="mathjax-tex">\(\{s_t(k,l):k=1, \ldots , K;l=1,\ldots ,L\}\)</span> estimating the likelihoods that two given 6DoF sensor streams are rigidly linked. This can be done repeatedly over time, possibly at each frame of the process. The scores were used to rank the possible linkages for each <i>k</i>, and at time step <i>t</i> the highest scoring hypothesis was chosen as the estimated correct hypothesis for sensor <span class="mathjax-tex">\(S_k\)</span> at time step <i>t</i>. Here, we assume that each sensor <span class="mathjax-tex">\(S_k\)</span> is attached to one sensor <span class="mathjax-tex">\(Q_l\)</span> from the other sensor system and no two sensors share the same parent sensor.</p><p>Additional filtering of the score was trialled in the form of <i>score accumulation</i>. The cumulative score <span class="mathjax-tex">\(\hat{s}_t(k,l)\)</span> of each hypothesis was updated with each new score value <span class="mathjax-tex">\(s_t(k,l)\)</span> such that</p><div id="Equ13" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \hat{s}_t(k,l) = \frac{s_t(k,l) \hat{s}_{t-1}(k,l)}{ \sum _{l=1}^L s_t(k,l) \hat{s}_{t-1}(k,l)} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (13)
                </div></div><p>and <span class="mathjax-tex">\(\hat{s}_0(k,l) = 1 / L\)</span>. The summation term ensures the likelihood scores are normalised across the possible matchings for sensor <i>k</i>. That is,</p><div id="Equ14" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \sum _{l=1}^L \hat{s}_t(k,l) = 1 \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (14)
                </div></div><p><i>Hypothesis elimination</i>, where hypothesis (<i>k</i>, <i>l</i>) was discarded when <span class="mathjax-tex">\(\hat{s}_t(k,l) &lt; \epsilon _s\)</span>, was also trialled. The likelihood score of any discarded hypothesis was not calculated to minimise computation time.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Simulation analysis</h2><div class="c-article-section__content" id="Sec8-content"><p>Simulated performance analysis of the calibration error (CE) and invariant functional (IF) matching algorithms were conducted. A Microsoft Kinect v1 camera was used to record a depth map of a user performing a series of poses. The OpenNI<sup><a href="#Fn9"><span class="u-visually-hidden">Footnote </span>9</a></sup> software library version 1.5.7 was used to fit a skeleton, consisting of a collection of six 6DoF rigid body poses, to the depth map. From these poses two virtual sensors were simulated, assuming rigid attachment to exactly one Kinect skeleton rigid body each. Kinect data was preferred over random rigid body motions because it provides measurements which conform to plausible human trajectories. For each simulation trial, the depth data was replayed with online skeleton fitting and virtual sensors were generated for the duration of the recording with simulation characteristics fixed for each trial. Simulated characteristics of the virtual sensors included latency, angular noise and translation noise.</p><p>The simulations were carried out on a 2.5GHz Intel Core i7 CPU with the implementation written in C++ using the Armadillo linear algebra library (Sanderson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Sanderson C (2010) Armadillo: An open source C++ linear algebra library for fast prototyping and computationally intensive experiments. Technical Report, NICTA, Australia" href="/article/10.1007/s10055-018-0341-8#ref-CR21" id="ref-link-section-d14777e5715">2010</a>). For each set of parameter values, a single simulation trial was performed. Each trial was based on 52 s of recorded Kinect depth map. The main measurement of performance was the <i>matching performance</i>, equal to the number of correct guesses divided by the total number of guesses over the trial. As there were two virtual sensors, two guesses were made whenever a new sample was recorded and there were <span class="mathjax-tex">\(N_s\)</span> samples available, leading to around 2000 guesses over each 52 s trial.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig5">5</a> profiles matching performance as a function of number of samples and noise level for the CE method. As expected, increasing the number of samples increases performance with noisy data. A graph of the computation time per hypothesis for the CE method as a function of number of samples <span class="mathjax-tex">\(N_s\)</span> is also included in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig5">5</a>. Over the interval [0, 200], the matching computation time per hypothesis <span class="mathjax-tex">\(\tau _m\)</span> is close to linear, with least squares fit yielding</p><div id="Equ15" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \tau _m (N_s) \approx (0.0018 * N_s + 0.033)\mathrm {ms} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (15)
                </div></div><p>The IF method always operates with just two samples, the latest sample and the earliest sample, and so runs in constant time after the initial samples have been gathered. Additional samples do not affect the IF method performance.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig5_HTML.gif?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig5_HTML.gif" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Simulation sample-noise and sample-timing analysis for the calibration error (CE) method. The heat map displays the fraction of correct identifications of the simulated sensors as a function of angular noise and number of samples (<i>left</i>). The sample-timing linear least squares relationship was <span class="mathjax-tex">\(\tau _m (N_s) \approx (0.0018 * N_s + 0.033)\mathrm {ms}\)</span> (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig6">6</a> shows the noise and latency performance for the IF and CE methods in the form of heat maps of matching performance. These experiments used <span class="mathjax-tex">\(N_s = 100\)</span> and <span class="mathjax-tex">\(\delta _s = 0.01\)</span> for the CE method, leading to an average total CE matching computation time of 2.57 ms, varying with standard deviation 0.35 ms. Neither accumulation filtering nor elimination was used for these experiments.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig6_HTML.gif?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig6_HTML.gif" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Simulated noise performance analysis of the calibration error (CE) and invariant functional (IF) matching algorithms. The performance is measured by the fraction of correct identifications of sensor dependencies over a 52 s simulation. Note that since the IF method only compares rotation data, it is expected to perform well independent of positional noise</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The IF method yielded a mean computation time of 0.14 ms varying with standard deviation of 0.06 ms. The simulation results suggest that sensors with sufficiently low angular measurement noise (<span class="mathjax-tex">\(\nu \le 0.5^\circ\)</span>) should be identifiable at a rate of over 60% by the CE method, even under conditions of high positional noise of <span class="mathjax-tex">\(\sigma = 10\)</span>cm. Note that the rate of correct matching given random selection is <span class="mathjax-tex">\(\frac{1}{6}\approx 0.167\)</span>. The simulation shows that the CE method has a small advantage over the IF method in terms of robustness to latency. The CE method also performs better than the IF method when there is no displacement noise present. However, that advantage diminishes when displacement noise is present. This observation is explained by the CE method discriminating between similar sensors due to its complete use of the available data. This characteristic also explains why the CE method is susceptible to noise in the translation components of the sensors. The IF method does not include position information and so is unaffected by the position error, but is also able to distinguish the sensors less effectively.</p></div></div></section><section aria-labelledby="Sec9"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9">Case study: offline experiment with real sensors</h2><div class="c-article-section__content" id="Sec9-content"><p>A Microsoft Kinect v1 depth camera was used to record depth data from a user performing a series of poses including waving of each arm, lifting of each leg and walking. The poses included facing the Kinect and facing sideways to the Kinect while walking. Each limb was also moved independently of the others at least once during the recording. Meanwhile, two 6DoF rigid bodies were tracked with an OptiTrack Flex-13 tracking system with OptiTrack’s Motive rigid body tracking software.<sup><a href="#Fn10"><span class="u-visually-hidden">Footnote </span>10</a></sup> The Kinect was also tracked with the OptiTrack system to provide the ground truth transform to the Kinect reference frame for qualitative verification of the collected data (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig8">8</a>, <i>top</i>). Example depth maps measured by the Kinect are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig7">7</a> with the sensor ground truth, OpenNI tracked skeleton and example matching results. The tracked rigid bodies were distinguishable but unlabelled and attached to the participants shoulder (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig8">8</a>, <i>bottom</i>) and opposite knee, corresponding to the knee and shoulder bones of the skeleton identified by the Kinect.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig7_HTML.gif?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig7_HTML.gif" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Data visualisation for the offline real sensor case study. <i>Left</i> shows the OptiTrack marker coordinate frames, transformed into the Kinect reference frame using ground truth (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig8">8</a>), and the detected user. A rigid body skeleton is generated with the OpenNI software platform version 1.5.7 (<i>middle</i>). The candidate Kinect rigid bodies have their corresponding coordinate frames drawn at their origins. The two 6DoF measurements from the sensors are matched with the one of six Kinect rigid body streams in real time (<i>right</i>). At this instance, the left knee (lower circle) has been correctly identified, while the right shoulder has been misidentified as the left shoulder (top circle)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The motion capture setup for measuring the Kinect reference frame for ground truth verification (<i>top</i>) and an example sensor placed on the shoulder and tracked by an OptiTrack system (<i>bottom</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>A skeleton pose was extracted from the recorded depth map using OpenNI software library version 1.5.7. The unlabeled sensors were matched to the Kinect skeleton rigid bodies using both the IF (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec5">3.1</a>) and CE matching (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec6">3.2</a>) techniques. The matching algorithms were tested with and without both score accumulation and hypothesis elimination, described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec7">3.3</a>. All computations, including skeleton tracking and sensor matching, were performed in real time at <span class="mathjax-tex">\(\ge\)</span>30Hz on a recorded depth map. For each frame, the two sensors were matched with one of six rigid bodies tracked by the Kinect: upper arms, lower arms and lower legs (below the knees). The resulting identifications were counted as correct or incorrect to calculate the matching performance for each sensor.</p><p>It was found that a latency of 76 ms existed between the OptiTrack motion capture stream and the Kinect stream. Until this was accounted for by manual correction the algorithm performed no better than chance. The parameters used in this experiment were <span class="mathjax-tex">\(N = 100\)</span> samples gathered before calibration was performed with a difference of <span class="mathjax-tex">\(\delta _s = 0.01\)</span> required between successive samples. When elimination was included, the elimination threshold was <span class="mathjax-tex">\(\epsilon _s = 0.1\)</span>.</p><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0341-8#Tab1">1</a> shows the results for the CE method. The accumulation filter increases performance by weighting previously incorrect hypotheses as less likely. However, this means it cannot recover as quickly from a change in sensor placement. Elimination with CE is very effective for this particular experiment. Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-018-0341-8#Tab2">2</a> shows results for the IF method. Elimination without accumulation for the IF method resulted in the knee sensor being identified with the wrong knee. This demonstrates how the IF method has more trouble distinguishing similar hypotheses compared to the CE method. The Kinect suffers from ambiguity in the 6DoF limb measurements and this instability has caused the IF method to perform poorly when elimination is used. The CE method is less susceptible to the instability because it uses more samples in this trial.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Calibration error (CE) method—fraction of correct identifications for real sensor experiment</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0341-8/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Invariant functional (IF) method—fraction of correct identifications for real sensor experiment</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-018-0341-8/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The best tracking result without elimination was the CE method with score accumulation. The worst matching score between the two sensors in this case was 76%. This is approximately 4.5 times better than random selection, which would score <span class="mathjax-tex">\(\frac{1}{6} \approx 16.7\%\)</span>, as there are six choices for each sensor identity. Even the worst result for the CE method scored <span class="mathjax-tex">\(40\%\)</span>, or 2.5 times better than random. Though direct comparison cannot be made, this result is similar to the results achieved by Bahle et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Bahle G, Lukowicz P, Kunze K, Kise K (2013) I see you: how to improve wearable activity recognition by leveraging information from environmental cameras. In: 2013 IEEE international conference on pervasive computing and communications workshops (PERCOM workshops), pp 409–412" href="/article/10.1007/s10055-018-0341-8#ref-CR1" id="ref-link-section-d14777e6712">2013</a>). However, our results used 6DoF sensors rather than 3DoF sensors and our study does not have comparable statistical significance.</p><p>The observation that the knee sensor matched in general better than the shoulder is explained through inspection of the skeleton which had been fitted to the depth map. The shoulder had increased ambiguity compared to the knee in that particular trial, leading to strange behaviour of the orientation of the upper arm and shoulder. As an example of instability, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig9">9</a> shows the rigid body measured for the knee in a separate trial swapping orientation by <span class="mathjax-tex">\(180^\circ\)</span> discontinuously. It is also possible that the assumption of rigidity breaks down for the data obtained from the Kinect and OpenNI.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The OpenNI Kinect skeleton tracking software has several instabilities which make it unsuitable for rigid body identification. For example, the knee orientation is ambiguous when outstretched (<i>left</i>); here the rigid body for the knee flips <span class="mathjax-tex">\(180^\circ\)</span> discontinuously (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Case study: interactive demonstration with real sensors</h2><div class="c-article-section__content" id="Sec10-content"><p>To verify the approach further, a third sensor system was created using Sony’s PlayStation Move (PS Move) controller and a PlayStation Eye (PS Eye) webcam. The open source PS Move API was used for connecting to, calibrating and tracking the PS Move controller with 6DoF (Pearl <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Pearl T (2012) Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion. Master’s thesis, Vienna University of Technology" href="/article/10.1007/s10055-018-0341-8#ref-CR19" id="ref-link-section-d14777e6802">2012</a>). The PS Move controller includes an accelerometer and gyroscope for orientation tracking. The positional tracking is performed using a webcam by tracking the screen position and size of a spherical coloured marker attached to the controller. The PS Eye webcam was used for the positional tracking. The PS Move tracking system was used to test the proposed techniques by matching the PS Move pose with that of motion capture markers from the OptiTrack motion capture system discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec9">5</a>.</p><p>A motion capture marker was used to track the position of the PS Eye camera so that the pose of the motion capture markers could be visualised on the PS Eye video feed alongside the PS Move tracking result. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig10">10</a> shows the tracking results for an OptiTrack marker and for the PS Move controller from the perspective of the PS Eye.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Tracking results for an OptiTrack marker (<i>left</i>) and a PS Move controller (<i>right</i>) from the perspective of the PS Eye camera. Note that the OptiTrack marker has been transformed to the PS Move sensor space for visualisation only; all computations use raw data in the native space</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig11">11</a> shows an example where the PS Move sensor is matched with a motion capture marker. The system was modified so that after all but one hypothesis was eliminated (according to Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec7">3.3</a>) the final hypothesis could also be eliminated if its raw score dropped below the threshold <span class="mathjax-tex">\(\epsilon _s\)</span>. If this happened, all samples would be erased, and the matching procedure was reset. This allowed the system to recover when the sensor dependencies changed.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig11_HTML.gif?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig11_HTML.gif" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Moment of sensor attachment (<i>left</i>) and moment of identification of the rigid link (<i>right</i>; visualised here with a (lower) white sphere. NB. the upper white sphere is a physical, illuminated component of the PS Move.) for the CE method. Typical time between the two events is 1–2 s, depending on the movement of the user (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig12">12</a>). The amount of movement required to match here was a <span class="mathjax-tex">\(90^\circ\)</span> rotation and a translation of about 15 cm. The IF method achieved similar results</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The system worked extremely well using either of the CE method or the IF method. Parameters which worked best were <span class="mathjax-tex">\(N_s = 10\)</span>, <span class="mathjax-tex">\(\delta _s = 0.1\)</span> and <span class="mathjax-tex">\(\epsilon _s = 0.1\)</span>. It is possible to attach sensors to the PS Move in an arbitrary rigid configuration and have them be correctly identified within 1–2 s, provided the controller is moved (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig12">12</a>). The amount of movement required to match here can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig11">11</a> to be a <span class="mathjax-tex">\(90^\circ\)</span> rotation and a translation of about 15 cm. Limitations include the assumption that exactly one sensor is attached to the controller. The problem the algorithm is solving is which of the two markers is the PS Move attached to. The sensors represent separate rigid bodies, for example, an arm and a leg. This results in false positives occasionally when neither of the sensors are attached and one false negative and instability when both are attached. This could be amended by using a slightly modified approach to elimination where sensors are eliminated based on their absolute score rather than their relative score. Further investigation is left for future work.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig12_HTML.gif?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig12_HTML.gif" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>An example trace where two motion capture markers are attached sequentially, one at a time, to the PS Move sensor. Here it is assumed that when the ground truth distance between a marker and the PS Move (<i>top</i>) is less than 30cm, the two are rigidly attached for ground truth. The matching result for the CE method (<i>bottom</i>) can be seen to match well with the ground truth estimator (<i>centre</i>). The broken and dotted lines indicate the times at which a sensor is attached and removed, respectively</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec11">Live Kinect application</h3><p>A live application of the system was created with the Microsoft Kinect, OpenNI and NiTE 2.2.<sup><a href="#Fn11"><span class="u-visually-hidden">Footnote </span>11</a></sup> The rigid bodies from the NiTE user tracker were matched against a single OptiTrack rigid body marker, as in the offline trials. The system demonstrates that the CE method is feasible for real-time identification of sensor placement on a complex articulated body. In this case, the marker is assigned one of 10 locations on the body, including upper and lower arms and legs, body and head. Time taken to identify the location of the marker ranged from 5 s on highly mobile body parts like the forearm, to 15 s when placed on less mobile parts like the shoulders or thighs. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig13">13</a> shows example screen captures of the program identifying a marker placed on the shoulder. The limitation of this system is centrally the quality of the tracked skeleton given by OpenNI: for ambiguous depth images it is impossible to correctly identify sensor placement.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig13_HTML.gif?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-018-0341-8/MediaObjects/10055_2018_341_Fig13_HTML.gif" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>An example of real-time interactive sensor matching for the Microsoft Kinect. The user is tracked with OpenNI and NiTE software libraries and the sensor’s true location (left shoulder) is drawn as 3D coordinate axes (<i>left</i>). Hypotheses are shown as small white spheres placed at the origin of each articulated body part, with the single larger sphere representing the most likely hypothesis (<i>centre</i>). After approximately 10 s of tracking, the correct location is determined and most other hypotheses are eliminated (<i>right</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-018-0341-8/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">Discussion</h2><div class="c-article-section__content" id="Sec12-content"><p>The methods examined in this work ignore some potentially useful sensor data. The first is that the matrix <span class="mathjax-tex">\(\mathbf {Y}\)</span> is actually constant for sensors belonging to the same pair of sensor systems, whereas matching assumed that <span class="mathjax-tex">\(\mathbf {Y}\)</span> would be different for each sensor pair. A limitation of the CE method is that it cannot match sensors between left and right-handed sensor systems. This can be compensated for, but requires the user to understand and identify the handedness of each system. Further development to account for this case would require modification of the calibration methods to allow <span class="mathjax-tex">\(\det (\mathbf {X}), \det (\mathbf {Y}) = \pm\, 1\)</span>, rather than restricting to <span class="mathjax-tex">\(+1\)</span>, as is the case with each of the discussed methods, i.e., Dornaika and Horaud (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–622" href="/article/10.1007/s10055-018-0341-8#ref-CR7" id="ref-link-section-d14777e7256">1998</a>), Li et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product. Int J Phys Sci 5(10):1530–1536" href="/article/10.1007/s10055-018-0341-8#ref-CR17" id="ref-link-section-d14777e7260">2010</a>), Shah (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e7263">2013</a>) and Zhuang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554" href="/article/10.1007/s10055-018-0341-8#ref-CR25" id="ref-link-section-d14777e7266">1994</a>). For example, in the method used in our system there is a step which restricts the determinants to be <span class="mathjax-tex">\(+\,1\)</span> (Shah <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007" href="/article/10.1007/s10055-018-0341-8#ref-CR24" id="ref-link-section-d14777e7295">2013</a>). It may be possible to simply compute the case of <span class="mathjax-tex">\(\det =-1\)</span>, and choose whichever has lesser calibration error.</p><p>A separate static noise analysis placed the noise values for the Kinect rigid bodies at <span class="mathjax-tex">\(\nu \approx 1^\circ\)</span>–<span class="mathjax-tex">\(2^\circ\)</span> and <span class="mathjax-tex">\(\sigma \approx 10\)</span>–20 cm. Comparing these values to Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-018-0341-8#Fig6">6</a>, we expect to see matching performance near 0.5. However, these heat maps assume that both sensors have the same values for <span class="mathjax-tex">\(\nu\)</span> and <span class="mathjax-tex">\(\sigma\)</span>. A static noise analysis for the motion capture system yielded <span class="mathjax-tex">\(\nu \approx 0.07^\circ\)</span> and <span class="mathjax-tex">\(\sigma \approx 0.1\)</span> cm. This suggests the low noise level of the motion capture system has improved the results to be better than expected for the offline case study (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-018-0341-8#Sec9">5</a>). Static noise analysis for the PS Move sensor, positioned 2 m from the tracking camera, yielded <span class="mathjax-tex">\(\nu \approx 0.5^\circ\)</span> and <span class="mathjax-tex">\(\sigma \approx 20\)</span>–50 cm, with position noise increasing with distance from the camera. This low rotation noise coupled with the low noise of the motion capture system gives very good matching results for two sensors even with relatively few samples (<span class="mathjax-tex">\(N_s = 10\)</span>).</p></div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Conclusion</h2><div class="c-article-section__content" id="Sec13-content"><p>Two techniques were analysed for their ability to identify rigidly linked 6DoF and 3DoF sensors. The invariant functional (IF) matching method compares the angle which each rigid body has rotated through since the stream began. The calibration error (CE) technique uses the error from a calibration algorithm, originally developed for industrial robots, to assess the likelihood that two sensors are rigidly linked. The methods perform well in simulation and a noise and latency analysis reveals that although the CE method is more robust, it is also slower and significantly more difficult to implement. However, both methods are fast enough to perform real-time matching, with the largest computation times on the order of a few milliseconds. It is recommended that IF is used for small systems, while CE is used when many sensors must be identified.</p><p>A case study was performed with offline data from a Microsoft Kinect v1 and an OptiTrack motion capture system. Considering the noise and instability inherent in extracting 6DoF rigid bodies from 3D depth data, excellent matching capability was achieved. The best result performed at least 4.5 times better than chance. A real-time interactive demonstration was also developed, demonstrating responsive and accurate sensor matching even when dealing with a large number of possible sensor locations. This allows the system to succeed in a very important case: locating sensors on a human skeleton, such as the skeleton tracked by the Microsoft Kinect and similar devices.</p><p>The work described here has focused on off-the-shelf interaction technologies as these are examples of accessible and widespread sensor technology. Such sensors are increasingly used together within virtual environment applications and by users whom complex manual calibration of sensors will be undesirable, if not impossible. Thus our methods represent a key step in creating highly accessible modular multi-device 3D virtual environments.</p><p>Ongoing work involves combining multiple tracking systems with a plug-and-play style interface (Fountain and Smith <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Fountain J, Smith SP (2017) Real-time ambient fusion of commodity tracking systems for virtual reality. In: International conference on artificial reality and telexistence and eurographics symposium on virtual environments (ICAT-EGVE 2017). Eurographics Association, pp 1–8" href="/article/10.1007/s10055-018-0341-8#ref-CR10" id="ref-link-section-d14777e7627">2017</a>).<sup><a href="#Fn12"><span class="u-visually-hidden">Footnote </span>12</a></sup> System combinations will include the HTC Vive with Kinect, multiple Kinects, and Leap Motion with inertial sensors. In particular, these systems will be applied to interaction in immersive interactive virtual environments.</p></div></div></section>
                        
                    

                    <section aria-labelledby="notes"><div class="c-article-section" id="notes-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="notes">Notes</h2><div class="c-article-section__content" id="notes-content"><ol class="c-article-footnote c-article-footnote--listed"><li class="c-article-footnote--listed__item" id="Fn1"><span class="c-article-footnote--listed__index">1.</span><div class="c-article-footnote--listed__content"><p>Constellation is the positional tracking system used with the Oculus Rift and Touch systems, see <a href="http://www.oculus.com">http://www.oculus.com</a>—accessed 04/01/2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn2"><span class="c-article-footnote--listed__index">2.</span><div class="c-article-footnote--listed__content"><p>The Lighthouse tracking system comprises of the two base stations for use with the HTC Vive virtual reality headset, see <a href="http://www.vive.com">http://www.vive.com</a>—accessed 04/01/2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn3"><span class="c-article-footnote--listed__index">3.</span><div class="c-article-footnote--listed__content"><p>See <a href="http://neuronmocap.com/products/perception_neuron">http://neuronmocap.com/products/perception_neuron</a>—accessed 04/01/2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn4"><span class="c-article-footnote--listed__index">4.</span><div class="c-article-footnote--listed__content"><p>The Microsoft Kinect is a consumer grade infrared structured light depth camera originally developed for gaming applications (<a href="http://dev.windows.com/en-us/kinect">http://dev.windows.com/en-us/kinect</a>—accessed 04/01/2018).</p></div></li><li class="c-article-footnote--listed__item" id="Fn5"><span class="c-article-footnote--listed__index">5.</span><div class="c-article-footnote--listed__content"><p>In real sensors, there will likely be different latency and sampling rates between the sensors. Here, we consider that data, for each time step <i>t</i>, is sampled in synchronisation with the lowest update rate and using the latest data from the faster sensors. Also we assume approximately synchronous samples.</p></div></li><li class="c-article-footnote--listed__item" id="Fn6"><span class="c-article-footnote--listed__index">6.</span><div class="c-article-footnote--listed__content"><p><i>SO</i>(3), or <i>special orthogonal group</i>, is the group that represents rotations in 3-dimensional Euclidean space <span class="mathjax-tex">\(\mathbb {R}^3\)</span>.</p></div></li><li class="c-article-footnote--listed__item" id="Fn7"><span class="c-article-footnote--listed__index">7.</span><div class="c-article-footnote--listed__content"><p><i>SE</i>(3), or <i>special Euclidean group</i>, is the symmetry group of 3-dimensional Euclidean space.</p></div></li><li class="c-article-footnote--listed__item" id="Fn8"><span class="c-article-footnote--listed__index">8.</span><div class="c-article-footnote--listed__content"><p>This is not actually a norm as <span class="mathjax-tex">\(\angle (\mathbf {R})\)</span> fails the triangle inequality and multiplicativity due to its periodicity. However, definiteness and positivity are the only properties needed for the purpose of measuring the size of the rotation of a matrix.</p></div></li><li class="c-article-footnote--listed__item" id="Fn9"><span class="c-article-footnote--listed__index">9.</span><div class="c-article-footnote--listed__content"><p>OpenNI is an open source API for depth cameras and other natural input devices (<a href="http://github.com/OpenNI/OpenNI">http://github.com/OpenNI/OpenNI</a>—accessed 04/01/2018).</p></div></li><li class="c-article-footnote--listed__item" id="Fn10"><span class="c-article-footnote--listed__index">10.</span><div class="c-article-footnote--listed__content"><p><a href="http://www.optitrack.com">http://www.optitrack.com</a>—accessed 04/01/2018.</p></div></li><li class="c-article-footnote--listed__item" id="Fn11"><span class="c-article-footnote--listed__index">11.</span><div class="c-article-footnote--listed__content"><p>NiTE open source tracking middleware: <a href="http://openni.ru/files/nite/">http://openni.ru/files/nite/</a> (accessed 04/01/2018).</p></div></li><li class="c-article-footnote--listed__item" id="Fn12"><span class="c-article-footnote--listed__index">12.</span><div class="c-article-footnote--listed__content"><p>A beta Unreal Engine 4 plug-in for skeleton-centered virtual reality sensor fusion based on this work is available at <a href="http://github.com/JakeFountain/Spooky">http://github.com/JakeFountain/Spooky</a>—accessed 08/01/2018.</p></div></li></ol></div></div></section><section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bahle G, Lukowicz P, Kunze K, Kise K (2013) I see you: how to improve wearable activity recognition by leverag" /><p class="c-article-references__text" id="ref-CR1">Bahle G, Lukowicz P, Kunze K, Kise K (2013) I see you: how to improve wearable activity recognition by leveraging information from environmental cameras. In: 2013 IEEE international conference on pervasive computing and communications workshops (PERCOM workshops), pp 409–412</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Banos O, Calatroni A, Damas M, Pomares H, Rojas I, Sagha H, del R Milln J, Tröster G, Chavarriaga R, Roggen D " /><p class="c-article-references__text" id="ref-CR2">Banos O, Calatroni A, Damas M, Pomares H, Rojas I, Sagha H, del R Milln J, Tröster G, Chavarriaga R, Roggen D (2012) Kinect= IMU? Learning MIMO signal mappings to automatically translate activity recognition systems across sensor modalities. In: 2012 16th international symposium on wearable computers (ISWC). IEEE, pp 92–99</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Calatroni A, Roggen D, Tröster G (2010) A methodology to use unknown new sensors for activity recognition by l" /><p class="c-article-references__text" id="ref-CR3">Calatroni A, Roggen D, Tröster G (2010) A methodology to use unknown new sensors for activity recognition by leveraging sporadic interactions with primitive sensors and behavioral assumptions. Eidgenössische Technische Hochschule Zürich, D-ITET, Institut für Elektronik</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Chavarriaga, H. Bayati, JdR. Milln, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Chavarriaga R, Bayati H, Milln JdR (2013) Unsupervised adaptation for acceleration-based activity recognition:" /><p class="c-article-references__text" id="ref-CR4">Chavarriaga R, Bayati H, Milln JdR (2013) Unsupervised adaptation for acceleration-based activity recognition: robustness to sensor displacement and rotation. Pers Ubiquitous Comput 17(3):479–490</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00779-011-0493-y" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unsupervised%20adaptation%20for%20acceleration-based%20activity%20recognition%3A%20robustness%20to%20sensor%20displacement%20and%20rotation&amp;journal=Pers%20Ubiquitous%20Comput&amp;volume=17&amp;issue=3&amp;pages=479-490&amp;publication_year=2013&amp;author=Chavarriaga%2CR&amp;author=Bayati%2CH&amp;author=Milln%2CJdR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Deng, N. Jiang, J. Chang, S. Guo, JJ. Zhang, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Deng S, Jiang N, Chang J, Guo S, Zhang JJ (2017) Understanding the impact of multimodal interaction using gaze" /><p class="c-article-references__text" id="ref-CR5">Deng S, Jiang N, Chang J, Guo S, Zhang JJ (2017) Understanding the impact of multimodal interaction using gaze informed mid-air gesture control in 3D virtual objects manipulation. Int J Human-Comput Stud 105(Supplement C):68–80</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.ijhcs.2017.04.002" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20the%20impact%20of%20multimodal%20interaction%20using%20gaze%20informed%20mid-air%20gesture%20control%20in%203D%20virtual%20objects%20manipulation&amp;journal=Int%20J%20Human-Comput%20Stud&amp;volume=105&amp;issue=Supplement%20C&amp;pages=68-80&amp;publication_year=2017&amp;author=Deng%2CS&amp;author=Jiang%2CN&amp;author=Chang%2CJ&amp;author=Guo%2CS&amp;author=Zhang%2CJJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Destelle F, Ahmadi A, O’Connor N, Moran K, Chatzitofis A, Zarpalas D, Daras P (2014) Low-cost accurate skeleto" /><p class="c-article-references__text" id="ref-CR6">Destelle F, Ahmadi A, O’Connor N, Moran K, Chatzitofis A, Zarpalas D, Daras P (2014) Low-cost accurate skeleton tracking based on fusion of kinect and wearable inertial sensors. In: 2014 Proceedings of the 22nd European signal processing conference (EUSIPCO), pp 371–375</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="F. Dornaika, R. Horaud, " /><meta itemprop="datePublished" content="1998" /><meta itemprop="headline" content="Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–6" /><p class="c-article-references__text" id="ref-CR7">Dornaika F, Horaud R (1998) Simultaneous robot-world and hand-eye calibration. IEEE Trans Robot Autom 14:617–622</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.704233" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20robot-world%20and%20hand-eye%20calibration&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=14&amp;pages=617-622&amp;publication_year=1998&amp;author=Dornaika%2CF&amp;author=Horaud%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Forster K, Roggen D, Tröster G (2009) Unsupervised classifier self-calibration through repeated context occure" /><p class="c-article-references__text" id="ref-CR8">Forster K, Roggen D, Tröster G (2009) Unsupervised classifier self-calibration through repeated context occurences: is there robustness against sensor displacement to gain? In: International symposium on wearable computers (ISWC), pp 77–84</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fountain J, Smith SP (2016) Automatic identification of rigidly linked 6DoF sensors. In: IEEE virtual reality " /><p class="c-article-references__text" id="ref-CR9">Fountain J, Smith SP (2016) Automatic identification of rigidly linked 6DoF sensors. In: IEEE virtual reality 2016. IEEE, pp 175–176</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Fountain J, Smith SP (2017) Real-time ambient fusion of commodity tracking systems for virtual reality. In: In" /><p class="c-article-references__text" id="ref-CR10">Fountain J, Smith SP (2017) Real-time ambient fusion of commodity tracking systems for virtual reality. In: International conference on artificial reality and telexistence and eurographics symposium on virtual environments (ICAT-EGVE 2017). Eurographics Association, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gottschalk S, Hughes JF (1993) Autocalibration for virtual environment tracking hardware. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR11">Gottschalk S, Hughes JF (1993) Autocalibration for virtual environment tracking hardware. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques. ACM, New York, NY, USA, SIGGRAPH ’93, pp 65–72</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kunze K, Lukowicz P (2008) Dealing with sensor displacement in motion-based onbody activity recognition system" /><p class="c-article-references__text" id="ref-CR12">Kunze K, Lukowicz P (2008) Dealing with sensor displacement in motion-based onbody activity recognition systems. In: Proceedings of the 10th international conference on ubiquitous computing. ACM, pp 20–29</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kunze K, Lukowicz P, Junker H, Trster G (2005) Where am I: recognizing on-body positions of wearable sensors. " /><p class="c-article-references__text" id="ref-CR13">Kunze K, Lukowicz P, Junker H, Trster G (2005) Where am I: recognizing on-body positions of wearable sensors. In: Strang T, Linnhoff-Popien C (eds) Location- and context-awareness, no. 3479 in Lecture notes in computer science. Springer, Berlin, pp 264–275</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kunze K, Lukowicz P, Partridge K, Begole B (2009) Which way am I facing: inferring horizontal device orientati" /><p class="c-article-references__text" id="ref-CR14">Kunze K, Lukowicz P, Partridge K, Begole B (2009) Which way am I facing: inferring horizontal device orientation from an accelerometer signal. In: International symposium on wearable computers, 2009. ISWC ’09, pp 149–150</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="JJ. LaViola, E. Kruijff, RP. McMahan, DA. Bowman, I. Poupyrev, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="LaViola JJ Jr, Kruijff E, McMahan RP, Bowman DA, Poupyrev I (2017) 3D user interfaces: theory and practice, 2n" /><p class="c-article-references__text" id="ref-CR15">LaViola JJ Jr, Kruijff E, McMahan RP, Bowman DA, Poupyrev I (2017) 3D user interfaces: theory and practice, 2nd edn. Addison-Wesley, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=3D%20user%20interfaces%3A%20theory%20and%20practice&amp;publication_year=2017&amp;author=LaViola%2CJJ&amp;author=Kruijff%2CE&amp;author=McMahan%2CRP&amp;author=Bowman%2CDA&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Lester, B. Hannaford, G. Borriello, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Lester J, Hannaford B, Borriello G (2004) “Are You with Me?”—Using accelerometers to determine if two devices " /><p class="c-article-references__text" id="ref-CR16">Lester J, Hannaford B, Borriello G (2004) “Are You with Me?”—Using accelerometers to determine if two devices are carried by the same person. In: Ferscha A, Mattern F (eds) Pervasive computing. Springer, Berlin, pp 33–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Pervasive%20computing&amp;pages=33-50&amp;publication_year=2004&amp;author=Lester%2CJ&amp;author=Hannaford%2CB&amp;author=Borriello%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Li, L. Wang, D. Wu, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kroneck" /><p class="c-article-references__text" id="ref-CR17">Li A, Wang L, Wu D (2010) Simultaneous robot-world and hand-eye calibration using dual-quaternions and Kronecker product. Int J Phys Sci 5(10):1530–1536</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20robot-world%20and%20hand-eye%20calibration%20using%20dual-quaternions%20and%20Kronecker%20product&amp;journal=Int%20J%20Phys%20Sci&amp;volume=5&amp;issue=10&amp;pages=1530-1536&amp;publication_year=2010&amp;author=Li%2CA&amp;author=Wang%2CL&amp;author=Wu%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Moser, Y. Itoh, K. Oshima, J. Swan, G. Klinker, C. Sandor, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Moser K, Itoh Y, Oshima K, Swan J, Klinker G, Sandor C (2015) Subjective evaluation of a semi-automatic optica" /><p class="c-article-references__text" id="ref-CR18">Moser K, Itoh Y, Oshima K, Swan J, Klinker G, Sandor C (2015) Subjective evaluation of a semi-automatic optical see-through head-mounted display calibration technique. IEEE Trans Vis Comput Graph 21(4):491–500</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2015.2391856" aria-label="View reference 18">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 18 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Subjective%20evaluation%20of%20a%20semi-automatic%20optical%20see-through%20head-mounted%20display%20calibration%20technique&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=21&amp;issue=4&amp;pages=491-500&amp;publication_year=2015&amp;author=Moser%2CK&amp;author=Itoh%2CY&amp;author=Oshima%2CK&amp;author=Swan%2CJ&amp;author=Klinker%2CG&amp;author=Sandor%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Pearl T (2012) Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion. Ma" /><p class="c-article-references__text" id="ref-CR19">Pearl T (2012) Cross-platform tracking of a 6DoF motion controller using computer vision and sensor fusion. Master’s thesis, Vienna University of Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Plopski, Y. Itoh, C. Nitschke, K. Kiyokawa, G. Klinker, H. Takemura, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Plopski A, Itoh Y, Nitschke C, Kiyokawa K, Klinker G, Takemura H (2015) Corneal-imaging calibration for optica" /><p class="c-article-references__text" id="ref-CR20">Plopski A, Itoh Y, Nitschke C, Kiyokawa K, Klinker G, Takemura H (2015) Corneal-imaging calibration for optical see-through head-mounted displays. IEEE Trans Vis Comput Graph 21(4):481–490</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTVCG.2015.2391857" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Corneal-imaging%20calibration%20for%20optical%20see-through%20head-mounted%20displays&amp;journal=IEEE%20Trans%20Vis%20Comput%20Graph&amp;volume=21&amp;issue=4&amp;pages=481-490&amp;publication_year=2015&amp;author=Plopski%2CA&amp;author=Itoh%2CY&amp;author=Nitschke%2CC&amp;author=Kiyokawa%2CK&amp;author=Klinker%2CG&amp;author=Takemura%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sanderson C (2010) Armadillo: An open source C++ linear algebra library for fast prototyping and computational" /><p class="c-article-references__text" id="ref-CR21">Sanderson C (2010) Armadillo: An open source C++ linear algebra library for fast prototyping and computationally intensive experiments. Technical Report, NICTA, Australia</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Schapansky K (2014) Jester: a device abstraction and data fusion API for skeletal tracking. Master’s Thesis, C" /><p class="c-article-references__text" id="ref-CR22">Schapansky K (2014) Jester: a device abstraction and data fusion API for skeletal tracking. Master’s Thesis, California Polytechnic State University</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Shah, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Shah M (2011) Comparing two sets of corresponding six degree of freedom data. Comput Vis Image Underst 115(10)" /><p class="c-article-references__text" id="ref-CR23">Shah M (2011) Comparing two sets of corresponding six degree of freedom data. Comput Vis Image Underst 115(10):1355–1362</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cviu.2011.05.007" aria-label="View reference 23">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 23 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparing%20two%20sets%20of%20corresponding%20six%20degree%20of%20freedom%20data&amp;journal=Comput%20Vis%20Image%20Underst&amp;volume=115&amp;issue=10&amp;pages=1355-1362&amp;publication_year=2011&amp;author=Shah%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Shah, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5" /><p class="c-article-references__text" id="ref-CR24">Shah M (2013) Solving the robot-world/hand-eye calibration problem using the Kronecker product. J Mech Robot 5(3):031,007–031,007</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1115%2F1.4024473" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Solving%20the%20robot-world%2Fhand-eye%20calibration%20problem%20using%20the%20Kronecker%20product&amp;journal=J%20Mech%20Robot&amp;volume=5&amp;issue=3&amp;pages=031%2C007-031%2C007&amp;publication_year=2013&amp;author=Shah%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Zhuang, ZS. Roth, R. Sudhakar, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneo" /><p class="c-article-references__text" id="ref-CR25">Zhuang H, Roth ZS, Sudhakar R (1994) Simultaneous robot/world and tool/flange calibration by solving homogeneous transformation equations of the form AX = YB. IEEE Trans Robot Autom 10(4):549–554</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F70.313105" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simultaneous%20robot%2Fworld%20and%20tool%2Fflange%20calibration%20by%20solving%20homogeneous%20transformation%20equations%20of%20the%20form%20AX%20%3D%20YB&amp;journal=IEEE%20Trans%20Robot%20Autom&amp;volume=10&amp;issue=4&amp;pages=549-554&amp;publication_year=1994&amp;author=Zhuang%2CH&amp;author=Roth%2CZS&amp;author=Sudhakar%2CR">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-018-0341-8-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by an Australian Postgraduate Allowance Scholarship and the Newcastle Robotics Laboratory at The University of Newcastle, Australia.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">School of Electrical Engineering and Computing, The University of Newcastle, Callaghan, New South Wales, 2308, Australia</p><p class="c-article-author-affiliation__authors-list">Jake Fountain &amp; Shamus P. Smith</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jake-Fountain"><span class="c-article-authors-search__title u-h3 js-search-name">Jake Fountain</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jake+Fountain&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jake+Fountain" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jake+Fountain%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Shamus_P_-Smith"><span class="c-article-authors-search__title u-h3 js-search-name">Shamus P. Smith</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Shamus P.+Smith&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shamus P.+Smith" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shamus P.+Smith%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-018-0341-8/email/correspondent/c1/new">Shamus P. Smith</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Detecting%20rigid%20links%20between%20sensors%20for%20automatic%20sensor%20space%20alignment%20in%20virtual%20environments&amp;author=Jake%20Fountain%20et%20al&amp;contentID=10.1007%2Fs10055-018-0341-8&amp;publication=1359-4338&amp;publicationDate=2018-03-31&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-018-0341-8" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-018-0341-8" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Fountain, J., Smith, S.P. Detecting rigid links between sensors for automatic sensor space alignment in virtual environments.
                    <i>Virtual Reality</i> <b>23, </b>71–84 (2019). https://doi.org/10.1007/s10055-018-0341-8</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-018-0341-8.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2017-01-22">22 January 2017</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03-12">12 March 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2018-03-31">31 March 2018</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-03-05">05 March 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-018-0341-8" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-018-0341-8</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Tracking</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Input devices</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Calibration</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Usability</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Sensors</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-018-0341-8.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=341;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

