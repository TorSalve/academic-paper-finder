<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Gesture-based interactive augmented reality content authoring system u"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/20/1.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Gesture-based interactive augmented reality content authoring system using HMD"/>

    <meta name="dc.source" content="Virtual Reality 2016 20:1"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2016-01-12"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2016 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a marker via gestures. The purpose of this system is to simplify augmented reality (AR) technology usage for ordinary users, especially parents and preschool children who are unfamiliar with AR technology. The system provides an immersive AR environment with a head-mounted display and recognizes users&#8217; gestures via an RGB-D camera. Users can freely create the AR content that they will be using without any special programming ability simply by connecting virtual objects stored in a database to the system. Following recognition of the marker via the system&#8217;s RGB-D camera worn by the user, he/she can apply various interactive features to the marker-based AR content using simple gestures. Interactive features applied to AR content can enlarge, shrink, rotate, and transfer virtual objects with hand gestures. In addition to this gesture-interactive feature, the proposed system also allows for tangible interaction using markers. The AR content that the user edits is stored in a database, and is retrieved whenever the markers are recognized. The results of comparative experiments conducted indicate that the proposed system is easier to use and has a higher interaction satisfaction level than AR environments such as fixed-monitor and touch-based interaction on mobile screens."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2016-01-12"/>

    <meta name="prism.volume" content="20"/>

    <meta name="prism.number" content="1"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="57"/>

    <meta name="prism.endingPage" content="69"/>

    <meta name="prism.copyright" content="2016 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-016-0282-z"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-016-0282-z"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-016-0282-z.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-016-0282-z"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Gesture-based interactive augmented reality content authoring system using HMD"/>

    <meta name="citation_volume" content="20"/>

    <meta name="citation_issue" content="1"/>

    <meta name="citation_publication_date" content="2016/03"/>

    <meta name="citation_online_date" content="2016/01/12"/>

    <meta name="citation_firstpage" content="57"/>

    <meta name="citation_lastpage" content="69"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-016-0282-z"/>

    <meta name="DOI" content="10.1007/s10055-016-0282-z"/>

    <meta name="citation_doi" content="10.1007/s10055-016-0282-z"/>

    <meta name="description" content="This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply inte"/>

    <meta name="dc.creator" content="Jinwook Shim"/>

    <meta name="dc.creator" content="Yoonsik Yang"/>

    <meta name="dc.creator" content="Nahyung Kang"/>

    <meta name="dc.creator" content="Jonghoon Seo"/>

    <meta name="dc.creator" content="Tack-Don Han"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="AndAR (2014) AndAR-Android augmented reality. 
                    http://code.google.com/p/andar/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="ARToolKit (2014) ARToolKit. 
                    http://www.hitl.washington.edu/artoolkit/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="citation_journal_title=Presence; citation_title=A survey of augmented reality; citation_author=RT Azuma; citation_volume=6; citation_issue=4; citation_publication_date=1997; citation_pages=355-385; citation_doi=10.1162/pres.1997.6.4.355; citation_id=CR3"/>

    <meta name="citation_reference" content="Azuma R, Bishop G (1994) Improving static and dynamic registration in an optical see-through HMD. In: Proceedings of the 21st ACM annual conference on computer graphics and interactive techniques, pp 197&#8211;204"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Recent advances in augmented reality; citation_author=R Azuma, Y Baillot, R Behringer, S Feiner, S Julier, B MacIntyre; citation_volume=21; citation_issue=6; citation_publication_date=2001; citation_pages=34-47; citation_doi=10.1109/38.963459; citation_id=CR5"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=Dynamic registration correction in video-based augmented reality systems; citation_author=M Bajura, U Neumann; citation_volume=15; citation_issue=5; citation_publication_date=1995; citation_pages=52-60; citation_doi=10.1109/38.403828; citation_id=CR6"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Comput Graph Appl; citation_title=The magicbook-moving seamlessly between reality and virtuality; citation_author=M Billinghurst, H Kato, I Poupyrev; citation_volume=21; citation_issue=3; citation_publication_date=2001; citation_pages=6-8; citation_id=CR7"/>

    <meta name="citation_reference" content="Billinghurst M, Hakkarainen M, Woodward C (2008) Augmented assembly using a mobile phone. In: Proceedings of the 7th ACM international conference on mobile and ubiquitous multimedia, pp 84&#8211;87"/>

    <meta name="citation_reference" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingARtips: Gesture based direct manipulation in augmented reality. In: Proceedings of the 2nd ACM international conference on computer graphics and interactive techniques in Australasia and South East Asia, pp 212&#8211;221"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=In-situ interactive image-based model building for augmented reality from a handheld device; citation_author=P Bunnun, S Subramanian, WW Mayol-Cuevas; citation_volume=17; citation_issue=2; citation_publication_date=2013; citation_pages=137-146; citation_doi=10.1007/s10055-011-0206-x; citation_id=CR10"/>

    <meta name="citation_reference" content="Coquillart S, G&#246;bel M (2004) Authoring of mixed reality applications including multi-marker calibration for mobile devices. In: Eurographics symposium on virtual environments, pp 1&#8211;9"/>

    <meta name="citation_reference" content="citation_title=Hand shape and 3D pose estimation using depth data from a single cluttered frame; citation_inbook_title=Advances in visual computing; citation_publication_date=2012; citation_pages=148-158; citation_id=CR12; citation_author=P Doliotis; citation_author=V Athitsos; citation_author=D Kosmopoulos; citation_author=S Perantonis; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Dorfmuller-Ulhaas K, Schmalstieg D (2001) Finger tracking for interaction in augmented environments. In: Proceedings IEEE and ACM international symposium on augmented reality, pp 55&#8211;64"/>

    <meta name="citation_reference" content="Grimm P, Haller M, Paelke V, Reinhold S, Reimann C, Zauner R (2002) AMIRE-authoring mixed reality. In: The first IEEE international workshop augmented reality toolkit, pp 2&#8211;pp"/>

    <meta name="citation_reference" content="Hackenberg G, McCall R, Broll W (2011) Lightweight palm and finger tracking for real-time 3d gesture control. In: 2011 IEEE virtual reality conference (VR), pp 19&#8211;26"/>

    <meta name="citation_reference" content="Harviainen T, Korkalo O, Woodward C (2009) Camera-based interactions for augmented reality. In: Proceedings of the ACM international conference on advances in computer entertainment technology, pp 307&#8211;310"/>

    <meta name="citation_reference" content="Henrysson A, Billinghurst M (2007) Using a mobile phone for 6 DOF mesh editing. In: Proceedings of the 8th ACM SIGCHI New Zealand chapter&#8217;s international conference on computer-human interaction: design centered HCI, pp 9&#8211;16"/>

    <meta name="citation_reference" content="Hoff WA, Nguyen K, Lyon T (1996) Computer-vision-based registration techniques for augmented reality. In: Photonics East&#8217;96, international society for optics and photonics, pp 538&#8211;548"/>

    <meta name="citation_reference" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings IWAR&#8217;99, pp 85&#8211;94"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ISAR; citation_title=Virtual object manipulation on a table-top AR environment; citation_author=H Kato, M Billinghurst, I Poupyrev, K Imamoto, K Tachibana; citation_volume=2000; citation_publication_date=2000; citation_pages=111-119; citation_id=CR20"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graph; citation_title=Mathematics and geometry education with collaborative augmented reality; citation_author=H Kaufmann, D Schmalstieg; citation_volume=27; citation_issue=3; citation_publication_date=2003; citation_pages=339-345; citation_doi=10.1016/S0097-8493(03)00028-1; citation_id=CR21"/>

    <meta name="citation_reference" content="citation_journal_title=Pers Ubiquit Comput; citation_title=Sketching up the world: in situ authoring for mobile augmented reality; citation_author=T Langlotz, S Mooslechner, S Zollmann, C Degendorfer, G Reitmayr, D Schmalstieg; citation_volume=16; citation_issue=6; citation_publication_date=2012; citation_pages=623-630; citation_doi=10.1007/s00779-011-0430-0; citation_id=CR22"/>

    <meta name="citation_reference" content="Ledermann F, Schmalstieg D (2005) April: a high-level framework for creating augmented reality presentations. In: Proceedings. VR 2005, pp 187&#8211;194"/>

    <meta name="citation_reference" content="Lee T, Hollerer T (2007) Handy AR: Markerless inspection of augmented reality objects using fingertip tracking. In: 11th IEEE international symposium on wearable computers, pp 83&#8211;90"/>

    <meta name="citation_reference" content="Lee U, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Proceedings of Asia Pacific conference on computing human interact, pp 705&#8211;706"/>

    <meta name="citation_reference" content="Lee GA, Nelles C, Billinghurst M, Kim GJ (2004) Immersive authoring of tangible augmented reality applications. In: Proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality, pp 172&#8211;181"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ICVRV; citation_title=Touchablear: a new experience of augmented reality; citation_author=D Li, D Weng, Y Li, J Xie; citation_volume=2013; citation_publication_date=2013; citation_pages=37-42; citation_id=CR27"/>

    <meta name="citation_reference" content="Liang H, Yuan J, Thalmann D (2012) 3D fingertip and palm tracking in depth image sequences. In: Proceedings of the 20th ACM international conference on Multimedia, pp 785&#8211;788"/>

    <meta name="citation_reference" content="citation_journal_title=Usability Interface; citation_title=Measuring usability with the use questionnaire; citation_author=A Lund; citation_volume=8; citation_issue=2; citation_publication_date=2001; citation_pages=3-6; citation_id=CR29"/>

    <meta name="citation_reference" content="MacIntyre B, Gandy M, Dow S, Bolter JD (2004) DART: A toolkit for rapid design exploration of augmented reality experiences. In: Proceedings of the 17th annual ACM symposium on user interface software and technology, pp 197&#8211;206"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ISMAR; citation_title=Designing and comparing two-handed gestures to confirm links between user controlled objects; citation_author=P Maier, M Tonnis, G Klinker; citation_volume=2010; citation_publication_date=2010; citation_pages=251-252; citation_id=CR31"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Interact Design Manuf (IJIDeM); citation_title=GARDE: a gesture-based augmented reality design evaluation system; citation_author=L Ng, S Oon, S Ong, A Nee; citation_volume=5; citation_issue=2; citation_publication_date=2011; citation_pages=85-94; citation_doi=10.1007/s12008-011-0117-9; citation_id=CR32"/>

    <meta name="citation_reference" content="NyARToolKit (2014) NyARToolKit. 
                    http://arforglass.org/?project=nyartoolkit/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="Oculus VR (2014) Oculus VR developer center. 
                    http://developer.oculusvr.com/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="citation_journal_title=Proc BMVC; citation_title=Efficient model-based 3D tracking of hand articulations using Kinect; citation_author=I Oikonomidis, N Kyriazis, AA Argyros; citation_volume=2011; citation_publication_date=2011; citation_pages=1-11; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=Proc CVPR; citation_title=Tracking the articulated motion of two strongly interacting hands; citation_author=I Oikonomidis, N Kyriazis, AA Argyros; citation_volume=2012; citation_publication_date=2012; citation_pages=1862-1869; citation_id=CR36"/>

    <meta name="citation_reference" content="Okuma T, Kiyokawa K, Takemura H, Yokoya N (1998) An augmented reality system using a real-time vision based registration. In: Proceedings fourteenth international conference on pattern recognition, vol 2, pp 1226&#8211;1229"/>

    <meta name="citation_reference" content="citation_journal_title=CIRP Ann-Manuf Technol; citation_title=Augmented assembly technologies based on 3D bare-hand interaction; citation_author=S Ong, Z Wang; citation_volume=60; citation_issue=1; citation_publication_date=2011; citation_pages=1-4; citation_doi=10.1016/j.cirp.2011.03.001; citation_id=CR38"/>

    <meta name="citation_reference" content="OpenNI (2014) OpenNI. 
                    https://github.com/OpenNI/OpenNI/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="citation_journal_title=Computer; citation_title=Developing a generic augmented-reality interface; citation_author=I Poupyrev, DS Tan, M Billinghurst, H Kato, H Regenbrecht, N Tetsutani; citation_volume=35; citation_issue=3; citation_publication_date=2002; citation_pages=44-50; citation_doi=10.1109/2.989929; citation_id=CR40"/>

    <meta name="citation_reference" content="QCAR (2014) QCAR. 
                    http://developer.qualcomm.com/dev/augmented-reality/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="Radu I, MacIntyre B (2009) Augmented-reality scratch: a tangible programming environment for children. In: Proceedings of conference on interaction design for children, Como, Italy"/>

    <meta name="citation_reference" content="Ren Z, Meng J, Yuan J, Zhang Z (2011a) Robust hand gesture recognition with Kinect sensor. In: Proceedings of the 19th ACM international conference on Multimedia, pp 759&#8211;760"/>

    <meta name="citation_reference" content="Ren Z, Yuan J, Zhang Z (2011b) Robust hand gesture recognition based on finger-earth mover&#8217;s distance with a commodity depth camera. In: Proceedings of the 19th ACM international conference on Multimedia, pp 1093&#8211;1096"/>

    <meta name="citation_reference" content="citation_journal_title=Presence Teleoper Virtual Environ; citation_title=The studierstube augmented reality project; citation_author=D Schmalstieg, A Fuhrmann, G Hesina, Z Szalav&#225;ri, LM Encarna&#231;ao, M Gervautz, W Purgathofer; citation_volume=11; citation_issue=1; citation_publication_date=2002; citation_pages=33-54; citation_doi=10.1162/105474602317343640; citation_id=CR45"/>

    <meta name="citation_reference" content="Seichter H, Looser J, Billinghurst M (2008) Composar: An intuitive tool for authoring AR applications. In: Proceedings of the 7th IEEE/ACM international symposium on mixed and augmented reality, pp 177&#8211;178"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ICCE; citation_title=Interactive features based augmented reality authoring tool; citation_author=J Shim, M Kong, Y Yang, J Seo, T-D Han; citation_volume=2014; citation_publication_date=2014; citation_pages=47-50; citation_id=CR47"/>

    <meta name="citation_reference" content="SoftKinetic (2014) SoftKinetic: the interface is you. 
                    http://www.softkinetic.com/
                    
                  . Accessed 30 May 2014"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Glove based user interaction techniques for augmented reality in an outdoor environment; citation_author=BH Thomas, W Piekarski; citation_volume=6; citation_issue=3; citation_publication_date=2002; citation_pages=167-180; citation_doi=10.1007/s100550200017; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Proc N Z Comput Sci Res Stud Conf; citation_title=An authoring tool for mobile phone AR environments; citation_author=Y Wang, T Langlotz, M Billinghurst, T Bell; citation_volume=9; citation_publication_date=2009; citation_pages=1-4; citation_id=CR50"/>

    <meta name="citation_reference" content="Whitton MC, Pisano ED, Fuchs H (1996) Technologies for augmented reality systems: Realizing ultrasound-guided needle biopsies. In: Proceedings of the 23rd annual conference on computer graphics and interactive techniques, pp 439&#8211;446"/>

    <meta name="citation_reference" content="Wilson AD (2010) Using a depth camera as a touch sensor. In: Proceedings ACM international conference on interactive tabletops and surfaces, pp 69&#8211;72"/>

    <meta name="citation_reference" content="Wozniewski M, Warne P (2011) Towards in situ authoring of augmented reality content. In: Proceedings ISMAR 2011"/>

    <meta name="citation_reference" content="citation_journal_title=Proc ICME; citation_title=Real-time hand pose estimation from RGB-D sensor; citation_author=Y Yao, Y Fu; citation_volume=2012; citation_publication_date=2012; citation_pages=705-710; citation_id=CR54"/>

    <meta name="citation_author" content="Jinwook Shim"/>

    <meta name="citation_author_email" content="jin99foryou@msl.yonsei.ac.kr"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Yonsei University, Seoul, Korea"/>

    <meta name="citation_author" content="Yoonsik Yang"/>

    <meta name="citation_author_email" content="yoonsikyang@msl.yonsei.ac.kr"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Yonsei University, Seoul, Korea"/>

    <meta name="citation_author" content="Nahyung Kang"/>

    <meta name="citation_author_email" content="nahyung.kang@msl.yonsei.ac.kr"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Yonsei University, Seoul, Korea"/>

    <meta name="citation_author" content="Jonghoon Seo"/>

    <meta name="citation_author_institution" content="Software Platform R&amp;D Lab, LG Electronics Advanced Research Institute, Seoul, Korea"/>

    <meta name="citation_author" content="Tack-Don Han"/>

    <meta name="citation_author_email" content="hantack55@gmail.com"/>

    <meta name="citation_author_institution" content="Department of Computer Science, Yonsei University, Seoul, Korea"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-016-0282-z&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2016/03/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-016-0282-z"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Gesture-based interactive augmented reality content authoring system using HMD"/>
        <meta property="og:description" content="This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a marker via gestures. The purpose of this system is to simplify augmented reality (AR) technology usage for ordinary users, especially parents and preschool children who are unfamiliar with AR technology. The system provides an immersive AR environment with a head-mounted display and recognizes users’ gestures via an RGB-D camera. Users can freely create the AR content that they will be using without any special programming ability simply by connecting virtual objects stored in a database to the system. Following recognition of the marker via the system’s RGB-D camera worn by the user, he/she can apply various interactive features to the marker-based AR content using simple gestures. Interactive features applied to AR content can enlarge, shrink, rotate, and transfer virtual objects with hand gestures. In addition to this gesture-interactive feature, the proposed system also allows for tangible interaction using markers. The AR content that the user edits is stored in a database, and is retrieved whenever the markers are recognized. The results of comparative experiments conducted indicate that the proposed system is easier to use and has a higher interaction satisfaction level than AR environments such as fixed-monitor and touch-based interaction on mobile screens."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Gesture-based interactive augmented reality content authoring system using HMD | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-016-0282-z","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Immersive augmented reality, Augmented reality authoring, Gesture interaction, Tangible interaction","kwrd":["Immersive_augmented_reality","Augmented_reality_authoring","Gesture_interaction","Tangible_interaction"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-016-0282-z","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-016-0282-z","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=282;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-016-0282-z">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Gesture-based interactive augmented reality content authoring system using HMD
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0282-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0282-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2016-01-12" itemprop="datePublished">12 January 2016</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Gesture-based interactive augmented reality content authoring system using HMD</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jinwook-Shim" data-author-popup="auth-Jinwook-Shim">Jinwook Shim</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Yonsei University" /><meta itemprop="address" content="grid.15444.30, 0000000404705454, Department of Computer Science, Yonsei University, 50, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Yoonsik-Yang" data-author-popup="auth-Yoonsik-Yang">Yoonsik Yang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Yonsei University" /><meta itemprop="address" content="grid.15444.30, 0000000404705454, Department of Computer Science, Yonsei University, 50, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Nahyung-Kang" data-author-popup="auth-Nahyung-Kang">Nahyung Kang</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Yonsei University" /><meta itemprop="address" content="grid.15444.30, 0000000404705454, Department of Computer Science, Yonsei University, 50, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jonghoon-Seo" data-author-popup="auth-Jonghoon-Seo">Jonghoon Seo</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="LG Electronics Advanced Research Institute" /><meta itemprop="address" content="grid.464630.3, 0000000106969566, Software Platform R&amp;D Lab, LG Electronics Advanced Research Institute, 19, Yangjae-daero 11gil, Seocho-gu, Seoul, 06772, Korea" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Tack_Don-Han" data-author-popup="auth-Tack_Don-Han" data-corresp-id="c1">Tack-Don Han<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Yonsei University" /><meta itemprop="address" content="grid.15444.30, 0000000404705454, Department of Computer Science, Yonsei University, 50, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 20</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">57</span>–<span itemprop="pageEnd">69</span>(<span data-test="article-publication-year">2016</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1329 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">8 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                        <li class="c-article-metrics-bar__item">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-016-0282-z/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a marker via gestures. The purpose of this system is to simplify augmented reality (AR) technology usage for ordinary users, especially parents and preschool children who are unfamiliar with AR technology. The system provides an immersive AR environment with a head-mounted display and recognizes users’ gestures via an RGB-D camera. Users can freely create the AR content that they will be using without any special programming ability simply by connecting virtual objects stored in a database to the system. Following recognition of the marker via the system’s RGB-D camera worn by the user, he/she can apply various interactive features to the marker-based AR content using simple gestures. Interactive features applied to AR content can enlarge, shrink, rotate, and transfer virtual objects with hand gestures. In addition to this gesture-interactive feature, the proposed system also allows for tangible interaction using markers. The AR content that the user edits is stored in a database, and is retrieved whenever the markers are recognized. The results of comparative experiments conducted indicate that the proposed system is easier to use and has a higher interaction satisfaction level than AR environments such as fixed-monitor and touch-based interaction on mobile screens.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Augmented reality (AR) technology enables users to access supplementary information by seamlessly overlaying virtual objects in the real world in real time (Azuma <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Azuma RT (1997) A survey of augmented reality. Presence 6(4):355–385" href="/article/10.1007/s10055-016-0282-z#ref-CR3" id="ref-link-section-d88954e369">1997</a>). Using AR technology, complex information can be displayed and procedural guidelines presented to assist users. Users can also work with digital elements in environments such as classrooms and industrial sites. Because of these features, AR technology is currently being incorporated into various fields, such as medicine, gaming, education, art, engineering design, military, and maintenance and repair (Azuma et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47" href="/article/10.1007/s10055-016-0282-z#ref-CR5" id="ref-link-section-d88954e372">2001</a>). In addition, users are provided with content that can actively connect to various systems via AR technology, unlike conventional one-way audiovisual services that can only passively receive content.</p><p>Much research has been conducted on AR environments with the goal of enabling users to easily create AR content. ARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="ARToolKit (2014) ARToolKit. &#xA;                    http://www.hitl.washington.edu/artoolkit/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR2" id="ref-link-section-d88954e378">2014</a>) is a representative marker-based AR application that uses computer vision technology. This application requires the user to know programming languages such as C/C++ (Wang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Wang Y, Langlotz T, Billinghurst M, Bell T (2009) An authoring tool for mobile phone AR environments. Proc N Z Comput Sci Res Stud Conf 9:1–4" href="/article/10.1007/s10055-016-0282-z#ref-CR50" id="ref-link-section-d88954e381">2009</a>). Many AR technologies, including Studierstube (Schmalstieg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Schmalstieg D, Fuhrmann A, Hesina G, Szalavári Z, Encarnaçao LM, Gervautz M, Purgathofer W (2002) The studierstube augmented reality project. Presence Teleoper Virtual Environ 11(1):33–54" href="/article/10.1007/s10055-016-0282-z#ref-CR45" id="ref-link-section-d88954e384">2002</a>), DART (MacIntyre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="MacIntyre B, Gandy M, Dow S, Bolter JD (2004) DART: A toolkit for rapid design exploration of augmented reality experiences. In: Proceedings of the 17th annual ACM symposium on user interface software and technology, pp 197–206" href="/article/10.1007/s10055-016-0282-z#ref-CR30" id="ref-link-section-d88954e387">2004</a>), APRIL (Ledermann and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ledermann F, Schmalstieg D (2005) April: a high-level framework for creating augmented reality presentations. In: Proceedings. VR 2005, pp 187–194" href="/article/10.1007/s10055-016-0282-z#ref-CR23" id="ref-link-section-d88954e390">2005</a>), and ComposAR (Seichter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Seichter H, Looser J, Billinghurst M (2008) Composar: An intuitive tool for authoring AR applications. In: Proceedings of the 7th IEEE/ACM international symposium on mixed and augmented reality, pp 177–178" href="/article/10.1007/s10055-016-0282-z#ref-CR46" id="ref-link-section-d88954e394">2008</a>) have subsequently been proposed based on ARToolKit. However, these applications still emphasize registration and arrangement of virtual objects. Thus, it is difficult for ordinary users, i.e., non-specialists, to create AR content themselves because general commercial AR systems require programming skills or professional training in AR technology.</p><p>The system proposed in this paper, depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig1">1</a>, provides an environment where users, with no programming skills at all, can easily apply interactive features to AR content. Our proposed system uses a head-mounted display (HMD) to provide users with an immersive AR environment (which makes it different from desktop PC-based AR environments) and enables intuitive hand-based 3D interaction via an RGB-D camera. Users are provided with a touchable area interface that allows them to connect a virtual object stored in the database to markers. The interactive features can then be applied to the virtual object. Because an RGB-D camera is used, interaction with the virtual object is done by hand, without the need for a sensor to establish the location of that hand.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Our AR content authoring system, which uses video see-through HMD and an RGB-D camera: <b>a</b> A user operating our system, <b>b</b> user interacting with the AR content</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>The remainder of this paper is organized as follows. Various AR technologies are presented and discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0282-z#Sec2">2</a>. The proposed system is presented and elaborated on in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0282-z#Sec7">3</a>. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0282-z#Sec10">4</a> illustrates the gesture-based interactive features and simple tangible interaction provided by the proposed system. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0282-z#Sec13">5</a> discusses the results of a comparative experiment conducted. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-016-0282-z#Sec19">6</a> presents our conclusion and outline plans for future work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">AR registration techniques</h3><p>The basic features of an AR system are user position, viewpoint direction, and virtual object registration. With these features in place, virtual objects can be shown seamlessly to users in the real world. Marker-based AR systems are robust and can easily provide a stable AR environment for users. Marker-based AR systems are used extensively. They obtain information on camera position and orientation through the geometric features of the marker. Although AR environments are primarily tested using a fixed monitor, tests using optical see-through (Azuma and Bishop <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Azuma R, Bishop G (1994) Improving static and dynamic registration in an optical see-through HMD. In: Proceedings of the 21st ACM annual conference on computer graphics and interactive techniques, pp 197–204" href="/article/10.1007/s10055-016-0282-z#ref-CR4" id="ref-link-section-d88954e458">1994</a>; Hoff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Hoff WA, Nguyen K, Lyon T (1996) Computer-vision-based registration techniques for augmented reality. In: Photonics East’96, international society for optics and photonics, pp 538–548" href="/article/10.1007/s10055-016-0282-z#ref-CR18" id="ref-link-section-d88954e461">1996</a>) and video see-through (Bajura and Neumann <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Bajura M, Neumann U (1995) Dynamic registration correction in video-based augmented reality systems. IEEE Comput Graph Appl 15(5):52–60" href="/article/10.1007/s10055-016-0282-z#ref-CR6" id="ref-link-section-d88954e464">1995</a>; Whitton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Whitton MC, Pisano ED, Fuchs H (1996) Technologies for augmented reality systems: Realizing ultrasound-guided needle biopsies. In: Proceedings of the 23rd annual conference on computer graphics and interactive techniques, pp 439–446" href="/article/10.1007/s10055-016-0282-z#ref-CR51" id="ref-link-section-d88954e467">1996</a>) HMD that combine the real world and virtual objects have also been conducted. However, the locations of the virtual objects have to be adjusted depending on the orientation and position of the user’s head in order for the real world and the virtual objects to be correctly merged in such AR systems. In research by Kato and Billinghurst (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings IWAR’99, pp 85–94" href="/article/10.1007/s10055-016-0282-z#ref-CR19" id="ref-link-section-d88954e470">1999</a>), the position and pose estimation of the markers were first acquired and used to calibrate the user’s viewpoint, camera, and HMD. Calibration of the camera required a perspective projection matrix, establishing the relationship between the respective coordinates for the camera and camera screen. The camera screen was then adjusted by finding the HMD calibration value with the perspective projection and pose estimation data of the markers identified. Okuma et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1998" title="Okuma T, Kiyokawa K, Takemura H, Yokoya N (1998) An augmented reality system using a real-time vision based registration. In: Proceedings fourteenth international conference on pattern recognition, vol 2, pp 1226–1229" href="/article/10.1007/s10055-016-0282-z#ref-CR37" id="ref-link-section-d88954e474">1998</a>) also investigated a vision-based video see-through AR system with a single camera using four colored markers as feature points.</p><h3 class="c-article__sub-heading" id="Sec4">AR-authoring applications</h3><p>Much research on the creation of AR content in AR environments is actively being conducted using ARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="ARToolKit (2014) ARToolKit. &#xA;                    http://www.hitl.washington.edu/artoolkit/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR2" id="ref-link-section-d88954e485">2014</a>). Studierstube (Schmalstieg et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Schmalstieg D, Fuhrmann A, Hesina G, Szalavári Z, Encarnaçao LM, Gervautz M, Purgathofer W (2002) The studierstube augmented reality project. Presence Teleoper Virtual Environ 11(1):33–54" href="/article/10.1007/s10055-016-0282-z#ref-CR45" id="ref-link-section-d88954e488">2002</a>), a comprehensive AR framework, is composed of scene graph-based rendering, content loading, tracking, and networking modules. It interacts with virtual objects via a pen and a tablet. Construct 3D (Kaufmann and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Kaufmann H, Schmalstieg D (2003) Mathematics and geometry education with collaborative augmented reality. Comput Graph 27(3):339–345" href="/article/10.1007/s10055-016-0282-z#ref-CR21" id="ref-link-section-d88954e491">2003</a>), which uses AR technology, can be utilized to teach mathematics and geometry. It employs an HMD and a Personal Interaction Panel (PIP) to interact with the virtual objects. DART (MacIntyre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="MacIntyre B, Gandy M, Dow S, Bolter JD (2004) DART: A toolkit for rapid design exploration of augmented reality experiences. In: Proceedings of the 17th annual ACM symposium on user interface software and technology, pp 197–206" href="/article/10.1007/s10055-016-0282-z#ref-CR30" id="ref-link-section-d88954e494">2004</a>), an authoring tool that acts as a plug-in for Adobe Director, has been developed to enable designers to easily generate AR content. APRIL (Ledermann and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Ledermann F, Schmalstieg D (2005) April: a high-level framework for creating augmented reality presentations. In: Proceedings. VR 2005, pp 187–194" href="/article/10.1007/s10055-016-0282-z#ref-CR23" id="ref-link-section-d88954e497">2005</a>) is an AR-authoring platform that serves as an extension to XML. AMIRE (Grimm et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Grimm P, Haller M, Paelke V, Reinhold S, Reimann C, Zauner R (2002) AMIRE-authoring mixed reality. In: The first IEEE international workshop augmented reality toolkit, pp 2–pp" href="/article/10.1007/s10055-016-0282-z#ref-CR14" id="ref-link-section-d88954e501">2002</a>) is an AR-authoring tool that makes GUI-based interaction possible as a visual representation; however, it is too complex for non-specialists to easily operate. ComposAR (Seichter et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Seichter H, Looser J, Billinghurst M (2008) Composar: An intuitive tool for authoring AR applications. In: Proceedings of the 7th IEEE/ACM international symposium on mixed and augmented reality, pp 177–178" href="/article/10.1007/s10055-016-0282-z#ref-CR46" id="ref-link-section-d88954e504">2008</a>) enables users to author AR and MR content personally and customize various third-party modules easily. This tool can be directly programmed and created using scripts.</p><p>Research is being conducted not only on desktop PC-based AR applications, but also mobile diverse environment-related AR applications. As regards the latter, Langlotz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Langlotz T, Mooslechner S, Zollmann S, Degendorfer C, Reitmayr G, Schmalstieg D (2012) Sketching up the world: in situ authoring for mobile augmented reality. Pers Ubiquit Comput 16(6):623–630" href="/article/10.1007/s10055-016-0282-z#ref-CR22" id="ref-link-section-d88954e510">2012</a>) proposed a method of interacting with augmented virtual objects using a mobile device for both indoor and outdoor work environments. In confined work environments, the target image is recognized by adopting a natural feature-based tracking system. Conversely, in open work environments, such as outdoors, it is recognized using panorama-based vision tracking and GPS data. The images recognized are copied as three-dimensional (3D) virtual objects and then augmented for interaction. Wozniewski and Warne (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Wozniewski M, Warne P (2011) Towards in situ authoring of augmented reality content. In: Proceedings ISMAR 2011" href="/article/10.1007/s10055-016-0282-z#ref-CR53" id="ref-link-section-d88954e513">2011</a>) introduced an application that can apply special effects and manufacture 3D assets directly at the site of the image filmed with a mobile device by the user. Bunnun et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Bunnun P, Subramanian S, Mayol-Cuevas WW (2013) In-situ interactive image-based model building for augmented reality from a handheld device. Virtual Real 17(2):137–146" href="/article/10.1007/s10055-016-0282-z#ref-CR10" id="ref-link-section-d88954e516">2013</a>) developed a system that can build wireframe models of real objects at the inputted image through two kinds of handheld devices equipped with cameras. Henrysson and Billibghurst (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Henrysson A, Billinghurst M (2007) Using a mobile phone for 6 DOF mesh editing. In: Proceedings of the 8th ACM SIGCHI New Zealand chapter’s international conference on computer-human interaction: design centered HCI, pp 9–16" href="/article/10.1007/s10055-016-0282-z#ref-CR17" id="ref-link-section-d88954e519">2007</a>) proposed an interaction method for editing 6-DOF 3D mesh using mobile devices. Their research demonstrates how mobile cameras can make 6-DOF interaction devices feasible. Coquillart and Göbel (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Coquillart S, Göbel M (2004) Authoring of mixed reality applications including multi-marker calibration for mobile devices. In: Eurographics symposium on virtual environments, pp 1–9" href="/article/10.1007/s10055-016-0282-z#ref-CR11" id="ref-link-section-d88954e522">2004</a>) proposed an application that users without programming skills can use to easily execute the mixed reality of each marker through 3D calibration and recognize multiple markers using the rear camera of a tablet PC. All of the research cited above—along with NyARToolKit (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="NyARToolKit (2014) NyARToolKit. &#xA;                    http://arforglass.org/?project=nyartoolkit/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR33" id="ref-link-section-d88954e526">2014</a>)—function as optimized versions of the ARToolKit library and support various programming languages and platforms. AndAR (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="AndAR (2014) AndAR-Android augmented reality. &#xA;                    http://code.google.com/p/andar/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR1" id="ref-link-section-d88954e529">2014</a>), a Java-based software library, supports Android and iOS platforms. Qualcomm augmented reality (QCAR) (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="QCAR (2014) QCAR. &#xA;                    http://developer.qualcomm.com/dev/augmented-reality/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR41" id="ref-link-section-d88954e532">2014</a>) supports the SDK and facilitates development of AR applications on Android mobiles.</p><p>As outlined above, although many AR systems are available, they all require that non-professional ordinary users have special skills in order to utilize them. This paper proposes an AR content-authoring system that enables ordinary users without programming skills to easily apply interactive functions to AR content.</p><h3 class="c-article__sub-heading" id="Sec5">Interaction for AR applications</h3><p>Various methods have been proposed for advanced applied interaction in AR. For instance, Kato et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. Proc ISAR 2000:111–119" href="/article/10.1007/s10055-016-0282-z#ref-CR20" id="ref-link-section-d88954e546">2000</a>) facilitate interaction within the AR environment by having the operator use a paddle. MagicBook (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook-moving seamlessly between reality and virtuality. IEEE Comput Graph Appl 21(3):6–8" href="/article/10.1007/s10055-016-0282-z#ref-CR7" id="ref-link-section-d88954e549">2001</a>) uses an actual book, for which content is provided to users by augmenting virtual objects on each page. Tiles System (Poupyrev et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-reality interface. Computer 35(3):44–50" href="/article/10.1007/s10055-016-0282-z#ref-CR40" id="ref-link-section-d88954e552">2002</a>) provides a friendly workspace to users by mapping the interaction and virtual objects to markers on several pages. Maier et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Maier P, Tonnis M, Klinker G (2010) Designing and comparing two-handed gestures to confirm links between user controlled objects. Proc ISMAR 2010:251–252" href="/article/10.1007/s10055-016-0282-z#ref-CR31" id="ref-link-section-d88954e555">2010</a>) presented a system that can observe and interact with 3D molecular models. Users can move the markers, combine two atoms, and observe the generation, relationship, and combinations of the molecules. In addition, much research is being conducted on the use of mobile devices as interaction tools (Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Billinghurst M, Hakkarainen M, Woodward C (2008) Augmented assembly using a mobile phone. In: Proceedings of the 7th ACM international conference on mobile and ubiquitous multimedia, pp 84–87" href="/article/10.1007/s10055-016-0282-z#ref-CR8" id="ref-link-section-d88954e558">2008</a>; Harviainen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Harviainen T, Korkalo O, Woodward C (2009) Camera-based interactions for augmented reality. In: Proceedings of the ACM international conference on advances in computer entertainment technology, pp 307–310" href="/article/10.1007/s10055-016-0282-z#ref-CR16" id="ref-link-section-d88954e562">2009</a>). Harviainen et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Harviainen T, Korkalo O, Woodward C (2009) Camera-based interactions for augmented reality. In: Proceedings of the ACM international conference on advances in computer entertainment technology, pp 307–310" href="/article/10.1007/s10055-016-0282-z#ref-CR16" id="ref-link-section-d88954e565">2009</a>) enable interaction with 3D content depending on the mobile camera’s movement. Billinghurst et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Billinghurst M, Hakkarainen M, Woodward C (2008) Augmented assembly using a mobile phone. In: Proceedings of the 7th ACM international conference on mobile and ubiquitous multimedia, pp 84–87" href="/article/10.1007/s10055-016-0282-z#ref-CR8" id="ref-link-section-d88954e568">2008</a>) proposed an AR assembly system that provides complex models resulting from interaction with a mobile phone. The application displays a 3D puzzle as an animation by recognizing multiple markers in pictures taken by the user and sending them to a server. Radu and Macintyre (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Radu I, MacIntyre B (2009) Augmented-reality scratch: a tangible programming environment for children. In: Proceedings of conference on interaction design for children, Como, Italy" href="/article/10.1007/s10055-016-0282-z#ref-CR42" id="ref-link-section-d88954e571">2009</a>) proposed an AR programming environment for children developed by expanding the Scratch platform. The AR technology is provided to children using simple interaction metaphors with minimized cognitive load. Using this environment, children can display virtual objects in real-world space with a camera and interact with the virtual world using physical objects.</p><p>Methods of interacting with physical objects by attaching markers to gloves have also been proposed (Dorfmuller-Ulhaas and Schmalstieg <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Dorfmuller-Ulhaas K, Schmalstieg D (2001) Finger tracking for interaction in augmented environments. In: Proceedings IEEE and ACM international symposium on augmented reality, pp 55–64" href="/article/10.1007/s10055-016-0282-z#ref-CR13" id="ref-link-section-d88954e577">2001</a>; Thomas and Piekarski <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Thomas BH, Piekarski W (2002) Glove based user interaction techniques for augmented reality in an outdoor environment. Virtual Real 6(3):167–180" href="/article/10.1007/s10055-016-0282-z#ref-CR49" id="ref-link-section-d88954e580">2002</a>). Ng et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ng L, Oon S, Ong S, Nee A (2011) GARDE: a gesture-based augmented reality design evaluation system. Int J Interact Design Manuf (IJIDeM) 5(2):85–94" href="/article/10.1007/s10055-016-0282-z#ref-CR32" id="ref-link-section-d88954e583">2011</a>) proposed a gesture-based AR design environment called GARDE. Using gestures, a designer can visualize a 3D model in GARDE, evaluate the design, and make modifications, which are then reflected in the model and also in a conventional CAD software application in real-time. Buchmann et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingARtips: Gesture based direct manipulation in augmented reality. In: Proceedings of the 2nd ACM international conference on computer graphics and interactive techniques in Australasia and South East Asia, pp 212–221" href="/article/10.1007/s10055-016-0282-z#ref-CR9" id="ref-link-section-d88954e586">2004</a>) proposed FingARTips, in which markers are attached to the thumb and index finger. The system recognizes these markers and converts the hands into virtual tongs that the user moves to pick up and transfer virtual objects with hand gestures. Research on direct interaction with the AR environment using bare hands, which provides a more intuitive and natural HCI environment than methods that use data glove, is also being conducted. Lee and Hollerer (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Lee T, Hollerer T (2007) Handy AR: Markerless inspection of augmented reality objects using fingertip tracking. In: 11th IEEE international symposium on wearable computers, pp 83–90" href="/article/10.1007/s10055-016-0282-z#ref-CR24" id="ref-link-section-d88954e589">2007</a>) proposed a method to augment and interact with a virtual object on the palm of the hand by tracking the user’s fingertips in the hand-segmented region. Lee et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Lee GA, Nelles C, Billinghurst M, Kim GJ (2004) Immersive authoring of tangible augmented reality applications. In: Proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality, pp 172–181" href="/article/10.1007/s10055-016-0282-z#ref-CR26" id="ref-link-section-d88954e593">2004</a>) proposed a system based on tangible AR in which the user creates content and a virtual interface using fingers and types of objects. This system tracks the marker and augments it showing the content and the interface to the user wearing the HMD. It enables the user to immediately visually verify the feedback that has occurred. This is possible because of the interaction using fingers and types of objects. Ong and Wang (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Ong S, Wang Z (2011) Augmented assembly technologies based on 3D bare-hand interaction. CIRP Ann-Manuf Technol 60(1):1–4" href="/article/10.1007/s10055-016-0282-z#ref-CR38" id="ref-link-section-d88954e596">2011</a>) examined 3D barehand interaction that assembles and creates virtual components in the augmented assembly environment. As such, the immersion and concentration of AR content can be enhanced when it is provided such that it can interact with AR content using the recognized users’ hands.</p><h3 class="c-article__sub-heading" id="Sec6">User hand recognition techniques</h3><p>
                           <i>Microsoft Kinect</i> has made it easier to recognize hands via RGB image and depth information. Lee and Tanaka (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Lee U, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Proceedings of Asia Pacific conference on computing human interact, pp 705–706" href="/article/10.1007/s10055-016-0282-z#ref-CR25" id="ref-link-section-d88954e610">2012</a>) presented a method in which hands are recognized in the depth image through a predefined threshold of the K-means clustering algorithm, and fingertips are found via convex hull analysis. Liang et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Liang H, Yuan J, Thalmann D (2012) 3D fingertip and palm tracking in depth image sequences. In: Proceedings of the 20th ACM international conference on Multimedia, pp 785–788" href="/article/10.1007/s10055-016-0282-z#ref-CR28" id="ref-link-section-d88954e613">2012</a>) and Hackenberg et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Hackenberg G, McCall R, Broll W (2011) Lightweight palm and finger tracking for real-time 3d gesture control. In: 2011 IEEE virtual reality conference (VR), pp 19–26" href="/article/10.1007/s10055-016-0282-z#ref-CR15" id="ref-link-section-d88954e616">2011</a>) presented a method in which hands and fingers or fingertips are found in the depth image using morphological constraints. Further, research into merging color information to improve the accuracy of hand detection is also being conducted (Oikonomidis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Oikonomidis I, Kyriazis N, Argyros AA (2011) Efficient model-based 3D tracking of hand articulations using Kinect. Proc BMVC 2011:1–11" href="/article/10.1007/s10055-016-0282-z#ref-CR35" id="ref-link-section-d88954e619">2011</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Oikonomidis I, Kyriazis N, Argyros AA (2012) Tracking the articulated motion of two strongly interacting hands. Proc CVPR 2012:1862–1869" href="/article/10.1007/s10055-016-0282-z#ref-CR36" id="ref-link-section-d88954e623">2012</a>). This research uses skin color as depth information as well as an RGB image for more accurate hand detection via a method that labels the hand regions. In addition, much research is being conducted into recognizing the posture of the hands. This method adopts shape-based hand posture recognition. Doliotis et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Doliotis P, Athitsos V, Kosmopoulos D, Perantonis S (2012) Hand shape and 3D pose estimation using depth data from a single cluttered frame. In: Bebis G, Boyle R, Parvin B, Koracin D, Fowlkes C (eds) Advances in visual computing. Springer, Berlin, Heidelberg, pp 148–158" href="/article/10.1007/s10055-016-0282-z#ref-CR12" id="ref-link-section-d88954e626">2012</a>) analyzed the contours of the hands to recognize postures by comparative evaluation with models of the hands. Ren et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011a" title="Ren Z, Meng J, Yuan J, Zhang Z (2011a) Robust hand gesture recognition with Kinect sensor. In: Proceedings of the 19th ACM international conference on Multimedia, pp 759–760" href="/article/10.1007/s10055-016-0282-z#ref-CR43" id="ref-link-section-d88954e629">2011a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Ren Z, Yuan J, Zhang Z (2011b) Robust hand gesture recognition based on finger-earth mover’s distance with a commodity depth camera. In: Proceedings of the 19th ACM international conference on Multimedia, pp 1093–1096" href="/article/10.1007/s10055-016-0282-z#ref-CR44" id="ref-link-section-d88954e632">b</a>) employed Finger-Earth Mover`s Distance to recognize posture using the differences in hand shapes. Yao and Fu (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Yao Y, Fu Y (2012) Real-time hand pose estimation from RGB-D sensor. Proc ICME 2012:705–710" href="/article/10.1007/s10055-016-0282-z#ref-CR54" id="ref-link-section-d88954e635">2012</a>) proposed a method in which hand posture is recognized by classifying each part of the hand using a colored glove. In their research, a more accurate 3D hand posture can be recognized. The above research recognizes the users’ hands using the RGB-D camera. The users can interact with virtual objects using the recognized hands.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Overview of the proposed system</h2><div class="c-article-section__content" id="Sec7-content"><h3 class="c-article__sub-heading" id="Sec8">Interactive AR content authoring system</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig2">2</a> is a flowchart of our proposed AR environment. Our interactive AR content authoring system comprises an HMD that is worn by the user, an RGB-D camera, and a desktop PC. OpenGL is used to render virtual objects, along with OpenCV, an open-source library, and an AR engine in a desktop PC that operates the system. Further, OpenNI (OpenNI <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="OpenNI (2014) OpenNI. &#xA;                    https://github.com/OpenNI/OpenNI/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR39" id="ref-link-section-d88954e654">2014</a>) and <i>iisu</i>, an engine of SoftKinetic (SoftKinetic <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="SoftKinetic (2014) SoftKinetic: the interface is you. &#xA;                    http://www.softkinetic.com/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR48" id="ref-link-section-d88954e660">2014</a>), are used for depth touch-sensing (Wilson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Wilson AD (2010) Using a depth camera as a touch sensor. In: Proceedings ACM international conference on interactive tabletops and surfaces, pp 69–72" href="/article/10.1007/s10055-016-0282-z#ref-CR52" id="ref-link-section-d88954e663">2010</a>) and user hand detection (Doliotis et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Doliotis P, Athitsos V, Kosmopoulos D, Perantonis S (2012) Hand shape and 3D pose estimation using depth data from a single cluttered frame. In: Bebis G, Boyle R, Parvin B, Koracin D, Fowlkes C (eds) Advances in visual computing. Springer, Berlin, Heidelberg, pp 148–158" href="/article/10.1007/s10055-016-0282-z#ref-CR12" id="ref-link-section-d88954e667">2012</a>; Ren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011a" title="Ren Z, Meng J, Yuan J, Zhang Z (2011a) Robust hand gesture recognition with Kinect sensor. In: Proceedings of the 19th ACM international conference on Multimedia, pp 759–760" href="/article/10.1007/s10055-016-0282-z#ref-CR43" id="ref-link-section-d88954e670">2011a</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference b" title="Ren Z, Yuan J, Zhang Z (2011b) Robust hand gesture recognition based on finger-earth mover’s distance with a commodity depth camera. In: Proceedings of the 19th ACM international conference on Multimedia, pp 1093–1096" href="/article/10.1007/s10055-016-0282-z#ref-CR44" id="ref-link-section-d88954e673">b</a>), with <i>Oculus Rift DK</i> (Oculus <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Oculus VR (2014) Oculus VR developer center. &#xA;                    http://developer.oculusvr.com/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR34" id="ref-link-section-d88954e679">2014</a>) to send the image to the HMD. The OpenCV-based AR engine module carries out marker detection, where the marker used has a cell-based structure, such as ARTag and ARToolKit Plus. Markers with this structure are used because the recognition speed of the AR template-matching technology decreases as the number of marker IDs increases, and there is a limit to the number of markers and objects provided by the system. In general, markers with a 4 × 4-sized cell structure, which can be adjusted up to 6 × 6, are used. The cells of the marker’s top-left, top-right, and bottom-left are used in an alignment pattern, and the leftover cell indicates the ID value. The virtual object connected to the marker uses the <i>Pose Estimation</i> function of ARToolKit (Kato and Billinghurst <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings IWAR’99, pp 85–94" href="/article/10.1007/s10055-016-0282-z#ref-CR19" id="ref-link-section-d88954e686">1999</a>) to render it to the calculated coordinate. The user can connect the virtual objects that are stored in the database of the markers through the interface shown in the HMD. The system proposed in this paper does not allow users to author virtual objects, but they can select and use the authored virtual objects in the database. The user can determine the desired interactive features in the object if the virtual object is connected to the marker. After AR content authoring is complete, the data from the interaction, the selected virtual object, and the marker ID are stored in the database. If the user recognizes a marker, the AR content data stored are retrieved, depending on the ID value of the marker, and the user augments the authored AR content with the marker.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Interactive AR content authoring system flowchart</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec9">Interactive AR contents authoring system interface</h3><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig3">3</a> shows an interaction use case diagram for the proposed system, while Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig4">4</a> shows the required flow and interface for authoring AR content through the system. All control for the proposed system is carried out through recognition of the user’s hand (SoftKinetic <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="SoftKinetic (2014) SoftKinetic: the interface is you. &#xA;                    http://www.softkinetic.com/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR48" id="ref-link-section-d88954e721">2014</a>) and perception of the user’s touch (Wilson <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Wilson AD (2010) Using a depth camera as a touch sensor. In: Proceedings ACM international conference on interactive tabletops and surfaces, pp 69–72" href="/article/10.1007/s10055-016-0282-z#ref-CR52" id="ref-link-section-d88954e724">2010</a>). The user only needs to select the desired feature via the touch interaction for authoring content. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig4">4</a>a, the screen for authoring AR content shows two important buttons after recognizing the marker ID: the <i>AR object</i> button, for connecting the virtual object to the marker, and the <i>Interaction</i> button, for assigning an interactive feature to authored AR content.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig3_HTML.gif?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig3_HTML.gif" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Interaction use case diagram</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Authoring AR content interface: <b>a</b> marker is perceived, <b>b</b> selecting an augmenting object to the marker, <b>c</b> selecting interaction applying to the object, <b>d</b> connecting the second marker in case of merge interaction, <b>e</b> selecting an augmenting object for the second marker, <b>f</b> selecting an object authored through merge interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig4">4</a>b, the virtual objects stored in the database that can augment the recognized marker are shown when the <i>AR object</i> button is selected. The user can connect the desired virtual object to the marker by selecting it by touch. After connecting the virtual object to the marker, the user selects an interactive feature that will be applied to the object with two tangible interactions and four gesture-based interactions. Even though the <i>click interaction</i> in the tangible interaction and four gesture-based interactions can be selected by activating a button, the <i>merge interaction,</i> which provides tangible interaction through two markers, requires another marker ID and needs two authoring steps. The user has to place another marker, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig4">4</a>d, and select the augmenting object, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig4">4</a>e, f, when <i>merge interaction</i> is established.</p></div></div></section><section aria-labelledby="Sec10"><div class="c-article-section" id="Sec10-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec10">Interactive features</h2><div class="c-article-section__content" id="Sec10-content"><p>Our proposed system provides users with various methods to interact with the AR content, such as gesture-based interaction (Buchmann et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingARtips: Gesture based direct manipulation in augmented reality. In: Proceedings of the 2nd ACM international conference on computer graphics and interactive techniques in Australasia and South East Asia, pp 212–221" href="/article/10.1007/s10055-016-0282-z#ref-CR9" id="ref-link-section-d88954e827">2004</a>) and tangible interaction (Kato et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. Proc ISAR 2000:111–119" href="/article/10.1007/s10055-016-0282-z#ref-CR20" id="ref-link-section-d88954e830">2000</a>; Maier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Maier P, Tonnis M, Klinker G (2010) Designing and comparing two-handed gestures to confirm links between user controlled objects. Proc ISMAR 2010:251–252" href="/article/10.1007/s10055-016-0282-z#ref-CR31" id="ref-link-section-d88954e833">2010</a>). Tangible interaction allows users to employ markers directly, and the gesture-based interaction provides users with continuous interaction, which facilitates manipulation of the virtual objects using hands.</p><h3 class="c-article__sub-heading" id="Sec11">Gesture-based interaction</h3><p>Interactive features such as enlarging and shrinking, rotating, and transferring of the augmented object in accordance to the user’s pinching gesture are accommodated by recognizing the user’s hand via Hand Detection (SoftKinetic <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="SoftKinetic (2014) SoftKinetic: the interface is you. &#xA;                    http://www.softkinetic.com/&#xA;                    &#xA;                  . Accessed 30 May 2014" href="/article/10.1007/s10055-016-0282-z#ref-CR48" id="ref-link-section-d88954e843">2014</a>). Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig5">5</a>a, b shows the user enlarging and shrinking the augmented object using her hands. The operation is carried out using the pinch gesture of both hands and interacting with the augmented object by widening and narrowing the distance between the hands. Further, the virtual object can be rotated or transferred to another location by utilizing the same pinch gesture. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig5">5</a>c shows the augmented object being rotated clockwise using two fingers, while Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig5">5</a>d shows the augmented object being transferred to another location. Gesture-based interaction, as noted above, is frequently used to manipulate AR content. The recognition rates for the pinch gesture recognized with the RGB-D camera and object selection are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig10">10</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Gesture-based interaction: <b>a</b> enlarge interaction, <b>b</b> shrink interaction, <b>c</b> rotation interaction, <b>d</b> transfer interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec12">Tangible interaction</h3><p>In addition to the gesture-based features for interacting with AR content, tangible interaction using markers is also provided. This includes two kinds of interactions for users: an interaction method using two markers, a merge interaction that augments a new virtual object by placing two markers close together, and a click interaction that augments the virtual object only when an occlusion of the marker occurs. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig6">6</a>a, the virtual object assigned to each marker is augmented when the two markers are far apart. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig6">6</a>b shows a new virtual object augmented by the two markers. Merge interaction operates by comparing the distance threshold between markers A and B. Merge interaction is recognized when the sum of the diagonals of markers A and B is larger than the distance between the central points of both markers. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig6">6</a>c shows a click interaction augmenting a virtual object when the marker is occluded. Even if the marker is connected to the virtual object, the object is not augmented because the marker is not occluded. However, augmentation of the object can be seen when a finger covers the marker. These tangible interactions are less natural than the gesture-based interaction, but they are effective for playing games, for instance in virtual experiments or in word matching and word pair matching for preschool children (Maier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Maier P, Tonnis M, Klinker G (2010) Designing and comparing two-handed gestures to confirm links between user controlled objects. Proc ISMAR 2010:251–252" href="/article/10.1007/s10055-016-0282-z#ref-CR31" id="ref-link-section-d88954e907">2010</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Tangible interaction: <b>a</b> when the markers are far apart, <b>b</b> when the markers are close together, <b>c</b> when the user clicks the marker</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec13"><div class="c-article-section" id="Sec13-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec13">Implementation and experimental evaluation</h2><div class="c-article-section__content" id="Sec13-content"><h3 class="c-article__sub-heading" id="Sec14">Implementation</h3><p>We implemented the system in a general office environment with illumination less than 900 lx. The system used in the experiment, shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig7">7</a>, was a video see-through HMD prototype, comprising an RGB-D camera, and SoftKinetic (model DS325) connected to an Oculus Rift VR HMD. The Oculus Rift had a resolution of 1280 × 800 (720p) and provided 640 × 800 resolution in each eye. The RGB-D camera (model DS325) had a resolution of 320 × 240. The display was designed so that it could be enlarged in accordance with the HMD display, depending on the experiment’s respective participants, through calibration seen on the HMD—or adjusted to counter any dizziness or eyestrain reported by participants. The desktop PC was equipped with a quad core 3.4 GHz CPU and 12 GB RAM. The size of the marker provided for the participants in the experiment was 80 × 80 mm. The participants interacted with the system by recognizing the marker and their hands with the RGB-D camera, and performing the tasks requested.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>The experimental apparatus: the RGB-D camera is attached to an HMD</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec15">Experiments</h3><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Design of the experiments</h4><p>We invited 28 students to participate in our experiment: 18 males and 10 females, ages ranging from 21 to 34 years. Among the participants, seven majored in computer science, and four of them had experience with AR technology. We showed the participants video clips of each environment before the experiment and let them practice with demo applications for between 30 min and 1 h with each application. We provided the participants with three environments: our proposed HMD-based AR environment, a fixed-monitor desktop PC AR environment (Shim et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Shim J, Kong M, Yang Y, Seo J, Han T-D (2014) Interactive features based augmented reality authoring tool. Proc ICCE 2014:47–50" href="/article/10.1007/s10055-016-0282-z#ref-CR47" id="ref-link-section-d88954e984">2014</a>), and a mobile AR environment. All three AR environments were prototyped from the same system to perform all tasks (outlined in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0282-z#Tab1">1</a> and depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig8">8</a>). The RGB-D camera for the fixed-monitor-based AR environment was set on the participant’s head 100 cm from the table. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig8">8</a>b, it was possible to connect a marker to a virtual object with a GUI-based interface and to apply interactive features to objects with a simple mouse click. For the mobile-based AR environment, we constructed a prototype iPhone5 interface that could be connected to the proposed system. Since a hand gesture is unlikely to be recognized in the mobile-based AR environment, interaction was performed using the touch screen of the mobile device.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Simple experimental tasks and activities carried out by the participants</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0282-z/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>The AR environment provided for each platform: <b>a</b> HMD-based AR environment, <b>b</b> fixed-monitor-based AR environment, <b>c</b> mobile-based AR environment</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The experiment conducted in each environment was divided into three parts: (1) a touch recognition rate experiment of the touchable area in the HMD-based AR environment; (2) a gesture recognition rate experiment provided in the HMD-based AR environment; and (3) a fixed-monitor-based AR environment. Comparative analysis was subsequently performed using four features common to all three AR environments: usefulness, ease of use, ease of learning, and satisfaction (Lund <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lund A (2001) Measuring usability with the use questionnaire. Usability Interface 8(2):3–6" href="/article/10.1007/s10055-016-0282-z#ref-CR29" id="ref-link-section-d88954e1150">2001</a>). A total of 30 questions were asked of the participants in the four categories. The major item, usefulness, was assessed by how well the system authored AR content and how useful the system that enabled AR content and interaction was for the users. The second, ease of use, assessed how fast and easy it was to author AR content and to operate the system. The third, ease of learning, assessed how easy it is to learn the method for authoring AR content with the interface and the method for interacting with AR content. The fourth, satisfaction, assessed how satisfied the users were with the system. For the final assessment, we conducted a one-way Analysis of Variance (ANOVA) test as a comparative experiment for mobile, fixed monitor, and HMD-based AR environments. A <i>t</i> test was carried out as post hoc analysis for each environment. We classified it according to the Likert scale, with values ranging from one (most negative) to seven (most positive).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Results of the recognition rate experiments</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig9">9</a>a shows the touchable area the participants used for recognizing the marker with the RGB-D camera. The touchable area is used as an interface when the user creates AR content in the HMD-based AR environment. In the experiment, a 40 × 60 cm overlay grid comprising 15 × 25 cm cells was presented. A blue cell was displayed in the grid for the participants to touch at random locations. The green cells are those that were touched by the participants. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig9">9</a>b is the result of the experiment; it displays regions with a touch recognition rate of more than 95 % in red. This experiment was designed to provide a button within the high touch recognition rate near 20 × 30 cm, the marker standard for the users.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig9_HTML.gif?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig9_HTML.gif" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Depth-sensing-based touchable area: <b>a</b> experiment task, <b>b</b> touchable area’s recognition rate result</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-016-0282-z#Fig10">10</a> shows the recognition rate of experimental results for the pinch gesture used in the HMD-based AR environment. In the experiment, the participants were asked to select the augmenting virtual object in the marker by using the pinch gesture. This experiment was divided into three sets and conducted over 3 weeks. In each experiment, the virtual objects were augmented for 5 min and then selected by a pinch gesture. <i>False Positive</i> signifies that the system assumed a pinch gesture even when the participant did the wrong gesture. <i>False Negative</i> signifies an unrecognized pinch gesture. <i>Miss Recognition</i> signifies that the participant’s hand deviated from the recognition area. In the third week, the recognition rate increased by 3 %, from 91 to 94 %. The recognition rate increase was low because the participants reported difficulty recognizing the virtual object in the space because haptic feedback was not provided during the selection of the virtual object (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Li D, Weng D, Li Y, Xie J (2013) Touchablear: a new experience of augmented reality. Proc ICVRV 2013:37–42" href="/article/10.1007/s10055-016-0282-z#ref-CR27" id="ref-link-section-d88954e1210">2013</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig10_HTML.gif?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-016-0282-z/MediaObjects/10055_2016_282_Fig10_HTML.gif" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Pinch gesture recognition rate experimental results</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-016-0282-z/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Result of usability experiments</h4><p>In Task 1 of the experiment, the participants created and assessed AR content with the authored interactive AR content provided in the respective AR environments. Task 2 involved the participants performing and assessing interaction with AR content (Lund <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Lund A (2001) Measuring usability with the use questionnaire. Usability Interface 8(2):3–6" href="/article/10.1007/s10055-016-0282-z#ref-CR29" id="ref-link-section-d88954e1240">2001</a>). Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0282-z#Tab2">2</a> displays the one-way ANOVA test results for each AR environment divided into four categories. It shows that there is a significant difference between the ANOVA test result values in all the respective categories.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 One-way ANOVA test results for Tasks 1 and 2</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0282-z/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The <i>t</i> tests comparing AR environments for each category were performed for post hoc analysis. The <i>t</i> test results for usefulness in Task 1 (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0282-z#Tab3">3</a>) indicate that the mobile and HMD environments have a significantly higher positive result compared to the fixed-monitor-based AR environment. The <i>t</i> test result for the mobile- and fixed-monitor environments is <i>t</i>(27) = 2.353, <i>p</i> = 0.026, and the <i>t</i> test result between fixed-monitor and HMD environments is <i>t</i>(27) = −4.117, <i>p</i> &lt; 0.05. However, the <i>t</i> test result comparing the HMD and mobile environments is <i>t</i>(27) = 0.931, <i>p</i> = 0.36. The participants determined that there is no significant difference between the two environments. Both environments received high scores because applications can be easily created and used in the mobile environment. In the HMD environment, however, a high immersion level was observed in the AR environment compared to the fixed-monitor environment. Furthermore, in terms of ease of use, the mobile environment (<i>t</i>(27) = 2.806, <i>p</i> = 0.009) and the HMD-based AR environment (<i>t</i>(27) = −5.605, <i>p</i> &lt; 0.05) earned higher points than the fixed-monitor environment. The fixed-monitor environment received a relatively low score because AR content is created through Windows, Icons, Menus, and Pointers (WIMP). As regards ease of learning, participants more easily created AR content in the mobile environment (<i>t</i>(27) = 2.201, <i>p</i> = 0.036) and the HMD environment (<i>t</i>(27) = −4.088, <i>p</i> &lt; 0.05) than in the fixed-monitor environment. With respect to satisfaction, participants reported preferring the mobile environment (<i>t</i>(27) = 2.580, <i>p</i> = 0.016) and HMD environment (<i>t</i>(27) = −4.869, <i>p</i> &lt; 0.05) to the fixed-monitor environment. The difference between the mobile and HMD (<i>t</i>(27) = 2.056, <i>p</i> = 0.05) environments was not particularly significant.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 
                                          <i>T</i> test results for Task 1</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0282-z/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>In Task 2, participants were asked to execute AR content and perform and assess interactive tasks. In terms of usefulness, the HMD environment was marginally preferred to the mobile environment (<i>t</i>(27) = 3.315, <i>p</i> = 0.003) and the fixed-monitor environment (<i>t</i>(27) = −2.423, <i>p</i> = 0.022), but the difference between the mobile and fixed-monitor environments was insignificant (<i>t</i>(27) = −1.279, <i>p</i> = 0.212). The participants reported that gesture-based interaction using hands would be more efficient than touch-based interaction on a mobile screen. For ease of use and ease of learning, results showed that interacting with AR content in the HMD environment is easier than in the fixed-monitor environment. Finally, as can be seen in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-016-0282-z#Tab4">4</a>, the HMD environment showed a higher satisfaction level in terms of interaction than the other two environments. Most participants did not have difficulty performing and understanding the tasks that involved interacting with virtual objects or authoring content in the HMD environment. However, it was necessary to prioritize the understanding of AR technology for several non-professional participants and those encountering AR technology for the first time.  </p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 
                                          <i>T</i> test results for Task 2</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-016-0282-z/tables/4"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           </div></div></section><section aria-labelledby="Sec19"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusion</h2><div class="c-article-section__content" id="Sec19-content"><p>This paper proposed an interactive AR content authoring system that simplifies AR content authoring by enabling users to use their barehands. The proposed system provides an immersive AR environment for ordinary users without professional knowledge of AR or programming skills. Prototypes for several AR environments were presented and compared. Our proposed system provides an immersive AR environment to users through the use of an HMD and is designed such that gesture interaction is possible as the system recognizes users’ hands using an RGB-D camera. In this research, we attempted to provide an environment where users can more easily author AR content. Natural user interaction features were added for users to more easily interact without supplementary effort. The purpose of this system is to provide a convenient way for users unfamiliar with AR environments to operate the technology, especially parents with children. This will enable such users to easily create simple applications and games such as word matching and word pair matching games. It will also help create virtual experiments for preschool children. As shown in the results above, the proposed system facilitates the interaction intended by the user, thereby generating interest.</p><p>Further research is necessary to investigate the interfacing of authoring tools for usability and convenience, and development of additional interaction technologies to create AR content that can be easily comprehended and applied by users. We believe that further gesture-based interaction research will entice users to actively participate in AR content and AR technology.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="AndAR (2014) AndAR-Android augmented reality. http://code.google.com/p/andar/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR1">AndAR (2014) AndAR-Android augmented reality. <a href="http://code.google.com/p/andar/">http://code.google.com/p/andar/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="ARToolKit (2014) ARToolKit. http://www.hitl.washington.edu/artoolkit/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR2">ARToolKit (2014) ARToolKit. <a href="http://www.hitl.washington.edu/artoolkit/">http://www.hitl.washington.edu/artoolkit/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="RT. Azuma, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Azuma RT (1997) A survey of augmented reality. Presence 6(4):355–385" /><p class="c-article-references__text" id="ref-CR3">Azuma RT (1997) A survey of augmented reality. Presence 6(4):355–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2Fpres.1997.6.4.355" aria-label="View reference 3">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20augmented%20reality&amp;journal=Presence&amp;volume=6&amp;issue=4&amp;pages=355-385&amp;publication_year=1997&amp;author=Azuma%2CRT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Azuma R, Bishop G (1994) Improving static and dynamic registration in an optical see-through HMD. In: Proceedi" /><p class="c-article-references__text" id="ref-CR4">Azuma R, Bishop G (1994) Improving static and dynamic registration in an optical see-through HMD. In: Proceedings of the 21st ACM annual conference on computer graphics and interactive techniques, pp 197–204</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Azuma, Y. Baillot, R. Behringer, S. Feiner, S. Julier, B. MacIntyre, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. " /><p class="c-article-references__text" id="ref-CR5">Azuma R, Baillot Y, Behringer R, Feiner S, Julier S, MacIntyre B (2001) Recent advances in augmented reality. IEEE Comput Graph Appl 21(6):34–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.963459" aria-label="View reference 5">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recent%20advances%20in%20augmented%20reality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=6&amp;pages=34-47&amp;publication_year=2001&amp;author=Azuma%2CR&amp;author=Baillot%2CY&amp;author=Behringer%2CR&amp;author=Feiner%2CS&amp;author=Julier%2CS&amp;author=MacIntyre%2CB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Bajura, U. Neumann, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Bajura M, Neumann U (1995) Dynamic registration correction in video-based augmented reality systems. IEEE Comp" /><p class="c-article-references__text" id="ref-CR6">Bajura M, Neumann U (1995) Dynamic registration correction in video-based augmented reality systems. IEEE Comput Graph Appl 15(5):52–60</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F38.403828" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dynamic%20registration%20correction%20in%20video-based%20augmented%20reality%20systems&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=15&amp;issue=5&amp;pages=52-60&amp;publication_year=1995&amp;author=Bajura%2CM&amp;author=Neumann%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Billinghurst, H. Kato, I. Poupyrev, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Billinghurst M, Kato H, Poupyrev I (2001) The magicbook-moving seamlessly between reality and virtuality. IEEE" /><p class="c-article-references__text" id="ref-CR7">Billinghurst M, Kato H, Poupyrev I (2001) The magicbook-moving seamlessly between reality and virtuality. IEEE Comput Graph Appl 21(3):6–8</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20magicbook-moving%20seamlessly%20between%20reality%20and%20virtuality&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=21&amp;issue=3&amp;pages=6-8&amp;publication_year=2001&amp;author=Billinghurst%2CM&amp;author=Kato%2CH&amp;author=Poupyrev%2CI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Billinghurst M, Hakkarainen M, Woodward C (2008) Augmented assembly using a mobile phone. In: Proceedings of t" /><p class="c-article-references__text" id="ref-CR8">Billinghurst M, Hakkarainen M, Woodward C (2008) Augmented assembly using a mobile phone. In: Proceedings of the 7th ACM international conference on mobile and ubiquitous multimedia, pp 84–87</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingARtips: Gesture based direct manipulation in augm" /><p class="c-article-references__text" id="ref-CR9">Buchmann V, Violich S, Billinghurst M, Cockburn A (2004) FingARtips: Gesture based direct manipulation in augmented reality. In: Proceedings of the 2nd ACM international conference on computer graphics and interactive techniques in Australasia and South East Asia, pp 212–221</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Bunnun, S. Subramanian, WW. Mayol-Cuevas, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Bunnun P, Subramanian S, Mayol-Cuevas WW (2013) In-situ interactive image-based model building for augmented r" /><p class="c-article-references__text" id="ref-CR10">Bunnun P, Subramanian S, Mayol-Cuevas WW (2013) In-situ interactive image-based model building for augmented reality from a handheld device. Virtual Real 17(2):137–146</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-011-0206-x" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=In-situ%20interactive%20image-based%20model%20building%20for%20augmented%20reality%20from%20a%20handheld%20device&amp;journal=Virtual%20Real&amp;volume=17&amp;issue=2&amp;pages=137-146&amp;publication_year=2013&amp;author=Bunnun%2CP&amp;author=Subramanian%2CS&amp;author=Mayol-Cuevas%2CWW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Coquillart S, Göbel M (2004) Authoring of mixed reality applications including multi-marker calibration for mo" /><p class="c-article-references__text" id="ref-CR11">Coquillart S, Göbel M (2004) Authoring of mixed reality applications including multi-marker calibration for mobile devices. In: Eurographics symposium on virtual environments, pp 1–9</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="P. Doliotis, V. Athitsos, D. Kosmopoulos, S. Perantonis, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Doliotis P, Athitsos V, Kosmopoulos D, Perantonis S (2012) Hand shape and 3D pose estimation using depth data " /><p class="c-article-references__text" id="ref-CR12">Doliotis P, Athitsos V, Kosmopoulos D, Perantonis S (2012) Hand shape and 3D pose estimation using depth data from a single cluttered frame. In: Bebis G, Boyle R, Parvin B, Koracin D, Fowlkes C (eds) Advances in visual computing. Springer, Berlin, Heidelberg, pp 148–158</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20visual%20computing&amp;pages=148-158&amp;publication_year=2012&amp;author=Doliotis%2CP&amp;author=Athitsos%2CV&amp;author=Kosmopoulos%2CD&amp;author=Perantonis%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Dorfmuller-Ulhaas K, Schmalstieg D (2001) Finger tracking for interaction in augmented environments. In: Proce" /><p class="c-article-references__text" id="ref-CR13">Dorfmuller-Ulhaas K, Schmalstieg D (2001) Finger tracking for interaction in augmented environments. In: Proceedings IEEE and ACM international symposium on augmented reality, pp 55–64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Grimm P, Haller M, Paelke V, Reinhold S, Reimann C, Zauner R (2002) AMIRE-authoring mixed reality. In: The fir" /><p class="c-article-references__text" id="ref-CR14">Grimm P, Haller M, Paelke V, Reinhold S, Reimann C, Zauner R (2002) AMIRE-authoring mixed reality. In: The first IEEE international workshop augmented reality toolkit, pp 2–pp</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hackenberg G, McCall R, Broll W (2011) Lightweight palm and finger tracking for real-time 3d gesture control. " /><p class="c-article-references__text" id="ref-CR15">Hackenberg G, McCall R, Broll W (2011) Lightweight palm and finger tracking for real-time 3d gesture control. In: 2011 IEEE virtual reality conference (VR), pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Harviainen T, Korkalo O, Woodward C (2009) Camera-based interactions for augmented reality. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR16">Harviainen T, Korkalo O, Woodward C (2009) Camera-based interactions for augmented reality. In: Proceedings of the ACM international conference on advances in computer entertainment technology, pp 307–310</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Henrysson A, Billinghurst M (2007) Using a mobile phone for 6 DOF mesh editing. In: Proceedings of the 8th ACM" /><p class="c-article-references__text" id="ref-CR17">Henrysson A, Billinghurst M (2007) Using a mobile phone for 6 DOF mesh editing. In: Proceedings of the 8th ACM SIGCHI New Zealand chapter’s international conference on computer-human interaction: design centered HCI, pp 9–16</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hoff WA, Nguyen K, Lyon T (1996) Computer-vision-based registration techniques for augmented reality. In: Phot" /><p class="c-article-references__text" id="ref-CR18">Hoff WA, Nguyen K, Lyon T (1996) Computer-vision-based registration techniques for augmented reality. In: Photonics East’96, international society for optics and photonics, pp 538–548</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferen" /><p class="c-article-references__text" id="ref-CR19">Kato H, Billinghurst M (1999) Marker tracking and HMD calibration for a video-based augmented reality conferencing system. In: Proceedings IWAR’99, pp 85–94</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Kato, M. Billinghurst, I. Poupyrev, K. Imamoto, K. Tachibana, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top A" /><p class="c-article-references__text" id="ref-CR20">Kato H, Billinghurst M, Poupyrev I, Imamoto K, Tachibana K (2000) Virtual object manipulation on a table-top AR environment. Proc ISAR 2000:111–119</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20object%20manipulation%20on%20a%20table-top%20AR%20environment&amp;journal=Proc%20ISAR&amp;volume=2000&amp;pages=111-119&amp;publication_year=2000&amp;author=Kato%2CH&amp;author=Billinghurst%2CM&amp;author=Poupyrev%2CI&amp;author=Imamoto%2CK&amp;author=Tachibana%2CK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Kaufmann, D. Schmalstieg, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Kaufmann H, Schmalstieg D (2003) Mathematics and geometry education with collaborative augmented reality. Comp" /><p class="c-article-references__text" id="ref-CR21">Kaufmann H, Schmalstieg D (2003) Mathematics and geometry education with collaborative augmented reality. Comput Graph 27(3):339–345</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0097-8493%2803%2900028-1" aria-label="View reference 21">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 21 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Mathematics%20and%20geometry%20education%20with%20collaborative%20augmented%20reality&amp;journal=Comput%20Graph&amp;volume=27&amp;issue=3&amp;pages=339-345&amp;publication_year=2003&amp;author=Kaufmann%2CH&amp;author=Schmalstieg%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Langlotz, S. Mooslechner, S. Zollmann, C. Degendorfer, G. Reitmayr, D. Schmalstieg, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Langlotz T, Mooslechner S, Zollmann S, Degendorfer C, Reitmayr G, Schmalstieg D (2012) Sketching up the world:" /><p class="c-article-references__text" id="ref-CR22">Langlotz T, Mooslechner S, Zollmann S, Degendorfer C, Reitmayr G, Schmalstieg D (2012) Sketching up the world: in situ authoring for mobile augmented reality. Pers Ubiquit Comput 16(6):623–630</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00779-011-0430-0" aria-label="View reference 22">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Sketching%20up%20the%20world%3A%20in%20situ%20authoring%20for%20mobile%20augmented%20reality&amp;journal=Pers%20Ubiquit%20Comput&amp;volume=16&amp;issue=6&amp;pages=623-630&amp;publication_year=2012&amp;author=Langlotz%2CT&amp;author=Mooslechner%2CS&amp;author=Zollmann%2CS&amp;author=Degendorfer%2CC&amp;author=Reitmayr%2CG&amp;author=Schmalstieg%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ledermann F, Schmalstieg D (2005) April: a high-level framework for creating augmented reality presentations. " /><p class="c-article-references__text" id="ref-CR23">Ledermann F, Schmalstieg D (2005) April: a high-level framework for creating augmented reality presentations. In: Proceedings. VR 2005, pp 187–194</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee T, Hollerer T (2007) Handy AR: Markerless inspection of augmented reality objects using fingertip tracking" /><p class="c-article-references__text" id="ref-CR24">Lee T, Hollerer T (2007) Handy AR: Markerless inspection of augmented reality objects using fingertip tracking. In: 11th IEEE international symposium on wearable computers, pp 83–90</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee U, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with K" /><p class="c-article-references__text" id="ref-CR25">Lee U, Tanaka J (2012) Hand controller: image manipulation interface using fingertips and palm tracking with Kinect depth data. In: Proceedings of Asia Pacific conference on computing human interact, pp 705–706</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lee GA, Nelles C, Billinghurst M, Kim GJ (2004) Immersive authoring of tangible augmented reality applications" /><p class="c-article-references__text" id="ref-CR26">Lee GA, Nelles C, Billinghurst M, Kim GJ (2004) Immersive authoring of tangible augmented reality applications. In: Proceedings of the 3rd IEEE/ACM international symposium on mixed and augmented reality, pp 172–181</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Li, D. Weng, Y. Li, J. Xie, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Li D, Weng D, Li Y, Xie J (2013) Touchablear: a new experience of augmented reality. Proc ICVRV 2013:37–42" /><p class="c-article-references__text" id="ref-CR27">Li D, Weng D, Li Y, Xie J (2013) Touchablear: a new experience of augmented reality. Proc ICVRV 2013:37–42</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Touchablear%3A%20a%20new%20experience%20of%20augmented%20reality&amp;journal=Proc%20ICVRV&amp;volume=2013&amp;pages=37-42&amp;publication_year=2013&amp;author=Li%2CD&amp;author=Weng%2CD&amp;author=Li%2CY&amp;author=Xie%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Liang H, Yuan J, Thalmann D (2012) 3D fingertip and palm tracking in depth image sequences. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR28">Liang H, Yuan J, Thalmann D (2012) 3D fingertip and palm tracking in depth image sequences. In: Proceedings of the 20th ACM international conference on Multimedia, pp 785–788</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Lund, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Lund A (2001) Measuring usability with the use questionnaire. Usability Interface 8(2):3–6" /><p class="c-article-references__text" id="ref-CR29">Lund A (2001) Measuring usability with the use questionnaire. Usability Interface 8(2):3–6</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="http://www.ams.org/mathscinet-getitem?mr=1148802" aria-label="View reference 29 on MathSciNet">MathSciNet</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 29 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Measuring%20usability%20with%20the%20use%20questionnaire&amp;journal=Usability%20Interface&amp;volume=8&amp;issue=2&amp;pages=3-6&amp;publication_year=2001&amp;author=Lund%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="MacIntyre B, Gandy M, Dow S, Bolter JD (2004) DART: A toolkit for rapid design exploration of augmented realit" /><p class="c-article-references__text" id="ref-CR30">MacIntyre B, Gandy M, Dow S, Bolter JD (2004) DART: A toolkit for rapid design exploration of augmented reality experiences. In: Proceedings of the 17th annual ACM symposium on user interface software and technology, pp 197–206</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Maier, M. Tonnis, G. Klinker, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Maier P, Tonnis M, Klinker G (2010) Designing and comparing two-handed gestures to confirm links between user " /><p class="c-article-references__text" id="ref-CR31">Maier P, Tonnis M, Klinker G (2010) Designing and comparing two-handed gestures to confirm links between user controlled objects. Proc ISMAR 2010:251–252</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 31 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Designing%20and%20comparing%20two-handed%20gestures%20to%20confirm%20links%20between%20user%20controlled%20objects&amp;journal=Proc%20ISMAR&amp;volume=2010&amp;pages=251-252&amp;publication_year=2010&amp;author=Maier%2CP&amp;author=Tonnis%2CM&amp;author=Klinker%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Ng, S. Oon, S. Ong, A. Nee, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ng L, Oon S, Ong S, Nee A (2011) GARDE: a gesture-based augmented reality design evaluation system. Int J Inte" /><p class="c-article-references__text" id="ref-CR32">Ng L, Oon S, Ong S, Nee A (2011) GARDE: a gesture-based augmented reality design evaluation system. Int J Interact Design Manuf (IJIDeM) 5(2):85–94</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs12008-011-0117-9" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=GARDE%3A%20a%20gesture-based%20augmented%20reality%20design%20evaluation%20system&amp;journal=Int%20J%20Interact%20Design%20Manuf%20%28IJIDeM%29&amp;volume=5&amp;issue=2&amp;pages=85-94&amp;publication_year=2011&amp;author=Ng%2CL&amp;author=Oon%2CS&amp;author=Ong%2CS&amp;author=Nee%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="NyARToolKit (2014) NyARToolKit. http://arforglass.org/?project=nyartoolkit/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR33">NyARToolKit (2014) NyARToolKit. <a href="http://arforglass.org/?project=nyartoolkit/">http://arforglass.org/?project=nyartoolkit/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Oculus VR (2014) Oculus VR developer center. http://developer.oculusvr.com/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR34">Oculus VR (2014) Oculus VR developer center. <a href="http://developer.oculusvr.com/">http://developer.oculusvr.com/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Oikonomidis, N. Kyriazis, AA. Argyros, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Oikonomidis I, Kyriazis N, Argyros AA (2011) Efficient model-based 3D tracking of hand articulations using Kin" /><p class="c-article-references__text" id="ref-CR35">Oikonomidis I, Kyriazis N, Argyros AA (2011) Efficient model-based 3D tracking of hand articulations using Kinect. Proc BMVC 2011:1–11</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20model-based%203D%20tracking%20of%20hand%20articulations%20using%20Kinect&amp;journal=Proc%20BMVC&amp;volume=2011&amp;pages=1-11&amp;publication_year=2011&amp;author=Oikonomidis%2CI&amp;author=Kyriazis%2CN&amp;author=Argyros%2CAA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Oikonomidis, N. Kyriazis, AA. Argyros, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Oikonomidis I, Kyriazis N, Argyros AA (2012) Tracking the articulated motion of two strongly interacting hands" /><p class="c-article-references__text" id="ref-CR36">Oikonomidis I, Kyriazis N, Argyros AA (2012) Tracking the articulated motion of two strongly interacting hands. Proc CVPR 2012:1862–1869</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Tracking%20the%20articulated%20motion%20of%20two%20strongly%20interacting%20hands&amp;journal=Proc%20CVPR&amp;volume=2012&amp;pages=1862-1869&amp;publication_year=2012&amp;author=Oikonomidis%2CI&amp;author=Kyriazis%2CN&amp;author=Argyros%2CAA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Okuma T, Kiyokawa K, Takemura H, Yokoya N (1998) An augmented reality system using a real-time vision based re" /><p class="c-article-references__text" id="ref-CR37">Okuma T, Kiyokawa K, Takemura H, Yokoya N (1998) An augmented reality system using a real-time vision based registration. In: Proceedings fourteenth international conference on pattern recognition, vol 2, pp 1226–1229</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Ong, Z. Wang, " /><meta itemprop="datePublished" content="2011" /><meta itemprop="headline" content="Ong S, Wang Z (2011) Augmented assembly technologies based on 3D bare-hand interaction. CIRP Ann-Manuf Technol" /><p class="c-article-references__text" id="ref-CR38">Ong S, Wang Z (2011) Augmented assembly technologies based on 3D bare-hand interaction. CIRP Ann-Manuf Technol 60(1):1–4</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cirp.2011.03.001" aria-label="View reference 38">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 38 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20assembly%20technologies%20based%20on%203D%20bare-hand%20interaction&amp;journal=CIRP%20Ann-Manuf%20Technol&amp;volume=60&amp;issue=1&amp;pages=1-4&amp;publication_year=2011&amp;author=Ong%2CS&amp;author=Wang%2CZ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="OpenNI (2014) OpenNI. https://github.com/OpenNI/OpenNI/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR39">OpenNI (2014) OpenNI. <a href="https://github.com/OpenNI/OpenNI/">https://github.com/OpenNI/OpenNI/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Poupyrev, DS. Tan, M. Billinghurst, H. Kato, H. Regenbrecht, N. Tetsutani, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-r" /><p class="c-article-references__text" id="ref-CR40">Poupyrev I, Tan DS, Billinghurst M, Kato H, Regenbrecht H, Tetsutani N (2002) Developing a generic augmented-reality interface. Computer 35(3):44–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F2.989929" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Developing%20a%20generic%20augmented-reality%20interface&amp;journal=Computer&amp;volume=35&amp;issue=3&amp;pages=44-50&amp;publication_year=2002&amp;author=Poupyrev%2CI&amp;author=Tan%2CDS&amp;author=Billinghurst%2CM&amp;author=Kato%2CH&amp;author=Regenbrecht%2CH&amp;author=Tetsutani%2CN">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="QCAR (2014) QCAR. http://developer.qualcomm.com/dev/augmented-reality/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR41">QCAR (2014) QCAR. <a href="http://developer.qualcomm.com/dev/augmented-reality/">http://developer.qualcomm.com/dev/augmented-reality/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Radu I, MacIntyre B (2009) Augmented-reality scratch: a tangible programming environment for children. In: Pro" /><p class="c-article-references__text" id="ref-CR42">Radu I, MacIntyre B (2009) Augmented-reality scratch: a tangible programming environment for children. In: Proceedings of conference on interaction design for children, Como, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren Z, Meng J, Yuan J, Zhang Z (2011a) Robust hand gesture recognition with Kinect sensor. In: Proceedings of " /><p class="c-article-references__text" id="ref-CR43">Ren Z, Meng J, Yuan J, Zhang Z (2011a) Robust hand gesture recognition with Kinect sensor. In: Proceedings of the 19th ACM international conference on Multimedia, pp 759–760</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren Z, Yuan J, Zhang Z (2011b) Robust hand gesture recognition based on finger-earth mover’s distance with a c" /><p class="c-article-references__text" id="ref-CR44">Ren Z, Yuan J, Zhang Z (2011b) Robust hand gesture recognition based on finger-earth mover’s distance with a commodity depth camera. In: Proceedings of the 19th ACM international conference on Multimedia, pp 1093–1096</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Schmalstieg, A. Fuhrmann, G. Hesina, Z. Szalavári, LM. Encarnaçao, M. Gervautz, W. Purgathofer, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Schmalstieg D, Fuhrmann A, Hesina G, Szalavári Z, Encarnaçao LM, Gervautz M, Purgathofer W (2002) The studiers" /><p class="c-article-references__text" id="ref-CR45">Schmalstieg D, Fuhrmann A, Hesina G, Szalavári Z, Encarnaçao LM, Gervautz M, Purgathofer W (2002) The studierstube augmented reality project. Presence Teleoper Virtual Environ 11(1):33–54</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1162%2F105474602317343640" aria-label="View reference 45">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20studierstube%20augmented%20reality%20project&amp;journal=Presence%20Teleoper%20Virtual%20Environ&amp;volume=11&amp;issue=1&amp;pages=33-54&amp;publication_year=2002&amp;author=Schmalstieg%2CD&amp;author=Fuhrmann%2CA&amp;author=Hesina%2CG&amp;author=Szalav%C3%A1ri%2CZ&amp;author=Encarna%C3%A7ao%2CLM&amp;author=Gervautz%2CM&amp;author=Purgathofer%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Seichter H, Looser J, Billinghurst M (2008) Composar: An intuitive tool for authoring AR applications. In: Pro" /><p class="c-article-references__text" id="ref-CR46">Seichter H, Looser J, Billinghurst M (2008) Composar: An intuitive tool for authoring AR applications. In: Proceedings of the 7th IEEE/ACM international symposium on mixed and augmented reality, pp 177–178</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Shim, M. Kong, Y. Yang, J. Seo, T-D. Han, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Shim J, Kong M, Yang Y, Seo J, Han T-D (2014) Interactive features based augmented reality authoring tool. Pro" /><p class="c-article-references__text" id="ref-CR47">Shim J, Kong M, Yang Y, Seo J, Han T-D (2014) Interactive features based augmented reality authoring tool. Proc ICCE 2014:47–50</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Interactive%20features%20based%20augmented%20reality%20authoring%20tool&amp;journal=Proc%20ICCE&amp;volume=2014&amp;pages=47-50&amp;publication_year=2014&amp;author=Shim%2CJ&amp;author=Kong%2CM&amp;author=Yang%2CY&amp;author=Seo%2CJ&amp;author=Han%2CT-D">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="SoftKinetic (2014) SoftKinetic: the interface is you. http://www.softkinetic.com/. Accessed 30 May 2014" /><p class="c-article-references__text" id="ref-CR48">SoftKinetic (2014) SoftKinetic: the interface is you. <a href="http://www.softkinetic.com/">http://www.softkinetic.com/</a>. Accessed 30 May 2014</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="BH. Thomas, W. Piekarski, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Thomas BH, Piekarski W (2002) Glove based user interaction techniques for augmented reality in an outdoor envi" /><p class="c-article-references__text" id="ref-CR49">Thomas BH, Piekarski W (2002) Glove based user interaction techniques for augmented reality in an outdoor environment. Virtual Real 6(3):167–180</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs100550200017" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Glove%20based%20user%20interaction%20techniques%20for%20augmented%20reality%20in%20an%20outdoor%20environment&amp;journal=Virtual%20Real&amp;volume=6&amp;issue=3&amp;pages=167-180&amp;publication_year=2002&amp;author=Thomas%2CBH&amp;author=Piekarski%2CW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Wang, T. Langlotz, M. Billinghurst, T. Bell, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Wang Y, Langlotz T, Billinghurst M, Bell T (2009) An authoring tool for mobile phone AR environments. Proc N Z" /><p class="c-article-references__text" id="ref-CR50">Wang Y, Langlotz T, Billinghurst M, Bell T (2009) An authoring tool for mobile phone AR environments. Proc N Z Comput Sci Res Stud Conf 9:1–4</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20authoring%20tool%20for%20mobile%20phone%20AR%20environments&amp;journal=Proc%20N%20Z%20Comput%20Sci%20Res%20Stud%20Conf&amp;volume=9&amp;pages=1-4&amp;publication_year=2009&amp;author=Wang%2CY&amp;author=Langlotz%2CT&amp;author=Billinghurst%2CM&amp;author=Bell%2CT">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Whitton MC, Pisano ED, Fuchs H (1996) Technologies for augmented reality systems: Realizing ultrasound-guided " /><p class="c-article-references__text" id="ref-CR51">Whitton MC, Pisano ED, Fuchs H (1996) Technologies for augmented reality systems: Realizing ultrasound-guided needle biopsies. In: Proceedings of the 23rd annual conference on computer graphics and interactive techniques, pp 439–446</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wilson AD (2010) Using a depth camera as a touch sensor. In: Proceedings ACM international conference on inter" /><p class="c-article-references__text" id="ref-CR52">Wilson AD (2010) Using a depth camera as a touch sensor. In: Proceedings ACM international conference on interactive tabletops and surfaces, pp 69–72</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wozniewski M, Warne P (2011) Towards in situ authoring of augmented reality content. In: Proceedings ISMAR 201" /><p class="c-article-references__text" id="ref-CR53">Wozniewski M, Warne P (2011) Towards in situ authoring of augmented reality content. In: Proceedings ISMAR 2011</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Yao, Y. Fu, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Yao Y, Fu Y (2012) Real-time hand pose estimation from RGB-D sensor. Proc ICME 2012:705–710" /><p class="c-article-references__text" id="ref-CR54">Yao Y, Fu Y (2012) Real-time hand pose estimation from RGB-D sensor. Proc ICME 2012:705–710</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 54 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real-time%20hand%20pose%20estimation%20from%20RGB-D%20sensor&amp;journal=Proc%20ICME&amp;volume=2012&amp;pages=705-710&amp;publication_year=2012&amp;author=Yao%2CY&amp;author=Fu%2CY">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-016-0282-z-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This work was supported by a National Research Foundation of Korea (NRF) Grant (No. NRF-2015R1A2A1A10055673) funded by the Korea government (MEST).</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, Yonsei University, 50, Yonsei-ro, Seodaemun-gu, Seoul, 03722, Korea</p><p class="c-article-author-affiliation__authors-list">Jinwook Shim, Yoonsik Yang, Nahyung Kang &amp; Tack-Don Han</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Software Platform R&amp;D Lab, LG Electronics Advanced Research Institute, 19, Yangjae-daero 11gil, Seocho-gu, Seoul, 06772, Korea</p><p class="c-article-author-affiliation__authors-list">Jonghoon Seo</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Jinwook-Shim"><span class="c-article-authors-search__title u-h3 js-search-name">Jinwook Shim</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jinwook+Shim&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jinwook+Shim" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jinwook+Shim%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Yoonsik-Yang"><span class="c-article-authors-search__title u-h3 js-search-name">Yoonsik Yang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Yoonsik+Yang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yoonsik+Yang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yoonsik+Yang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Nahyung-Kang"><span class="c-article-authors-search__title u-h3 js-search-name">Nahyung Kang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Nahyung+Kang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Nahyung+Kang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Nahyung+Kang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jonghoon-Seo"><span class="c-article-authors-search__title u-h3 js-search-name">Jonghoon Seo</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jonghoon+Seo&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jonghoon+Seo" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jonghoon+Seo%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Tack_Don-Han"><span class="c-article-authors-search__title u-h3 js-search-name">Tack-Don Han</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Tack-Don+Han&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tack-Don+Han" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Tack-Don+Han%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-016-0282-z/email/correspondent/c1/new">Tack-Don Han</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Gesture-based%20interactive%20augmented%20reality%20content%20authoring%20system%20using%20HMD&amp;author=Jinwook%20Shim%20et%20al&amp;contentID=10.1007%2Fs10055-016-0282-z&amp;publication=1359-4338&amp;publicationDate=2016-01-12&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-016-0282-z" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-016-0282-z" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Shim, J., Yang, Y., Kang, N. <i>et al.</i> Gesture-based interactive augmented reality content authoring system using HMD.
                    <i>Virtual Reality</i> <b>20, </b>57–69 (2016). https://doi.org/10.1007/s10055-016-0282-z</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-016-0282-z.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2014-06-19">19 June 2014</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-01-04">04 January 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-01-12">12 January 2016</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2016-03">March 2016</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-016-0282-z" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-016-0282-z</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Immersive augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality authoring</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Gesture interaction</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Tangible interaction</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-016-0282-z.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=282;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

