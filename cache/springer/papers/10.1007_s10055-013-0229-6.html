<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Experiences in mixed reality-based collocated after action review"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpose of learning. AAR has often been..."/>

    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10055/17/3.jpg"/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Experiences in mixed reality-based collocated after action review"/>

    <meta name="dc.source" content="Virtual Reality 2013 17:3"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2013-08-31"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2013 Springer-Verlag London"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpose of learning. AAR has often been conducted with video-based systems whereby a video of the action is reviewed afterward, usually at another location. This paper proposes collocated AAR of training experiences through mixed reality (MR). Collocated AAR allows users to review past training experiences in situ with the user&#8217;s current, real-world experience, i.e., the AAR is conducted at the same location where the action being reviewed occurred. MR enables a user-controlled egocentric viewpoint, augmentation such as a visual overlay of virtual information like conceptual visualizations, and playback of recorded training experiences collocated with the user&#8217;s current experience or that of an expert. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization, and interactive debriefing system. The system enables collocated AAR in two applications related to anesthesia training: anesthesia machine operation training and skin disinfection training with a mannequin patient simulator. Collocated AAR was evaluated in two informal pilot studies by students (n&#160;=&#160;19) and an educator (n&#160;=&#160;1) not directly affiliated with the project. We review the anecdotal data collected from the studies and point toward ways to refine and improve collocated AAR."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2013-08-31"/>

    <meta name="prism.volume" content="17"/>

    <meta name="prism.number" content="3"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="239"/>

    <meta name="prism.endingPage" content="252"/>

    <meta name="prism.copyright" content="2013 Springer-Verlag London"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-013-0229-6"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-013-0229-6"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-013-0229-6.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-013-0229-6"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Experiences in mixed reality-based collocated after action review"/>

    <meta name="citation_volume" content="17"/>

    <meta name="citation_issue" content="3"/>

    <meta name="citation_publication_date" content="2013/09"/>

    <meta name="citation_online_date" content="2013/08/31"/>

    <meta name="citation_firstpage" content="239"/>

    <meta name="citation_lastpage" content="252"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-013-0229-6"/>

    <meta name="DOI" content="10.1007/s10055-013-0229-6"/>

    <meta name="citation_doi" content="10.1007/s10055-013-0229-6"/>

    <meta name="description" content="After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpos"/>

    <meta name="dc.creator" content="John Quarles"/>

    <meta name="dc.creator" content="Samsun Lampotang"/>

    <meta name="dc.creator" content="Ira Fischler"/>

    <meta name="dc.creator" content="Paul Fishwick"/>

    <meta name="dc.creator" content="Benjamin Lok"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="Bier EA, Stone MC, Pier K, Buxton W, DeRose TD (1993) Toolglass and magic lenses: the see-through interface. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, pp 73&#8211;80"/>

    <meta name="citation_reference" content="citation_journal_title=Proc IEEE Virtual Real; citation_title=Training for physical tasks in virtual environments: Tai Chi; citation_author=PT Chua, R Crivella, B Daly, N Hu, R Schaaf, D Ventura, T Camill, J Hodgins, R Pausch; citation_volume=2003; citation_publication_date=2003; citation_pages=87-94; citation_id=CR2"/>

    <meta name="citation_reference" content="Department of the Army (1993) Washington, DC. Training Circular 25&#8211;20: A leader&#8217;s guide to after-action reviews"/>

    <meta name="citation_reference" content="citation_journal_title=Simul Health Care; citation_title=Understanding of anesthesia machine function is enhanced with a transparent reality simulation; citation_author=I Fischler, CE Kaschub, DE Lizdas, S Lampotang; citation_volume=3; citation_publication_date=2008; citation_pages=26-32; citation_doi=10.1097/SIH.0b013e31816366d3; citation_id=CR4"/>

    <meta name="citation_reference" content="citation_title=Simulation model design and execution: building digital worlds; citation_publication_date=1995; citation_id=CR5; citation_author=PA Fishwick; citation_publisher=Prentice Hall"/>

    <meta name="citation_reference" content="citation_journal_title=Simulation; citation_title=Toward an integrative multimodeling interface: a human-computer interface approach to interrelating model structures; citation_author=PA Fishwick; citation_volume=80; citation_issue=9; citation_publication_date=2004; citation_pages=421; citation_doi=10.1177/0037549704044081; citation_id=CR6"/>

    <meta name="citation_reference" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the SIGCHI conference on Human factors in computing systems, pp 234&#8211;241"/>

    <meta name="citation_reference" content="citation_title=Computer simulation in biology; a basic introduction; citation_publication_date=1992; citation_id=CR8; citation_author=R Keen; citation_author=J Spain; citation_publisher=Wiley"/>

    <meta name="citation_reference" content="citation_journal_title=Contemp Educ Psychol; citation_title=Just-in-time information presentation: improving learning a troubleshooting skill; citation_author=L Kester, P Kirschner, J Merrinboer; citation_volume=31; citation_issue=2; citation_publication_date=2006; citation_pages=167-185; citation_doi=10.1016/j.cedpsych.2005.04.002; citation_id=CR9"/>

    <meta name="citation_reference" content="Knerr BW, Lampton DR, Martin GA, Washburn DA, Cope D (2002) Developing an after action review system for virtual dismounted infantry simulations. In the interservice industry training, simulation &amp; education conference (I/ITSEC)
"/>

    <meta name="citation_reference" content="citation_journal_title=Educ Technol; citation_title=Transparent reality, a simulation based on interactive dynamic graphical models emphasizing visualization; citation_author=S Lampotang, DE Lizdas, N Gravenstein, EB Liem; citation_volume=46; citation_issue=1; citation_publication_date=2006; citation_pages=55-59; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_journal_title=Simul Healthcare.; citation_title=Transparent reality simulation of skin prepping; citation_author=DE Lizdas, N Gravenstein, S Lampotang; citation_volume=4; citation_issue=5; citation_publication_date=2009; citation_pages=193; citation_id=CR12"/>

    <meta name="citation_reference" content="Looser J, Billinghurst M, Cockburn A (2004) Through the looking glass: the use of lenses as an interface tool for Augmented Reality interfaces. In: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia, pp 204&#8211;211"/>

    <meta name="citation_reference" content="Lynch C, Ashley K, Aleven V, Pinkwart N (2006) Defining &#8220;ill-defined domains&#8221;; a literature survey. In: Proceedings of the workshop on intelligent tutoring systems for ill-defined domains at the 8th international conference on intelligent tutoring systems (pp 1&#8211;10) National Central University, Jhongli (Taiwan)"/>

    <meta name="citation_reference" content="citation_journal_title=IEICE Trans Inf Syst; citation_title=A taxonomy of mixed reality visual displays; citation_author=P Milgram, F Kishino; citation_volume=77; citation_issue=12; citation_publication_date=1994; citation_pages=1321-1329; citation_id=CR15"/>

    <meta name="citation_reference" content="citation_journal_title=Appl Psychophysiol Biofeedback; citation_title=Development of a data management tool for investigating multivariate space and free will experiences in virtual reality; citation_author=J Morie, K Iyer, D Luigi, J Williams, A Dozois, A Rizzo; citation_volume=30; citation_issue=3; citation_publication_date=2005; citation_pages=319-331; citation_doi=10.1007/s10484-005-6386-y; citation_id=CR16"/>

    <meta name="citation_reference" content="citation_journal_title=Simulation; citation_title=Integrating dynamic and geometry model components through ontology-based inference; citation_author=M Park, PA Fishwick; citation_volume=81; citation_issue=12; citation_publication_date=2005; citation_pages=795; citation_doi=10.1177/0037549705064359; citation_id=CR17"/>

    <meta name="citation_reference" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract and concrete knowledge. IEEE Virtual Real, Reno, pp 27&#8211;34"/>

    <meta name="citation_reference" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spatial cognition. IEEE 3D User Interfaces Reno, pp 11&#8211;18"/>

    <meta name="citation_reference" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008c) Collocated AAR: augmenting after action review with mixed reality. IEEE Int Symp Mixed Augment Reality Cambridge, pp 107&#8211;116"/>

    <meta name="citation_reference" content="Raij A, Lok B (2008) IPSVIZ: an after-action review tool for human virtual human experiences. IEEE Virt Reality, Reno, pp 91&#8211;98"/>

    <meta name="citation_reference" content="Sielhorst T, Blum T, Navab N (2005) Synchronizing 3D movements for quantitative comparison and simultaneous visualization of actions. In: Proceedings of the Fourth IEEE and ACM International Symposium on Mixed and Augmented, 2005, pp  38&#8211;47"/>

    <meta name="citation_reference" content="Smith R, Allen G (1994) After action review in military training simulations. In WSC&#8217;94: Proceedings of the 26th conference on Winter Simulation, 845&#8211;849, San Diego, USA"/>

    <meta name="citation_reference" content="Studiocode (2008) Retrieved 28 April 2008 from 
                    http://www.studiocodegroup.com
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Artif Intell Med; citation_title=A Bayesian approach to generating tutorial hints in a collaborative medical problem-based learning system; citation_author=S Suebnukarn, P Haddawy; citation_volume=38; citation_issue=1; citation_publication_date=2006; citation_pages=5-24; citation_doi=10.1016/j.artmed.2005.04.003; citation_id=CR25"/>

    <meta name="citation_reference" content="Viega J, Conway MJ, Williams G, Pausch R (1996) 3D magic lenses. In: Proceedings of the 9th annual ACM symposium on User interface software and technology, pp 51&#8211;58"/>

    <meta name="citation_reference" content="Virtual anesthesia machine (2010) Retrieved 21 April 2010, from 
                    http://vam.anest.ufl.edu/wip.html
                    
                  
                        "/>

    <meta name="citation_reference" content="citation_journal_title=Human Resour Dev Q; citation_title=Theories supporting transfer of training; citation_author=S Yamnill, G McLean; citation_volume=12; citation_issue=2; citation_publication_date=2001; citation_pages=195; citation_doi=10.1002/hrdq.7; citation_id=CR28"/>

    <meta name="citation_author" content="John Quarles"/>

    <meta name="citation_author_email" content="jpq@cs.utsa.edu"/>

    <meta name="citation_author_institution" content="Department of Computer Science, University of Texas at San Antonio, San Antonio, USA"/>

    <meta name="citation_author" content="Samsun Lampotang"/>

    <meta name="citation_author_institution" content="Department of Anesthesiology, University of Florida, Gainesville, USA"/>

    <meta name="citation_author" content="Ira Fischler"/>

    <meta name="citation_author_institution" content="Department of Psychology, University of Florida, Gainesville, USA"/>

    <meta name="citation_author" content="Paul Fishwick"/>

    <meta name="citation_author_institution" content="Department of CISE, University of Florida, Gainesville, USA"/>

    <meta name="citation_author" content="Benjamin Lok"/>

    <meta name="citation_author_institution" content="Department of CISE, University of Florida, Gainesville, USA"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-013-0229-6&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>

    <meta name="citation_cover_date" content="2013/09/01"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-013-0229-6"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Experiences in mixed reality-based collocated after action review"/>
        <meta property="og:description" content="After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpose of learning. AAR has often been conducted with video-based systems whereby a video of the action is reviewed afterward, usually at another location. This paper proposes collocated AAR of training experiences through mixed reality (MR). Collocated AAR allows users to review past training experiences in situ with the user’s current, real-world experience, i.e., the AAR is conducted at the same location where the action being reviewed occurred. MR enables a user-controlled egocentric viewpoint, augmentation such as a visual overlay of virtual information like conceptual visualizations, and playback of recorded training experiences collocated with the user’s current experience or that of an expert. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization, and interactive debriefing system. The system enables collocated AAR in two applications related to anesthesia training: anesthesia machine operation training and skin disinfection training with a mannequin patient simulator. Collocated AAR was evaluated in two informal pilot studies by students (n&amp;nbsp;&#x3D;&amp;nbsp;19) and an educator (n&amp;nbsp;&#x3D;&amp;nbsp;1) not directly affiliated with the project. We review the anecdotal data collected from the studies and point toward ways to refine and improve collocated AAR."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Experiences in mixed reality-based collocated after action review | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-013-0229-6","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Mixed reality, After action review, Anesthesia machine, Human patient simulator, User studies, Skin prepping","kwrd":["Mixed_reality","After_action_review","Anesthesia_machine","Human_patient_simulator","User_studies","Skin_prepping"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-013-0229-6","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-013-0229-6","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=229;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-013-0229-6">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Experiences in mixed reality-based collocated after action review
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0229-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0229-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2013-08-31" itemprop="datePublished">31 August 2013</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Experiences in mixed reality-based collocated after action review</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-John-Quarles" data-author-popup="auth-John-Quarles" data-corresp-id="c1">John Quarles<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Texas at San Antonio" /><meta itemprop="address" content="grid.215352.2, 0000000121845633, Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, San Antonio, TX, 78254, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Samsun-Lampotang" data-author-popup="auth-Samsun-Lampotang">Samsun Lampotang</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Florida" /><meta itemprop="address" content="grid.15276.37, 0000000419368091, Department of Anesthesiology, University of Florida, PO Box 100254, Gainesville, FL, 32610, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Ira-Fischler" data-author-popup="auth-Ira-Fischler">Ira Fischler</a></span><sup class="u-js-hide"><a href="#Aff3">3</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Florida" /><meta itemprop="address" content="grid.15276.37, 0000000419368091, Department of Psychology, University of Florida, P.O. Box 112250, Gainesville, FL, 32611, USA" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Paul-Fishwick" data-author-popup="auth-Paul-Fishwick">Paul Fishwick</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Florida" /><meta itemprop="address" content="grid.15276.37, 0000000419368091, Department of CISE, University of Florida, PO Box 116120, Gainesville, FL, 32611, USA" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Benjamin-Lok" data-author-popup="auth-Benjamin-Lok">Benjamin Lok</a></span><sup class="u-js-hide"><a href="#Aff4">4</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="University of Florida" /><meta itemprop="address" content="grid.15276.37, 0000000419368091, Department of CISE, University of Florida, PO Box 116120, Gainesville, FL, 32611, USA" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 17</b>, <span class="u-visually-hidden">pages</span><span itemprop="pageStart">239</span>–<span itemprop="pageEnd">252</span>(<span data-test="article-publication-year">2013</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">519 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">2 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-013-0229-6/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpose of learning. AAR has often been conducted with video-based systems whereby a video of the action is reviewed afterward, usually at another location. This paper proposes collocated AAR of training experiences through mixed reality (MR). Collocated AAR allows users to review past training experiences in situ with the user’s current, real-world experience, i.e., the AAR is conducted at the same location where the action being reviewed occurred. MR enables a user-controlled egocentric viewpoint, augmentation such as a visual overlay of virtual information like conceptual visualizations, and playback of recorded training experiences collocated with the user’s current experience or that of an expert. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization, and interactive debriefing system. The system enables collocated AAR in two applications related to anesthesia training: anesthesia machine operation training and skin disinfection training with a mannequin patient simulator. Collocated AAR was evaluated in two informal pilot studies by students (<i>n</i> = 19) and an educator (<i>n</i> = 1) not directly affiliated with the project. We review the anecdotal data collected from the studies and point toward ways to refine and improve collocated AAR.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In medical education, most current after action review (AAR) systems consist of reviewing videos of a student’s training experience, which allows students and educators to playback, critique, and assess performance. However, video-based review consists of fixed viewpoints (determined by the video camera location and its pan-tilt-zoom setting), often takes place at a different location from where the action occurred, and primarily only real-world information (i.e., no virtual overlay or augmentations as found in mixed reality (MR)) (Milgram and Kishino <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst 77(12):1321–1329" href="/article/10.1007/s10055-013-0229-6#ref-CR15" id="ref-link-section-d84870e425">1994</a>). We propose to augment AAR with MR to facilitate collocated AAR. The overlay of virtual information and user-controlled egocentric viewpoints obtained with MR may enhance AAR and provide novel interaction and visualization that is not possible with current AAR systems.</p><p>Specifically, this paper presents a MR-based collocated AAR system—the Augmented Anesthesia Machine Visualization and Interactive Debriefing system (AAMVID). It merges the recording and playback features of video-based AAR with the augmentation features of MR. AAMVID features include a user-controlled review experience from a first-person viewpoint. For example, users can review an abstract simulation of an anesthesia machine’s internal workings that are registered to a real anesthesia machine (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig1">1</a>). Subsequently, during the AAR, previous interactions are collocated with current real-time interactions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig1">1</a> bottom)—enabling interactive instruction and correction of previous mistakes in situ (e.g., in place with the anesthesia machine at the location where the previous action occurred).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig1_HTML.jpg" alt="figure1" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>
                                    <i>Top</i> a student view of an anesthesia machine from a magic lens. <i>Bottom</i> a student mimics the collocated expert interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>To be useful, collocated AAR must meet the pedagogical needs of educators and students. Students need directed instruction, repetition (deliberate practice), and feedback to bring them to a level of competency (Department of the Army <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Department of the Army (1993) Washington, DC. Training Circular 25–20: A leader’s guide to after-action reviews" href="/article/10.1007/s10055-013-0229-6#ref-CR3" id="ref-link-section-d84870e465">1993</a>). Educators need to assess students’ approaches to problems. To make this assessment, they need to identify means, outliers, and class-wide trends.</p><p>Because of the different educational needs of educators and students, two versions of AAMVID were created and evaluated separately. The student version (AAMVID-S in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec12">5</a>) enables students to review and interact with both their own previous interactions and an expert’s previous interactions. The educator version (AAMVID-E in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec21">7</a>) enables educators to visualize and interact with the aggregated performance of multiple students.</p><p>By definition, an expert has above-average understanding and knowledge and as such is a rare person, usually with restricted time availability. This bottleneck in expert availability is one of the factors promoting the concept of self-debriefing. The ability of students to compare their actions with those of experts in AAMVID-S facilitates self-debriefing and makes the experts’ expertise more readily available.</p><p>The approach is evaluated in two informal pilot studies. In the first study (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec20">6</a>), 19 students learned about anesthesia machines and then used AAMVID-S to review their experiences. Then, an educator used AAMVID-E to review the aggregate data obtained from the 19 study participants (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec27">8</a>). Based on these preliminary evaluations, we review advantages and disadvantages of MR-based collocated AAR and discuss how to improve and refine the approach.</p><p>This paper is an extended version of (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008c" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008c) Collocated AAR: augmenting after action review with mixed reality. IEEE Int Symp Mixed Augment Reality Cambridge, pp 107–116" href="/article/10.1007/s10055-013-0229-6#ref-CR20" id="ref-link-section-d84870e492">2008c</a>). To extend this paper, Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec19">9</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec37">10</a> describe the implementation of collocated AAR for a second application: skin disinfection training with a mannequin patient simulator. In contrast to the anesthesia machine operation training described previously, skin preparation training has different needs for visualization and interaction. We discuss our approach to meet these needs (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec19">9</a>), and we discuss how collocated AAR can be improved and generalized for other applications (Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec37">10</a>).</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Previous work</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Magic lens displays</h3><p>In AAMVID, the main visual display is a tracked 6 degrees of freedom (6DOF) tablet PC. Magic lenses were originally created as 2D interfaces (Bier et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Bier EA, Stone MC, Pier K, Buxton W, DeRose TD (1993) Toolglass and magic lenses: the see-through interface. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, pp 73–80" href="/article/10.1007/s10055-013-0229-6#ref-CR1" id="ref-link-section-d84870e519">1993</a>) and were later extended into 3D (Viega et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1996" title="Viega J, Conway MJ, Williams G, Pausch R (1996) 3D magic lenses. In: Proceedings of the 9th annual ACM symposium on User interface software and technology, pp 51–58" href="/article/10.1007/s10055-013-0229-6#ref-CR26" id="ref-link-section-d84870e522">1996</a>). In mixed and augmented reality, these lenses have again been extended as in (Looser et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Looser J, Billinghurst M, Cockburn A (2004) Through the looking glass: the use of lenses as an interface tool for Augmented Reality interfaces. In: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia, pp 204–211" href="/article/10.1007/s10055-013-0229-6#ref-CR13" id="ref-link-section-d84870e525">2004</a>). The MR/AR lens enables a see-through window into the user’s surrounding augmented environment and can be used as a 6DOF tangible user interface (Ishii and Ullmer <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the SIGCHI conference on Human factors in computing systems, pp 234–241" href="/article/10.1007/s10055-013-0229-6#ref-CR7" id="ref-link-section-d84870e528">1997</a>; Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spatial cognition. IEEE 3D User Interfaces Reno, pp 11–18" href="/article/10.1007/s10055-013-0229-6#ref-CR19" id="ref-link-section-d84870e531">2008b</a>). Tangible user interfaces use physical objects as the interface to semantically related digital media (i.e., a tablet PC can be used as a physical window into a digital world). This work was influential to AAMVID because the main visual display is a tracked 6DOF magic lens. AAMVID takes the display and interactive capabilities of magic lenses a step further by integrating an abstract model of a system (i.e., an anesthesia machine) with the real system. This combination of abstract model and real system can be considered an integrative model.</p><h3 class="c-article__sub-heading" id="Sec4">Integrative modeling</h3><p>Integrative modeling—the concept of linking dynamic and geometric models together in the user interface—is introduced in (Fishwick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2004" title="Fishwick PA (2004) Toward an integrative multimodeling interface: a human-computer interface approach to interrelating model structures. Simulation 80(9):421" href="/article/10.1007/s10055-013-0229-6#ref-CR6" id="ref-link-section-d84870e542">2004</a>; Park and Fishwick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Park M, Fishwick PA (2005) Integrating dynamic and geometry model components through ontology-based inference. Simulation 81(12):795" href="/article/10.1007/s10055-013-0229-6#ref-CR17" id="ref-link-section-d84870e545">2005</a>). In this work, integrative modeling is posed as a grand challenge to simulation and exemplified in an interactive computer graphics-based air traffic control simulation, where the interface to the simulation was integrated with the visual representation of the planes and their movement patterns. AAMVID utilizes this concept in that it uses MR to explicitly visualize this linkage and facilitate an effective form of human–machine interaction. Our main innovation here is using MR to take the concept of integrative modeling into the real world (i.e., a real anesthesia machine) and extending it to after action review.</p><h3 class="c-article__sub-heading" id="Sec5">After action review</h3><p>In AAR, trainees and trainers review training experiences and reflect upon them for the purpose of learning. The concept originally stems from the “war games” practiced in military command strategy review (e.g., outcomes after moving troops). AAR allows soldiers “to discover for themselves what happened, why it happened, and how to sustain strengths and improve on weaknesses” (Department of the Army <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1993" title="Department of the Army (1993) Washington, DC. Training Circular 25–20: A leader’s guide to after-action reviews" href="/article/10.1007/s10055-013-0229-6#ref-CR3" id="ref-link-section-d84870e556">1993</a>). Since then, AAR has been extended into the industrial, medical, and educational domains.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec6">After action review systems</h4><p>There are AAR systems in many fields of training. For military training, TAARUS (Smith and Allen<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Smith R, Allen G (1994) After action review in military training simulations. In WSC’94: Proceedings of the 26th conference on Winter Simulation, 845–849, San Diego, USA" href="/article/10.1007/s10055-013-0229-6#ref-CR23" id="ref-link-section-d84870e566">1994</a>) and DIVAARS (
Knerr et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Knerr BW, Lampton DR, Martin GA, Washburn DA, Cope D (2002) Developing an after action review system for virtual dismounted infantry simulations. In the interservice industry training, simulation &amp; education conference (I/ITSEC)&#xA;" href="/article/10.1007/s10055-013-0229-6#ref-CR10" id="ref-link-section-d84870e569">2002</a>) use maps and graphs to allow AAR of troop movements and of battlefield simulations. More generally, behavior has been studied using AAR. For example, Phloem (Morie et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Morie J, Iyer K, Luigi D, Williams J, Dozois A, Rizzo A (2005) Development of a data management tool for investigating multivariate space and free will experiences in virtual reality. Appl Psychophysiol Biofeedback 30(3):319–331" href="/article/10.1007/s10055-013-0229-6#ref-CR16" id="ref-link-section-d84870e572">2005</a>) visualizes large sets of behavioral data. IPSVis (Raij and Lok <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Raij A, Lok B (2008) IPSVIZ: an after-action review tool for human virtual human experiences. IEEE Virt Reality, Reno, pp 91–98" href="/article/10.1007/s10055-013-0229-6#ref-CR21" id="ref-link-section-d84870e575">2008</a>) is an AAR system geared toward reviewing interpersonal simulation, specifically human–virtual human interaction. All of these systems offer users methods of reviewing past experiences and have been shown to be highly effective learning tools. The goal of AAMVID is to increase the effectiveness of AAR by providing additional real-world context and interaction methods.</p><p>Outside of AAR, there has also been some relevant work in using expert interactions and MR to direct training. Chua et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Chua PT, Crivella R, Daly B, Hu N, Schaaf R, Ventura D, Camill T, Hodgins J, Pausch R (2003) Training for physical tasks in virtual environments: Tai Chi. Proc IEEE Virtual Real 2003:87–94" href="/article/10.1007/s10055-013-0229-6#ref-CR2" id="ref-link-section-d84870e581">2003</a>) created a system to train students with expert Tai Chi movements from a user-controlled, first-person perspective. Sielhorst et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Sielhorst T, Blum T, Navab N (2005) Synchronizing 3D movements for quantitative comparison and simultaneous visualization of actions. In: Proceedings of the Fourth IEEE and ACM International Symposium on Mixed and Augmented, 2005, pp  38–47" href="/article/10.1007/s10055-013-0229-6#ref-CR22" id="ref-link-section-d84870e584">2005</a>) created new ways of quantitatively comparing expert and novice 3D interactions in augmented reality with a training application for childbirth using forceps. In addition, Sielhorst et al. effectively collocated the novice and expert interaction visualizations.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec7">Video-based AAR in education</h4><p>Current video-based AAR systems offer educators and students the ability to perform standard video playback operations (e.g., play, fast-forward, rewind, and pause). This enables students and educators to review training sessions repeatedly and at their own pace. Some video-based AAR systems (Studiocode <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Studiocode (2008) Retrieved 28 April 2008 from &#xA;                    http://www.studiocodegroup.com&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0229-6#ref-CR24" id="ref-link-section-d84870e595">2008</a>) allow educators to manually annotate the video time line—to highlight important moments in the video (e.g., when a mistake was made and what kind of mistake). This type of annotation helps to direct student instruction and educator assessment.</p><p>However, video-based review consists of fixed viewpoints and primarily real-world information (i.e., the video is minimally augmented with virtual information). Thus, during AAR, students and educators may not experience the cognitive, interactive, and visual advantages of collocating real and virtual information in MR.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Anesthesia machine simulation</h2><div class="c-article-section__content" id="Sec8-content"><p>This section summarizes our previous work in anesthesia machine simulation and describes the progression toward AAMVID.</p><h3 class="c-article__sub-heading" id="Sec9">Transparent reality simulations</h3><p>Visualizations have been used to present information for millennia such as the first maps. Improvements in computer graphics led to increased use of visualization in all forms of human endeavor. Transparent reality simulation is a form of visualization where the internal structure, functions, and processes are made explicit and visible, thus enhancing the understanding of the object or organism being simulated (Lampotang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Lampotang S, Lizdas DE, Gravenstein N, Liem EB (2006) Transparent reality, a simulation based on interactive dynamic graphical models emphasizing visualization. Educ Technol 46(1):55–59" href="/article/10.1007/s10055-013-0229-6#ref-CR11" id="ref-link-section-d84870e616">2006</a>; Fischler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Fischler I, Kaschub CE, Lizdas DE, Lampotang S (2008) Understanding of anesthesia machine function is enhanced with a transparent reality simulation. Simul Health Care 3:26–32" href="/article/10.1007/s10055-013-0229-6#ref-CR4" id="ref-link-section-d84870e619">2008</a>). In the case of the virtual anesthesia machine (Virtual Anesthesia Machine <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Virtual anesthesia machine (2010) Retrieved 21 April 2010, from &#xA;                    http://vam.anest.ufl.edu/wip.html&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0229-6#ref-CR27" id="ref-link-section-d84870e622">2010</a>), the pneumatic circuitry inside an anesthesia machine is made transparent and the flow of gases within the plumbing is made visible and color-coded (according to established medical gas color codes). Using animation techniques controlled by mathematical models, the simulation responds in real time to user interventions and can be used as the virtual component in a MR-based simulation.</p><h3 class="c-article__sub-heading" id="Sec10">Mixed reality simulation</h3><p>The augmented anesthesia machine (AAM) (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract and concrete knowledge. IEEE Virtual Real, Reno, pp 27–34" href="/article/10.1007/s10055-013-0229-6#ref-CR18" id="ref-link-section-d84870e633">2008a</a>) is a MR-based simulation that was created to enhance learning and training in anesthesia education. AAMVID was originally built as an AAR module upon the core AAM system and then was further extended to include human patient simulators. This section describes the training benefits of the AAM and aims to motivate the use of AAMVID to enable similar benefits in collocated AAR.</p><p>Currently, some anesthesia students first train with the virtual anesthesia machine (VAM) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig2">2</a>), a 2D abstract, transparent reality simulation (Lampotang et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Lampotang S, Lizdas DE, Gravenstein N, Liem EB (2006) Transparent reality, a simulation based on interactive dynamic graphical models emphasizing visualization. Educ Technol 46(1):55–59" href="/article/10.1007/s10055-013-0229-6#ref-CR11" id="ref-link-section-d84870e642">2006</a>) of an anesthesia machine. Although the VAM is very effective for teaching anesthesia concepts such as invisible gas flow (Fischler et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Fischler I, Kaschub CE, Lizdas DE, Lampotang S (2008) Understanding of anesthesia machine function is enhanced with a transparent reality simulation. Simul Health Care 3:26–32" href="/article/10.1007/s10055-013-0229-6#ref-CR4" id="ref-link-section-d84870e645">2008</a>), some VAM-trained students still have difficulty interacting with a real anesthesia machine due to the real machine’s lack of transparency and different spatial layout (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spatial cognition. IEEE 3D User Interfaces Reno, pp 11–18" href="/article/10.1007/s10055-013-0229-6#ref-CR19" id="ref-link-section-d84870e648">2008b</a>) .</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig2_HTML.gif?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig2_HTML.gif" alt="figure2" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A screenshot from the VAM simulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <p>In previous research, the AAM was presented as a potential solution to this problem. The AAM offers students the ability to (1) use a tracked 6DOF magic lens (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig1">1</a> bottom) to visualize an abstract 3D simulation of the anesthesia machine’s internal components and invisible gas flow (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig3">3</a>), while (2) interacting with the real anesthesia machine. The AAM helped users to better transfer their abstract knowledge of the machine (i.e., invisible gas flows) to a concrete domain (i.e., physical interaction with the machine) (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract and concrete knowledge. IEEE Virtual Real, Reno, pp 27–34" href="/article/10.1007/s10055-013-0229-6#ref-CR18" id="ref-link-section-d84870e678">2008a</a>). Later, it was shown that this improvement was in part the result of the AAM compensating for low spatial cognition (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008b" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spatial cognition. IEEE 3D User Interfaces Reno, pp 11–18" href="/article/10.1007/s10055-013-0229-6#ref-CR19" id="ref-link-section-d84870e681">2008b</a>). Based on these findings, we expect that the AAMs benefits will transfer from collocated training to collocated AAR.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig3_HTML.jpg?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig3_HTML.jpg" alt="figure3" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>A zoomed-out screenshot of the AAM simulation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        </div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Collocated AAR: challenges for MR</h2><div class="c-article-section__content" id="Sec11-content"><p>The main challenge of MR-based collocated AAR is to merge the abilities of MR with the goals of AAR.</p><p>Video-based AAR system goals:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Enable users to review, assess, and learn from videos of trainees and videos of experts.</p>
                  </li>
                  <li>
                    <p>User-controlled playback of the video for ease of review (e.g., fast-forward, pause, and rewind).</p>
                  </li>
                </ul>
                     <p>MR abilities:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>Visual overlay of virtual information (e.g., the abstract simulation in the AAM).</p>
                  </li>
                  <li>
                    <p>Interaction with the real world (e.g., turning knobs on the anesthesia machine) that affects the visual overlay.</p>
                  </li>
                  <li>
                    <p>User-controlled, tracked viewpoint.</p>
                  </li>
                </ul>
                     <p>For collocated AAR, a MR system must collocate things that are not typically collocated, such as:</p><ul class="u-list-style-bullet">
                  <li>
                    <p>
                                 <i>Time</i>—collocating playback time with real time.</p>
                  </li>
                  <li>
                    <p>
                                 <i>Interaction</i>—collocating recorded expert or student interactions with current interactions.</p>
                  </li>
                  <li>
                    <p>
                                 <i>Visualization</i>—collocating recorded users’ viewpoints and virtual information with the current user-controlled view.</p>
                  </li>
                </ul>
                     <p>The following sections describe how we addressed these issues to enable collocated AAR.</p></div></div></section><section aria-labelledby="Sec12"><div class="c-article-section" id="Sec12-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec12">AAMVID-S for students</h2><div class="c-article-section__content" id="Sec12-content"><p>For students, AAMVID-S enables collocated AAR for anesthesia machine fault tests in anesthesia education. First, a fault is caused in the machine—a problem in the machine unknown to the student and intentionally caused by the educator, such as a disabled component. Then, students attempt to diagnose and correct the machine fault by interacting with the real anesthesia machine (with no help from additional visualizations or simulations). Once this test is completed, students can use AAMVID-S for the collocated AAR of the test. This section describes the AAMVID-S implementation and presents the results of a usability study with 19 students.</p><h3 class="c-article__sub-heading" id="Sec13">AAMVID-S system features</h3><p>The goals of AAMVID-S are to allow students to (1) review their performance in situ, (2) review an expert’s performance for the same fault in situ, (3) interact with the physical anesthesia machine while following a collocated expert guided tutorial, and (4) observe a collocated visualization of the machine’s internal workings during (1), (2),and (3). To realize these goals in MR, we used a tracked 6DOF magic lens display and developed software that logged student and expert interactions. During the AAR, AAMVID-S allows a student to playback previous interactions, visualize the chain of events that made up the previous interactions, and visualize where the user and the expert were each looking during their respective interactions.</p><p>One important design decision is that AAMVID only allows students to control the playback of one previous experience at a time (e.g., a user’s previous experience or the expert’s experience). The purpose of this decision is to decrease student confusion. However, AAMVID does visualize interactions from a recorded experience in situ with the user’s current real-world experience and interactions.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec14">Logging student and expert interaction</h4><p>To generate visualizations for collocated AAR, two types of data are logged during the fault test: head-gaze and anesthesia machine states. For head-gaze, the user wears a hat (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig1">1</a> bottom), tracked with retroreflective tape, and IR sensing Web cams. This enables the system to log the head-gaze direction of the user. For the anesthesia machine state, the AAM tracking system, described in (Quarles et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008a" title="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract and concrete knowledge. IEEE Virtual Real, Reno, pp 27–34" href="/article/10.1007/s10055-013-0229-6#ref-CR18" id="ref-link-section-d84870e808">2008a</a>), tracks the states of the machine. The changes in these states are then processed to determine when the user interacted with the machine.</p><p>A student log is recorded when a student performs a fault test prior to the collocated AAR. Our expert log data were recorded when Dr. Samsun Lampotang—an anesthesia educator and a co-author of this paper—performed each of the fault tests.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec15">Abstract visualization of machine faults</h4><p>In AAMVID-S, students physically interact with the real machine and use a 6DOF magic lens to visualize how these interactions affect the internal workings and invisible gas flows of the real machine. Similarly, to visualize fault behavior, specific faults were physically caused in the real machine and triggered in the abstract simulation. For example, one fault involves a faulty inspiratory valve, which can be potentially harmful to a patient. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig4">4</a> top left is what the student sees in a real machine. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig4">4</a> bottom is what the student sees on the magic lens during the AAR. Because the magic lens visualizes abstract concepts like invisible gas flow, AAMVID-S allows students to observe how a faulty inspiratory valve affects gas flow in situ. Notice how the abstract valve icons are both open (e.g., the horizontal line is located at the top of the icon, which denotes an open valve).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig4_HTML.jpg" alt="figure4" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>
                                          <i>Top left</i> real-world view of a user touching an incompetent inspiratory valve. <i>Bottom</i> AAMVID view of an incompetent inspiratory valve during AAR</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec16">Event chain visualization</h4><p>To learn from and critique their fault tests, students need to review the specific actions they performed during the fault test and compare their actions to an expert’s actions. To meet this need, AAMVID-S enables students to visualize the chain of interaction events that occurred during the fault test. For example, a student or expert might have turned the O<sub>2</sub> flow control knob, then turned on the ventilator, and then pressed the oxygen flush. AAMVID-S discretizes this series of events on the fly during the fault test.</p><p>To discretize these events, the AAMVID-S uses a logging system coupled with the internal simulation of the machine. The AAMVID-S logging system is built upon the AAM system, which simulates the gas flow and internal states of the components with a rule-based finite state machine (FSM) (Fishwick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Fishwick PA (1995) Simulation model design and execution: building digital worlds. Prentice Hall, USA&#xA;" href="/article/10.1007/s10055-013-0229-6#ref-CR5" id="ref-link-section-d84870e866">1995</a>). This FSM takes input from the AAM tracking system. Based upon this input, the AAM updates the visualized internal machine states and gas flows. Changes in the internal states then are used to detect specific interactions events (e.g., when the user turns the O2 knob, the FSM changes state, and the simulated O2 particles visually increase in speed). When an interaction is detected, the state of the simulation is key framed (i.e., saved in memory) for later playback.</p><p>When an event occurs during playback, an “interaction event box” appears that is collocated with the corresponding control (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig5">5</a>). For example, when the student turned the O2 knob, an interaction box pops up next to the control and reads that the student increased the O2 flow by a specific percentage. To direct the user’s attention to the next event, a 3D red line is rendered that slowly extends from the last interaction event position and toward the position of the next upcoming event. Lines between older events are blue lines indicating that the events have passed. By the end of the playback time line, these lines connect all the interactions that were performed in the experience. This forms a directed graph where the interaction boxes are the nodes and the lines are the transitions between them.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig5_HTML.jpg" alt="figure5" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Past interaction boxes are collocated with the real controls and describe past interactions. The <i>boxes</i> are connected with <i>lines</i>, denoting a chain of interaction events</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec17">Playback: manipulating virtual time</h4><p>An advantage of traditional video-based AAR systems is the ability to play, pause, rewind, and fast-forward. AAMVID-S implements this playback interface with 2D buttons on the magic lens display that users click with a pen interface. AAMVID-S users are able to jump (fast-forward) to the next interaction event, jump (rewind) to the previous event if they missed something, or pause the playback to observe the interaction at their own pace. One additional advantage of AAMVID-S is that it allows students to view any point in time from a user-controlled viewpoint. For example, students can pause the interaction playback and then move to a different viewpoint to visualize a key point in time or previously occluded information (i.e., internal gas flows).</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec18">Look-at indicator</h4><p>One of the difficulties that students experience in a fault test (and in the AAR of a fault test) is in knowing where to direct their attention. There are many concurrent processes in an anesthesia machine and it can be difficult for students to know where to look to find the faults. To resolve this problem in the collocated AAR, students can see a visualization of where they were looking or where the expert was looking during the fault test (although not at the same time). To generate this visualization, we tracked the head of each student and of our expert. The resulting “look-at indicator” (the highlighted red spotlight in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig6">6</a>) helps students to direct their attention in the AAR and allows them to compare their own observations to the expert’s observations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig6_HTML.jpg" alt="figure6" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>The student can see what an expert was looking at, denoted by the large <i>red spotlight</i> (color figure online)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec19">Viewing modes</h3><p>One important design decision was that AAMVID-S only allows students to control the playback of one previous experience at a time. To minimize confusion and information overload, there are three different viewing modes that visualize data from different sources. Each of these modes corresponds to specific sets of data that are being collocated with the real world:</p><p>
                           <i>User View Mode</i> visualizes the student’s fault test collocated with the real machine.</p><p>
                           <i>Expert View Mode</i> visualizes the expert’s fault test collocated with the machine.</p><p>
                           <i>Expert Tutorial Mode</i> directs student attention to an interaction with the overlaid interaction event boxes and look-at indicator of the expert, but the student must perform the interactions because the abstract simulation visualization comes from the real-time tracking data of the anesthesia machine, which is turned on during this mode. This promotes a more hands-on learning experience as the students metaphorically walk in the footsteps of the expert.</p></div></div></section><section aria-labelledby="Sec20"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">AAMVID-S informal pilot study</h2><div class="c-article-section__content" id="Sec20-content"><p>A pilot study was conducted to collect preliminary data on AAMVID-S and to begin to identify the advantages and disadvantages of collocated AAR. In the pilot study, the population was 19 students enrolled in an Introductory Psychology course. While anesthesia residents are the target population for the system, it is more difficult to investigate basic learning effects due to residents’ high variability in experience and knowledge of anesthesia machines. Instead of residents, we used psychology students that had no previous knowledge of anesthesia machines as a baseline to study learning effects.</p><p>The participants were first trained using the AAM. Then, they were given three machine fault tests. After each test, they used AAMVID-S for AAR, which was performed without an expert present (self-debriefing) and with minimal assistance from the experimenter (i.e., the experimenter would answer interface-related questions but not anesthesia-related questions). Each participant was given a questionnaire before and after the AAR to gauge how the AAR affected (1) understanding of the machine faults and (2) their level of confidence in their answers. As expected, participants’ average understanding and confidence significantly increased from &lt;1 on a scale of 0 to 4 to &gt;3 (<i>p</i> &lt; .001).</p><p>We also gave participants an opportunity to give subjective feedback in the open comments of the questionnaires. Most participants expressed that they would use the magic lens for machine fault study in the future because the magic lens was a useful tool in helping them to understand machine faults. Specifically, participants expressed that the “expert tutorial mode” was the most helpful AAMVID mode. Expert tutorial mode overlaid an expert’s interaction boxes and “look-at indicator” in situ with the real anesthesia machine and the abstract simulation. This mode allowed participants to observe and physically mimic the expert’s collocated interactions. Participants found the expert tutorial’s interaction boxes and look-at indicator easy to follow and noted how they preferred the mode’s more “hands-on experience.” This suggests that AAMVID’s collocated interaction boxes and look-at indicator approaches are an effective way to focus a student’s attention and direct them to the location in the training space where they need to interact.</p></div></div></section><section aria-labelledby="Sec21"><div class="c-article-section" id="Sec21-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec21">AAMVID-E for educators</h2><div class="c-article-section__content" id="Sec21-content"><p>Instead of reviewing individual student interaction, educators may be interested in identifying trends (e.g., many students may approach the same problem incorrectly) and outliers (e.g., the few students who perform exceptionally well or poorly) in groups of students. To meet this need, AAMVID-E can combine data from multiple students and visualize these data via the magic lens. Currently, AAMVID-E displays aggregate head-gaze and interactions (i.e., turning knobs and pressing buttons).</p><h3 class="c-article__sub-heading" id="Sec22">Gaze maps</h3><p>A visualization of gaze can help educators better understand the main components that students focus on during a fault test and allows them to adjust their education plans (e.g., lectures) accordingly. To enable gaze visualization, AAMVID-E generates a heat-mapped (i.e., the places where participants focused on appear more “hot” in color) visualization of where students were looking during a fault test (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig7">7</a>). This map is generated off-line.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig7_HTML.jpg" alt="figure7" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>A gaze map collocated with the machine. In this case, many students were looking at the flow meters during the fault test. These data can be interactively filtered using the slider controls in the <i>top left</i>
                                    </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec23">Implementation of gaze maps</h4><p>Our method of generating gaze maps is described in this section. This method requires a 3D model of the geometry that the gaze maps will overlay. In the case of AAMVID-E, we used a full scale 3D model of the real machine (shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig7">7</a>). This model must be registered to the real object (e.g., the anesthesia machine) that users were gazing at during the experience.</p><p>Preprocessing for every data point in a student’s log:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Query the log for head position and orientation.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Project 4 rays of the viewing frustum (generated by the head tracking data) into the texture space of the 3D model.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Additively blend a gray scale Gaussian into the current gaze map texture, which is initially transparent. The Gaussian is scaled and positioned based upon a quadrilateral formed by the ray intersection points.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>Map the additively blended textures to a heat scale—a 1D array of RGB values that visually appear “hotter” as the array index.</p>
                        
                      </li>
                    </ol>
                           <p>In our specific implementation, the gaze maps are alpha blended with underlying textured machine geometry. For example, the machine shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig7">7</a> is a textured 3D model. However, gaze mapping could be extended to a see-through (e.g., video or optical) display. If the registered 3D machine model was not rendered, then the gaze maps could be blended with a real-time video stream instead.</p><p>Although preprocessing time increases based on the number of data points, the preprocessing step needs only to be performed once. After the gaze maps are generated, they are written to textures, which can be stored on a hard disk for future runs of the application. Then, the gaze maps can be rendered at an interactive rate (e.g., 60 fps).</p><h3 class="c-article__sub-heading" id="Sec24">Markov model of class interaction</h3><p>Trends in student interaction are important to improving educators’ pedagogical approaches. This type of information is useful in determining whether students unknowingly perform interactions that are potentially harmful to a patient (e.g., over inflating the lungs with the oxygen flush valve). If the educator is able to isolate such a trend, then they can adjust their lesson plans accordingly. To meet this need, AAMVID-E aggregates and visualizes the interactions of an entire class of students.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec25">Implementation</h4><p>Each student’s interaction event chain (explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec16">5.1.3</a>) is integrated into a simple Markov model (Fishwick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Fishwick PA (1995) Simulation model design and execution: building digital worlds. Prentice Hall, USA&#xA;" href="/article/10.1007/s10055-013-0229-6#ref-CR5" id="ref-link-section-d84870e1104">1995</a>). A Markov model can be represented as a directed graph in which each arc has a probability associated with it. For a given node, all of the arc weights stemming from that node add up to 1. When traversing this graph as in a simulation, the arc weights represent the probability that a subsequent node will be visited. For example, a set of user logs contains a finite set of discrete interaction events. These events form the nodes of the directed graph. The sequence of events forms the arcs of the graph and the frequency of these sequences determines the weights on the arcs.</p><p>Based upon the sequences and frequencies of multiple users’ events, we can generate a probability (e.g., the percentage of students that performed the sequence) that a specific sequence of events will occur. These probabilities are the basis for the resulting Markov model.</p><p>Given such a model, educators can generate the probability that a student in the class will first increase the O<sub>2</sub> flow and second decrease the N<sub>2</sub>O flow. These data are visualized as a directed graph (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig8">8</a>), which can be collocated with the anesthesia machine and visualized using the magic lens (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig9">9</a>). To interact with the interaction model, instructors press buttons and turn knobs on the machine in an order of their choosing, that is, by interacting with the machine, the educators traverse the directed graph of possible interactions. Then, the model generates the probability that a student in the class would perform that sequence of actions.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig8_HTML.jpg" alt="figure8" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>A heat-mapped (on frequency of interaction), directed graph of aggregate student interaction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                              <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig9_HTML.jpg" alt="figure9" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>The interaction graph is collocated with the machine. Educators can test the probability of interaction sequences, highlighted by the <i>icons</i> at the <i>top</i>
                                       </p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <h3 class="c-article__sub-heading" id="Sec26">Data filtering</h3><p>For educators to more effectively identify class trends, it is helpful to be able to filter the data based on certain parameters such as class performance or standardized test results. To meet this need, AAMVID-E allows educators to interactively filter the data based on the parameters that the educator defines before runtime. For example, if the educator wanted to only visualize the gaze data of students with low spatial cognition, they can enter spatial ability test values for each student in the aggregate log files. At runtime, the expert can interactively manipulate sliders to select the range of spatial ability to more effectively isolate and identify trends and outliers in the class.</p></div></div></section><section aria-labelledby="Sec27"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27">Informal expert evaluation of AAMVID-E</h2><div class="c-article-section__content" id="Sec27-content"><p>To evaluate the potential benefits of collocated AAR for educators, an expert in anesthesia education informally assessed the AAMVID-E. The expert was Nikolaus Gravenstein, M.D., Professor and former Chair of the Anesthesiology Department at the University of Florida and was not directly affiliated with this project outside of being employed in University of Florida Anesthesia. Dr. Gravenstein used AAMVID-E to visualize and interact with the aggregate training data obtained from the 19 AAMVID-S study participants. Then, he was interviewed about his experience. In general, he thought AAMVID-E was usable and generalizable:</p><p>“You are presenting us with a new way to look at this kind of stuff in our weird environment. And the application isn’t unique to our environment; the application is really in any environment where there are degrees of ability—especially where there are lots of steps and complexities that you have to sort out.”—Gravenstein.</p><p>In the interview, we also asked questions about the advantages and disadvantages of the system. In particular, we asked whether he would prefer using a desktop version over the MR version of the system. He replied that he would prefer both desktop and the MR versions. For convenience (and because the lens can be cumbersome at times), they would like to use a desktop version for personal review—to visualize the data in the office or on his computer. He would use the MR version for external review to (1) perform an instructor-assisted AAR with a student, (2) visualize the data for non-educators (such as anesthesia machine engineers), and (3) to physically interact with the machine to manipulate the data. This identifies general use cases for the system an points to the fact that MR would likely be effective for collaborative visualization but less effective for personal visualization. In the future, we will conduct formal studies to investigate the collaborative aspects of collocated AAR.</p></div></div></section><section aria-labelledby="Sec28"><div class="c-article-section" id="Sec28-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec28">AAMVID integration with a mannequin patient simulator</h2><div class="c-article-section__content" id="Sec28-content"><p>The purpose of this section is to demonstrate the generalizability of AAMVID’s visual and interactive approach through its application to a slightly different task. Here, we extend AAMVID to also include a simulated patient (a physical mannequin simulation of a patient, Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig10">10</a>). By observing the development, extension, and integration of AAMVID with machine simulation and patient simulation, we aim to derive general design guidelines for future applications (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec37">10</a> for discussion).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig10_HTML.jpg" alt="figure10" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Disinfecting the patient (i.e., the HPS) with a ChloraPrep™ applicator (<i>inset</i>)</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                     <p>In anesthesia training, the human patient is often simulated with a mannequin patient simulator, such as the METI Human Patient Simulator™ (HPS) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig10">10</a>). These mannequins physically simulate processes in the human body such as breathing, heart rate, and drug actions with a high degree of fidelity. We have extended the AAMVID collocated AAR system to integrate a human patient simulator, giving students additional insight into the consequences of their interactions with patients.</p><p>However, extending AAMVID for the HPS was not as straightforward as replacing the 3D machine model with a 3D human model. As compared to a collocated AAR with the anesthesia machine, collocated AAR with the HPS requires additional interactions, visualizations, and infrastructure. We hope that the implementation presented here will offer insight into how to design more generalizable MR-based AAR frameworks in the future.</p><h3 class="c-article__sub-heading" id="Sec29">Procedures for skin preparation and disinfection</h3><p>We extended AAMVID to provide AAR for reviewing training experiences in skin preparation and disinfection techniques. As a precursor to describe how AAMVID was extended for this application, this section overviews the basic procedure and practice of skin preparation and disinfection that is conducted as part of surgical and anesthesia.</p><p>Skin can be considered a mechanical barrier that prevents infectious organisms from entering our body. Any time that the skin is punctured (e.g., incision for surgical procedures or catheter insertion in anesthesia), there is concern that infectious organisms on the skin are thereby afforded entry into the body. To reduce this risk, the skin at and around the intended site of puncture is “prepped” (i.e., scrubbed and disinfected) prior to puncturing or cutting the skin. Improper skin preparation is believed to be a cause of hospital-acquired infection that may result in increased morbidity and mortality and lengthened hospital stays. Thus, proper skin prepping is a basic and fundamental procedural skill that has profound impact on patient safety and health care costs.</p><p>Before breaking the skin, the skin must be clipped, scrubbed, and disinfected to minimize the risk of infection. To disinfect the area, practitioners may use a ChloraPrep™ applicator (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig10">10</a>), which, once activated (i.e., through breaking an inner seal), releases a disinfecting liquid through a pad on the tip of the applicator. Some applicators contain an added dye that colors the skin to indicate the parts of the skin where the pad has already been applied while other colorless applicators contain no dye. Practitioners then attempt to thoroughly scrub a designated skin area with the pad.</p><p>However, scrubbing technique may be sub-optimal and lead to subsequent infection. When a dye is present, the colored liquid marks the areas that have been scrubbed but does not indicate how well the area has actually been disinfected, which can be dependent on how much the area has been rescrubbed and the technique of scrubbing. For example, practitioners may traditionally scrub the area in a circular or outward spiral motion. With this technique, infectious organisms may hide under the hair stubbles on the skin (i.e., after clipping, there are still hair stubbles present). Scrubbing in a circular or outward spiral motion lays the hair stubbles flat in a particular direction, such that the skin under one side of the hairs is not disinfected.</p><p>A recommended technique for disinfecting the skin is to scrub each skin area from 4 orthogonal directions (i.e., in a grid pattern: east to west, west to east, north to south, and south to north). This technique is believed to help reduce the risk of infection and has been demonstrated in an online simulation (Lizdas et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Lizdas DE, Gravenstein N, Lampotang S (2009) Transparent reality simulation of skin prepping. Simul Healthcare. 4(5):193. &#xA;                    http://vam.anest.ufl.edu/skinprep/&#xA;                    &#xA;                  &#xA;                        " href="/article/10.1007/s10055-013-0229-6#ref-CR12" id="ref-link-section-d84870e1250">2009</a>).</p><h3 class="c-article__sub-heading" id="Sec30">Augmenting the skin with heat maps of disinfection effectiveness</h3><p>Although some ChloraPrep™ applicator pads dye the skin to indicate where the disinfecting liquid has been applied, they do not indicate the effectiveness of the user’s skin prepping technique. This could lead to poor disinfection in practice and could be one of the reasons for hospital-acquired infections. To address this problem, we created an interactive visualization for disinfection effectiveness that can be utilized during the collocated AAR.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec31">Tracking of scrubbing interaction</h4><p>To visualize the disinfection effectiveness, the user interaction with the applicator must be tracked. We tracked both the applicator and the HPS, using the same IR-marker-based vision tracking system that is used for the magic lens (Jitter: 5 mm, Latency 70 ms). First, markers are affixed to the HPS and the applicator. Then, these two objects are manually calibrated to enable an approximate registration between the HPS, the virtual representation of the HPS, and the disinfection pad.</p><p>The tracking markers of the HPS are then used to define a local coordinate system, which enables the system to evaluate the directions of the scrubbing motions (e.g., using a grid pattern of motion versus a circular pattern). This system first determines whether a collision between the HPS and the scrub pad has occurred, using collision meshes for both the pad and the HPS. Upon collision, the system tracks the number of passes over each part of the prep area and the direction of scrubbing (i.e., using a threshold for scrubbing speed to ensure the user is actually making continuous passes over the scrub area). For the most effective disinfection, the user must scrub in four directions. This information is then used to generate the visualization of scrubbing effectiveness.</p><p>Pressure applied with the applicator is not being tracked, although it may affect the disinfection effectiveness. In the future, we may consider instrumenting the applicator with a pressure sensor and incorporating these data into the visualization.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec32">Heat map visualization implementation</h4><p>Based on the tracking of scrubbing motion, the system generates a heat map (i.e., a scrub map) of the scrubbed area of the patient in real time. “Hotter” areas indicate more effective disinfection (i.e., white indicates excellent disinfection). To generate this heat map in real time, the system uses the tracked collisions between the HPS and the pad and the user’s motion pattern (i.e., in four directions as visualized in the squares on the left of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig11">11</a>). The system uses a similar, yet modified, heat mapping algorithm similar to the algorithm described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec23">7.1.1</a>. This algorithm has been modified to incorporate the directionality of the scrubber and has been optimized to enable rendering at interactive rates (e.g., 60 fps).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig11_HTML.jpg" alt="figure11" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>A heat map of disinfection effectiveness (shown on the <i>chest</i>) as viewed from the magic lens. The four maps to the <i>left</i> show the coverage in each of four directions</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>An interactive heat mapping algorithm is performed whenever a collision between the HPS and disinfection pad is detected:</p><ol class="u-list-style-none">
                      <li>
                        <span class="u-custom-list-number">1.</span>
                        
                          <p>Query the tracker for positions and orientations of pad and HPS.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">2.</span>
                        
                          <p>Project a gray scale Gaussian, that is, the same diameter as the circular pad into the texture space of the HPS mesh.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">3.</span>
                        
                          <p>Additively blend the Gaussian into one or more of four collocated texture maps, each representing a particular direction (e.g., if the user scrubs left, the Gaussian is added to the “left” texture map).</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">4.</span>
                        
                          <p>As displayed in the four boxes in the left of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig11">11</a>, map each of the four directional textures to a heat scale—a 1D array of RGB values that visually appear “hotter” as the array index.</p>
                        
                      </li>
                      <li>
                        <span class="u-custom-list-number">5.</span>
                        
                          <p>Additively blend and normalize all four of the texture maps and display the combined heat map collocated with the HPS mesh (as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig11">11</a> on the HPS’s chest area).</p>
                        
                      </li>
                    </ol>
                           <p>It is important to only update the textures in the region defined by the collision of the HPS and the disinfection pad. This significantly decreases the number of texels that are updated each frame (as opposed to updating all of the texels in all five textures each frame). This helps to ensure high frame rates and real-time interactivity.</p><p>For an example interaction, if the user scrubs in only two directions on the HPS’s chest (e.g., right to left and left to right), only two of the textures (i.e., shown in the left of Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig11">11</a>) will be updated and the combined texture map on the chest will be “cooler,” indicating that the area is not adequately disinfected.</p><h3 class="c-article__sub-heading" id="Sec33">Bacterial simulation and visualization</h3><p>One of the main purposes of collocated AAR is to better enable students to understand the consequences of procedural decisions and to consider those consequences when interacting with patients. To enable students to recognize these consequences for disinfection, we implemented a visualization of bacteria infecting the patient post-procedurally.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec34">Simulation and visualization implementation</h4><p>In the visualization, the simulated rate and spread of bacterial infection is dependent on how effectively the student disinfected the skin. When the student presses a button to start the simulation, the percent coverage of the skin disinfection area is computed and used to drive the spread of bacteria throughout the body (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig12">12</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig12_HTML.jpg" alt="figure12" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>A bacterial simulation interactively spreads over the whole body when the skin is not effectively disinfected</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                           <p>The HPS model has a single texture for all of the skin that is essentially wrapped around the mesh. After the simulation begins, images of bacteria are blended into the skin texture until the bacteria growth period has elapsed. Although the simulated period is accelerated since bacterial growth may occur over several days, the growth rate increases and decreases, based on a model of bacterial growth (Keen and Spain <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Keen R, Spain J (1992) Computer simulation in biology; a basic introduction. Wiley, New York" href="/article/10.1007/s10055-013-0229-6#ref-CR8" id="ref-link-section-d84870e1421">1992</a>). The bacteria images then spread throughout the body until the simulation growth period has elapsed.</p><p>The growth area is controlled by a “window” that increases over the entire skin texture, that is, when the simulation starts, the window is the disinfection area (e.g., the small square on the chest).Then, the window grows to simulate bacteria spreading over the body. The bacteria textures are randomly drawn into the window, so that as the window increases, the bacteria are more sparsely distributed (i.e., a typical bacterial growth rate model is exponential in the beginning, then constant, and then exponentially decreases). With bacteria textures drawn at random positions into the window, we simulate bacterial growth throughout the body as the window grows.</p><p>While the simulation is based on a widely used bacterial growth model, there are no current models for, and minimal data on, how bacteria actually distributes through the body, that is, this simulation is largely demonstrative in that regard. The point of this simulation is not to educate students about the specifics of how bacterial infection distributes through the body, but more to enable the students to recognize the potential consequences of improper disinfection procedure.</p><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec35">Synchronization with the HPS</h4><p>The HPS has the ability to simulate the physiological and pharmacological reactions of a real human. These reactions of the HPS are synchronized to the skin prep simulation and thereby enhance the realism of the experience and reinforce the consequences of disinfection effectiveness. When the bacterial simulation is triggered, the HPS will react accordingly based on the intensity of the bacterial growth. If the area was poorly disinfected, the HPS may simulate sepsis. For an example of sepsis, the heart rate of the patient (i.e., indicated by the heart rate monitor, the palpable pulse, and the rate of the pulse oximeter tone) significantly increases and the patient becomes unconscious.</p><p>This synchronization was initially performed by manually synchronizing the HPS and the bacterial simulation through a Wizard of Oz interface. Since this initial work, we now have communication access to the HPS via its HIDEP protocol. HIDEP allows systems to directly send commands to the HPS and receive the HPS state in real time.</p><h3 class="c-article__sub-heading" id="Sec36">Usage</h3><p>This interactive visualization can be viewed either during training, as part of the AAR, or both. For example, the student can practice sterilizing the skin without augmentation through visualization. Then, during the AAR, the student can use the magic lens to visualize how effectively the skin was disinfected. Next, the student can trigger the bacterial simulation and visualize the consequences of their disinfection procedure—the spread of bacteria. Finally, the student can reset the simulation and then scrub the area again, while interactively visualizing their disinfection effectiveness (i.e., the heat map described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec30">9.2</a>). Upon running the bacterial simulation again, students will see a significant decrease in bacterial growth and the patient may not have sepsis (i.e., assuming the student disinfected the area effectively enough the second time). In the future, we aim to formally study the learning effects of this system.</p></div></div></section><section aria-labelledby="Sec37"><div class="c-article-section" id="Sec37-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec37">Collocated AAR: design considerations and lessons learned</h2><div class="c-article-section__content" id="Sec37-content"><p>Although the two applications of collocated AAR presented in this paper (i.e., anesthesia machine operation and skin disinfection) are both integral in anesthesia training, they each include fundamentally different skills that must be trained and different information. Each of these applications presents different challenges for visualization and interaction. Our collocated AAR addressed these challenges in different ways. Feedback from the usability testing of anesthesia machine operation training (Sects. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec20">6</a>, <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-013-0229-6#Sec27">8</a>) and informal demos of both applications greatly influenced our design decisions and future research directions. This section overviews the lessons learned from problems we encountered and offers potential solutions with the intent of ultimately generalizing collocated AAR to a wider range of applications.</p><h3 class="c-article__sub-heading" id="Sec38">Problem: information overload</h3><p>The main problem that we have encountered with designing collocated AAR systems is that of information overload, that is, collocated AAR attempts to collocate multiple information streams (e.g., interactions of novices and experts) and information types (e.g., physical and virtual) into the same context. The purpose is to enable users to better understand the semantic links between the disparate concepts. However, in complex applications such as anesthesia, there can be so much collocated information users become overloaded and may have difficulty focusing.</p><p>For example, it is important for anesthesia students to learn how anesthesia affects the body at multiple scales (e.g., regional anesthesia, the nervous system, a neuron, and a synapse). For students to understand how these scales are semantically and spatially related, the AAR system can collocate this information. However, if we collocated all of this information into one context, it would most likely result in information overload, that is, care must be taken when deciding on collocation methodology.</p><h3 class="c-article__sub-heading" id="Sec39">Proposed solution 1: tangible interfaces for information filtering</h3><p>A possible way to reduce information overload is to enable the user to interactively filter this information as needed. However, one has to be careful to design an interface that preserves the semantic and spatial relationships in the information (e.g., relationships between how anesthesia operates at the neural level and how it operates at the synaptic level). To address this, we developed a technology demonstration for the HPS that enables the user to control the current scale (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-013-0229-6#Fig13">13</a>) with the magic lens as a tangible interface, that is, the user controls how much information they are seeing at each scale by interacting with the magic lens to semantically zoom to smaller and smaller levels. As users physically move closer to the mannequin, the visuals can be exponentially magnified. This semantic magnifying glass metaphor preserves the spatial and semantic relationships between the scales and still gives the user control over the information presented. The lesson here is that it may be possible to minimize information overload in collocated AAR through the use of information visualization techniques and a tangible interface.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-013-0229-6/MediaObjects/10055_2013_229_Fig13_HTML.jpg" alt="figure13" loading="lazy" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>The magic lens is used to zoom to anesthesia in different levels of the body, (<i>top left</i>) full body, (<i>top right</i>) regional, (<i>bottom left</i>) neuron, (<i>bottom right</i>) synapse</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-013-0229-6/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div>
                        <h3 class="c-article__sub-heading" id="Sec40">Proposed solution 2: just-in-time feedback for information filtering</h3><p>Alternatively, the system itself could offer program-controlled information filtering through just-in-time feedback, that is, instead of delaying the AAR until after the fault test or skin prep, the system could intelligently track how well the student is performing the procedure and adaptively offer feedback based on this. The timing and nature of this feedback could be based on intelligent tutoring approaches (Lynch et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Lynch C, Ashley K, Aleven V, Pinkwart N (2006) Defining “ill-defined domains”; a literature survey. In: Proceedings of the workshop on intelligent tutoring systems for ill-defined domains at the 8th international conference on intelligent tutoring systems (pp 1–10) National Central University, Jhongli (Taiwan)" href="/article/10.1007/s10055-013-0229-6#ref-CR14" id="ref-link-section-d84870e1522">2006</a>, Suebnukarn and Haddaway <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Suebnukarn S, Haddawy P (2006) A Bayesian approach to generating tutorial hints in a collaborative medical problem-based learning system. Artif Intell Med 38(1):5–24" href="/article/10.1007/s10055-013-0229-6#ref-CR25" id="ref-link-section-d84870e1525">2006</a>), for example. The system could effectively pause the simulation, alerting the student that they need to perform differently. This blurs the line between training and AAR, but it could be helpful to reinforce reflective learning, as demonstrated in some training transfer literature (Yamnill and McLean <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yamnill S, McLean G (2001) Theories supporting transfer of training. Human Resour Dev Q 12(2):195" href="/article/10.1007/s10055-013-0229-6#ref-CR28" id="ref-link-section-d84870e1528">2001</a>).</p><p>However, care must still be taken when providing timely information so as not to overload the user with too much information (Kester et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Kester L, Kirschner P, van Merrinboer J (2006) Just-in-time information presentation: improving learning a troubleshooting skill. Contemp Educ Psychol 31(2):167–185" href="/article/10.1007/s10055-013-0229-6#ref-CR9" id="ref-link-section-d84870e1534">2006</a>). Different applications may have different requirements, and further research is needed to effectively employ adaptive just-in-time learning approaches.</p></div></div></section><section aria-labelledby="Sec41"><div class="c-article-section" id="Sec41-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec41">Conclusions</h2><div class="c-article-section__content" id="Sec41-content"><p>We proposed the idea of collocated AAR using MR. MR-based collocated AAR augments the traditional AAR process by (1) allowing a user-controlled egocentric viewpoint, (2) overlaying virtual information that enhances learning (i.e., abstract simulation and automatic annotation of interaction events), and (3) collocating training experiences in situ with the real training area (i.e., collocating one’s own previous experience, an expert’s previous experience, and current real-time experience).</p><p>The main challenges to MR are how to collocate multiple past experiences and the current experience with respect to time, interaction, and visualization. Our approach to addressing these challenges is exemplified with AAMVID—a MR-based system for collocated AAR in anesthesia education. We presented two systems: (1) AAR of anesthesia machine operation training experiences and (2) AAR of skin disinfection with a mannequin patient simulator.</p><p>Students and an educator evaluated AAMVID in pilot studies. Their feedback suggests that (1) collocated AAR is a viable type of AAR that can effectively direct a student’s attention and interaction, and (2) collocated AAR offers educators’ novel assessment tools with potential advantages in collaborative visualization. In the future, we will conduct formal comparative studies to determine the specific educational benefits of collocated AAR.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bier EA, Stone MC, Pier K, Buxton W, DeRose TD (1993) Toolglass and magic lenses: the see-through interface. I" /><p class="c-article-references__text" id="ref-CR1">Bier EA, Stone MC, Pier K, Buxton W, DeRose TD (1993) Toolglass and magic lenses: the see-through interface. In: Proceedings of the 20th annual conference on computer graphics and interactive techniques, pp 73–80</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PT. Chua, R. Crivella, B. Daly, N. Hu, R. Schaaf, D. Ventura, T. Camill, J. Hodgins, R. Pausch, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Chua PT, Crivella R, Daly B, Hu N, Schaaf R, Ventura D, Camill T, Hodgins J, Pausch R (2003) Training for phys" /><p class="c-article-references__text" id="ref-CR2">Chua PT, Crivella R, Daly B, Hu N, Schaaf R, Ventura D, Camill T, Hodgins J, Pausch R (2003) Training for physical tasks in virtual environments: Tai Chi. Proc IEEE Virtual Real 2003:87–94</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 2 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Training%20for%20physical%20tasks%20in%20virtual%20environments%3A%20Tai%20Chi&amp;journal=Proc%20IEEE%20Virtual%20Real&amp;volume=2003&amp;pages=87-94&amp;publication_year=2003&amp;author=Chua%2CPT&amp;author=Crivella%2CR&amp;author=Daly%2CB&amp;author=Hu%2CN&amp;author=Schaaf%2CR&amp;author=Ventura%2CD&amp;author=Camill%2CT&amp;author=Hodgins%2CJ&amp;author=Pausch%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Department of the Army (1993) Washington, DC. Training Circular 25–20: A leader’s guide to after-action review" /><p class="c-article-references__text" id="ref-CR3">Department of the Army (1993) Washington, DC. Training Circular 25–20: A leader’s guide to after-action reviews</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="I. Fischler, CE. Kaschub, DE. Lizdas, S. Lampotang, " /><meta itemprop="datePublished" content="2008" /><meta itemprop="headline" content="Fischler I, Kaschub CE, Lizdas DE, Lampotang S (2008) Understanding of anesthesia machine function is enhanced" /><p class="c-article-references__text" id="ref-CR4">Fischler I, Kaschub CE, Lizdas DE, Lampotang S (2008) Understanding of anesthesia machine function is enhanced with a transparent reality simulation. Simul Health Care 3:26–32</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1097%2FSIH.0b013e31816366d3" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20of%20anesthesia%20machine%20function%20is%20enhanced%20with%20a%20transparent%20reality%20simulation&amp;journal=Simul%20Health%20Care&amp;volume=3&amp;pages=26-32&amp;publication_year=2008&amp;author=Fischler%2CI&amp;author=Kaschub%2CCE&amp;author=Lizdas%2CDE&amp;author=Lampotang%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PA. Fishwick, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Fishwick PA (1995) Simulation model design and execution: building digital worlds. Prentice Hall, USA&#xA;" /><p class="c-article-references__text" id="ref-CR5">Fishwick PA (1995) Simulation model design and execution: building digital worlds. Prentice Hall, USA
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 5 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Simulation%20model%20design%20and%20execution%3A%20building%20digital%20worlds&amp;publication_year=1995&amp;author=Fishwick%2CPA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="PA. Fishwick, " /><meta itemprop="datePublished" content="2004" /><meta itemprop="headline" content="Fishwick PA (2004) Toward an integrative multimodeling interface: a human-computer interface approach to inter" /><p class="c-article-references__text" id="ref-CR6">Fishwick PA (2004) Toward an integrative multimodeling interface: a human-computer interface approach to interrelating model structures. Simulation 80(9):421</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F0037549704044081" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Toward%20an%20integrative%20multimodeling%20interface%3A%20a%20human-computer%20interface%20approach%20to%20interrelating%20model%20structures&amp;journal=Simulation&amp;volume=80&amp;issue=9&amp;publication_year=2004&amp;author=Fishwick%2CPA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Procee" /><p class="c-article-references__text" id="ref-CR7">Ishii H, Ullmer B (1997) Tangible bits: towards seamless interfaces between people, bits and atoms. In: Proceedings of the SIGCHI conference on Human factors in computing systems, pp 234–241</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Keen, J. Spain, " /><meta itemprop="datePublished" content="1992" /><meta itemprop="headline" content="Keen R, Spain J (1992) Computer simulation in biology; a basic introduction. Wiley, New York" /><p class="c-article-references__text" id="ref-CR8">Keen R, Spain J (1992) Computer simulation in biology; a basic introduction. Wiley, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Computer%20simulation%20in%20biology%3B%20a%20basic%20introduction&amp;publication_year=1992&amp;author=Keen%2CR&amp;author=Spain%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Kester, P. Kirschner, J. Merrinboer, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Kester L, Kirschner P, van Merrinboer J (2006) Just-in-time information presentation: improving learning a tro" /><p class="c-article-references__text" id="ref-CR9">Kester L, Kirschner P, van Merrinboer J (2006) Just-in-time information presentation: improving learning a troubleshooting skill. Contemp Educ Psychol 31(2):167–185</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.cedpsych.2005.04.002" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Just-in-time%20information%20presentation%3A%20improving%20learning%20a%20troubleshooting%20skill&amp;journal=Contemp%20Educ%20Psychol&amp;volume=31&amp;issue=2&amp;pages=167-185&amp;publication_year=2006&amp;author=Kester%2CL&amp;author=Kirschner%2CP&amp;author=Merrinboer%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Knerr BW, Lampton DR, Martin GA, Washburn DA, Cope D (2002) Developing an after action review system for virtu" /><p class="c-article-references__text" id="ref-CR10">Knerr BW, Lampton DR, Martin GA, Washburn DA, Cope D (2002) Developing an after action review system for virtual dismounted infantry simulations. In the interservice industry training, simulation &amp; education conference (I/ITSEC)
</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Lampotang, DE. Lizdas, N. Gravenstein, EB. Liem, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Lampotang S, Lizdas DE, Gravenstein N, Liem EB (2006) Transparent reality, a simulation based on interactive d" /><p class="c-article-references__text" id="ref-CR11">Lampotang S, Lizdas DE, Gravenstein N, Liem EB (2006) Transparent reality, a simulation based on interactive dynamic graphical models emphasizing visualization. Educ Technol 46(1):55–59</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transparent%20reality%2C%20a%20simulation%20based%20on%20interactive%20dynamic%20graphical%20models%20emphasizing%20visualization&amp;journal=Educ%20Technol&amp;volume=46&amp;issue=1&amp;pages=55-59&amp;publication_year=2006&amp;author=Lampotang%2CS&amp;author=Lizdas%2CDE&amp;author=Gravenstein%2CN&amp;author=Liem%2CEB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="DE. Lizdas, N. Gravenstein, S. Lampotang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Lizdas DE, Gravenstein N, Lampotang S (2009) Transparent reality simulation of skin prepping. Simul Healthcare" /><p class="c-article-references__text" id="ref-CR12">Lizdas DE, Gravenstein N, Lampotang S (2009) Transparent reality simulation of skin prepping. Simul Healthcare. 4(5):193. <a href="http://vam.anest.ufl.edu/skinprep/">http://vam.anest.ufl.edu/skinprep/</a>
                        </p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Transparent%20reality%20simulation%20of%20skin%20prepping&amp;journal=Simul%20Healthcare.&amp;volume=4&amp;issue=5&amp;publication_year=2009&amp;author=Lizdas%2CDE&amp;author=Gravenstein%2CN&amp;author=Lampotang%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Looser J, Billinghurst M, Cockburn A (2004) Through the looking glass: the use of lenses as an interface tool " /><p class="c-article-references__text" id="ref-CR13">Looser J, Billinghurst M, Cockburn A (2004) Through the looking glass: the use of lenses as an interface tool for Augmented Reality interfaces. In: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia, pp 204–211</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lynch C, Ashley K, Aleven V, Pinkwart N (2006) Defining “ill-defined domains”; a literature survey. In: Procee" /><p class="c-article-references__text" id="ref-CR14">Lynch C, Ashley K, Aleven V, Pinkwart N (2006) Defining “ill-defined domains”; a literature survey. In: Proceedings of the workshop on intelligent tutoring systems for ill-defined domains at the 8th international conference on intelligent tutoring systems (pp 1–10) National Central University, Jhongli (Taiwan)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Milgram, F. Kishino, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst 77(12):1321–1329" /><p class="c-article-references__text" id="ref-CR15">Milgram P, Kishino F (1994) A taxonomy of mixed reality visual displays. IEICE Trans Inf Syst 77(12):1321–1329</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 15 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20taxonomy%20of%20mixed%20reality%20visual%20displays&amp;journal=IEICE%20Trans%20Inf%20Syst&amp;volume=77&amp;issue=12&amp;pages=1321-1329&amp;publication_year=1994&amp;author=Milgram%2CP&amp;author=Kishino%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Morie, K. Iyer, D. Luigi, J. Williams, A. Dozois, A. Rizzo, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Morie J, Iyer K, Luigi D, Williams J, Dozois A, Rizzo A (2005) Development of a data management tool for inves" /><p class="c-article-references__text" id="ref-CR16">Morie J, Iyer K, Luigi D, Williams J, Dozois A, Rizzo A (2005) Development of a data management tool for investigating multivariate space and free will experiences in virtual reality. Appl Psychophysiol Biofeedback 30(3):319–331</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10484-005-6386-y" aria-label="View reference 16">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 16 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Development%20of%20a%20data%20management%20tool%20for%20investigating%20multivariate%20space%20and%20free%20will%20experiences%20in%20virtual%20reality&amp;journal=Appl%20Psychophysiol%20Biofeedback&amp;volume=30&amp;issue=3&amp;pages=319-331&amp;publication_year=2005&amp;author=Morie%2CJ&amp;author=Iyer%2CK&amp;author=Luigi%2CD&amp;author=Williams%2CJ&amp;author=Dozois%2CA&amp;author=Rizzo%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Park, PA. Fishwick, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Park M, Fishwick PA (2005) Integrating dynamic and geometry model components through ontology-based inference." /><p class="c-article-references__text" id="ref-CR17">Park M, Fishwick PA (2005) Integrating dynamic and geometry model components through ontology-based inference. Simulation 81(12):795</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F0037549705064359" aria-label="View reference 17">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Integrating%20dynamic%20and%20geometry%20model%20components%20through%20ontology-based%20inference&amp;journal=Simulation&amp;volume=81&amp;issue=12&amp;publication_year=2005&amp;author=Park%2CM&amp;author=Fishwick%2CPA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract an" /><p class="c-article-references__text" id="ref-CR18">Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008a) A mixed reality approach for merging abstract and concrete knowledge. IEEE Virtual Real, Reno, pp 27–34</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spat" /><p class="c-article-references__text" id="ref-CR19">Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008b) Tangible user interfaces compensate for low spatial cognition. IEEE 3D User Interfaces Reno, pp 11–18</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008c) Collocated AAR: augmenting after action review w" /><p class="c-article-references__text" id="ref-CR20">Quarles J, Lampotang S, Fischler I, Fishwick P, Lok B (2008c) Collocated AAR: augmenting after action review with mixed reality. IEEE Int Symp Mixed Augment Reality Cambridge, pp 107–116</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Raij A, Lok B (2008) IPSVIZ: an after-action review tool for human virtual human experiences. IEEE Virt Realit" /><p class="c-article-references__text" id="ref-CR21">Raij A, Lok B (2008) IPSVIZ: an after-action review tool for human virtual human experiences. IEEE Virt Reality, Reno, pp 91–98</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Sielhorst T, Blum T, Navab N (2005) Synchronizing 3D movements for quantitative comparison and simultaneous vi" /><p class="c-article-references__text" id="ref-CR22">Sielhorst T, Blum T, Navab N (2005) Synchronizing 3D movements for quantitative comparison and simultaneous visualization of actions. In: Proceedings of the Fourth IEEE and ACM International Symposium on Mixed and Augmented, 2005, pp  38–47</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Smith R, Allen G (1994) After action review in military training simulations. In WSC’94: Proceedings of the 26" /><p class="c-article-references__text" id="ref-CR23">Smith R, Allen G (1994) After action review in military training simulations. In WSC’94: Proceedings of the 26th conference on Winter Simulation, 845–849, San Diego, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Studiocode (2008) Retrieved 28 April 2008 from http://www.studiocodegroup.com&#xA;                        " /><p class="c-article-references__text" id="ref-CR24">Studiocode (2008) Retrieved 28 April 2008 from <a href="http://www.studiocodegroup.com">http://www.studiocodegroup.com</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Suebnukarn, P. Haddawy, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Suebnukarn S, Haddawy P (2006) A Bayesian approach to generating tutorial hints in a collaborative medical pro" /><p class="c-article-references__text" id="ref-CR25">Suebnukarn S, Haddawy P (2006) A Bayesian approach to generating tutorial hints in a collaborative medical problem-based learning system. Artif Intell Med 38(1):5–24</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.artmed.2005.04.003" aria-label="View reference 25">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20Bayesian%20approach%20to%20generating%20tutorial%20hints%20in%20a%20collaborative%20medical%20problem-based%20learning%20system&amp;journal=Artif%20Intell%20Med&amp;volume=38&amp;issue=1&amp;pages=5-24&amp;publication_year=2006&amp;author=Suebnukarn%2CS&amp;author=Haddawy%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Viega J, Conway MJ, Williams G, Pausch R (1996) 3D magic lenses. In: Proceedings of the 9th annual ACM symposi" /><p class="c-article-references__text" id="ref-CR26">Viega J, Conway MJ, Williams G, Pausch R (1996) 3D magic lenses. In: Proceedings of the 9th annual ACM symposium on User interface software and technology, pp 51–58</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Virtual anesthesia machine (2010) Retrieved 21 April 2010, from http://vam.anest.ufl.edu/wip.html&#xA;            " /><p class="c-article-references__text" id="ref-CR27">Virtual anesthesia machine (2010) Retrieved 21 April 2010, from <a href="http://vam.anest.ufl.edu/wip.html">http://vam.anest.ufl.edu/wip.html</a>
                        </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Yamnill, G. McLean, " /><meta itemprop="datePublished" content="2001" /><meta itemprop="headline" content="Yamnill S, McLean G (2001) Theories supporting transfer of training. Human Resour Dev Q 12(2):195" /><p class="c-article-references__text" id="ref-CR28">Yamnill S, McLean G (2001) Theories supporting transfer of training. Human Resour Dev Q 12(2):195</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1002%2Fhrdq.7" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Theories%20supporting%20transfer%20of%20training&amp;journal=Human%20Resour%20Dev%20Q&amp;volume=12&amp;issue=2&amp;publication_year=2001&amp;author=Yamnill%2CS&amp;author=McLean%2CG">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-013-0229-6-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgments</h2><div class="c-article-section__content" id="Ack1-content"><p>This research is supported in part by National Science Foundation Grant IIS-0643557. A special thanks goes to Nikolaus Gravenstein, David Lizdas, Andrew Raij, Kyle Johnsen, Cynthia Kaschub, and the study participants. Some of the technology mentioned here is UF patent pending.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, San Antonio, TX, 78254, USA</p><p class="c-article-author-affiliation__authors-list">John Quarles</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Department of Anesthesiology, University of Florida, PO Box 100254, Gainesville, FL, 32610, USA</p><p class="c-article-author-affiliation__authors-list">Samsun Lampotang</p></li><li id="Aff3"><p class="c-article-author-affiliation__address">Department of Psychology, University of Florida, P.O. Box 112250, Gainesville, FL, 32611, USA</p><p class="c-article-author-affiliation__authors-list">Ira Fischler</p></li><li id="Aff4"><p class="c-article-author-affiliation__address">Department of CISE, University of Florida, PO Box 116120, Gainesville, FL, 32611, USA</p><p class="c-article-author-affiliation__authors-list">Paul Fishwick &amp; Benjamin Lok</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-John-Quarles"><span class="c-article-authors-search__title u-h3 js-search-name">John Quarles</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;John+Quarles&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=John+Quarles" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22John+Quarles%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Samsun-Lampotang"><span class="c-article-authors-search__title u-h3 js-search-name">Samsun Lampotang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Samsun+Lampotang&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Samsun+Lampotang" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Samsun+Lampotang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Ira-Fischler"><span class="c-article-authors-search__title u-h3 js-search-name">Ira Fischler</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Ira+Fischler&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ira+Fischler" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ira+Fischler%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Paul-Fishwick"><span class="c-article-authors-search__title u-h3 js-search-name">Paul Fishwick</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Paul+Fishwick&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Paul+Fishwick" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Paul+Fishwick%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Benjamin-Lok"><span class="c-article-authors-search__title u-h3 js-search-name">Benjamin Lok</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Benjamin+Lok&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Benjamin+Lok" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Benjamin+Lok%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-013-0229-6/email/correspondent/c1/new">John Quarles</a>.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Experiences%20in%20mixed%20reality-based%20collocated%20after%20action%20review&amp;author=John%20Quarles%20et%20al&amp;contentID=10.1007%2Fs10055-013-0229-6&amp;publication=1359-4338&amp;publicationDate=2013-08-31&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Quarles, J., Lampotang, S., Fischler, I. <i>et al.</i> Experiences in mixed reality-based collocated after action review.
                    <i>Virtual Reality</i> <b>17, </b>239–252 (2013). https://doi.org/10.1007/s10055-013-0229-6</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-013-0229-6.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2009-11-20">20 November 2009</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2010-11-15">15 November 2010</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-08-31">31 August 2013</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2013-09">September 2013</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-013-0229-6" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-013-0229-6</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">After action review</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Anesthesia machine</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Human patient simulator</span></li><li class="c-article-subject-list__subject"><span itemprop="about">User studies</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Skin prepping</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-013-0229-6.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=229;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

