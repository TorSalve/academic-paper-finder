<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="Application of immersive technologies and natural language to hyper-re"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a large number of degrees of freedom, so they..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="Application of immersive technologies and natural language to hyper-redundant robot teleoperation"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-12-05"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a large number of degrees of freedom, so they often exhibit complex configurations frustrating their spatial understanding. This work aims to contrast two hypotheses; first, if immersive interfaces enhance the telepresence and efficiency against conventional ones; and second, if natural language reduces workload and improves performance against other conventional tools. A total of 2 interfaces and 6 interaction tools have been tested by 50 people. As a result, immersive interfaces were more efficient, improved situational awareness and visual feedback, and were chosen by 94% of participants against conventional ones. On the other hand, participants performed better using natural language than conventional tools despite having less previous experience with the first ones. Additionally, according to 52% of the population, the preferred interaction tool was a mixed strategy that combined voice recognition and hand gestures. Therefore, it is concluded that immersive realities and natural language should play a very important role in the near future of hyper-redundant robots and their teleoperation."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-12-05"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="15"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00414-9"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00414-9"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00414-9.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00414-9"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="Application of immersive technologies and natural language to hyper-redundant robot teleoperation"/>

    <meta name="citation_online_date" content="2019/12/05"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="15"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00414-9"/>

    <meta name="DOI" content="10.1007/s10055-019-00414-9"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00414-9"/>

    <meta name="description" content="This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a larg"/>

    <meta name="dc.creator" content="Andr&#233;s Mart&#237;n-Barrio"/>

    <meta name="dc.creator" content="Juan Jes&#250;s Rold&#225;n"/>

    <meta name="dc.creator" content="Silvia Terrile"/>

    <meta name="dc.creator" content="Jaime del Cerro"/>

    <meta name="dc.creator" content="Antonio Barrientos"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Control Eng Pract; citation_title=Telerobotic system for live-power line maintenance: ROBTET; citation_author=R Aracil, M Ferre, M Hernando, E Pinto, J Sebastian; citation_volume=10; citation_publication_date=2002; citation_pages=1271-1281; citation_doi=10.1016/S0967-0661(02)00182-X; citation_id=CR1"/>

    <meta name="citation_reference" content="Aracil R, Buss M, Cobos S, Ferre M, Hirche S, Kuschel M, Peer A (2007) The human role in telerobotics. In: Advances in telerobotics. Springer, pp 11&#8211;24"/>

    <meta name="citation_reference" content="citation_title=Fundamentos de rob&#243;tica; citation_publication_date=2007; citation_id=CR3; citation_author=A Barrientos; citation_author=LF Pe&#241;in; citation_author=C Balaguer; citation_author=R Aracil; citation_publisher=McGrawHill"/>

    <meta name="citation_reference" content="citation_journal_title=Procedia Comput Sci; citation_title=Kinematics and teleoperation of tendon driven continuum robot; citation_author=S Bhattacherjee, S Chattopadhayay, V Rao, S Seth, S Mukherjee, A Sengupta, S Bhaumik; citation_volume=133; citation_publication_date=2018; citation_pages=879-886; citation_doi=10.1016/j.procs.2018.07.106; citation_id=CR4"/>

    <meta name="citation_reference" content="Britton N, Yoshida K, Walker J, Nagatani K, Taylor G, Dauphin L (2015) Lunar micro rover design for exploration through virtual reality tele-operations. In: Field and service robotics. Springer, pp 259&#8211;272"/>

    <meta name="citation_reference" content="Bugalia N, Sen A, Kalra P, Kumar S (2015) Immersive environment for robotic tele-operation. In: Proceedings of the 2015 conference on advances in robotics. ACM, p 49"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part C (Appl Rev); citation_title=Human performance issues and user interface design for teleoperated robots; citation_author=JY Chen, EC Haas, MJ Barnes; citation_volume=37; citation_publication_date=2007; citation_pages=1231-1245; citation_doi=10.1109/TSMCC.2007.905819; citation_id=CR7"/>

    <meta name="citation_reference" content="Cheng-jun D, Ping D, Ming-lu Z, Yan-fang Z (2009) Design of mobile robot teleoperation system based on virtual reality. In: IEEE international conference on automation and logistics. IEEE, pp 2024&#8211;2029"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Robot Autom Mag; citation_title=A hyper-redundant manipulator; citation_author=GS Chirikjian, JW Burdick; citation_volume=1; citation_publication_date=1994; citation_pages=22-29; citation_doi=10.1109/100.388263; citation_id=CR9"/>

    <meta name="citation_reference" content="Collins T, Shen W-M (2016) PASO: an integrated, scalable PSO-based optimization framework for hyper-redundant manipulator path planning and inverse kinematics. Information Sciences Institute Technical Report"/>

    <meta name="citation_reference" content="Daniel WW (1990) Kolmogorov&#8211;Smirnov one-sample test. In: Applied nonparametric statistics, vol 2"/>

    <meta name="citation_reference" content="citation_journal_title=Proceedings of the Human Factors and Ergonomics Society Annual Meeting; citation_title=Manual Versus Speech Input for Unmanned Aerial Vehicle Control Station Operations; citation_author=Mark Draper, Gloria Calhoun, Heath Ruff, David Williamson, Timothy Barry; citation_volume=47; citation_issue=1; citation_publication_date=2003; citation_pages=109-113; citation_doi=10.1177/154193120304700123; citation_id=CR12"/>

    <meta name="citation_reference" content="citation_journal_title=Fusion Eng Des; citation_title=Virtual reality applications in remote handling development for tokamaks in India; citation_author=P Dutta, N Rastogi, KK Gotewal; citation_volume=118; citation_publication_date=2017; citation_pages=73-80; citation_doi=10.1016/j.fusengdes.2017.03.047; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Proceedings of the Human Factors Society Annual Meeting; citation_title=Design and Evaluation for Situation Awareness Enhancement; citation_author=Mica R. Endsley; citation_volume=32; citation_issue=2; citation_publication_date=1988; citation_pages=97-101; citation_doi=10.1177/154193128803200221; citation_id=CR14"/>

    <meta name="citation_reference" content="Endsley MR (1988b) Situation awareness global assessment technique (SAGAT). In: Proceedings of the IEEE 1988 National Aerospace and Electronics Conference. IEEE, pp 789&#8211;795"/>

    <meta name="citation_reference" content="Espinoza MS, Goncalves J, Leitao P, Sanchez JLG, Herreros A (2012) Inverse kinematics of a 10 DOF modular hyper-redundant robot resorting to exhaustive and error-optimization methods: a comparative study. In: Robotics Symposium and Latin American Robotics Symposium (SBR-LARS), 2012 Brazilian. IEEE, pp 125&#8211;130"/>

    <meta name="citation_reference" content="citation_title=Advances in telerobotics; citation_publication_date=2007; citation_id=CR17; citation_author=M Ferre; citation_author=R Aracil; citation_author=C Balaguer; citation_author=M Buss; citation_author=C Melchiorri; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Frigola M, Fernandez J, Aranda J (2003) Visual human machine interface by gestures. In: IEEE international conference on robotics and automation (Cat. No. 03CH37422). IEEE, pp 386&#8211;391"/>

    <meta name="citation_reference" content="Gravagne IA, Walker ID (2000) Kinematic transformations for remotely-actuated planar continuum robots. In: IEEE international conference on robotics and automation. Proceedings. ICRA&#8217;00. IEEE, pp 19&#8211;26"/>

    <meta name="citation_reference" content="citation_journal_title=Proceedings of the Human Factors and Ergonomics Society Annual Meeting; citation_title=Nasa-Task Load Index (NASA-TLX); 20 Years Later; citation_author=Sandra G. Hart; citation_volume=50; citation_issue=9; citation_publication_date=2006; citation_pages=904-908; citation_doi=10.1177/154193120605000909; citation_id=CR20"/>

    <meta name="citation_reference" content="Hart SG, Staveland LE (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research. In: Advances in psychology, vol 52. Elsevier, pp 139&#8211;183"/>

    <meta name="citation_reference" content="Hedayati H, Walker M, Szafir D (2018) Improving collocated robot teleoperation with augmented reality. In: Proceedings of the 2018 ACM/IEEE International conference on human&#8211;robot interaction. ACM, pp 78&#8211;86"/>

    <meta name="citation_reference" content="Hu C, Meng MQ, Liu PX, Wang X (2003) Visual gesture recognition for human-machine interface of robot teleoperation. In: Proceedings 2003 IEEE/RSJ international conference on intelligent robots and systems (IROS 2003) (Cat. No. 03CH37453). IEEE, pp 1560&#8211;1565"/>

    <meta name="citation_reference" content="Kapadia AD, Walker ID, Tatlicioglu E (2012) Teleoperation control of a redundant continuum manipulator using a non-redundant rigid-link master. In: IEEE/RSJ international conference on intelligent robots and systems. IEEE, pp 3105&#8211;3110"/>

    <meta name="citation_reference" content="Koizumi S, Kanda T, Shiomi M, Ishiguro H, Hagita N (2006) Preliminary field trial for teleoperated communication robots. In: The 15th IEEE international symposium on robot and human interactive communication. IEEE, pp 145&#8211;150"/>

    <meta name="citation_reference" content="citation_journal_title=Int J Adv Rob Syst; citation_title=Application of virtual reality in teleoperation of the military mobile robotic system TAROS; citation_author=T Kot, P Nov&#225;k; citation_volume=15; citation_publication_date=2018; citation_pages=1729881417751545; citation_id=CR26"/>

    <meta name="citation_reference" content="Kuno Y, Nakanishi S, Murashima T, Shimada N, Shirai Y (1999) Robotic wheelchair with three control modes. In: Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C). IEEE, pp 2590&#8211;2595"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Syst Man Cybern Part B (Cybern); citation_title=Comparative analysis of 3-D robot teleoperation interfaces with novice users; citation_author=D Labonte, P Boissy, F Michaud; citation_volume=40; citation_publication_date=2010; citation_pages=1331-1342; citation_doi=10.1109/TSMCB.2009.2038357; citation_id=CR28"/>

    <meta name="citation_reference" content="Li N, Cartwright S, Shekhar Nittala A, Sharlin E, Costa Sousa M (2015) Flying frustum: a spatial interface for enhancing human-UAV awareness. In: Proceedings of the 3rd international conference on human&#8211;agent interaction. ACM, pp 27&#8211;31"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Robot Autom Lett; citation_title=Baxter&#8217;s homunculus: virtual reality spaces for teleoperation in manufacturing; citation_author=JI Lipton, AJ Fay, D Rus; citation_volume=3; citation_publication_date=2018; citation_pages=179-186; citation_doi=10.1109/LRA.2017.2737046; citation_id=CR30"/>

    <meta name="citation_reference" content="Lu Y, Liu L, Chen S, Huang Q (2010) Voice based control for humanoid teleoperation. In: International conference on intelligent system design and engineering application. IEEE, pp 814&#8211;818"/>

    <meta name="citation_reference" content="Lueth TC, Laengle T, Herzog G, Stopp E, Rembold U (1994) KANTRA-human&#8211;machine interaction for intelligent robots using natural language. In: Proceedings of 3rd IEEE international workshop on robot and human communication. IEEE, pp 106&#8211;111"/>

    <meta name="citation_reference" content="Mantec&#243;n T, del-Blanco CR, Jaureguizar F, Garc&#237;a N (2016) Hand gesture recognition using infrared imagery provided by leap motion controller. In: International conference on advanced concepts for intelligent vision systems. Springer, pp 47&#8211;57"/>

    <meta name="citation_reference" content="Marques L, Dinis J, Coimbra AP, Cris&#243;stomo MM, Ferreira JP (2009) 3D hyper-redundant robot. Paper presented at the 11th Spanish Portuguese Conference on Electrical Engineering, Zaragoza, Spain"/>

    <meta name="citation_reference" content="citation_journal_title=Soft Robot; citation_title=The Natural-CCD algorithm: a novel method to control hyper-redundant and soft robots; citation_author=A Mart&#237;n-Barrio, A Barrientos, J Cerro; citation_volume=5; citation_publication_date=2018; citation_pages=242-257; citation_doi=10.1089/soro.2017.0009; citation_id=CR35"/>

    <meta name="citation_reference" content="citation_journal_title=Revista Iberoamericana de Autom&#225;tica e Inform&#225;tica Industrial; citation_title=Hyper-redundant robots: classification, state-of-the-art and issues; citation_author=A Mart&#237;n-Barrio, S Terrile, A Barrientos, J Cerro; citation_volume=15; citation_publication_date=2018; citation_pages=351-362; citation_doi=10.4995/riai.2018.9207; citation_id=CR36"/>

    <meta name="citation_reference" content="Mart&#237;n-Barrio A, Terrile S, D&#237;az-Carrasco M, Del Cerro J, Barrientos A (2019) Modeling the soft robot Kyma based on Real-Time Finite Element Method (under review)"/>

    <meta name="citation_reference" content="Mashood A, Noura H, Jawhar I, Mohamed N (2015) A gesture based kinect for quadrotor control. In: International conference on information and communication technology research (ICTRC). IEEE, pp 298&#8211;301"/>

    <meta name="citation_reference" content="Matuszek C, Herbst E, Zettlemoyer L, Fox D (2013) Learning to parse natural language commands to a robot control system. In: Experimental robotics. Springer, pp 403&#8211;415"/>

    <meta name="citation_reference" content="citation_journal_title=Procedia CIRP; citation_title=Augmented reality (AR) applications for supporting human-robot interactive cooperation; citation_author=G Michalos, P Karagiannis, S Makris, &#214; Tok&#231;alar, G Chryssolouris; citation_volume=41; citation_publication_date=2016; citation_pages=370-375; citation_doi=10.1016/j.procir.2015.12.005; citation_id=CR40"/>

    <meta name="citation_reference" content="Mostefa M, El Boudadi LK, Loukil A, Mohamed K, Amine D (2015) Design of mobile robot teleoperation system based on virtual reality. In: 3rd international conference on control, engineering &amp; information technology (CEIT). IEEE, pp 1&#8211;6"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Robot; citation_title=Ecological interfaces for improving mobile robot teleoperation; citation_author=CW Nielsen, MA Goodrich, RW Ricks; citation_volume=23; citation_publication_date=2007; citation_pages=927-941; citation_doi=10.1109/TRO.2007.907479; citation_id=CR42"/>

    <meta name="citation_reference" content="citation_journal_title=Langenbeck&#8217;s Arch Surg; citation_title=Towards cybernetic surgery: robotic and augmented reality-assisted liver segmentectomy; citation_author=P Pessaux, M Diana, L Soler, T Piardi, D Mutter, J Marescaux; citation_volume=400; citation_publication_date=2015; citation_pages=381-385; citation_doi=10.1007/s00423-014-1256-9; citation_id=CR43"/>

    <meta name="citation_reference" content="citation_journal_title=Robotica; citation_title=Command-based voice teleoperation of a mobile robot via a human-robot interface; citation_author=A Poncela, L Gallardo-Estrella; citation_volume=33; citation_publication_date=2015; citation_pages=1-18; citation_doi=10.1017/S0263574714000010; citation_id=CR44"/>

    <meta name="citation_reference" content="citation_title=The humane interface: new directions for designing interactive systems; citation_publication_date=2000; citation_id=CR45; citation_author=J Raskin; citation_publisher=Addison-Wesley Professional"/>

    <meta name="citation_reference" content="Ren L, Omisore OM, Han S, Wang L (2017) A master-slave control system with workspaces isomerism for teleoperation of a snake robot. In: 39th annual international conference of the IEEE engineering in medicine and biology society (EMBC). IEEE, pp 4343&#8211;4346"/>

    <meta name="citation_reference" content="citation_title=Virtual reality: exploring the brave new technologies of artificial experience and interactive worlds-from cyberspace to teledildonics; citation_publication_date=1991; citation_id=CR47; citation_author=H Rheingold; citation_publisher=Secker &amp; Warburg"/>

    <meta name="citation_reference" content="citation_journal_title=Eur J Cardiothorac Surg; citation_title=ARTEMIS. A telemanipulator for cardiac surgery; citation_author=H Rininsland; citation_volume=16; citation_publication_date=1999; citation_pages=S106-S111; citation_id=CR48"/>

    <meta name="citation_reference" content="citation_journal_title=Sensors; citation_title=Multi-robot interfaces and operator situational awareness: study of the impact of immersion and prediction; citation_author=J Rold&#225;n, E Pe&#241;a-Tapia, A Mart&#237;n-Barrio, M Olivares-M&#233;ndez, J Cerro, A Barrientos; citation_volume=17; citation_publication_date=2017; citation_pages=1720; citation_doi=10.3390/s17081720; citation_id=CR49"/>

    <meta name="citation_reference" content="citation_journal_title=Robot Comput Integr Manuf; citation_title=A training system for Industry 4.0 operators in complex assemblies based on virtual reality and process mining; citation_author=JJ Rold&#225;n, E Crespo, A Mart&#237;n Barrio, E Pe&#241;a-Tapia, A Barrientos; citation_volume=59; citation_publication_date=2019; citation_pages=305-316; citation_doi=10.1016/j.rcim.2019.05.004; citation_id=CR50"/>

    <meta name="citation_reference" content="Rold&#225;n JJ, Pe&#241;a-Tapia E, Garz&#243;n-Ramos D, de Le&#243;n J, Garz&#243;n M, del Cerro J, Barrientos A (2019b) Multi-robot systems, virtual reality and ROS: developing a new generation of operator interfaces. In: Robot Operating System (ROS). Springer, pp 29-64"/>

    <meta name="citation_reference" content="citation_title=Remote control robotics; citation_publication_date=1999; citation_id=CR52; citation_author=C Sayers; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Shilling RD, Shinn-Cunningham B (2002) Virtual auditory displays. Handbook of Virtual Environment Technology, pp 65&#8211;92"/>

    <meta name="citation_reference" content="Shirwalkar S, Singh A, Sharma K, Singh N (2013) Telemanipulation of an industrial robotic arm using gesture recognition with Kinect. In: International conference on control, automation, robotics and embedded systems (CARE). IEEE, pp 1&#8211;6"/>

    <meta name="citation_reference" content="Stern H, Wachs J, Edan Y (2007) A method for selection of optimal hand gesture vocabularies. In: International Gesture Workshop. Springer, pp 57&#8211;68"/>

    <meta name="citation_reference" content="citation_title=Teleoperation and robotics: applications and technology; citation_publication_date=2013; citation_id=CR56; citation_author=J Vertut; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Yoshizaki M, Kuno Y, Nakamura A (2001) Human&#8211;robot interface based on the mutual assistance between speech and vision. In: Proceedings of the Workshop on Perceptive user Interfaces. ACM, pp 1&#8211;4"/>

    <meta name="citation_reference" content="citation_title=Real time languages: design and development; citation_publication_date=1982; citation_id=CR58; citation_author=SJ Young; citation_publisher=Ellis Horwood Ltd"/>

    <meta name="citation_author" content="Andr&#233;s Mart&#237;n-Barrio"/>

    <meta name="citation_author_email" content="andres.mb@upm.es"/>

    <meta name="citation_author_institution" content="Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_author" content="Juan Jes&#250;s Rold&#225;n"/>

    <meta name="citation_author_institution" content="Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_author" content="Silvia Terrile"/>

    <meta name="citation_author_institution" content="Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_author" content="Jaime del Cerro"/>

    <meta name="citation_author_institution" content="Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_author" content="Antonio Barrientos"/>

    <meta name="citation_author_institution" content="Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Polit&#233;cnica de Madrid, Madrid, Spain"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00414-9&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00414-9"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="Application of immersive technologies and natural language to hyper-redundant robot teleoperation"/>
        <meta property="og:description" content="This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a large number of degrees of freedom, so they often exhibit complex configurations frustrating their spatial understanding. This work aims to contrast two hypotheses; first, if immersive interfaces enhance the telepresence and efficiency against conventional ones; and second, if natural language reduces workload and improves performance against other conventional tools. A total of 2 interfaces and 6 interaction tools have been tested by 50 people. As a result, immersive interfaces were more efficient, improved situational awareness and visual feedback, and were chosen by 94% of participants against conventional ones. On the other hand, participants performed better using natural language than conventional tools despite having less previous experience with the first ones. Additionally, according to 52% of the population, the preferred interaction tool was a mixed strategy that combined voice recognition and hand gestures. Therefore, it is concluded that immersive realities and natural language should play a very important role in the near future of hyper-redundant robots and their teleoperation."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>Application of immersive technologies and natural language to hyper-redundant robot teleoperation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00414-9","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"Virtual reality, Augmented reality, Mixed reality, Natural language, Hyper-redundant robot, Soft robot, Teleoperation","kwrd":["Virtual_reality","Augmented_reality","Mixed_reality","Natural_language","Hyper-redundant_robot","Soft_robot","Teleoperation"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00414-9","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00414-9","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=414;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00414-9">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            Application of immersive technologies and natural language to hyper-redundant robot teleoperation
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00414-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00414-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-12-05" itemprop="datePublished">05 December 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">Application of immersive technologies and natural language to hyper-redundant robot teleoperation</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Andr_s-Mart_n_Barrio" data-author-popup="auth-Andr_s-Mart_n_Barrio" data-corresp-id="c1">Andrés Martín-Barrio<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0002-2051-9155"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-2051-9155</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000 0001 2151 2978, Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Juan_Jes_s-Rold_n" data-author-popup="auth-Juan_Jes_s-Rold_n">Juan Jesús Roldán</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000 0001 2151 2978, Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Silvia-Terrile" data-author-popup="auth-Silvia-Terrile">Silvia Terrile</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000 0001 2151 2978, Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Jaime-Cerro" data-author-popup="auth-Jaime-Cerro">Jaime del Cerro</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000 0001 2151 2978, Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Antonio-Barrientos" data-author-popup="auth-Antonio-Barrientos">Antonio Barrientos</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad Politécnica de Madrid" /><meta itemprop="address" content="grid.5690.a, 0000 0001 2151 2978, Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">184 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00414-9/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a large number of degrees of freedom, so they often exhibit complex configurations frustrating their spatial understanding. This work aims to contrast two hypotheses; first, if immersive interfaces enhance the telepresence and efficiency against conventional ones; and second, if <i>natural language</i> reduces workload and improves performance against other conventional tools. A total of 2 interfaces and 6 interaction tools have been tested by 50 people. As a result, immersive interfaces were more efficient, improved situational awareness and visual feedback, and were chosen by 94% of participants against conventional ones. On the other hand, participants performed better using natural language than conventional tools despite having less previous experience with the first ones. Additionally, according to 52% of the population, the preferred interaction tool was a mixed strategy that combined voice recognition and hand gestures. Therefore, it is concluded that immersive realities and natural language should play a very important role in the near future of hyper-redundant robots and their teleoperation.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>In every robotic device, the concept of <i>Degree of Freedom</i> (DoF) is very important. It can be defined as the number of movements or independent parameters that define the configuration of a robot (Barrientos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Barrientos A, Peñin LF, Balaguer C, Aracil R (2007) Fundamentos de robótica. McGrawHill, Madrid" href="/article/10.1007/s10055-019-00414-9#ref-CR3" id="ref-link-section-d76161e306">2007</a>). Traditionally, manipulator robots have been designed to have up to 6 DoF because it is the minimum number for which a robot can properly position and orient its end-effector in a three-dimensional (3D) space. However, robots with a higher number of DoF have some advantages against traditional ones: they have higher kinematic skills, and they can recover from a failure in some of their joints and have better abilities to actuate in complex environments with obstacles. Those robots with more than 6 DoF are called <i>redundant</i>, while those with more than 12 DoF can be named as <i>hyper</i>-<i>redundant</i> (Martín-Barrio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018b" title="Martín-Barrio A, Terrile S, Barrientos A, del Cerro J (2018b) Hyper-redundant robots: classification, state-of-the-art and issues. Revista Iberoamericana de Automática e Informática Industrial 15:351–362" href="/article/10.1007/s10055-019-00414-9#ref-CR36" id="ref-link-section-d76161e319">2018b</a>).</p><p>Traditionally, hyper-redundant robots have been conceived to be discrete, with a high number of distinguishable links. However, one of the most researched topics in the robotics scope is currently focused on <i>continuous</i> robots, especially those made by <i>soft</i> materials. These robots are characterized to continuously deform instead of being moved by the concatenation of a set of rigid links. Therefore, compared to traditional ones, they exhibit more flexibility and inherent safety to interact with their environment.</p><p>To control any robot, a prior knowledge of a <i>model</i> is usually required to extract its behaviour and predict its movements. At a kinematic level, two main problems are usually studied: the direct and inverse kinematics. The <i>direct kinematics</i> aims to determine the end-effector position and orientation when the joint values—the <i>configuration</i> or <i>pose</i>—are known. This problem is relatively easy to be solved for hyper-redundant robots using traditional techniques. On the other hand, the <i>inverse kinematics</i> problem is usually more interesting since it aims to solve the configuration that leads to a determined position for its end-effector. This problem has more difficulties in the hyper-redundant scope since such devices offer an infinite number of configurations that lead to a single position and orientation for the end-effector (Chirikjian and Burdick <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Chirikjian GS, Burdick JW (1994) A hyper-redundant manipulator. IEEE Robot Autom Mag 1:22–29. &#xA;https://doi.org/10.1109/100.388263&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00414-9#ref-CR9" id="ref-link-section-d76161e350">1994</a>). Choosing the best solution can be a great challenge because it is very relative and dependant on every situation. Several works have focussed on this matter (Collins and Shen <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Collins T, Shen W-M (2016) PASO: an integrated, scalable PSO-based optimization framework for hyper-redundant manipulator path planning and inverse kinematics. Information Sciences Institute Technical Report" href="/article/10.1007/s10055-019-00414-9#ref-CR10" id="ref-link-section-d76161e353">2016</a>; Espinoza et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Espinoza MS, Goncalves J, Leitao P, Sanchez JLG, Herreros A (2012) Inverse kinematics of a 10 DOF modular hyper-redundant robot resorting to exhaustive and error-optimization methods: a comparative study. In: Robotics Symposium and Latin American Robotics Symposium (SBR-LARS), 2012 Brazilian. IEEE, pp 125–130" href="/article/10.1007/s10055-019-00414-9#ref-CR16" id="ref-link-section-d76161e356">2012</a>; Marques et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Marques L, Dinis J, Coimbra AP, Crisóstomo MM, Ferreira JP (2009) 3D hyper-redundant robot. Paper presented at the 11th Spanish Portuguese Conference on Electrical Engineering, Zaragoza, Spain" href="/article/10.1007/s10055-019-00414-9#ref-CR34" id="ref-link-section-d76161e359">2009</a>; Martín-Barrio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018a" title="Martín-Barrio A, Barrientos A, del Cerro J (2018a) The Natural-CCD algorithm: a novel method to control hyper-redundant and soft robots. Soft Robot 5:242–257" href="/article/10.1007/s10055-019-00414-9#ref-CR35" id="ref-link-section-d76161e362">2018a</a>).</p><p>However, commanding the end-effector location of a hyper-redundant robot offers a limited number of applications. Besides, methods aiming to solve the inverse kinematics problem usually do not exploit all the kinematic capabilities for such robots. Thus, <i>shape controls</i> emerge as an alternative. This work will focus on the application of this type of control scheme. Under this paradigm, the entire configuration of the robot is controlled to perform tasks such as local avoidance of obstacles, surrounding objects or follow the leader guidance. Therefore, multiple references are needed to control such robots and not just one.</p><p>Strategies to adequately define this set of references are usually studied in <i>telerobotics</i>. This outlook implies linking a human operator and a robot in order to execute a remote task. Thus, <i>teleoperation</i> of the shape of a hyper-redundant manipulator should study how to properly associate a set of user-defined references to move the robot pose (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig1">1</a>). This field is highly related to <i>telepresence</i>, which means that the information about the remote environment is naturally displayed to the operator, implying a feeling of presence at the remote site. A good degree of telepresence guarantees the feasibility of a determined manipulation task (Aracil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Aracil R, Buss M, Cobos S, Ferre M, Hirche S, Kuschel M, Peer A (2007) The human role in telerobotics. In: Advances in telerobotics. Springer, pp 11–24" href="/article/10.1007/s10055-019-00414-9#ref-CR2" id="ref-link-section-d76161e387">2007</a>). Since hyper-redundant robots often exhibit intricate shapes, hindering their spatial understanding, maybe emerging <i>immersive technologies</i> can be a good solution to solve this issue.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig1_HTML.jpg?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig1_HTML.jpg" alt="figure1" loading="lazy" width="685" height="441" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Teleoperation of a continuous robot for space exploration using a <i>Virtual Reality</i> headset and a <i>Leap Motion</i> controller</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>According to the previous definition, <i>immersive realities</i> could be understood as those real or simulated environments in which a perceiver experiences telepresence (Rheingold <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Rheingold H (1991) Virtual reality: exploring the brave new technologies of artificial experience and interactive worlds-from cyberspace to teledildonics. Secker &amp; Warburg, Portland" href="/article/10.1007/s10055-019-00414-9#ref-CR47" id="ref-link-section-d76161e414">1991</a>; Vertut <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Vertut J (2013) Teleoperation and robotics: applications and technology, vol 3. Springer, Berlin" href="/article/10.1007/s10055-019-00414-9#ref-CR56" id="ref-link-section-d76161e417">2013</a>). For example, <i>Virtual Reality</i> (VR) provides interaction of virtual elements in virtual environments, <i>Augmented Reality</i> (AR) features interaction of virtual elements in real environments, and <i>Mixed Reality</i> (MR) offers interaction of real and virtual elements through real or virtual environments (Roldán et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019b" title="Roldán JJ, Peña-Tapia E, Garzón-Ramos D, de León J, Garzón M, del Cerro J, Barrientos A (2019b) Multi-robot systems, virtual reality and ROS: developing a new generation of operator interfaces. In: Robot Operating System (ROS). Springer, pp 29-64" href="/article/10.1007/s10055-019-00414-9#ref-CR51" id="ref-link-section-d76161e430">2019b</a>). Nowadays, all these realities are achieved using headsets, headphones, glasses or controllers among several other devices, providing stimulus to the human senses including, but not limited to, vision, audition and touch. This feedback can be used within a teleoperation system to properly manage the pose of hyper-redundant robots.</p><p>On the other hand, traditional telerobotic systems have often relied on a <i>master</i>, intended to command the movements of a remote robot, called <i>slave</i>. These master–slave architectures were conceived in this manner to be <i>transparent</i>, in other words, to exactly reproduce the remote environment (Aracil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Aracil R, Buss M, Cobos S, Ferre M, Hirche S, Kuschel M, Peer A (2007) The human role in telerobotics. In: Advances in telerobotics. Springer, pp 11–24" href="/article/10.1007/s10055-019-00414-9#ref-CR2" id="ref-link-section-d76161e445">2007</a>). However, they often implied the design and construction of a physical interface to reproduce the real-robot movements. Immersive technologies could replace those physical masters within the virtual world while enhancing the immersion and telepresence. But such transparency will highly rely on the <i>interaction tools</i> used to replace those physical architectures. In that context, <i>natural language</i> could be used as one possible tool. It is understood as a rich, intuitive mechanism by which humans can interact with systems around them, offering sufficient signal to support robot task planning (Matuszek et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Matuszek C, Herbst E, Zettlemoyer L, Fox D (2013) Learning to parse natural language commands to a robot control system. In: Experimental robotics. Springer, pp 403–415" href="/article/10.1007/s10055-019-00414-9#ref-CR39" id="ref-link-section-d76161e455">2013</a>).</p><p>Accordingly, this work aims to contrast two hypotheses: the first one states that immersive interfaces are better in terms of efficiency and situational awareness against conventional ones to teleoperate hyper-redundant robots; the second one states that <i>natural language</i> can improve the performance and reduce the workload against conventional tools for the same purpose.</p><p>The first main contribution of this work is the comparison of emergent immersive technologies against conventional ones to teleoperate the shape of hyper-redundant manipulators. Furthermore, an analysis of natural language and other tools used to interact within the immersive scenarios has been conducted and compared against conventional tools. Additionally, an experimentation strategy is proposed in order to measure both subjective and objective parameters, relevant to contrast the conjectures. In summary, this work is expected to clear the path to which the research of teleoperation of hyper-redundant, continuous and soft robots should be focused on.</p><p>This document will be structured as follows: Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec2">2</a> introduces the up-to-date state of the art in telerobotics, teleoperation, immersive technologies and natural language. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec3">3</a> introduces the system setup, in order to contextualize the following chapters. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec4">4</a> explains of the developed interfaces to contrast the benefits of using immersive realities or conventional ones. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec7">5</a> explains the interaction tools and strategies used to perform in immersive scenarios for further comparisons. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec14">6</a> presents the experiments performed to contrast the hypotheses and the results, and Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec17">7</a> outlines the conclusions derived from this work.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">State of the art</h2><div class="c-article-section__content" id="Sec2-content"><p>This section presents the up-to-date research in robot teleoperation, specifically in human–robot interfaces and interaction strategies, intended for many types of robots and purposes. Hopefully, this will help to clarify the current research, potential applications and limitations related to this work.</p><p>One of the major challenges in robot teleoperation is the difficulty of achieving a precise and reliable representation of the environment. Remote perception and manipulation are affected by factors such as limited <i>field of view</i> (FOV), orientation, camera viewpoint, depth perception, degraded video image, time delay and motion. Researchers have shown that enhancing teleoperator feedback plays an important role in decreasing task difficulty and creating a greater sense of operator immersion in a teleoperation environment (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Chen JY, Haas EC, Barnes MJ (2007) Human performance issues and user interface design for teleoperated robots. IEEE Trans Syst Man Cybern Part C (Appl Rev) 37:1231–1245" href="/article/10.1007/s10055-019-00414-9#ref-CR7" id="ref-link-section-d76161e503">2007</a>). Accordingly, some authors have studied the strengths and weaknesses of conventional 2D interfaces against immersive 3D ones. Some of them predict that immersive interfaces can help users to achieve a higher level of understanding of the robot’s point of view and situational awareness (Hedayati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Hedayati H, Walker M, Szafir D (2018) Improving collocated robot teleoperation with augmented reality. In: Proceedings of the 2018 ACM/IEEE International conference on human–robot interaction. ACM, pp 78–86" href="/article/10.1007/s10055-019-00414-9#ref-CR22" id="ref-link-section-d76161e506">2018</a>).</p><p>Several works related to mobile robotic applications agree at selecting virtual and augmented reality interfaces in order to improve performance (Cheng-jun et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Cheng-jun D, Ping D, Ming-lu Z, Yan-fang Z (2009) Design of mobile robot teleoperation system based on virtual reality. In: IEEE international conference on automation and logistics. IEEE, pp 2024–2029" href="/article/10.1007/s10055-019-00414-9#ref-CR8" id="ref-link-section-d76161e512">2009</a>; Kot and Novák <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Kot T, Novák P (2018) Application of virtual reality in teleoperation of the military mobile robotic system TAROS. Int J Adv Rob Syst 15:1729881417751545" href="/article/10.1007/s10055-019-00414-9#ref-CR26" id="ref-link-section-d76161e515">2018</a>; Mostefa et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Mostefa M, El Boudadi LK, Loukil A, Mohamed K, Amine D (2015) Design of mobile robot teleoperation system based on virtual reality. In: 3rd international conference on control, engineering &amp; information technology (CEIT). IEEE, pp 1–6" href="/article/10.1007/s10055-019-00414-9#ref-CR41" id="ref-link-section-d76161e518">2015</a>; Nielsen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Nielsen CW, Goodrich MA, Ricks RW (2007) Ecological interfaces for improving mobile robot teleoperation. IEEE Trans Robot 23:927–941" href="/article/10.1007/s10055-019-00414-9#ref-CR42" id="ref-link-section-d76161e521">2007</a>). One example of enhancing situational awareness is <i>Flying Frustum</i>, an AR interface aimed to control semi-autonomous <i>Unmanned Aerial Vehicles</i> (UAV) using a 3D interactive printout of the terrain. In this work, UAV operators could use pen-based interactions to input flight paths and commands to the UAVs by sketching directly on the physical topographical model of the terrain (Li et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Li N, Cartwright S, Shekhar Nittala A, Sharlin E, Costa Sousa M (2015) Flying frustum: a spatial interface for enhancing human-UAV awareness. In: Proceedings of the 3rd international conference on human–agent interaction. ACM, pp 27–31" href="/article/10.1007/s10055-019-00414-9#ref-CR29" id="ref-link-section-d76161e531">2015</a>).</p><p>Moreover, many works claim that VR technologies applied to robotics can reduce the cost of expensive specialized systems in manufacturing (Bugalia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Bugalia N, Sen A, Kalra P, Kumar S (2015) Immersive environment for robotic tele-operation. In: Proceedings of the 2015 conference on advances in robotics. ACM, p 49" href="/article/10.1007/s10055-019-00414-9#ref-CR6" id="ref-link-section-d76161e537">2015</a>; Lipton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Lipton JI, Fay AJ, Rus D (2018) Baxter’s homunculus: virtual reality spaces for teleoperation in manufacturing. IEEE Robot Autom Lett 3:179–186" href="/article/10.1007/s10055-019-00414-9#ref-CR30" id="ref-link-section-d76161e540">2018</a>; Roldán et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019a" title="Roldán JJ, Crespo E, Martín Barrio A, Peña-Tapia E, Barrientos A (2019a) A training system for Industry 4.0 operators in complex assemblies based on virtual reality and process mining. Robot Comput Integr Manuf 59:305–316" href="/article/10.1007/s10055-019-00414-9#ref-CR50" id="ref-link-section-d76161e543">2019a</a>).</p><p>Concerning the teleoperation of service robots, immersive realities can be used for surveillance and remote interventions in our homes. (Labonte et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Labonte D, Boissy P, Michaud F (2010) Comparative analysis of 3-D robot teleoperation interfaces with novice users. IEEE Trans Syst Man Cybern Part B (Cybern) 40:1331–1342" href="/article/10.1007/s10055-019-00414-9#ref-CR28" id="ref-link-section-d76161e550">2010</a>) conducted an experiment in which two MR visualization modalities were compared with standard video-centric and map-centric perspectives. As a result, novice operators experienced a higher workload and were less efficient than expert ones. Regarding the interfaces, the performance was significantly inconclusive. According to the authors, this is due to a high complexity of different factors such as the operation environment, the mission’s objectives, the robots’ capabilities or the choice of performance metrics. In another work, teleoperation enabled operators to establish effective communications between humanoid robots and people in order to avoid the automatic recognition difficulties of spoken language (Koizumi et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Koizumi S, Kanda T, Shiomi M, Ishiguro H, Hagita N (2006) Preliminary field trial for teleoperated communication robots. In: The 15th IEEE international symposium on robot and human interactive communication. IEEE, pp 145–150" href="/article/10.1007/s10055-019-00414-9#ref-CR25" id="ref-link-section-d76161e553">2006</a>).</p><p>In the hyper-redundant robotics scope, teleoperation schemes often follow traditional master–slave architectures. This is reasonable since these robots with a large number of DoF are usually perceived to need a replica with the same number of DoF in order to exploit all their kinematic skills. However, using masters and slaves with different number of DoF is a common practice. For example, a <i>Phantom Omni</i> haptic device (master) was used to accurately teleoperate the end-effector of an articulated snake with 12 DoF (slave) for minimally invasive surgical procedures. In these situations where the number of DoF of the master is lower than in the slave, it is necessary to obtain a mapping relation between each other (Ren et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Ren L, Omisore OM, Han S, Wang L (2017) A master-slave control system with workspaces isomerism for teleoperation of a snake robot. In: 39th annual international conference of the IEEE engineering in medicine and biology society (EMBC). IEEE, pp 4343–4346" href="/article/10.1007/s10055-019-00414-9#ref-CR46" id="ref-link-section-d76161e562">2017</a>). Another example uses the same master to teleoperate a tendon-driven continuous robot by using a VR interface in real time, also intended for surgical procedures. This research underlines that the kinematic structure of the master is very dissimilar to the kinematic model of the continuum robot, so the teleoperation strategy may not be the best (Bhattacherjee et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Bhattacherjee S, Chattopadhayay S, Rao V, Seth S, Mukherjee S, Sengupta A, Bhaumik S (2018) Kinematics and teleoperation of tendon driven continuum robot. Procedia Comput Sci 133:879–886" href="/article/10.1007/s10055-019-00414-9#ref-CR4" id="ref-link-section-d76161e565">2018</a>). Such dissimilarities were also studied trying to teleoperate a redundant continuous manipulator using a non-redundant rigid-link master. These differences were minimized using feedback linearizing task-space controllers (Kapadia et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Kapadia AD, Walker ID, Tatlicioglu E (2012) Teleoperation control of a redundant continuum manipulator using a non-redundant rigid-link master. In: IEEE/RSJ international conference on intelligent robots and systems. IEEE, pp 3105–3110" href="/article/10.1007/s10055-019-00414-9#ref-CR24" id="ref-link-section-d76161e568">2012</a>). In all these works, the inverse kinematics is interrelated between the master and slave for the end-effector only. Then, only the inverse kinematics problem is solved, but shape controls are not managed. This work will study such whole-body movements to be the ones that fully take advantage of the kinematic capabilities of these robots for remote operations.</p><p>Different interaction strategies are used to teleoperate robots, varying from traditional and mechanic peripherals to multimodal ones. On the one hand, physical tools vary from game controllers, electronic devices, keyboards, mouse devices, joysticks or <i>Phantom</i> haptic devices, among others. Depending on the field of application and the robot kinematics, one can be chosen against another. For example, to fly a UAV, specific physical controllers with buttons and axis are commonly used (Hedayati et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Hedayati H, Walker M, Szafir D (2018) Improving collocated robot teleoperation with augmented reality. In: Proceedings of the 2018 ACM/IEEE International conference on human–robot interaction. ACM, pp 78–86" href="/article/10.1007/s10055-019-00414-9#ref-CR22" id="ref-link-section-d76161e577">2018</a>).</p><p>Human gestures without physical intermediaries are another way of controlling robots in teleoperation. They can be advantageous because are easy to use, robust, fast and can be used in a wide range within the field of view (Hu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Hu C, Meng MQ, Liu PX, Wang X (2003) Visual gesture recognition for human-machine interface of robot teleoperation. In: Proceedings 2003 IEEE/RSJ international conference on intelligent robots and systems (IROS 2003) (Cat. No. 03CH37453). IEEE, pp 1560–1565" href="/article/10.1007/s10055-019-00414-9#ref-CR23" id="ref-link-section-d76161e583">2003</a>). Such interfaces are oriented to tasks that can be performed hands free. In fact, some authors claim a higher prominence of devices like the <i>Leap Motion</i> sensor for the future of human–machine interaction applications (Mantecón et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Mantecón T, del-Blanco CR, Jaureguizar F, García N (2016) Hand gesture recognition using infrared imagery provided by leap motion controller. In: International conference on advanced concepts for intelligent vision systems. Springer, pp 47–57" href="/article/10.1007/s10055-019-00414-9#ref-CR33" id="ref-link-section-d76161e589">2016</a>). Some works describe vision system interfaces that track human gestures for relatively simple robot commands such as up-down, stop, turn, approach and go (Frigola et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Frigola M, Fernandez J, Aranda J (2003) Visual human machine interface by gestures. In: IEEE international conference on robotics and automation (Cat. No. 03CH37422). IEEE, pp 386–391" href="/article/10.1007/s10055-019-00414-9#ref-CR18" id="ref-link-section-d76161e592">2003</a>). Others developed a robotic wheelchair controlled using face movements (Kuno et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Kuno Y, Nakanishi S, Murashima T, Shimada N, Shirai Y (1999) Robotic wheelchair with three control modes. In: Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C). IEEE, pp 2590–2595" href="/article/10.1007/s10055-019-00414-9#ref-CR27" id="ref-link-section-d76161e595">1999</a>). Moreover, it is worth highlighting that these strategies are also used to control UAV by using body postures (Mashood et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Mashood A, Noura H, Jawhar I, Mohamed N (2015) A gesture based kinect for quadrotor control. In: International conference on information and communication technology research (ICTRC). IEEE, pp 298–301" href="/article/10.1007/s10055-019-00414-9#ref-CR38" id="ref-link-section-d76161e599">2015</a>). Usually, vocabulary rules or ad hoc methods are used to select hand gestures. Some authors affirm that analytical approaches are better to design gesture vocabularies for multiple objectives for psycho-physiological and gesture recognition factors (Stern et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Stern H, Wachs J, Edan Y (2007) A method for selection of optimal hand gesture vocabularies. In: International Gesture Workshop. Springer, pp 57–68" href="/article/10.1007/s10055-019-00414-9#ref-CR55" id="ref-link-section-d76161e602">2007</a>). Accordingly, gesture-based techniques can be a valid alternative to physical master–slave architectures since they can be cheaper and more intuitive (Shirwalkar et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Shirwalkar S, Singh A, Sharma K, Singh N (2013) Telemanipulation of an industrial robotic arm using gesture recognition with Kinect. In: International conference on control, automation, robotics and embedded systems (CARE). IEEE, pp 1–6" href="/article/10.1007/s10055-019-00414-9#ref-CR54" id="ref-link-section-d76161e605">2013</a>).</p><p>Voice commands are also useful especially in environments where manual control is difficult or when both hands of the operator are busy, as in the case of <i>ROBTET</i>, a robotic system for maintaining live power lines. In such a system, users use voice commands, while both hands perform other control tasks (Aracil et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Aracil R, Ferre M, Hernando M, Pinto E, Sebastian J (2002) Telerobotic system for live-power line maintenance: ROBTET. Control Eng Pract 10:1271–1281" href="/article/10.1007/s10055-019-00414-9#ref-CR1" id="ref-link-section-d76161e614">2002</a>). Other projects compared the utility of manual and speech inputs for several UAV control tasks. Results showed that using speech was significantly better than manual inputs in terms of task completion time, accuracy, flight and navigation measures and pilot ratings (Draper et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Draper M, Calhoun G, Ruff H, Williamson D, Barry T (2003) Manual versus speech input for unmanned aerial vehicle control station operations. In: Proceedings of the human factors and ergonomics society annual meeting, vol 1. SAGE Publications Sage CA: Los Angeles, CA, pp 109–113" href="/article/10.1007/s10055-019-00414-9#ref-CR12" id="ref-link-section-d76161e617">2003</a>). Voice commands have been used in telerobotic surgical systems as well, as in <i>ARTEMIS</i>, an Advanced Robotics and Telemanipulator System for Minimally Invasive Surgery. The main components are two master–slave units guiding the surgical instruments and a remotely controlled endoscope guiding system. This last one can be guided by a simple joystick, voice control or automatic camera tracking (Rininsland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Rininsland H (1999) ARTEMIS. A telemanipulator for cardiac surgery. Eur J Cardiothorac Surg 16:S106–S111" href="/article/10.1007/s10055-019-00414-9#ref-CR48" id="ref-link-section-d76161e623">1999</a>). In addition, voice can be a powerful tool to use in teleoperation of humanoid robots for being convenient and simple, with less workload and learning time involved (Lu et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Lu Y, Liu L, Chen S, Huang Q (2010) Voice based control for humanoid teleoperation. In: International conference on intelligent system design and engineering application. IEEE, pp 814–818" href="/article/10.1007/s10055-019-00414-9#ref-CR31" id="ref-link-section-d76161e627">2010</a>; Poncela and Gallardo-Estrella <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Poncela A, Gallardo-Estrella L (2015) Command-based voice teleoperation of a mobile robot via a human-robot interface. Robotica 33:1–18" href="/article/10.1007/s10055-019-00414-9#ref-CR44" id="ref-link-section-d76161e630">2015</a>).</p><p>Natural-language interfaces can be applied to make the use of intelligent robots more flexible, such as in the case of the autonomous mobile two-arm robot <i>KAMRO</i> (Lueth et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1994" title="Lueth TC, Laengle T, Herzog G, Stopp E, Rembold U (1994) KANTRA-human–machine interaction for intelligent robots using natural language. In: Proceedings of 3rd IEEE international workshop on robot and human communication. IEEE, pp 106–111" href="/article/10.1007/s10055-019-00414-9#ref-CR32" id="ref-link-section-d76161e639">1994</a>). Combination of gesture and voice strategies gives place to multimodal interfaces, providing a large range of interactions, intuitive to humans and advantageous to service robots (Yoshizaki et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Yoshizaki M, Kuno Y, Nakamura A (2001) Human–robot interface based on the mutual assistance between speech and vision. In: Proceedings of the Workshop on Perceptive user Interfaces. ACM, pp 1–4" href="/article/10.1007/s10055-019-00414-9#ref-CR57" id="ref-link-section-d76161e642">2001</a>). There is evidence that multimodal displays and input controls have great potential towards improving the teleoperation performance (Chen et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Chen JY, Haas EC, Barnes MJ (2007) Human performance issues and user interface design for teleoperated robots. IEEE Trans Syst Man Cybern Part C (Appl Rev) 37:1231–1245" href="/article/10.1007/s10055-019-00414-9#ref-CR7" id="ref-link-section-d76161e645">2007</a>). In fact, some authors claim that these approaches increase awareness of surroundings, cue visual attention and convey a variety of complex information, especially when the visual channel is heavily loaded (Shilling and Shinn-Cunningham <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2002" title="Shilling RD, Shinn-Cunningham B (2002) Virtual auditory displays. Handbook of Virtual Environment Technology, pp 65–92" href="/article/10.1007/s10055-019-00414-9#ref-CR53" id="ref-link-section-d76161e648">2002</a>).</p></div></div></section><section aria-labelledby="Sec3"><div class="c-article-section" id="Sec3-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec3">System setup</h2><div class="c-article-section__content" id="Sec3-content"><p>The system arrangement will rely on the information exchanged between the user and the robot in both directions. Thus, communication is named <i>bilateral</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig2">2</a>). In other words, the user sends information to operate the robot in one direction, whereas the robot transmits information to the user in the other direction in order to report its current pose, both in real time. As a reminder, a <i>real</i>-<i>time</i> system can be understood as an information processing activity that responds to an externally generated stimulus in a previously determined frame of time (Young <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1982" title="Young SJ (1982) Real time languages: design and development. Ellis Horwood Ltd, Chichester" href="/article/10.1007/s10055-019-00414-9#ref-CR58" id="ref-link-section-d76161e671">1982</a>). In the presented system, this bilateral communication performs a cycle with a frequency of 10 Hz, which depending on the application can be acceptable or not. In this work, it was high enough to test the capabilities of the interfaces that will be explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec4">4</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="247" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Bilateral scheme for the robot teleoperation</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The bilateral controller implemented in this work follows a position–position scheme. Thus, the robot will move to a determined position previously commanded by the user. Although this scheme is very simple, it is the best suited when the master–slave size relation is similar, there are no limitations in the slave speed, a high level of telepresence is required, and the task involves a high number of degrees of freedom (Ferre et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ferre M, Aracil R, Balaguer C, Buss M, Melchiorri C (2007) Advances in telerobotics, vol 31. Springer, Madrid" href="/article/10.1007/s10055-019-00414-9#ref-CR17" id="ref-link-section-d76161e688">2007</a>).</p><p>In this work, the hyper-redundant and soft manipulator <i>Kyma</i> will be used to conduct the experiments (Martín-Barrio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Martín-Barrio A, Terrile S, Díaz-Carrasco M, Del Cerro J, Barrientos A (2019) Modeling the soft robot Kyma based on Real-Time Finite Element Method (under review)" href="/article/10.1007/s10055-019-00414-9#ref-CR37" id="ref-link-section-d76161e697">2019</a>) (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig3">3</a>). Its structure consists of a set of bellows that allow both flexion and prismatic movements. It is divided into 4 sections, each one actuated by 3 angularly equidistant tendons and motors. Therefore, teleoperating this robot will rely on 4 input positions. Moreover, the robot is also equipped with a gripper for pick and place operations.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="831" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Kyma, a soft continuous manipulator robot actuated by tendons</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Additionally, a computer is used to host the interfaces and control the robot movements. It uses different peripherals to transmit or capture information from the user such as a display monitor, a VR headset, a keyboard or a gaming controller. The robot feedback to the user will be visual, so it will rely on displaying its shape, obtained from the onboard sensory system based on <i>Inertial Measurement Units</i> (IMUs).</p><p>Therefore, the developed setup corresponds to <i>Mixed Reality</i> (MR), since interaction occurs with both real and virtual elements through real and virtual environments. This MR can be achieved from a <i>Virtual Reality</i> (VR) perspective, by introducing the user in a virtual world, or from an <i>Augmented Reality</i> (AR) one, by overlapping virtual elements over the real scene. In this work, the developments have been made in MR from both perspectives using the <i>HTC Vive</i> VR headset and the <i>Hololens</i> AR glasses, respectively, although the experiments have been entirely conducted on the first ones.</p><p>It is worth highlighting that virtual and augmented environments are not competitors, but rather different perspectives for different purposes and applications. Usually, AR is thought to be useful in local environments, enriching the real world, while VR is oriented to replicate remote or imaginary places by immersing the user in the environment. For example, teleoperating a robot in a nuclear reactor (Dutta et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Dutta P, Rastogi N, Gotewal KK (2017) Virtual reality applications in remote handling development for tokamaks in India. Fusion Eng Des 118:73–80" href="/article/10.1007/s10055-019-00414-9#ref-CR13" id="ref-link-section-d76161e738">2017</a>), or in a remote place such as the moon (Britton et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Britton N, Yoshida K, Walker J, Nagatani K, Taylor G, Dauphin L (2015) Lunar micro rover design for exploration through virtual reality tele-operations. In: Field and service robotics. Springer, pp 259–272" href="/article/10.1007/s10055-019-00414-9#ref-CR5" id="ref-link-section-d76161e741">2015</a>), is an application better suited for VR. However, teleoperating a robot in a surgery room (Pessaux et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Pessaux P, Diana M, Soler L, Piardi T, Mutter D, Marescaux J (2015) Towards cybernetic surgery: robotic and augmented reality-assisted liver segmentectomy. Langenbeck’s Arch Surg 400:381–385" href="/article/10.1007/s10055-019-00414-9#ref-CR43" id="ref-link-section-d76161e744">2015</a>), or in a cooperative environment for in situ assemblies (Michalos et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Michalos G, Karagiannis P, Makris S, Tokçalar Ö, Chryssolouris G (2016) Augmented reality (AR) applications for supporting human-robot interactive cooperation. Procedia CIRP 41:370–375" href="/article/10.1007/s10055-019-00414-9#ref-CR40" id="ref-link-section-d76161e747">2016</a>), are perspectives more appropriate for AR.</p></div></div></section><section aria-labelledby="Sec4"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4">Human–robot interfaces</h2><div class="c-article-section__content" id="Sec4-content"><p>This work aims to determine whether immersive interfaces are better or worse than conventional ones to teleoperate hyper-redundant robots. Hence, both interfaces have been created based on the same constitutive elements in <i>Unity3D</i>, a cross-platform real-time engine. A <i>human</i>–<i>robot interface</i> can be understood as a set of elements that enable the communication and interaction between a person and a robotic system. In this context, a good interface would be clear, concise, familiar, responsive, consistent, aesthetic, efficient and forgiving. Binding such features will give place to an efficient interface while also providing high situational awareness.</p><p>Teleoperation of hyper-redundant robots, similarly to multi-robot systems, usually requires a relatively high number of inputs to appropriately manage their shape (Roldán et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Roldán J, Peña-Tapia E, Martín-Barrio A, Olivares-Méndez M, Del Cerro J, Barrientos A (2017) Multi-robot interfaces and operator situational awareness: study of the impact of immersion and prediction. Sensors 17:1720" href="/article/10.1007/s10055-019-00414-9#ref-CR49" id="ref-link-section-d76161e770">2017</a>). And it is important to highlight that human beings perform better paying full attention to one thing at one time (Raskin <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Raskin J (2000) The humane interface: new directions for designing interactive systems. Addison-Wesley Professional, Boston" href="/article/10.1007/s10055-019-00414-9#ref-CR45" id="ref-link-section-d76161e773">2000</a>). Therefore, choosing the most convenient interface for this purpose is very important in order to maximize efficiency. In the context of teleoperating robots, e<i>fficiency</i> will be defined as the ability to maximize precision and speed by commanding a robot to perform a determined task.</p><p>In this context, the number of inputs will be highly dependent on the number of degrees of freedom, but not always strictly the same. For example, several hyper-redundant devices such as continuous or soft robots can be considered as under-actuated (Martín-Barrio et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018b" title="Martín-Barrio A, Terrile S, Barrientos A, del Cerro J (2018b) Hyper-redundant robots: classification, state-of-the-art and issues. Revista Iberoamericana de Automática e Informática Industrial 15:351–362" href="/article/10.1007/s10055-019-00414-9#ref-CR36" id="ref-link-section-d76161e782">2018b</a>). Such systems have a higher number of degrees of freedom than actuators, so their teleoperation will usually rely on a <i>model</i> to estimate their pose when being teleoperated. In the case of <i>Kyma</i> (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig3">3</a>), it has 4 sections so the number of inputs will be the same, although the virtual robot will rely on model based on <i>Piecewise Constant Curvature</i> (PCC) to reconstruct its shape (Gravagne and Walker <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2000" title="Gravagne IA, Walker ID (2000) Kinematic transformations for remotely-actuated planar continuum robots. In: IEEE international conference on robotics and automation. Proceedings. ICRA’00. IEEE, pp 19–26" href="/article/10.1007/s10055-019-00414-9#ref-CR19" id="ref-link-section-d76161e798">2000</a>). This is the main difference between commanding non-redundant robots and the shape of continuous or soft ones.</p><p>On the other hand, <i>situational awareness</i> can be defined as the perception of elements in the environment within a volume of time and space, the comprehension of their meaning and the projection of their status in the near future (Endsley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988a" title="Endsley MR (1988a) Design and evaluation for situation awareness enhancement. In: Proceedings of the Human Factors Society annual meeting, vol 2. SAGE Publications Sage CA: Los Angeles, CA, pp 97–101" href="/article/10.1007/s10055-019-00414-9#ref-CR14" id="ref-link-section-d76161e807">1988a</a>). Under these circumstances, a high level of situational awareness is considered a result of a high level of telepresence. Thus, situational awareness will be measured later to infer the quality of telepresence.</p><p>Therefore, two interfaces were created in order to make further comparisons: a <i>conventional</i> one that shows the information in a regular monitor, and an <i>immersive</i> one that uses a virtual reality headset to display the same information.</p><p>The scenes are originally developed in a three-dimensional space, although in the conventional interface the information is shown in a two-dimensional (2D) display. As it will be explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec14">6</a>, in both interfaces the user is expected to move the input towards 5 predefined positions and, in case of making a selection, the robot will move accordingly. The pose of the robot will be updated in the interface in real time using the onboard sensory system explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec3">3</a>.</p><h3 class="c-article__sub-heading" id="Sec5">Conventional interface</h3><p>In this interface, the user is sitting looking at the monitor and can use the keyboard to move in 3D a sphere that symbolizes the position where the robot’s end-effector is commanded to move (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig4">4</a>). The real robot was located in the same space so the users could see the robot being commanded. However, in a real scenario, the robot could be located in a remote environment, so the current pose of the robot is adequately displayed in real time using the bilateral scheme explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec3">3</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig4_HTML.jpg?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig4_HTML.jpg" alt="figure4" loading="lazy" width="685" height="383" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Conventional interface. Here, the scene is displayed in two dimensions by a monitor. The robot is commanded to move using a keyboard, and its pose is displayed in the interface in real time</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec6">Immersive interface</h3><p>In the immersive interface, the same information is displayed, but in this case an immersive headset with three-dimensional (3D) stereoscopic vision is used. For this practice, the user is asked to stand up and make small movements to command the robot position. Here, the user moves the 3D sphere that symbolizes the end-effector position by pushing it using the bare hands, tracked by the <i>Leap Motion</i> device (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig5">5</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig5_HTML.jpg?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig5_HTML.jpg" alt="figure5" loading="lazy" width="685" height="383" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Immersive interface. The setup is nearly the same than in the conventional interface but using a VR headset instead of a monitor</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>As a remainder, these interfaces were developed to contrast which one is better in terms of efficiency and situational awareness. In later experiments, only visual feedback will be further analysed to enlighten which one of such interfaces is better suited to enhance telepresence and efficiency, regardless of the interaction strategy used.</p><p>Next, Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec7">5</a> will introduce different interaction tools to contrast if <i>natural language</i> can improve the performance and reduce the workload against conventional tools to teleoperate hyper-redundant robots. Specific details of the experiments and results for both hypotheses are explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec14">6</a>.</p></div></div></section><section aria-labelledby="Sec7"><div class="c-article-section" id="Sec7-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec7">Interaction tools</h2><div class="c-article-section__content" id="Sec7-content"><p>One of the main purposes of this work aims at determining which strategy is more appropriate to teleoperate hyper-redundant robots in an immersive interface in terms of efficiency and workload.</p><p>In this context, <i>efficiency</i> will be defined in the same way as the previous Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec4">4</a>, as the ability to maximize precision and speed. On the other hand, the <i>workload</i> can be defined as a hypothetical construct that represents the cost incurred by a human operator to achieve a particular level of performance (Hart and Staveland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Hart SG, Staveland LE (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research. In: Advances in psychology, vol 52. Elsevier, pp 139–183" href="/article/10.1007/s10055-019-00414-9#ref-CR21" id="ref-link-section-d76161e906">1988</a>).</p><p>As a result, 6 different approaches have been developed in <i>Unity3D</i>, making use of a computer, the <i>HTC Vive</i> VR headset and, in some cases, <i>Xbox</i> or <i>Leap Motion</i> controllers. These strategies are the following: controller, master–slave, local gestures, remote gestures, voice commands and a hybrid approach of gestures and voice recognition.</p><p>The strategy based on a <i>controller</i> was chosen to illustrate one of many possible conventional tools based on a physical device to command the movements. The <i>master</i>–<i>slave</i> was inspired by past efforts based on physical replicas of the robot; but in this case, the replica was designed in the virtual world, so it is easily scalable, the teleoperation transparency is preserved, and the construction and maintenance costs are minimized. The <i>local gestures</i> strategy was developed to be the most intuitive since it relied on shaping the real-size robot with the hands. While <i>remote gestures</i> were also conceived to manipulate the robot with the hands, they promise better interactions when the robot is too big or too small. Additionally, v<i>oice commands</i> were implemented to let the robot be moved when the operator hands are busy or their movement is hindered. Finally, the <i>gestures and voice</i> strategy combines two natural-language approaches to determine if they synergize and give place to better results.</p><p>In every method, the user has to exploit all the available resources to adequately manage the pose of a virtual robot, which has to be approached towards a predefined virtual reference. This reference was only conceived to measure the performance and conduct the experiments. A more detailed description of each interaction tool is given below. Basically, in all these interaction tools the user will be introduced in a <i>Virtual Reality</i> scenario and then will be requested to move a virtual robot (green) towards a virtual reference (blue). In all these experiments, the participants needed to complete the tasks as fast and precise as possible, as it will be explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec14">6</a>.</p><h3 class="c-article__sub-heading" id="Sec8">Controller</h3><p>This strategy uses an <i>Xbox</i> gaming controller to teleoperate the robot, so it can be defined as a conventional technique against others developed in this work based on natural language. The user, provided with a <i>VR</i> headset and the controller, will rely on pressing buttons and axis to move the virtual robot towards the reference.</p><p>First, the desired joint must be selected using some buttons, and then, it can be moved along the Cartesian directions using the controller’s axis. The implementation allows simultaneous inputs so the user can combine the three directions to perform 3D movements. The joints’ speed can be adjusted by the quantity of displacement in the axis. In fact, these inputs have been filtered to minimize drifting or undesired sensibility issues (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig6">6</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig6_HTML.jpg?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig6_HTML.jpg" alt="figure6" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Controller strategy. In this interaction tool, a gaming controller is used to select and move the robot joints one by one using buttons and axis</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Master–slave</h3><p>Traditionally, the master–slave architecture often implied the design and construction of one physical device (master) to reproduce the real-robot movements (slave). In this context, the system is more transparent if the similarity of the master and the slave is higher. However, creating a physical replica of the robot with onboard sensors usually involved additional cost, maintenance and construction time.</p><p>Currently, immersive realities offer the possibility of rapidly and easily creating a virtual master with the same features of the slave without any physical restriction on its construction, and any additional cost or maintenance. According to the specific literature, all the presented strategies in this chapter have a master and a slave. However, this strategy takes its name inspired from traditional master–slave physical models, since the robot relies on a virtual model of different size. Thus, this strategy could be most beneficial when the robot size is too big or too small to be adequately manipulated using regular gestures.</p><p>In this scenario, the user’s hands were tracked using the <i>Leap Motion</i> controller, which uses a set of infrared <i>light</i>-<i>emitting diodes</i> (LED) and cameras to track the movements of hands and fingers. The sense of <i>proprioception</i> using this device is very high, so the users feel like touching virtual objects, although they are not.</p><p>Thus, when the user raises one hand, a smaller replica of the robot is created. Additionally, it will follow the hand movements until it closes so it can be positioned as desired. Then, the other hand can be raised to gently push the robot joints towards the desired reference by applying contact. By doing this, the slave will be moved the same way as the master is being manipulated. But in any case the user will enter in contact with the slave directly (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig7">7</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig7_HTML.jpg?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig7_HTML.jpg" alt="figure7" loading="lazy" width="685" height="384" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Master–slave strategy. Here, a little replica of the virtual robot (master) is manipulated so the bigger one (slave) is identically moved towards the desired reference</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec10">Local gestures</h3><p>This strategy also uses hand tracking to capture gestures. However, the main difference is that the user must approach the virtual real-size robot and use the hands to push the robot joints towards the desired reference by direct contact. It was found that pushing enhanced efficiency and reduced workload against grabbing, so the joints could not be grabbed but only pushed using this approach.</p><p>Here, the performance is highly dependent on the relative size of the robot and the user. If the virtual robot is too small, it will be difficult to be precise. On the other hand, if the virtual robot is too big, then the user will be very slow and will need to move more in the workspace. However, in <i>VR</i> is possible to scale all the environment so the perfect size of the robot can be chosen to the most adequate even if the real one has other dimensions (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig8">8</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig8_HTML.jpg?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig8_HTML.jpg" alt="figure8" loading="lazy" width="685" height="384" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Local gestures strategy. The user’s hands are tracked using a Leap Motion system so they can be used to directly interact with the virtual robot</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec11">Remote gestures</h3><p>Next, similarly to the master–slave strategy, remote gestures allow the robot to be moved without direct contact with the virtual robot model. In this case, the virtual robot model is moved without any intermediary using hand gestures as well.</p><p>Therefore, the user can open the index finger on the one hand, so a virtual laser appears as an extension to easily select the joint to move. Then, when the other hand is opened, it will move the selected joint accordingly to its own movement. When this hand closes, the joint does not move.</p><p>This implemented technique is called <i>reindexing</i>. It can be defined as the temporary removal of connection between master and slave when the range of motion is smaller for the first than for the second one (Sayers <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Sayers C (1999) Remote control robotics. Springer, New York" href="/article/10.1007/s10055-019-00414-9#ref-CR52" id="ref-link-section-d76161e1065">1999</a>). It is the same principle as when a computer mouse is raised when getting closer to the edge of a table.</p><p>Since this strategy relies on remote gestures, the operator is not expected to move a lot, so reindexing is very important to minimize the difference of size between master and the slave (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig9">9</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig9_HTML.jpg?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig9_HTML.jpg" alt="figure9" loading="lazy" width="685" height="386" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Remote gestures strategy. In this setup, the user can select the joint to move by pointing with a virtual laser that emanates from the left index finger. Then, the selected joint remotely moves when the right hand is opened</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec12">Voice commands</h3><p>This following tool is based on speech recognition to command the robot movements. This approach was created so the user can teleoperate the robot even if the hands are busy or their movement is hindered. Therefore, the user needed to remember some intuitive commands previously defined in a dictionary. Such commands could activate the movement of the robot by small increments or continuously by defining when to start and stop. This work opted for the second alternative because it was found to increase the precision and dramatically reduce the number of commands.</p><p>Therefore, the joint to move is first selected by pronouncing numbers, in this case, from 1 to 4. Then, the speed can be defined as slow, medium or fast. Only three levels of speed are taken to reduce the complexity of the strategy. However, they are tuned for the robot dimensions, so the slowest one is aimed to be more precise, while the fastest is more adequate to move long distances. Finally, the user can speak in which direction to move the selected joint among left, right, forward, backward, up and down.</p><p>These directions have not been allowed to be combined using this method because the result was unfamiliar to the users. Alternatively, this method could be implemented using voice intensity as a variable to select the movement speed (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig10">10</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig10_HTML.jpg?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig10_HTML.jpg" alt="figure10" loading="lazy" width="685" height="383" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Voice commands strategy. The VR headset records the user’s voice to recognize simple commands in order to select the joint to move, its speed and the direction</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">Gestures and voice</h3><p>If the user can speak and move in the real environment, another approach is based on using both voice and gestures to command the robot. Thus, it is the only multimodal approach in this work. Several methods could be proposed in this matter, but it was finally decided to combine what voice and gestures do better.</p><p>Therefore, and similarly to the previous method, <i>Voice Commands</i> are programmed to select the desired joint by pronouncing which joint to move from 1 to 4. Then, similarly to the <i>Remote Gestures</i> strategy, the user can raise the hand to move the joint with reindexing, as explained before. Hence, the voice is proposed as an alternative of pointing with the other hand to unequivocally select the joints, and gestures are selected to efficiently move the joint. This method can be understood as a hybrid method within the natural-language strategies (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig11">11</a>).</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig11_HTML.jpg?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig11_HTML.jpg" alt="figure11" loading="lazy" width="685" height="385" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Gestures and voice strategy. This strategy uses the voice to select a joint and hand gestures to remotely move it</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section aria-labelledby="Sec14"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14">Experiments and results</h2><div class="c-article-section__content" id="Sec14-content"><p>Two experiments were conducted between February and March of 2019 to validate the proposed hypotheses of this work. As a reminder, the first one wonders whether immersive 3D interfaces or conventional 2D ones are better suited to teleoperate hyper-redundant robots. On the other hand, the second one aims at selecting which interaction tools are more efficient and reduce workload for the same purpose.</p><p>For both experiments, a sample of 50 volunteers was selected in order to be heterogeneous enough to lead to generalizable results, while also homogeneous enough to minimize variances due to factors not relevant in the context of this work. For example, an engineering student with experience using computers or videogames is expected to perform differently compared to an elderly person. Since this work is not intended to compare people skills, but the interfaces and interaction tools instead, the population sample should exhibit a compromise between heterogeneity and homogeneity.</p><p>Therefore, the participants have different academic backgrounds but all of them come from a university context, they have different genders and ages but come from the same generation, and have different previous experience in immersive realities, robotics and videogames (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00414-9#Tab1">1</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Mean and variance values of the participant ages and their subjective previous experience (the higher, the more experience)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00414-9/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Since this sample is just one sector of the potential population, the obtained data are first evaluated to measure the goodness of fit to a determined statistical distribution. For this purpose, the <i>Kolmogorov</i>–<i>Smirnov test</i> is applied to all the normalized data using the software <i>MATLAB</i> (Daniel <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Daniel WW (1990) Kolmogorov–Smirnov one-sample test. In: Applied nonparametric statistics, vol 2" href="/article/10.1007/s10055-019-00414-9#ref-CR11" id="ref-link-section-d76161e1217">1990</a>). As a result, it has been found that the data follow a <i>normal distribution</i>. Based on this assumption, determining if the means of two sets of data are significantly different from each other, require the use of a <i>Student’s t test</i> or <i>one</i>-<i>way analysis of variance</i> (ANOVA), so they will be applied throughout this chapter. If not mentioned, the chosen significance level is <i>α</i> = 0.05.</p><h3 class="c-article__sub-heading" id="Sec15">Human–robot interfaces</h3><p>In order to test the objective efficiency of the developed interfaces in terms of precision and time, the participants were asked to move a blue sphere as a representation to where the end-effector of the robot is desired to move. They were warned to do it equally as fast and precise as possible. This input could be moved towards 5 numbered and predefined target positions, depicted by grey spheres of the same size. The participants could practice for a while until they felt ready, in order to minimize the learning factor between interfaces. They could press a begin button and move the input towards 3 of the 5 target positions, previously specified. Then, a timer began to run, and the precision was measured as the mean of the Euclidean distances between the input and the desired targets. Both time and precision variables were first rescaled—min–max normalization—(Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>) among the data relative to both interfaces to allow further comparisons, then averaged (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ2">2</a>), and then rescaled again (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>).</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$x^{{\prime }} = \frac{{x - { \hbox{min} }\left( x \right)}}{{\hbox{max} \left( x \right) - { \hbox{min} }\left( x \right)}}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\bar{x} = \frac{1}{n}\mathop \sum \limits_{i = 1}^{n} x_{i}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>where <i>x</i> is a single data for each participant, and <i>x</i>′ the scaled value relative to all the population for both interfaces, <span class="mathjax-tex">\(\bar{x}\)</span> the mean value between time and precision and <i>n</i> the number of participants. By this method, speed and precision are measured the same to test efficiency since both parameters can be considered as equally important to teleoperate a generic hyper-redundant robot. However, it must be remarked that one variable could be more important than the other depending on the robot and its purpose.</p><p>As a result, the efficiency is a 33.55% higher on average and the variance a 41.98% lower for the immersive interface than for the conventional one (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig12">12</a>a). The results between immersive and conventional interfaces are significantly different between each other based on both the <i>Student’s t test</i> (<i>p </i>= 1.9 × 10<sup>−6</sup>) and the <i>ANOVA</i> (<i>p </i>= 6.6 × 10<sup>−7</sup>). Therefore, their comparison is statistically reliable.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig12_HTML.png?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig12_HTML.png" alt="figure12" loading="lazy" width="685" height="176" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Results for the human–robot interfaces test. <b>a</b> Efficiency in terms of precision and time, <b>b</b> situational awareness, <b>c</b> subjective visual feedback and <b>d</b> subjective preference</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The situational awareness (<i>S</i><sub>A</sub>) is measured in this document inspired by previous works (Endsley <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988b" title="Endsley MR (1988b) Situation awareness global assessment technique (SAGAT). In: Proceedings of the IEEE 1988 National Aerospace and Electronics Conference. IEEE, pp 789–795" href="/article/10.1007/s10055-019-00414-9#ref-CR15" id="ref-link-section-d76161e1386">1988b</a>). Since such variable is subjective and depends on the environment setup, the questionnaire is usually custom-made. In this test, 3 questions were made to quantify the user perception of what was happening in the real world, compared to the images displayed on the interface. Therefore, the participants were able to see both the real robot and the interface in order to make an inference.</p><p>It is worth highlighting that the interface was programmed to hide the target number labels, so the participants were previously asked to remember them in order to test memory retention (<i>s</i><sub>1</sub>). Moreover, they were asked to focus on which of the 3 commanded movements they thought their precision was higher (<i>s</i><sub>2</sub>). To finish, they were asked to evaluate visual feedback from both interfaces (<i>s</i><sub>3</sub>). Parameters <i>s</i><sub>1</sub> and <i>s</i><sub>2</sub> were binary variables which could take value 0 if the answer was wrong or 1 if it was right. Variable <i>s</i><sub>3</sub>, the visual feedback, it was measured from 1 to 10. All these variables were normalized (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>) to allow further averaging (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ3">3</a>).</p><p>Furthermore, the influence of <i>s</i><sub>3</sub> was increased to reduce the variance of the results a 10% (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ3">3</a>).</p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$S_{A} = \frac{{s_{1} + s_{2} + 3s_{3} }}{5}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p>Afterwards, the resulting data are again rescaled (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>) in order to facilitate comparisons between interfaces. The results show that situational awareness is a 73.71% higher in the immersive interface than for the conventional one (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig12">12</a>b). Concerning the visual feedback evaluation, the participants were able to see the real robot and the model within the interface simultaneously and in real time, so they could make reliable comparisons between them. As a result, they perceived that the visual feedback was a 39.47% higher for the immersive interface than for the conventional one on average (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig12">12</a>c). These results are also significantly different based on the <i>Student’s t test</i> (<i>p</i> = 1.8 × 10<sup>−10</sup>) and the <i>ANOVA</i> (<i>p</i> = 4.3 × 10<sup>−12</sup>).</p><p>To finish, and regardless of previous questions, the participants were asked to subjectively choose between both interfaces assuming they had to perform the same task. Consequently, 94% of the participants preferred to use the immersive interface (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig12">12</a>d). Therefore, both subjective and objective parameters agree to select the immersive 3D interface to teleoperate hyper-redundant robots as the best to enhance efficiency and telepresence.</p><h3 class="c-article__sub-heading" id="Sec16">Interaction tools</h3><p>The human–robot interfaces test shows that immersive ones obtain better results to teleoperate hyper-redundant robots. The next inquiry is based on selecting which interaction strategy is better within this immersive reality for the same purpose. As a reminder, there are 6 developed strategies, as explained in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00414-9#Sec7">5</a>: controller (CR), master–slave (MS), local gestures (LG), remote gestures (RG), voice commands (VC) and gestures and voice (GV). In this test, the participants used a VR headset to command the movements of a hypothetical virtual robot towards a randomly defined set of references by using the previously mentioned interaction approaches.</p><p>On the one hand, the efficiency was similarly measured using both elapsed time and precision. For each one of the approaches, the user could also practice before the timer began in order to reduce the learning factor. And the precision was measured and averaged as the Euclidean distance between every 4 sections of the virtual robot and the relative ones of the reference, differently coloured to ease comprehension.</p><p>In all the strategies, the user had to repeat the process 3 times, moving the virtual robot towards different references in order to increase the data normalization. Such references were randomly defined but bounded to a predefined and limited space so they would not differ too much from one participant to another. Therefore, the virtual robot joints were moved 3600 times in total in these experiments. Accordingly, the learning factor was minimized, whereas the randomness and repetitions standardize the data in order to achieve generalization.</p><p>Similarly to the previous experiment, the speed and time were first rescaled (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>), then averaged (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ2">2</a>), and then rescaled again (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>), giving place to the previously defined <i>efficiency</i>. In this case, the obtained <i>p values</i> using the <i>Student’s t test</i> in pairs show that the interaction tools are significantly different except for two combinations (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00414-9#Tab2">2</a>). In those cases, it is not possible to determine which one of them is statistically better or worse.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Pair-wise <i>p</i> values to show the significance between the results of efficiency in the interaction tool test</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00414-9/tables/2"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Therefore, as seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>a, the efficiency is indistinguishably the best for both <i>local gestures</i> and <i>gestures and voice</i>, with mean values of around 90%. They are closely followed by <i>remote gestures</i> and the <i>controller</i>, also indistinguishable between each other with efficiencies of around 85% on average. And finally, the <i>master</i>–<i>slave</i> and the <i>voice commands</i> are, respectively, the ones with less objective efficiency.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig13_HTML.png?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig13_HTML.png" alt="figure13" loading="lazy" width="685" height="454" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Results for the interaction-tools test. <b>a</b> Efficiency in terms of precision and speed, <b>b</b> workload, <b>c</b> subjective preference, <b>d</b> subjective importance of variables in workload, <b>e</b> subjective and objective performance comparison and <b>f</b> previous experience influence in performance</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>On the other hand, the most common method to determine the workload (W) of a mission is the <i>NASA Task Load Index</i> (NASA-TLX) (Hart <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergonomics society annual meeting, vol 9. Sage publications Sage CA: Los Angeles, CA, pp 904-908" href="/article/10.1007/s10055-019-00414-9#ref-CR20" id="ref-link-section-d76161e1632">2006</a>; Hart and Staveland <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1988" title="Hart SG, Staveland LE (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research. In: Advances in psychology, vol 52. Elsevier, pp 139–183" href="/article/10.1007/s10055-019-00414-9#ref-CR21" id="ref-link-section-d76161e1635">1988</a>). It is based on rating 6 variables related to the influence of workload: mental demands, physical demands, temporal demands, performance, efforts and frustration. These parameters were first defined to ease their comprehension, and then, the users could evaluate them using natural numbers (<i>w</i><sub><i>i</i></sub> = [0, 20]). On the other hand, the participants ordered from lowest to highest the subjective relevance of such parameters in workload (<i>r</i><sub><i>i</i></sub> = [0, 5]), so the obtained scores were weighted and normalized according to (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ4">4</a>).</p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$W = \frac{{\mathop \sum \nolimits_{i = 1}^{6} w_{i} r_{i} }}{{\mathop \sum \nolimits_{i = 1}^{6} 20r_{i} }}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p>Afterwards, the resulting data are again rescaled (Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10055-019-00414-9#Equ1">1</a>) in order to facilitate comparisons between strategies. Again, the <i>p values</i> using the <i>Student’s t test</i> in pairs have been analysed to objectively make reliable comparisons from the results showed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>b (Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00414-9#Tab3">3</a>).</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Pair-wise <i>p</i> values to show the significance between the results of the workload in the interaction tools test</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00414-9/tables/3"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Thus, local <i>gestures</i>, <i>controller</i>, <i>gestures and voice</i> and <i>remote gestures</i> have half of the workload compared to <i>master</i>–<i>slave</i> and <i>voice commands</i> with mean values of approximately 32% and 61%, respectively.</p><p>Besides, the participants selected the frustration as the most important factor to influence workload, followed by performance, mental demands, efforts, time demands and physical demands as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>d. Furthermore, it is possible to compare the objective efficiency represented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>a with the subjective performance obtained from the workload test. Surprisingly, the participants were very accurate evaluating their performance and it was found to be highly influenced by the level of frustration (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>e).</p><p>As explained in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00414-9#Tab1">1</a>, the participants were asked about their previous experience in VR and AR, robotics and videogames. If that experience is averaged and plotted against the objective mean efficiency for both tests, the variables are found as directly proportional (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig14">14</a>). Therefore, previous experience can be understood as a very important factor to achieve a high level of efficiency.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig14_HTML.png?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00414-9/MediaObjects/10055_2019_414_Fig14_HTML.png" alt="figure14" loading="lazy" width="685" height="477" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Previous experience influence in efficiency both for the human–robot interfaces and the interaction tools experiments</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00414-9/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Although the <i>master</i>–<i>slave</i> strategy is controlled by gestures, it has been inspired by the physical architecture. Therefore, this work proposes to consider this tool as conventional, together with the <i>controller</i> one. In this context, the results show that despite having less experience with natural language than conventional tools, the participants performed better using the first ones against the last ones (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>f).</p><p>Finally, and regardless of previous results, the participants were asked to choose among their preferred interaction tool to teleoperate hyper-redundant robots. More than half of the participants, exactly 52%, chose the <i>gestures and voice</i> strategy, followed by <i>local gestures</i> with 28% and <i>remote gestures</i> with 14% (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00414-9#Fig13">13</a>c). If the number of conventional and natural-language tools were the same, 92.31% of the participants would have preferred to use a natural-language tool against a conventional one.</p></div></div></section><section aria-labelledby="Sec17"><div class="c-article-section" id="Sec17-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec17">Conclusions</h2><div class="c-article-section__content" id="Sec17-content"><p>This work aims to clear the path to focus the future research of hyper-redundant robot’s teleoperation. This kind of robots has numerous degrees of freedom, so their spatial understanding is often difficult to be perceived. Teleoperation is a very important field in robotics since it is essential for several applications such as surgery, spatial missions, inspection or surveillance. A bilateral communication based on a position–position scheme has been developed to be the most adequate to teleoperate hyper-redundant robot applications.</p><p>On the one hand, this work aimed to determine whether immersive 3D interfaces are better or worse than conventional 2D ones in order to teleoperate hyper-redundant robots. Experiments with 50 participants show that immersive interfaces exhibit 33.55% mean higher efficiency in terms of speed and precision and 73.71% mean higher situational awareness than conventional ones. As a result, immersive interfaces were chosen to teleoperate this kind of robots by 94% of the participants.</p><p>On the other hand, some interaction tools were tested by the same population in order to determine which strategy is better suited for the same purpose within an immersive reality. A total of 6 methods were developed: <i>controller</i>, <i>master</i>–<i>slave</i>, <i>local gestures</i>, <i>remote gestures</i>, <i>voice commands</i> and <i>gestures and voice.</i> In this case, efficiency is indistinguishably the best for both <i>local gestures</i> and <i>gestures and voice</i>, with mean values of around 90%, closely followed by <i>remote gestures</i> and <i>controller</i>. Also, a workload test showed that <i>local gestures</i>, <i>controller</i>, <i>gestures and voice</i> and <i>remote gestures</i> have half of the workload than the other two approaches.</p><p>Besides, the participants were very accurate in evaluating their performance, which was found to be highly influenced by their level of frustration. Moreover, results showed that natural-language tools are more intuitive since exhibited higher performance despite the less previous experience. Finally, more than half of the users chose the <i>gestures and voice</i> strategy as the preferred one. This is understandable since we, as human beings, are used to communicate by simultaneously gesticulating and speaking. This is closely related to the <i>Principle of Least Astonishment</i> (<i>POLA</i>), which states that a user interface should behave in the way that most users will expect it to behave. Thus, it can be deduced that this combined strategy was the most intuitive, while, at the same time, the distribution of tasks to different stimulus did not increase the subjective workload to the users. Moreover, 92.31% selected an interaction tool based on natural language against a conventional one. Future work in this matter should be focused on proposing and classifying new interaction strategies in order to select the most appropriate, depending on the hyper-redundant robot specifications and its applications.</p><p>Therefore, this work concludes that immersive realities and natural-language tools should play an important role in the near future of hyper-redundant robots and their teleoperation.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="R. Aracil, M. Ferre, M. Hernando, E. Pinto, J. Sebastian, " /><meta itemprop="datePublished" content="2002" /><meta itemprop="headline" content="Aracil R, Ferre M, Hernando M, Pinto E, Sebastian J (2002) Telerobotic system for live-power line maintenance:" /><p class="c-article-references__text" id="ref-CR1">Aracil R, Ferre M, Hernando M, Pinto E, Sebastian J (2002) Telerobotic system for live-power line maintenance: ROBTET. Control Eng Pract 10:1271–1281</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2FS0967-0661%2802%2900182-X" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Telerobotic%20system%20for%20live-power%20line%20maintenance%3A%20ROBTET&amp;journal=Control%20Eng%20Pract&amp;volume=10&amp;pages=1271-1281&amp;publication_year=2002&amp;author=Aracil%2CR&amp;author=Ferre%2CM&amp;author=Hernando%2CM&amp;author=Pinto%2CE&amp;author=Sebastian%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aracil R, Buss M, Cobos S, Ferre M, Hirche S, Kuschel M, Peer A (2007) The human role in telerobotics. In: Adv" /><p class="c-article-references__text" id="ref-CR2">Aracil R, Buss M, Cobos S, Ferre M, Hirche S, Kuschel M, Peer A (2007) The human role in telerobotics. In: Advances in telerobotics. Springer, pp 11–24</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="A. Barrientos, LF. Peñin, C. Balaguer, R. Aracil, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Barrientos A, Peñin LF, Balaguer C, Aracil R (2007) Fundamentos de robótica. McGrawHill, Madrid" /><p class="c-article-references__text" id="ref-CR3">Barrientos A, Peñin LF, Balaguer C, Aracil R (2007) Fundamentos de robótica. McGrawHill, Madrid</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 3 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Fundamentos%20de%20rob%C3%B3tica&amp;publication_year=2007&amp;author=Barrientos%2CA&amp;author=Pe%C3%B1in%2CLF&amp;author=Balaguer%2CC&amp;author=Aracil%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Bhattacherjee, S. Chattopadhayay, V. Rao, S. Seth, S. Mukherjee, A. Sengupta, S. Bhaumik, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Bhattacherjee S, Chattopadhayay S, Rao V, Seth S, Mukherjee S, Sengupta A, Bhaumik S (2018) Kinematics and tel" /><p class="c-article-references__text" id="ref-CR4">Bhattacherjee S, Chattopadhayay S, Rao V, Seth S, Mukherjee S, Sengupta A, Bhaumik S (2018) Kinematics and teleoperation of tendon driven continuum robot. Procedia Comput Sci 133:879–886</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.procs.2018.07.106" aria-label="View reference 4">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 4 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Kinematics%20and%20teleoperation%20of%20tendon%20driven%20continuum%20robot&amp;journal=Procedia%20Comput%20Sci&amp;volume=133&amp;pages=879-886&amp;publication_year=2018&amp;author=Bhattacherjee%2CS&amp;author=Chattopadhayay%2CS&amp;author=Rao%2CV&amp;author=Seth%2CS&amp;author=Mukherjee%2CS&amp;author=Sengupta%2CA&amp;author=Bhaumik%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Britton N, Yoshida K, Walker J, Nagatani K, Taylor G, Dauphin L (2015) Lunar micro rover design for exploratio" /><p class="c-article-references__text" id="ref-CR5">Britton N, Yoshida K, Walker J, Nagatani K, Taylor G, Dauphin L (2015) Lunar micro rover design for exploration through virtual reality tele-operations. In: Field and service robotics. Springer, pp 259–272</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Bugalia N, Sen A, Kalra P, Kumar S (2015) Immersive environment for robotic tele-operation. In: Proceedings of" /><p class="c-article-references__text" id="ref-CR6">Bugalia N, Sen A, Kalra P, Kumar S (2015) Immersive environment for robotic tele-operation. In: Proceedings of the 2015 conference on advances in robotics. ACM, p 49</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JY. Chen, EC. Haas, MJ. Barnes, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Chen JY, Haas EC, Barnes MJ (2007) Human performance issues and user interface design for teleoperated robots." /><p class="c-article-references__text" id="ref-CR7">Chen JY, Haas EC, Barnes MJ (2007) Human performance issues and user interface design for teleoperated robots. IEEE Trans Syst Man Cybern Part C (Appl Rev) 37:1231–1245</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCC.2007.905819" aria-label="View reference 7">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 7 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Human%20performance%20issues%20and%20user%20interface%20design%20for%20teleoperated%20robots&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20C%20%28Appl%20Rev%29&amp;volume=37&amp;pages=1231-1245&amp;publication_year=2007&amp;author=Chen%2CJY&amp;author=Haas%2CEC&amp;author=Barnes%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cheng-jun D, Ping D, Ming-lu Z, Yan-fang Z (2009) Design of mobile robot teleoperation system based on virtual" /><p class="c-article-references__text" id="ref-CR8">Cheng-jun D, Ping D, Ming-lu Z, Yan-fang Z (2009) Design of mobile robot teleoperation system based on virtual reality. In: IEEE international conference on automation and logistics. IEEE, pp 2024–2029</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GS. Chirikjian, JW. Burdick, " /><meta itemprop="datePublished" content="1994" /><meta itemprop="headline" content="Chirikjian GS, Burdick JW (1994) A hyper-redundant manipulator. IEEE Robot Autom Mag 1:22–29. https://doi.org/" /><p class="c-article-references__text" id="ref-CR9">Chirikjian GS, Burdick JW (1994) A hyper-redundant manipulator. IEEE Robot Autom Mag 1:22–29. <a href="https://doi.org/10.1109/100.388263">https://doi.org/10.1109/100.388263</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2F100.388263" aria-label="View reference 9">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20hyper-redundant%20manipulator&amp;journal=IEEE%20Robot%20Autom%20Mag&amp;doi=10.1109%2F100.388263&amp;volume=1&amp;pages=22-29&amp;publication_year=1994&amp;author=Chirikjian%2CGS&amp;author=Burdick%2CJW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Collins T, Shen W-M (2016) PASO: an integrated, scalable PSO-based optimization framework for hyper-redundant " /><p class="c-article-references__text" id="ref-CR10">Collins T, Shen W-M (2016) PASO: an integrated, scalable PSO-based optimization framework for hyper-redundant manipulator path planning and inverse kinematics. Information Sciences Institute Technical Report</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Daniel WW (1990) Kolmogorov–Smirnov one-sample test. In: Applied nonparametric statistics, vol 2" /><p class="c-article-references__text" id="ref-CR11">Daniel WW (1990) Kolmogorov–Smirnov one-sample test. In: Applied nonparametric statistics, vol 2</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Mark. Draper, Gloria. Calhoun, Heath. Ruff, David. Williamson, Timothy. Barry, " /><meta itemprop="datePublished" content="2003" /><meta itemprop="headline" content="Draper M, Calhoun G, Ruff H, Williamson D, Barry T (2003) Manual versus speech input for unmanned aerial vehic" /><p class="c-article-references__text" id="ref-CR12">Draper M, Calhoun G, Ruff H, Williamson D, Barry T (2003) Manual versus speech input for unmanned aerial vehicle control station operations. In: Proceedings of the human factors and ergonomics society annual meeting, vol 1. SAGE Publications Sage CA: Los Angeles, CA, pp 109–113</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F154193120304700123" aria-label="View reference 12">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Manual%20Versus%20Speech%20Input%20for%20Unmanned%20Aerial%20Vehicle%20Control%20Station%20Operations&amp;journal=Proceedings%20of%20the%20Human%20Factors%20and%20Ergonomics%20Society%20Annual%20Meeting&amp;volume=47&amp;issue=1&amp;pages=109-113&amp;publication_year=2003&amp;author=Draper%2CMark&amp;author=Calhoun%2CGloria&amp;author=Ruff%2CHeath&amp;author=Williamson%2CDavid&amp;author=Barry%2CTimothy">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Dutta, N. Rastogi, KK. Gotewal, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Dutta P, Rastogi N, Gotewal KK (2017) Virtual reality applications in remote handling development for tokamaks" /><p class="c-article-references__text" id="ref-CR13">Dutta P, Rastogi N, Gotewal KK (2017) Virtual reality applications in remote handling development for tokamaks in India. Fusion Eng Des 118:73–80</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.fusengdes.2017.03.047" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%20applications%20in%20remote%20handling%20development%20for%20tokamaks%20in%20India&amp;journal=Fusion%20Eng%20Des&amp;volume=118&amp;pages=73-80&amp;publication_year=2017&amp;author=Dutta%2CP&amp;author=Rastogi%2CN&amp;author=Gotewal%2CKK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Mica R.. Endsley, " /><meta itemprop="datePublished" content="1988" /><meta itemprop="headline" content="Endsley MR (1988a) Design and evaluation for situation awareness enhancement. In: Proceedings of the Human Fac" /><p class="c-article-references__text" id="ref-CR14">Endsley MR (1988a) Design and evaluation for situation awareness enhancement. In: Proceedings of the Human Factors Society annual meeting, vol 2. SAGE Publications Sage CA: Los Angeles, CA, pp 97–101</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F154193128803200221" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Design%20and%20Evaluation%20for%20Situation%20Awareness%20Enhancement&amp;journal=Proceedings%20of%20the%20Human%20Factors%20Society%20Annual%20Meeting&amp;volume=32&amp;issue=2&amp;pages=97-101&amp;publication_year=1988&amp;author=Endsley%2CMica%20R.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Endsley MR (1988b) Situation awareness global assessment technique (SAGAT). In: Proceedings of the IEEE 1988 N" /><p class="c-article-references__text" id="ref-CR15">Endsley MR (1988b) Situation awareness global assessment technique (SAGAT). In: Proceedings of the IEEE 1988 National Aerospace and Electronics Conference. IEEE, pp 789–795</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Espinoza MS, Goncalves J, Leitao P, Sanchez JLG, Herreros A (2012) Inverse kinematics of a 10 DOF modular hype" /><p class="c-article-references__text" id="ref-CR16">Espinoza MS, Goncalves J, Leitao P, Sanchez JLG, Herreros A (2012) Inverse kinematics of a 10 DOF modular hyper-redundant robot resorting to exhaustive and error-optimization methods: a comparative study. In: Robotics Symposium and Latin American Robotics Symposium (SBR-LARS), 2012 Brazilian. IEEE, pp 125–130</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Ferre, R. Aracil, C. Balaguer, M. Buss, C. Melchiorri, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Ferre M, Aracil R, Balaguer C, Buss M, Melchiorri C (2007) Advances in telerobotics, vol 31. Springer, Madrid" /><p class="c-article-references__text" id="ref-CR17">Ferre M, Aracil R, Balaguer C, Buss M, Melchiorri C (2007) Advances in telerobotics, vol 31. Springer, Madrid</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Advances%20in%20telerobotics&amp;publication_year=2007&amp;author=Ferre%2CM&amp;author=Aracil%2CR&amp;author=Balaguer%2CC&amp;author=Buss%2CM&amp;author=Melchiorri%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Frigola M, Fernandez J, Aranda J (2003) Visual human machine interface by gestures. In: IEEE international con" /><p class="c-article-references__text" id="ref-CR18">Frigola M, Fernandez J, Aranda J (2003) Visual human machine interface by gestures. In: IEEE international conference on robotics and automation (Cat. No. 03CH37422). IEEE, pp 386–391</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Gravagne IA, Walker ID (2000) Kinematic transformations for remotely-actuated planar continuum robots. In: IEE" /><p class="c-article-references__text" id="ref-CR19">Gravagne IA, Walker ID (2000) Kinematic transformations for remotely-actuated planar continuum robots. In: IEEE international conference on robotics and automation. Proceedings. ICRA’00. IEEE, pp 19–26</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Sandra G.. Hart, " /><meta itemprop="datePublished" content="2006" /><meta itemprop="headline" content="Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergon" /><p class="c-article-references__text" id="ref-CR20">Hart SG (2006) NASA-task load index (NASA-TLX); 20 years later. In: Proceedings of the human factors and ergonomics society annual meeting, vol 9. Sage publications Sage CA: Los Angeles, CA, pp 904-908</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1177%2F154193120605000909" aria-label="View reference 20">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nasa-Task%20Load%20Index%20%28NASA-TLX%29%3B%2020%20Years%20Later&amp;journal=Proceedings%20of%20the%20Human%20Factors%20and%20Ergonomics%20Society%20Annual%20Meeting&amp;volume=50&amp;issue=9&amp;pages=904-908&amp;publication_year=2006&amp;author=Hart%2CSandra%20G.">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hart SG, Staveland LE (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical r" /><p class="c-article-references__text" id="ref-CR21">Hart SG, Staveland LE (1988) Development of NASA-TLX (Task Load Index): results of empirical and theoretical research. In: Advances in psychology, vol 52. Elsevier, pp 139–183</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hedayati H, Walker M, Szafir D (2018) Improving collocated robot teleoperation with augmented reality. In: Pro" /><p class="c-article-references__text" id="ref-CR22">Hedayati H, Walker M, Szafir D (2018) Improving collocated robot teleoperation with augmented reality. In: Proceedings of the 2018 ACM/IEEE International conference on human–robot interaction. ACM, pp 78–86</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Hu C, Meng MQ, Liu PX, Wang X (2003) Visual gesture recognition for human-machine interface of robot teleopera" /><p class="c-article-references__text" id="ref-CR23">Hu C, Meng MQ, Liu PX, Wang X (2003) Visual gesture recognition for human-machine interface of robot teleoperation. In: Proceedings 2003 IEEE/RSJ international conference on intelligent robots and systems (IROS 2003) (Cat. No. 03CH37453). IEEE, pp 1560–1565</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kapadia AD, Walker ID, Tatlicioglu E (2012) Teleoperation control of a redundant continuum manipulator using a" /><p class="c-article-references__text" id="ref-CR24">Kapadia AD, Walker ID, Tatlicioglu E (2012) Teleoperation control of a redundant continuum manipulator using a non-redundant rigid-link master. In: IEEE/RSJ international conference on intelligent robots and systems. IEEE, pp 3105–3110</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Koizumi S, Kanda T, Shiomi M, Ishiguro H, Hagita N (2006) Preliminary field trial for teleoperated communicati" /><p class="c-article-references__text" id="ref-CR25">Koizumi S, Kanda T, Shiomi M, Ishiguro H, Hagita N (2006) Preliminary field trial for teleoperated communication robots. In: The 15th IEEE international symposium on robot and human interactive communication. IEEE, pp 145–150</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="T. Kot, P. Novák, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Kot T, Novák P (2018) Application of virtual reality in teleoperation of the military mobile robotic system TA" /><p class="c-article-references__text" id="ref-CR26">Kot T, Novák P (2018) Application of virtual reality in teleoperation of the military mobile robotic system TAROS. Int J Adv Rob Syst 15:1729881417751545</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Application%20of%20virtual%20reality%20in%20teleoperation%20of%20the%20military%20mobile%20robotic%20system%20TAROS&amp;journal=Int%20J%20Adv%20Rob%20Syst&amp;volume=15&amp;publication_year=2018&amp;author=Kot%2CT&amp;author=Nov%C3%A1k%2CP">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Kuno Y, Nakanishi S, Murashima T, Shimada N, Shirai Y (1999) Robotic wheelchair with three control modes. In: " /><p class="c-article-references__text" id="ref-CR27">Kuno Y, Nakanishi S, Murashima T, Shimada N, Shirai Y (1999) Robotic wheelchair with three control modes. In: Proceedings 1999 IEEE international conference on robotics and automation (Cat. No. 99CH36288C). IEEE, pp 2590–2595</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="D. Labonte, P. Boissy, F. Michaud, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Labonte D, Boissy P, Michaud F (2010) Comparative analysis of 3-D robot teleoperation interfaces with novice u" /><p class="c-article-references__text" id="ref-CR28">Labonte D, Boissy P, Michaud F (2010) Comparative analysis of 3-D robot teleoperation interfaces with novice users. IEEE Trans Syst Man Cybern Part B (Cybern) 40:1331–1342</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTSMCB.2009.2038357" aria-label="View reference 28">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Comparative%20analysis%20of%203-D%20robot%20teleoperation%20interfaces%20with%20novice%20users&amp;journal=IEEE%20Trans%20Syst%20Man%20Cybern%20Part%20B%20%28Cybern%29&amp;volume=40&amp;pages=1331-1342&amp;publication_year=2010&amp;author=Labonte%2CD&amp;author=Boissy%2CP&amp;author=Michaud%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Li N, Cartwright S, Shekhar Nittala A, Sharlin E, Costa Sousa M (2015) Flying frustum: a spatial interface for" /><p class="c-article-references__text" id="ref-CR29">Li N, Cartwright S, Shekhar Nittala A, Sharlin E, Costa Sousa M (2015) Flying frustum: a spatial interface for enhancing human-UAV awareness. In: Proceedings of the 3rd international conference on human–agent interaction. ACM, pp 27–31</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JI. Lipton, AJ. Fay, D. Rus, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Lipton JI, Fay AJ, Rus D (2018) Baxter’s homunculus: virtual reality spaces for teleoperation in manufacturing" /><p class="c-article-references__text" id="ref-CR30">Lipton JI, Fay AJ, Rus D (2018) Baxter’s homunculus: virtual reality spaces for teleoperation in manufacturing. IEEE Robot Autom Lett 3:179–186</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FLRA.2017.2737046" aria-label="View reference 30">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 30 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Baxter%E2%80%99s%20homunculus%3A%20virtual%20reality%20spaces%20for%20teleoperation%20in%20manufacturing&amp;journal=IEEE%20Robot%20Autom%20Lett&amp;volume=3&amp;pages=179-186&amp;publication_year=2018&amp;author=Lipton%2CJI&amp;author=Fay%2CAJ&amp;author=Rus%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lu Y, Liu L, Chen S, Huang Q (2010) Voice based control for humanoid teleoperation. In: International conferen" /><p class="c-article-references__text" id="ref-CR31">Lu Y, Liu L, Chen S, Huang Q (2010) Voice based control for humanoid teleoperation. In: International conference on intelligent system design and engineering application. IEEE, pp 814–818</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lueth TC, Laengle T, Herzog G, Stopp E, Rembold U (1994) KANTRA-human–machine interaction for intelligent robo" /><p class="c-article-references__text" id="ref-CR32">Lueth TC, Laengle T, Herzog G, Stopp E, Rembold U (1994) KANTRA-human–machine interaction for intelligent robots using natural language. In: Proceedings of 3rd IEEE international workshop on robot and human communication. IEEE, pp 106–111</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mantecón T, del-Blanco CR, Jaureguizar F, García N (2016) Hand gesture recognition using infrared imagery prov" /><p class="c-article-references__text" id="ref-CR33">Mantecón T, del-Blanco CR, Jaureguizar F, García N (2016) Hand gesture recognition using infrared imagery provided by leap motion controller. In: International conference on advanced concepts for intelligent vision systems. Springer, pp 47–57</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Marques L, Dinis J, Coimbra AP, Crisóstomo MM, Ferreira JP (2009) 3D hyper-redundant robot. Paper presented at" /><p class="c-article-references__text" id="ref-CR34">Marques L, Dinis J, Coimbra AP, Crisóstomo MM, Ferreira JP (2009) 3D hyper-redundant robot. Paper presented at the 11th Spanish Portuguese Conference on Electrical Engineering, Zaragoza, Spain</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Martín-Barrio, A. Barrientos, J. Cerro, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Martín-Barrio A, Barrientos A, del Cerro J (2018a) The Natural-CCD algorithm: a novel method to control hyper-" /><p class="c-article-references__text" id="ref-CR35">Martín-Barrio A, Barrientos A, del Cerro J (2018a) The Natural-CCD algorithm: a novel method to control hyper-redundant and soft robots. Soft Robot 5:242–257</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1089%2Fsoro.2017.0009" aria-label="View reference 35">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 35 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20Natural-CCD%20algorithm%3A%20a%20novel%20method%20to%20control%20hyper-redundant%20and%20soft%20robots&amp;journal=Soft%20Robot&amp;volume=5&amp;pages=242-257&amp;publication_year=2018&amp;author=Mart%C3%ADn-Barrio%2CA&amp;author=Barrientos%2CA&amp;author=Cerro%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Martín-Barrio, S. Terrile, A. Barrientos, J. Cerro, " /><meta itemprop="datePublished" content="2018" /><meta itemprop="headline" content="Martín-Barrio A, Terrile S, Barrientos A, del Cerro J (2018b) Hyper-redundant robots: classification, state-of" /><p class="c-article-references__text" id="ref-CR36">Martín-Barrio A, Terrile S, Barrientos A, del Cerro J (2018b) Hyper-redundant robots: classification, state-of-the-art and issues. Revista Iberoamericana de Automática e Informática Industrial 15:351–362</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4995%2Friai.2018.9207" aria-label="View reference 36">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 36 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Hyper-redundant%20robots%3A%20classification%2C%20state-of-the-art%20and%20issues&amp;journal=Revista%20Iberoamericana%20de%20Autom%C3%A1tica%20e%20Inform%C3%A1tica%20Industrial&amp;volume=15&amp;pages=351-362&amp;publication_year=2018&amp;author=Mart%C3%ADn-Barrio%2CA&amp;author=Terrile%2CS&amp;author=Barrientos%2CA&amp;author=Cerro%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Martín-Barrio A, Terrile S, Díaz-Carrasco M, Del Cerro J, Barrientos A (2019) Modeling the soft robot Kyma bas" /><p class="c-article-references__text" id="ref-CR37">Martín-Barrio A, Terrile S, Díaz-Carrasco M, Del Cerro J, Barrientos A (2019) Modeling the soft robot Kyma based on Real-Time Finite Element Method (under review)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mashood A, Noura H, Jawhar I, Mohamed N (2015) A gesture based kinect for quadrotor control. In: International" /><p class="c-article-references__text" id="ref-CR38">Mashood A, Noura H, Jawhar I, Mohamed N (2015) A gesture based kinect for quadrotor control. In: International conference on information and communication technology research (ICTRC). IEEE, pp 298–301</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Matuszek C, Herbst E, Zettlemoyer L, Fox D (2013) Learning to parse natural language commands to a robot contr" /><p class="c-article-references__text" id="ref-CR39">Matuszek C, Herbst E, Zettlemoyer L, Fox D (2013) Learning to parse natural language commands to a robot control system. In: Experimental robotics. Springer, pp 403–415</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="G. Michalos, P. Karagiannis, S. Makris, Ö. Tokçalar, G. Chryssolouris, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Michalos G, Karagiannis P, Makris S, Tokçalar Ö, Chryssolouris G (2016) Augmented reality (AR) applications fo" /><p class="c-article-references__text" id="ref-CR40">Michalos G, Karagiannis P, Makris S, Tokçalar Ö, Chryssolouris G (2016) Augmented reality (AR) applications for supporting human-robot interactive cooperation. Procedia CIRP 41:370–375</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.procir.2015.12.005" aria-label="View reference 40">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 40 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Augmented%20reality%20%28AR%29%20applications%20for%20supporting%20human-robot%20interactive%20cooperation&amp;journal=Procedia%20CIRP&amp;volume=41&amp;pages=370-375&amp;publication_year=2016&amp;author=Michalos%2CG&amp;author=Karagiannis%2CP&amp;author=Makris%2CS&amp;author=Tok%C3%A7alar%2C%C3%96&amp;author=Chryssolouris%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Mostefa M, El Boudadi LK, Loukil A, Mohamed K, Amine D (2015) Design of mobile robot teleoperation system base" /><p class="c-article-references__text" id="ref-CR41">Mostefa M, El Boudadi LK, Loukil A, Mohamed K, Amine D (2015) Design of mobile robot teleoperation system based on virtual reality. In: 3rd international conference on control, engineering &amp; information technology (CEIT). IEEE, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="CW. Nielsen, MA. Goodrich, RW. Ricks, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Nielsen CW, Goodrich MA, Ricks RW (2007) Ecological interfaces for improving mobile robot teleoperation. IEEE " /><p class="c-article-references__text" id="ref-CR42">Nielsen CW, Goodrich MA, Ricks RW (2007) Ecological interfaces for improving mobile robot teleoperation. IEEE Trans Robot 23:927–941</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTRO.2007.907479" aria-label="View reference 42">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 42 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Ecological%20interfaces%20for%20improving%20mobile%20robot%20teleoperation&amp;journal=IEEE%20Trans%20Robot&amp;volume=23&amp;pages=927-941&amp;publication_year=2007&amp;author=Nielsen%2CCW&amp;author=Goodrich%2CMA&amp;author=Ricks%2CRW">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="P. Pessaux, M. Diana, L. Soler, T. Piardi, D. Mutter, J. Marescaux, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Pessaux P, Diana M, Soler L, Piardi T, Mutter D, Marescaux J (2015) Towards cybernetic surgery: robotic and au" /><p class="c-article-references__text" id="ref-CR43">Pessaux P, Diana M, Soler L, Piardi T, Mutter D, Marescaux J (2015) Towards cybernetic surgery: robotic and augmented reality-assisted liver segmentectomy. Langenbeck’s Arch Surg 400:381–385</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs00423-014-1256-9" aria-label="View reference 43">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 43 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20cybernetic%20surgery%3A%20robotic%20and%20augmented%20reality-assisted%20liver%20segmentectomy&amp;journal=Langenbeck%E2%80%99s%20Arch%20Surg&amp;volume=400&amp;pages=381-385&amp;publication_year=2015&amp;author=Pessaux%2CP&amp;author=Diana%2CM&amp;author=Soler%2CL&amp;author=Piardi%2CT&amp;author=Mutter%2CD&amp;author=Marescaux%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Poncela, L. Gallardo-Estrella, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Poncela A, Gallardo-Estrella L (2015) Command-based voice teleoperation of a mobile robot via a human-robot in" /><p class="c-article-references__text" id="ref-CR44">Poncela A, Gallardo-Estrella L (2015) Command-based voice teleoperation of a mobile robot via a human-robot interface. Robotica 33:1–18</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1017%2FS0263574714000010" aria-label="View reference 44">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 44 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Command-based%20voice%20teleoperation%20of%20a%20mobile%20robot%20via%20a%20human-robot%20interface&amp;journal=Robotica&amp;volume=33&amp;pages=1-18&amp;publication_year=2015&amp;author=Poncela%2CA&amp;author=Gallardo-Estrella%2CL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Raskin, " /><meta itemprop="datePublished" content="2000" /><meta itemprop="headline" content="Raskin J (2000) The humane interface: new directions for designing interactive systems. Addison-Wesley Profess" /><p class="c-article-references__text" id="ref-CR45">Raskin J (2000) The humane interface: new directions for designing interactive systems. Addison-Wesley Professional, Boston</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 45 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20humane%20interface%3A%20new%20directions%20for%20designing%20interactive%20systems&amp;publication_year=2000&amp;author=Raskin%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Ren L, Omisore OM, Han S, Wang L (2017) A master-slave control system with workspaces isomerism for teleoperat" /><p class="c-article-references__text" id="ref-CR46">Ren L, Omisore OM, Han S, Wang L (2017) A master-slave control system with workspaces isomerism for teleoperation of a snake robot. In: 39th annual international conference of the IEEE engineering in medicine and biology society (EMBC). IEEE, pp 4343–4346</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="H. Rheingold, " /><meta itemprop="datePublished" content="1991" /><meta itemprop="headline" content="Rheingold H (1991) Virtual reality: exploring the brave new technologies of artificial experience and interact" /><p class="c-article-references__text" id="ref-CR47">Rheingold H (1991) Virtual reality: exploring the brave new technologies of artificial experience and interactive worlds-from cyberspace to teledildonics. Secker &amp; Warburg, Portland</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 47 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Virtual%20reality%3A%20exploring%20the%20brave%20new%20technologies%20of%20artificial%20experience%20and%20interactive%20worlds-from%20cyberspace%20to%20teledildonics&amp;publication_year=1991&amp;author=Rheingold%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="H. Rininsland, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Rininsland H (1999) ARTEMIS. A telemanipulator for cardiac surgery. Eur J Cardiothorac Surg 16:S106–S111" /><p class="c-article-references__text" id="ref-CR48">Rininsland H (1999) ARTEMIS. A telemanipulator for cardiac surgery. Eur J Cardiothorac Surg 16:S106–S111</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 48 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=ARTEMIS.%20A%20telemanipulator%20for%20cardiac%20surgery&amp;journal=Eur%20J%20Cardiothorac%20Surg&amp;volume=16&amp;pages=S106-S111&amp;publication_year=1999&amp;author=Rininsland%2CH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="J. Roldán, E. Peña-Tapia, A. Martín-Barrio, M. Olivares-Méndez, J. Cerro, A. Barrientos, " /><meta itemprop="datePublished" content="2017" /><meta itemprop="headline" content="Roldán J, Peña-Tapia E, Martín-Barrio A, Olivares-Méndez M, Del Cerro J, Barrientos A (2017) Multi-robot inter" /><p class="c-article-references__text" id="ref-CR49">Roldán J, Peña-Tapia E, Martín-Barrio A, Olivares-Méndez M, Del Cerro J, Barrientos A (2017) Multi-robot interfaces and operator situational awareness: study of the impact of immersion and prediction. Sensors 17:1720</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.3390%2Fs17081720" aria-label="View reference 49">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 49 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Multi-robot%20interfaces%20and%20operator%20situational%20awareness%3A%20study%20of%20the%20impact%20of%20immersion%20and%20prediction&amp;journal=Sensors&amp;volume=17&amp;publication_year=2017&amp;author=Rold%C3%A1n%2CJ&amp;author=Pe%C3%B1a-Tapia%2CE&amp;author=Mart%C3%ADn-Barrio%2CA&amp;author=Olivares-M%C3%A9ndez%2CM&amp;author=Cerro%2CJ&amp;author=Barrientos%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JJ. Roldán, E. Crespo, A. Martín Barrio, E. Peña-Tapia, A. Barrientos, " /><meta itemprop="datePublished" content="2019" /><meta itemprop="headline" content="Roldán JJ, Crespo E, Martín Barrio A, Peña-Tapia E, Barrientos A (2019a) A training system for Industry 4.0 op" /><p class="c-article-references__text" id="ref-CR50">Roldán JJ, Crespo E, Martín Barrio A, Peña-Tapia E, Barrientos A (2019a) A training system for Industry 4.0 operators in complex assemblies based on virtual reality and process mining. Robot Comput Integr Manuf 59:305–316</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.rcim.2019.05.004" aria-label="View reference 50">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 50 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20training%20system%20for%20Industry%204.0%20operators%20in%20complex%20assemblies%20based%20on%20virtual%20reality%20and%20process%20mining&amp;journal=Robot%20Comput%20Integr%20Manuf&amp;volume=59&amp;pages=305-316&amp;publication_year=2019&amp;author=Rold%C3%A1n%2CJJ&amp;author=Crespo%2CE&amp;author=Mart%C3%ADn%20Barrio%2CA&amp;author=Pe%C3%B1a-Tapia%2CE&amp;author=Barrientos%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Roldán JJ, Peña-Tapia E, Garzón-Ramos D, de León J, Garzón M, del Cerro J, Barrientos A (2019b) Multi-robot sy" /><p class="c-article-references__text" id="ref-CR51">Roldán JJ, Peña-Tapia E, Garzón-Ramos D, de León J, Garzón M, del Cerro J, Barrientos A (2019b) Multi-robot systems, virtual reality and ROS: developing a new generation of operator interfaces. In: Robot Operating System (ROS). Springer, pp 29-64</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="C. Sayers, " /><meta itemprop="datePublished" content="1999" /><meta itemprop="headline" content="Sayers C (1999) Remote control robotics. Springer, New York" /><p class="c-article-references__text" id="ref-CR52">Sayers C (1999) Remote control robotics. Springer, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 52 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Remote%20control%20robotics&amp;publication_year=1999&amp;author=Sayers%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shilling RD, Shinn-Cunningham B (2002) Virtual auditory displays. Handbook of Virtual Environment Technology, " /><p class="c-article-references__text" id="ref-CR53">Shilling RD, Shinn-Cunningham B (2002) Virtual auditory displays. Handbook of Virtual Environment Technology, pp 65–92</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Shirwalkar S, Singh A, Sharma K, Singh N (2013) Telemanipulation of an industrial robotic arm using gesture re" /><p class="c-article-references__text" id="ref-CR54">Shirwalkar S, Singh A, Sharma K, Singh N (2013) Telemanipulation of an industrial robotic arm using gesture recognition with Kinect. In: International conference on control, automation, robotics and embedded systems (CARE). IEEE, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Stern H, Wachs J, Edan Y (2007) A method for selection of optimal hand gesture vocabularies. In: International" /><p class="c-article-references__text" id="ref-CR55">Stern H, Wachs J, Edan Y (2007) A method for selection of optimal hand gesture vocabularies. In: International Gesture Workshop. Springer, pp 57–68</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="J. Vertut, " /><meta itemprop="datePublished" content="2013" /><meta itemprop="headline" content="Vertut J (2013) Teleoperation and robotics: applications and technology, vol 3. Springer, Berlin" /><p class="c-article-references__text" id="ref-CR56">Vertut J (2013) Teleoperation and robotics: applications and technology, vol 3. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 56 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Teleoperation%20and%20robotics%3A%20applications%20and%20technology&amp;publication_year=2013&amp;author=Vertut%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Yoshizaki M, Kuno Y, Nakamura A (2001) Human–robot interface based on the mutual assistance between speech and" /><p class="c-article-references__text" id="ref-CR57">Yoshizaki M, Kuno Y, Nakamura A (2001) Human–robot interface based on the mutual assistance between speech and vision. In: Proceedings of the Workshop on Perceptive user Interfaces. ACM, pp 1–4</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="SJ. Young, " /><meta itemprop="datePublished" content="1982" /><meta itemprop="headline" content="Young SJ (1982) Real time languages: design and development. Ellis Horwood Ltd, Chichester" /><p class="c-article-references__text" id="ref-CR58">Young SJ (1982) Real time languages: design and development. Ellis Horwood Ltd, Chichester</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 58 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Real%20time%20languages%3A%20design%20and%20development&amp;publication_year=1982&amp;author=Young%2CSJ">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00414-9-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>The present work is the result of research activities carried out at the Centre for Automation and Robotics, CAR (CSIC-UPM), within the Robotics and Cybernetics research group (RobCib). Supported by the RoboCity2030-DIH-CM project (RoboCity2030 - Madrid Robotics Digital Innovation Hub, P2018/NMT-4331), the UPM Program project VJIDOCUPM18JCG, and by the project PRIC (Proteccion Robotizada de Infraestructuras Criticas, DPI2014-56985-R), funded by the Ministry of Economy and Competitiveness (Government of Spain). Also, our special thanks to all the participants that made possible the experimentation of this work.</p></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Centre for Automation and Robotics (CAR), CSIC-UPM, Universidad Politécnica de Madrid, c/ José Gutiérrez Abascal, No. 2, 28006, Madrid, Spain</p><p class="c-article-author-affiliation__authors-list">Andrés Martín-Barrio, Juan Jesús Roldán, Silvia Terrile, Jaime del Cerro &amp; Antonio Barrientos</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Andr_s-Mart_n_Barrio"><span class="c-article-authors-search__title u-h3 js-search-name">Andrés Martín-Barrio</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Andr%C3%A9s+Mart%C3%ADn-Barrio&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Andr%C3%A9s+Mart%C3%ADn-Barrio" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Andr%C3%A9s+Mart%C3%ADn-Barrio%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Juan_Jes_s-Rold_n"><span class="c-article-authors-search__title u-h3 js-search-name">Juan Jesús Roldán</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Juan Jes%C3%BAs+Rold%C3%A1n&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Juan Jes%C3%BAs+Rold%C3%A1n" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Juan Jes%C3%BAs+Rold%C3%A1n%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Silvia-Terrile"><span class="c-article-authors-search__title u-h3 js-search-name">Silvia Terrile</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Silvia+Terrile&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Silvia+Terrile" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Silvia+Terrile%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Jaime-Cerro"><span class="c-article-authors-search__title u-h3 js-search-name">Jaime del Cerro</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Jaime+del+Cerro&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jaime+del+Cerro" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jaime+del+Cerro%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Antonio-Barrientos"><span class="c-article-authors-search__title u-h3 js-search-name">Antonio Barrientos</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Antonio+Barrientos&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Antonio+Barrientos" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Antonio+Barrientos%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00414-9/email/correspondent/c1/new">Andrés Martín-Barrio</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="Sec18"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec18-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.

</p><div id="MOESM1"><div class="video" id="mijsvdiv1350014"><script src="https://www.edge-cdn.net/videojs_1350014?jsdiv=mijsvdiv1350014&amp;playerskin=37016" defer="defer"></script></div><div class="serif suppress-bottom-margin add-top-margin standard-space-below" data-test="bottom-caption"><p>Supplementary material 1 (MP4 259799 kb)</p></div></div></div></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Application%20of%20immersive%20technologies%20and%20natural%20language%20to%20hyper-redundant%20robot%20teleoperation&amp;author=Andr%C3%A9s%20Mart%C3%ADn-Barrio%20et%20al&amp;contentID=10.1007%2Fs10055-019-00414-9&amp;publication=1359-4338&amp;publicationDate=2019-12-05&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00414-9" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00414-9" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Martín-Barrio, A., Roldán, J.J., Terrile, S. <i>et al.</i> Application of immersive technologies and natural language to hyper-redundant robot teleoperation.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00414-9</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00414-9.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-04-23">23 April 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-11-25">25 November 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-05">05 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00414-9" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00414-9</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">Virtual reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Augmented reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Mixed reality</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Natural language</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Hyper-redundant robot</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Soft robot</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Teleoperation</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00414-9.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=414;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

