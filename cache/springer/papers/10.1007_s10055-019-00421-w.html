<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="access" content="Yes">

    

    <meta name="twitter:card" content="summary"/>

    <meta name="twitter:title" content="A model for nonverbal interaction cues in collaborative virtual enviro"/>

    <meta name="twitter:site" content="@SpringerLink"/>

    <meta name="twitter:description" content="People&#8217;s social interaction is a complex process that involves our verbal and nonverbal behavior. In collaborative virtual environments (CVEs) technologies, interaction is achieved employing..."/>

    <meta name="twitter:image:alt" content="Content cover image"/>

    <meta name="journal_id" content="10055"/>

    <meta name="dc.title" content="A model for nonverbal interaction cues in collaborative virtual environments"/>

    <meta name="dc.source" content="Virtual Reality 2019"/>

    <meta name="dc.format" content="text/html"/>

    <meta name="dc.publisher" content="Springer"/>

    <meta name="dc.date" content="2019-12-21"/>

    <meta name="dc.type" content="OriginalPaper"/>

    <meta name="dc.language" content="En"/>

    <meta name="dc.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="dc.description" content="People&#8217;s social interaction is a complex process that involves our verbal and nonverbal behavior. In collaborative virtual environments (CVEs) technologies, interaction is achieved employing a graphical representation, namely avatar. In this primarily visual media, interactions are constrained by the available possibilities linked to technical and design issues. The focus of this work is the automatic analysis of collaborative interaction among actors in CVEs, based on basic action units of study. For that, a designed domain ontology to establish the basis for the definition and automation of nonverbal interaction cues is presented. Also, for the analysis of collaboration, a structure to infer higher indicators through individual interaction cues was developed. Finally, a case of study applying this proposal is presented."/>

    <meta name="prism.issn" content="1434-9957"/>

    <meta name="prism.publicationName" content="Virtual Reality"/>

    <meta name="prism.publicationDate" content="2019-12-21"/>

    <meta name="prism.section" content="OriginalPaper"/>

    <meta name="prism.startingPage" content="1"/>

    <meta name="prism.endingPage" content="14"/>

    <meta name="prism.copyright" content="2019 Springer-Verlag London Ltd., part of Springer Nature"/>

    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>

    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10055-019-00421-w"/>

    <meta name="prism.doi" content="doi:10.1007/s10055-019-00421-w"/>

    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10055-019-00421-w.pdf"/>

    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10055-019-00421-w"/>

    <meta name="citation_journal_title" content="Virtual Reality"/>

    <meta name="citation_journal_abbrev" content="Virtual Reality"/>

    <meta name="citation_publisher" content="Springer London"/>

    <meta name="citation_issn" content="1434-9957"/>

    <meta name="citation_title" content="A model for nonverbal interaction cues in collaborative virtual environments"/>

    <meta name="citation_online_date" content="2019/12/21"/>

    <meta name="citation_firstpage" content="1"/>

    <meta name="citation_lastpage" content="14"/>

    <meta name="citation_article_type" content="Original Article"/>

    <meta name="citation_language" content="en"/>

    <meta name="dc.identifier" content="doi:10.1007/s10055-019-00421-w"/>

    <meta name="DOI" content="10.1007/s10055-019-00421-w"/>

    <meta name="citation_doi" content="10.1007/s10055-019-00421-w"/>

    <meta name="description" content="People&#8217;s social interaction is a complex process that involves our verbal and nonverbal behavior. In collaborative virtual environments (CVEs) techno"/>

    <meta name="dc.creator" content="Adriana Pe&#241;a P&#233;rez Negr&#243;n"/>

    <meta name="dc.creator" content="Edrisi Mu&#241;oz"/>

    <meta name="dc.creator" content="Graciela Lara L&#243;pez"/>

    <meta name="dc.subject" content="Computer Graphics"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Artificial Intelligence"/>

    <meta name="dc.subject" content="Image Processing and Computer Vision"/>

    <meta name="dc.subject" content="User Interfaces and Human Computer Interaction"/>

    <meta name="citation_reference" content="citation_journal_title=Virtual Real; citation_title=Intermodal audio&#8211;haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks; citation_author=M Ammi, BF Katz; citation_volume=19; citation_issue=3&#8211;4; citation_publication_date=2015; citation_pages=235-252; citation_doi=10.1007/s10055-015-0273-5; citation_id=CR1"/>

    <meta name="citation_reference" content="Aylett R, Krenn B, Pelachaud C, Shimodaira H (eds.) (2013) Intelligent virtual agents. In: Proceedings of the 13th international conference on intelligent virtual agents, IVA&#39;13 (Lecture notes on computer science), vol 8108. Springer Verlag"/>

    <meta name="citation_reference" content="Brdiczka O, Maisonnasse J, Reignier P (2005) Automatic detection of interaction groups. In: ICMI, Trento, Italy"/>

    <meta name="citation_reference" content="Breazeal C, Kidd CD, Thomaz AL, Hoffman G, Berlin M (2005) Effects of nonverbal communication on efficiency and robustness in human&#8211;robot teamwork. In: IEEE/RSJ international conference on intelligent robots and systems (IROS), pp 383&#8211;388"/>

    <meta name="citation_reference" content="Capin TK, Pandzic IS, Thalmann NM, Thalmann D (1997) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: From desktop to webtop: virtual environments on the internet, WWW and networks, international conference, Bradford, UK"/>

    <meta name="citation_reference" content="citation_journal_title=Int J e-Collab; citation_title=Towards an automated model to evaluate collaboration through non-verbal interaction in collaborative virtual environments; citation_author=L Casillas, A Pe&#241;a, A Guti&#233;rrez; citation_volume=12; citation_issue=4; citation_publication_date=2016; citation_pages=7-23; citation_doi=10.4018/IJeC.2016100102; citation_id=CR6"/>

    <meta name="citation_reference" content="Cerrato L, Skhiri M (2003) Analysis and measurement of head movements signalling feedback in face-to-face human dialogues. In: First nordic symposium on multimodal communication, pp 43&#8211;52"/>

    <meta name="citation_reference" content="citation_journal_title=Adv Exp Soc Psychol; citation_title=Dimensions of group process: amount and structure of vocal interaction; citation_author=JM Dabbs, RB Ruback; citation_volume=20; citation_publication_date=1987; citation_pages=123-165; citation_id=CR8"/>

    <meta name="citation_reference" content="citation_title=Origins and elements of virtual environments; citation_publication_date=1995; citation_id=CR9; citation_author=S Ellis; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="citation_journal_title=Lect Notes Comput Sci; citation_title=An event-based architecture to manage virtual human non-verbal communication in 3D chatting environment; citation_author=S Gobron, J Ahn, D Garcia, Q Silvestre, D Thalmann, R Boulic; citation_volume=7378; citation_publication_date=2012; citation_pages=58-68; citation_doi=10.1007/978-3-642-31567-1_6; citation_id=CR10"/>

    <meta name="citation_reference" content="citation_journal_title=Curr Anthropol; citation_title=Proxemics; citation_author=E Hall; citation_volume=9; citation_publication_date=1968; citation_pages=83-108; citation_doi=10.1086/200975; citation_id=CR11"/>

    <meta name="citation_reference" content="citation_title=The Influence of Users&#8217; Personality on the Perception of Intelligent Virtual Agents&#8217; Personality and the Trust Within a Collaborative Context; citation_inbook_title=Communications in Computer and Information Science; citation_publication_date=2015; citation_pages=31-47; citation_id=CR12; citation_author=Nader Hanna; citation_author=Deborah Richards; citation_publisher=Springer International Publishing"/>

    <meta name="citation_reference" content="citation_journal_title=Procedia Comput Sci; citation_title=Estimating collaborative attitudes based on non-verbal features in collaborative learning interaction; citation_author=Y Hayashi, H Morita, YI Nakano; citation_volume=35; citation_publication_date=2014; citation_pages=986-993; citation_doi=10.1016/j.procs.2014.08.184; citation_id=CR13"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Supported Coop Work; citation_title=Unpacking collaboration: The interactional organisation of trading in a city dealing room; citation_author=C Heath, M Jirotka, P Luff, J Hindmarsh; citation_volume=3; citation_issue=2; citation_publication_date=1995; citation_pages=147-165; citation_doi=10.1007/BF00773445; citation_id=CR14"/>

    <meta name="citation_reference" content="Honold F, Sch&#252;ssel F, Panayotova K, Weber M (2012) The nonverbal toolkit: towards a framework for automatic integration of nonverbal communication into virtual environments. In: 2012 8th international conference on intelligent environments (IE), IEEE, pp 243&#8211;250"/>

    <meta name="citation_reference" content="Jofr&#233; LN, Rodr&#237;guez GB, Alvarado YM, Fern&#225;ndez JM, Guerrero RA (2016) Non-verbal communication interface using a data glove. In: IEEE CACIDI 2016-IEEE conference on computer sciences, IEEE, pp 1&#8211;6"/>

    <meta name="citation_reference" content="citation_title=Emotion, affect and personality in speech: the bias of language and paralanguage; citation_publication_date=2015; citation_id=CR17; citation_author=S Johar; citation_publisher=Springer"/>

    <meta name="citation_reference" content="Jovanov E, Hanish N, Courson V, Stidham J, Stinson H, Webb C, Denny K (2009) Avatar&#8212;a multi-sensory system for real time body position monitoring. In: Annual international conference of the IEEE engineering in medicine and biology society, 2009. EMBC 2009, pp 2462&#8211;2465"/>

    <meta name="citation_reference" content="citation_title=Vocal expression of affect; citation_inbook_title=The new handbook of methods in nonverbal behavior research; citation_publication_date=2005; citation_pages=65-135; citation_id=CR19; citation_author=PN Juslin; citation_author=KR Scherer; citation_author=J Harrigan; citation_author=R Rosenthal; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="citation_title=Nonverbal communication in human interaction; citation_publication_date=2010; citation_id=CR20; citation_author=M Knapp; citation_author=J Hall; citation_publisher=Thomson Wadsworth"/>

    <meta name="citation_reference" content="Lange B, Rizzo A, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC), Las Vegas, NV, pp 1&#8211;8"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Graphics Proc ACM SIGGRAPH; citation_title=Moving objects in space: exploiting proprioception in virtual-environment interaction; citation_author=M Mine, FP Brook, CH Sequin; citation_volume=1997; citation_publication_date=1997; citation_pages=19-26; citation_id=CR22"/>

    <meta name="citation_reference" content="Munoz E, Espu&#241;a A, Puigjaner L (2010) Towards an ontological infrastructure for chemical batch process management. In: Computers &amp; Chemical Engineering, 34(5), 668&#8211;682, Selected paper of symposium ESCAPE 19, Krakow, Poland, June 14&#8211;17, 2009"/>

    <meta name="citation_reference" content="citation_journal_title=Image Vis Comput; citation_title=Visual recognition of pointing gestures for human&#8211;robot interaction; citation_author=K Nickel, R Stiefelhagen; citation_volume=25; citation_issue=12; citation_publication_date=2007; citation_pages=1875-1884; citation_doi=10.1016/j.imavis.2005.12.020; citation_id=CR24"/>

    <meta name="citation_reference" content="citation_journal_title=Comput Sci Inf Technol; citation_title=A collaboration facilitator model for learning virtual environments; citation_author=A Pe&#241;a; citation_volume=2; citation_issue=2; citation_publication_date=2014; citation_pages=100-107; citation_id=CR25"/>

    <meta name="citation_reference" content="citation_journal_title=J Multimodal User Interfaces; citation_title=Nonverbal interaction contextualized in collaborative virtual environments; citation_author=A Pe&#241;a, N Rangel, G Lara; citation_volume=9; citation_issue=3; citation_publication_date=2015; citation_pages=253-260; citation_doi=10.1007/s12193-015-0193-4; citation_id=CR26"/>

    <meta name="citation_reference" content="citation_title=Being there together: social interaction in shared virtual environments; citation_publication_date=2010; citation_id=CR27; citation_author=R Schroeder; citation_publisher=Oxford University Press"/>

    <meta name="citation_reference" content="citation_journal_title=World Acad Sci Eng Technol Int J Comput Electr Autom Control Inf Eng; citation_title=Recognizing an individual, their topic of conversation, and cultural background from 3D body movement; citation_author=GJ Shahrour, MJ Russell; citation_volume=9; citation_issue=1; citation_publication_date=2015; citation_pages=311-316; citation_id=CR28"/>

    <meta name="citation_reference" content="Spante M, Heldal I, Steed A, Axelsson A, Schroeder R (2003) Strangers and friends in networked immersive environments: virtual spaces for future living. In: Home oriented informatics and telematics (HOIT), Irvine, California, USA"/>

    <meta name="citation_reference" content="Wolff R, Roberts D, Steed A, Otto O (2005) A review of tele-collaboration technologies with respect to closely coupled collaboration. In: International Journal of Computer Applications in Technology"/>

    <meta name="citation_reference" content="Wolff R, Roberts D, Murgia A, Murray N, Rae J, Steptoe W, Sharkey P (2008) Communicating eye gaze across a distance without rooting participants to the spot. In: 12th IEEE international symposium on distributed simulation and real-time applications, 2008. DS-RT 2008, Vancouver, British Columbia, Canada"/>

    <meta name="citation_reference" content="citation_journal_title=IEEE Trans Pattern Anal Mach Intell; citation_title=A survey of affect recognition methods: audio, visual, and spontaneous expressions; citation_author=Z Zeng, M Pantic, GI Roisman, TS Huang; citation_volume=31; citation_publication_date=2009; citation_pages=39-58; citation_doi=10.1109/TPAMI.2008.52; citation_id=CR32"/>

    <meta name="citation_author" content="Adriana Pe&#241;a P&#233;rez Negr&#243;n"/>

    <meta name="citation_author_email" content="adriana.pena@cucei.udg.mx"/>

    <meta name="citation_author_institution" content="CUCEI, Universidad de Guadalajara, Guadalajara, Mexico"/>

    <meta name="citation_author" content="Edrisi Mu&#241;oz"/>

    <meta name="citation_author_email" content="emunoz@cimat.mx"/>

    <meta name="citation_author_institution" content="Centro de Investigaci&#243;n en Matem&#225;ticas, A.C., Unidad Zacatecas, Zacatecas, Mexico"/>

    <meta name="citation_author" content="Graciela Lara L&#243;pez"/>

    <meta name="citation_author_email" content="graciela.lara@academicos.udg.mx"/>

    <meta name="citation_author_institution" content="CUCEI, Universidad de Guadalajara, Guadalajara, Mexico"/>

    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s10055-019-00421-w&amp;api_key="/>

    <meta name="format-detection" content="telephone=no"/>


    
        <meta property="og:url" content="https://link.springer.com/article/10.1007/s10055-019-00421-w"/>
        <meta property="og:type" content="article"/>
        <meta property="og:site_name" content="Virtual Reality"/>
        <meta property="og:title" content="A model for nonverbal interaction cues in collaborative virtual environments"/>
        <meta property="og:description" content="People’s social interaction is a complex process that involves our verbal and nonverbal behavior. In collaborative virtual environments (CVEs) technologies, interaction is achieved employing a graphical representation, namely avatar. In this primarily visual media, interactions are constrained by the available possibilities linked to technical and design issues. The focus of this work is the automatic analysis of collaborative interaction among actors in CVEs, based on basic action units of study. For that, a designed domain ontology to establish the basis for the definition and automation of nonverbal interaction cues is presented. Also, for the analysis of collaboration, a structure to infer higher indicators through individual interaction cues was developed. Finally, a case of study applying this proposal is presented."/>
        <meta property="og:image" content="https://media.springernature.com/w110/springer-static/cover/journal/10055.jpg"/>
    

    <title>A model for nonverbal interaction cues in collaborative virtual environments | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    <link rel="stylesheet" href=/oscar-static/app-springerlink/css/core-article-b0cd12fb00.css media="screen">
    <link rel="stylesheet" id="js-mustard" href=/oscar-static/app-springerlink/css/enhanced-article-6aad73fdd0.css media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
    

    
    <script type="text/javascript">
        window.dataLayer = [{"Country":"DK","doi":"10.1007-s10055-019-00421-w","Journal Title":"Virtual Reality","Journal Id":10055,"Keywords":"CVE, Avatars, Nonverbal communication, HCI","kwrd":["CVE","Avatars","Nonverbal_communication","HCI"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":["doNotAutoAssociate"],"Open Access":"N","hasAccess":"Y","bypassPaywall":"N","user":{"license":{"businessPartnerID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"businessPartnerIDString":"1600006921|2000421697|3000092219|3000209025|3000236495"}},"Access Type":"subscription","Bpids":"1600006921, 2000421697, 3000092219, 3000209025, 3000236495","Bpnames":"Copenhagen University Library Royal Danish Library Copenhagen, DEFF Consortium Danish National Library Authority, Det Kongelige Bibliotek The Royal Library, DEFF Danish Agency for Culture, 1236 DEFF LNCS","BPID":["1600006921","2000421697","3000092219","3000209025","3000236495"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10055-019-00421-w","Full HTML":"Y","Subject Codes":["SCI","SCI22013","SCI21000","SCI22021","SCI18067"],"pmc":["I","I22013","I21000","I22021","I18067"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1434-9957","pissn":"1359-4338"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Computer Graphics","2":"Artificial Intelligence","3":"Artificial Intelligence","4":"Image Processing and Computer Vision","5":"User Interfaces and Human Computer Interaction"},"secondarySubjectCodes":{"1":"I22013","2":"I21000","3":"I21000","4":"I22021","5":"I18067"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article","GA Key":"UA-26408784-1","DOI":"10.1007/s10055-019-00421-w","Page":"article","page":{"attributes":{"environment":"live"}}}];
        var event = new CustomEvent('dataLayerCreated');
        document.dispatchEvent(event);
    </script>


    


<script src="/oscar-static/js/jquery-220afd743d.js" id="jquery"></script>

<script>
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>


    <script data-test="onetrust-link" type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>





    
    
        <!-- Google Tag Manager -->
        <script data-test="gtm-head">
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');
        </script>
        <!-- End Google Tag Manager -->
    


    <script class="js-entry">
    (function(w, d) {
        window.config = window.config || {};
        window.config.mustardcut = false;

        var ctmLinkEl = d.getElementById('js-mustard');

        if (ctmLinkEl && w.matchMedia && w.matchMedia(ctmLinkEl.media).matches) {
            window.config.mustardcut = true;

            
            
            
                window.Component = {};
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                { 'src' : '/oscar-static/js/polyfill-es5-bundle-ab77bcf8ba.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es5-bundle-6b407673fa.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/airbrake-es6-bundle-e7bd4589ff.js', 'async': false, 'module': true}
            ];

            var bodyScripts = [
                { 'src' : '/oscar-static/js/app-es5-bundle-867d07d044.js', 'async': false, 'module': false},
                { 'src' : '/oscar-static/js/app-es6-bundle-8ebcb7376d.js', 'async': false, 'module': true}
                
                
                    , { 'src' : '/oscar-static/js/global-article-es5-bundle-12442a199c.js', 'async': false, 'module': false},
                    { 'src' : '/oscar-static/js/global-article-es6-bundle-7ae1776912.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i=0; i<headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i=0; i<bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        }
    })(window, document);
</script>

</head>
<body class="shared-article-renderer">
    
    
    
        <!-- Google Tag Manager (noscript) -->
        <noscript data-test="gtm-body">
            <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
            height="0" width="0" style="display:none;visibility:hidden"></iframe>
        </noscript>
        <!-- End Google Tag Manager (noscript) -->
    


    <div class="u-vh-full">
        
    <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-LB1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="728x90" data-gpt-targeting="pos=LB1;articleid=421;"></div>
        </div>
    </aside>

<div class="u-position-relative">
    <header class="c-header u-mb-24" data-test="publisher-header">
        <div class="c-header__container">
            <div class="c-header__brand">
                
    <a id="logo" class="u-display-block" href="/" title="Go to homepage" data-test="springerlink-logo">
        <picture>
            <source type="image/svg+xml" srcset=/oscar-static/images/springerlink/svg/springerlink-253e23a83d.svg>
            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt="SpringerLink" width="148" height="30" data-test="header-academic">
        </picture>
        
        
    </a>


            </div>
            <div class="c-header__navigation">
                
    
        <button type="button"
                class="c-header__link u-button-reset u-mr-24"
                data-expander
                data-expander-target="#popup-search"
                data-expander-autofocus="firstTabbable"
                data-test="header-search-button">
            <span class="u-display-flex u-align-items-center">
                Search
                <svg class="c-icon u-ml-8" width="22" height="22" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </span>
        </button>
        <nav>
            <ul class="c-header__menu">
                
                <li class="c-header__item">
                    <a data-test="login-link" class="c-header__link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10055-019-00421-w">Log in</a>
                </li>
                

                
            </ul>
        </nav>
    

    



            </div>
        </div>
    </header>

    
        <div id="popup-search" class="c-popup-search u-mb-16 js-header-search u-js-hide">
            <div class="c-popup-search__content">
                <div class="u-container">
                    <div class="c-popup-search__container" data-test="springerlink-popup-search">
                        <div class="app-search">
    <form role="search" method="GET" action="/search" >
        <label for="search" class="app-search__label">Search SpringerLink</label>
        <div class="app-search__content">
            <input id="search" class="app-search__input" data-search-input autocomplete="off" role="textbox" name="query" type="text" value="">
            <button class="app-search__button" type="submit">
                <span class="u-visually-hidden">Search</span>
                <svg class="c-icon" width="14" height="14" aria-hidden="true" focusable="false">
                    <use xlink:href="#global-icon-search"></use>
                </svg>
            </button>
            
                <input type="hidden" name="searchType" value="publisherSearch">
            
            
        </div>
    </form>
</div>

                    </div>
                </div>
            </div>
        </div>
    
</div>

        

    <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
        <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
            
                
                <div class="c-context-bar u-hide" data-component="context-bar" aria-hidden="true">
                    <div class="c-context-bar__container u-container">
                        <div class="c-context-bar__title">
                            A model for nonverbal interaction cues in collaborative virtual environments
                        </div>
                        
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00421-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                    </div>
                </div>
            
            
            
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00421-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

            <article itemscope itemtype="http://schema.org/ScholarlyArticle" lang="en">
                <div class="c-article-header">
                    <header>
                        <ul class="c-article-identifiers" data-test="article-identifier">
                            
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Article</li>
    
    
    

                            <li class="c-article-identifiers__item"><a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2019-12-21" itemprop="datePublished">21 December 2019</time></a></li>
                        </ul>

                        
                        <h1 class="c-article-title" data-test="article-title" data-article-title="" itemprop="name headline">A model for nonverbal interaction cues in collaborative virtual environments</h1>
                        <ul class="c-author-list js-etal-collapsed" data-etal="25" data-etal-small="3" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Adriana-Pe_a_P_rez_Negr_n" data-author-popup="auth-Adriana-Pe_a_P_rez_Negr_n">Adriana Peña Pérez Negrón</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Guadalajara" /><meta itemprop="address" content="grid.412890.6, 0000 0001 2158 0196, CUCEI, Universidad de Guadalajara, Blvd. Marcelino García Barragán #1421, C.P. 44430, Guadalajara, Jalisco, Mexico" /></span></sup>, </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Edrisi-Mu_oz" data-author-popup="auth-Edrisi-Mu_oz">Edrisi Muñoz</a></span><sup class="u-js-hide"><a href="#Aff2">2</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Unidad Zacatecas" /><meta itemprop="address" content="grid.454267.6, Centro de Investigación en Matemáticas, A.C., Unidad Zacatecas, Parque Quantum, Ciudad del Conocimiento. Av. Lassec, Andador Galileo Galilei, Manzana 3, Lote 7, 98160, Zacatecas, Mexico" /></span></sup> &amp; </li><li class="c-author-list__item" itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person"><span itemprop="name"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Graciela-Lara_L_pez" data-author-popup="auth-Graciela-Lara_L_pez" data-corresp-id="c1">Graciela Lara López<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-email"></use></svg></a></span><span class="u-js-hide"> 
            <a class="js-orcid" itemprop="url" href="http://orcid.org/0000-0003-2766-8134"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0003-2766-8134</a></span><sup class="u-js-hide"><a href="#Aff1">1</a><span itemprop="affiliation" itemscope="itemscope" itemtype="http://schema.org/Organization" class="u-visually-hidden"><meta itemprop="name" content="Universidad de Guadalajara" /><meta itemprop="address" content="grid.412890.6, 0000 0001 2158 0196, CUCEI, Universidad de Guadalajara, Blvd. Marcelino García Barragán #1421, C.P. 44430, Guadalajara, Jalisco, Mexico" /></span></sup> </li></ul>
                        <p class="c-article-info-details" data-container-section="info">
                            
    <a data-test="journal-link" href="/journal/10055"><i data-test="journal-title">Virtual Reality</i></a>

                            (<span data-test="article-publication-year">2019</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                        </p>
                        
    

                        <div data-test="article-metrics">
                            <div id="altmetric-container">
    
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">143 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007%2Fs10055-019-00421-w/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    
</div>

                        </div>
                        
                        
                        
                    </header>
                </div>

                <div data-article-body="true" data-track-component="article body" class="c-article-body">
                    <section aria-labelledby="Abs1" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>People’s social interaction is a complex process that involves our verbal and nonverbal behavior. In collaborative virtual environments (CVEs) technologies, interaction is achieved employing a graphical representation, namely avatar. In this primarily visual media, interactions are constrained by the available possibilities linked to technical and design issues. The focus of this work is the automatic analysis of collaborative interaction among actors in CVEs, based on basic action units of study. For that, a designed domain ontology to establish the basis for the definition and automation of nonverbal interaction cues is presented. Also, for the analysis of collaboration, a structure to infer higher indicators through individual interaction cues was developed. Finally, a case of study applying this proposal is presented.</p></div></div></section>
                    
    


                    

                    

                    
                        
                            <section aria-labelledby="Sec1"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>Current technological trends and computational evolution during the last decades have strongly influenced our communication process. In this context, collaborative virtual environments (CVEs), which are computer-generated interfaces to cause users the feeling of being together in an environment different than the one they actually are (Ellis <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Ellis S (1995) Origins and elements of virtual environments. Oxford University Press, New York" href="/article/10.1007/s10055-019-00421-w#ref-CR9" id="ref-link-section-d82258e315">1995</a>; Schroeder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Schroeder R (2010) Being there together: social interaction in shared virtual environments. Oxford University Press, New York" href="/article/10.1007/s10055-019-00421-w#ref-CR27" id="ref-link-section-d82258e318">2010</a>), provide a shared place with common objects for multiple users to collaborate at a distance. CVEs then represent a media that brings remote people into spatial and social proximity, facilitating communication awareness (Wolff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Wolff R, Roberts D, Murgia A, Murray N, Rae J, Steptoe W, Sharkey P (2008) Communicating eye gaze across a distance without rooting participants to the spot. In: 12th IEEE international symposium on distributed simulation and real-time applications, 2008. DS-RT 2008, Vancouver, British Columbia, Canada" href="/article/10.1007/s10055-019-00421-w#ref-CR31" id="ref-link-section-d82258e321">2008</a>).</p><p>Two types of actors can inhabit a CVE: computer users and intelligent virtual agents (IVAs). IVAs are interactive characters that exhibit human-like qualities. They can communicate with humans or other IVAs using natural human modalities, with the purpose to simulate real-time perception, cognition and actions (Aylett et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Aylett R, Krenn B, Pelachaud C, Shimodaira H (eds.) (2013) Intelligent virtual agents. In: Proceedings of the 13th international conference on intelligent virtual agents, IVA'13 (Lecture notes on computer science), vol 8108. Springer Verlag" href="/article/10.1007/s10055-019-00421-w#ref-CR2" id="ref-link-section-d82258e327">2013</a>). Both types of actors are embodied in the virtual scenario through avatars, their graphical representation in the CVE and their means for interacting with the virtual world and with others.</p><p>As in real life, actors’ interaction in CVEs is achieved via verbal and/or nonverbal channels. The IVAs’ display of nonverbal communication represents a broad research area mainly aimed to the design of human-like artificial behavior for both robots and avatars (e.g. Breazeal et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Breazeal C, Kidd CD, Thomaz AL, Hoffman G, Berlin M (2005) Effects of nonverbal communication on efficiency and robustness in human–robot teamwork. In: IEEE/RSJ international conference on intelligent robots and systems (IROS), pp 383–388" href="/article/10.1007/s10055-019-00421-w#ref-CR4" id="ref-link-section-d82258e333">2005</a>, Gobron et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Gobron S, Ahn J, Garcia D, Silvestre Q, Thalmann D, Boulic R (2012) An event-based architecture to manage virtual human non-verbal communication in 3D chatting environment. Lect Notes Comput Sci 7378:58–68. &#xA;https://doi.org/10.1007/978-3-642-31567-1_6&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00421-w#ref-CR10" id="ref-link-section-d82258e336">2012</a>). IVAs nonverbal interaction has also been recreated for collaborative situations.</p><p>In Hanna and Richards (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Hanna N, Richards D (2015) The influence of users’ personality on the perception of intelligent virtual agents’ personality and the trust within a collaborative context. In: International workshop on multiagent foundations of social computing, Springer, Cham, pp 31–47" href="/article/10.1007/s10055-019-00421-w#ref-CR12" id="ref-link-section-d82258e342">2015</a>), authors manage different parameters (e.g. the spatial extent to perform expressions, time to perform an expression, repetition of movements, and close physical postures) to provide IVAs with certain personalities expressed through verbal and nonverbal behavior. According to Jofré et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Jofré LN, Rodríguez GB, Alvarado YM, Fernández JM, Guerrero RA (2016) Non-verbal communication interface using a data glove. In: IEEE CACIDI 2016-IEEE conference on computer sciences, IEEE, pp 1–6" href="/article/10.1007/s10055-019-00421-w#ref-CR16" id="ref-link-section-d82258e345">2016</a>), nonverbal behavior is a primary factor during human–avatar interaction as much as it is in human–human interaction. In their work, they presented a human–computer interaction (HCI) system for virtual reality (VR) based on nonverbal cues. Nevertheless, for CVEs, there are scarce studies about the nonverbal cues that users can display through their avatar, such as deictic gestures or gazes. Despite some isolated effort for the automatic recognition of nonverbal interaction, it reminds as an open issue.</p><p>In real-life conditions, in Shahrour and Russell (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Shahrour GJ, Russell MJ (2015) Recognizing an individual, their topic of conversation, and cultural background from 3D body movement. World Acad Sci Eng Technol Int J Comput Electr Autom Control Inf Eng 9(1):311–316" href="/article/10.1007/s10055-019-00421-w#ref-CR28" id="ref-link-section-d82258e352">2015</a>), body trackers were applied for people’s nonverbal interaction automatic recognition to establish the topic of conversation and the cultural background of the subjects. For a collaborative situation, in Hayashi et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Hayashi Y, Morita H, Nakano YI (2014) Estimating collaborative attitudes based on non-verbal features in collaborative learning interaction. Procedia Comput Sci 35:986–993" href="/article/10.1007/s10055-019-00421-w#ref-CR13" id="ref-link-section-d82258e355">2014</a>), the analysis of collaborative learning was conducted through eye-tracking glasses, microphone audio detection, and a pen device, to collect data of gazes, speech intervals, and task implementation to determine participation and learning attitude.</p><p>For digital environments, the recent interest in the area of affective computing has increased the study of the users’ nonverbal behavior focused on affective states (Zeng et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Zeng Z, Pantic M, Roisman GI, Huang TS (2009) A survey of affect recognition methods: audio, visual, and spontaneous expressions. IEEE Trans Pattern Anal Mach Intell 31:39–58" href="/article/10.1007/s10055-019-00421-w#ref-CR32" id="ref-link-section-d82258e361">2009</a>). Honold et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Honold F, Schüssel F, Panayotova K, Weber M (2012) The nonverbal toolkit: towards a framework for automatic integration of nonverbal communication into virtual environments. In: 2012 8th international conference on intelligent environments (IE), IEEE, pp 243–250" href="/article/10.1007/s10055-019-00421-w#ref-CR15" id="ref-link-section-d82258e364">2012</a>) presented a framework to acquire, analyze, and present nonverbal communication, to classify emotions. Also, particularly for users, an overview of the principal conditions of nonverbal interaction contextualized within CVEs was presented in Peña et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Peña A, Rangel N, Lara G (2015) Nonverbal interaction contextualized in collaborative virtual environments. J Multimodal User Interfaces 9(3):253–260. &#xA;https://doi.org/10.1007/s12193-015-0193-4&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00421-w#ref-CR26" id="ref-link-section-d82258e367">2015</a>).</p><p>Using CVEs, in Peña (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Peña A (2014) A collaboration facilitator model for learning virtual environments. Comput Sci Inf Technol 2(2):100–107" href="/article/10.1007/s10055-019-00421-w#ref-CR25" id="ref-link-section-d82258e373">2014</a>), collaborative learning is diagnosed through nonverbal cues automatic recognition to create a facilitator for collaboration. Another proposal to establish collaboration in CVEs was presented in Casillas et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Casillas L, Peña A, Gutiérrez A (2016) Towards an automated model to evaluate collaboration through non-verbal interaction in collaborative virtual environments. Int J e-Collab 12(4):7–23. &#xA;https://doi.org/10.4018/IJeC.2016100102&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00421-w#ref-CR6" id="ref-link-section-d82258e376">2016</a>), where a multilayer model using fuzzy classification, a rule-based inference was used to somehow assess group collaboration. None of them deals with the automatic collection of nonverbal interaction from a general point of view.</p><p>It is worth highlighting that CVEs present a particular scenario where the display of the users’ nonverbal cues is constrained by the technology that in contrast, facilitates the collaborative process better than other media technologies.</p><p>Understanding and effectively using nonverbal behavior is crucial in every social experience, both in artificial and real life. Verbal and nonverbal channels work together for the communication process, in which nonverbal interaction is used to repeat, conflict, complement, substitute, accent, moderate, and/or regulate verbal communication (Knapp and Hall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" href="/article/10.1007/s10055-019-00421-w#ref-CR20" id="ref-link-section-d82258e385">2010</a>). Nonverbal communication provides a tremendous quantity of informational cues. Furthermore, collaborative work requires to be conscious of the presence of other participants and to understand what they are doing (Ammi and Katz <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Ammi M, Katz BF (2015) Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks. Virtual Real 19(3–4):235–252" href="/article/10.1007/s10055-019-00421-w#ref-CR1" id="ref-link-section-d82258e388">2015</a>), for which communication is a key aspect. Hence, we propose that the comprehension of the collaboration flow can be achieved through the interaction that takes place during such collaborative interaction.</p><p>CVEs resource can service activities such as socializing or gaming. However, the work presented here is centered on nonverbal interaction suitable to achieve collaborative tasks. Also, because CVEs are predominantly visual, they provide elements to center attention on tasks that involve the use of space and objects; otherwise, this technology might not be necessary (Schroeder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Schroeder R (2010) Being there together: social interaction in shared virtual environments. Oxford University Press, New York" href="/article/10.1007/s10055-019-00421-w#ref-CR27" id="ref-link-section-d82258e394">2010</a>; Spante et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Spante M, Heldal I, Steed A, Axelsson A, Schroeder R (2003) Strangers and friends in networked immersive environments: virtual spaces for future living. In: Home oriented informatics and telematics (HOIT), Irvine, California, USA" href="/article/10.1007/s10055-019-00421-w#ref-CR29" id="ref-link-section-d82258e397">2003</a>). Likewise, unlike in real life, the focus in a CVE will be narrowed on a few things and constantly engaged because there is an ongoing reason for being there (Schroeder <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Schroeder R (2010) Being there together: social interaction in shared virtual environments. Oxford University Press, New York" href="/article/10.1007/s10055-019-00421-w#ref-CR27" id="ref-link-section-d82258e400">2010</a>). Keeping these considerations in mind, to establish the actors’ interactions in CVEs, the design of a domain ontology is here presented. The ontological model is composed of a taxonomy and its relations (object and data) for the processes to retrieve nonverbal interaction cues from a CVE. As well, some guidelines for higher indicators of a collaborative session are discussed. Then, this model is applied and presented in a case of study.</p><p>We consider that this domain ontology will provide insights for better understanding of nonverbal interaction in CVEs, and it constitutes the support to automatize its classification. In turn, this will provide the means to make computer analysis during the collaborative session and/or afterward.</p></div></div></section><section aria-labelledby="Sec2"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Nonverbal interaction cues in CVEs</h2><div class="c-article-section__content" id="Sec2-content"><p>The richness of face-to-face verbal and nonverbal interactions is not readily available in CVEs, a condition that requires to be carefully considered to describe this domain.</p><p>As Knapp and Hall (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" href="/article/10.1007/s10055-019-00421-w#ref-CR20" id="ref-link-section-d82258e418">2010</a>) pointed out, defining nonverbal communication as the communication effected by other means than words is generally useful, although not completely precise. Separating verbal and nonverbal behavior is virtually impossible. For example, hand gestures are often classified as nonverbal communication, but sign languages are mostly linguistic, or not all spoken words are clearly or singularly verbal like onomatopoeic. Knapp and Hall (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" href="/article/10.1007/s10055-019-00421-w#ref-CR20" id="ref-link-section-d82258e421">2010</a>) also stated that another way to classify nonverbal behavior is by looking at what people study in this regard. They found that the theory and research associated with nonverbal communication focus on three primary units:</p><ol class="u-list-style-none"><li><span class="u-custom-list-number">1.</span><p><i>The communication environment</i>, those elements that impinge on the human relationship, but that are not directly part of it, constituted by <i>environmental factors</i> such as furniture, lighting conditions or temperature, and <i>Proxemics</i>, the study of the spatial environment (Hall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1968" title="Hall E (1968) Proxemics. Curr Anthropol 9:83–108" href="/article/10.1007/s10055-019-00421-w#ref-CR11" id="ref-link-section-d82258e438">1968</a>).</p></li><li><span class="u-custom-list-number">2.</span><p><i>The communicators’ physical characteristics</i> such as body shape or skin color, which includes artifacts as clothes, hairstyle, or jewelry.</p></li><li><span class="u-custom-list-number">3.</span><p><i>Body movement and position,</i> known as <i>Kinesics</i>, which includes gestures, movements of the limbs, posture, touching behavior, facial expressions, eye, and vocal behavior.</p></li></ol><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10055-019-00421-w#Tab1">1</a> shows the nonverbal interaction units of study that were transposed to their expression in CVEs, see Peña et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Peña A, Rangel N, Lara G (2015) Nonverbal interaction contextualized in collaborative virtual environments. J Multimodal User Interfaces 9(3):253–260. &#xA;https://doi.org/10.1007/s12193-015-0193-4&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00421-w#ref-CR26" id="ref-link-section-d82258e464">2015</a>) for details.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Nonverbal interaction units of study transposed to CVEs (Peña et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Peña A, Rangel N, Lara G (2015) Nonverbal interaction contextualized in collaborative virtual environments. J Multimodal User Interfaces 9(3):253–260. &#xA;https://doi.org/10.1007/s12193-015-0193-4&#xA;&#xA;&#xA;" href="/article/10.1007/s10055-019-00421-w#ref-CR26" id="ref-link-section-d82258e473">2015</a>)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10055-019-00421-w/tables/1"><span>Full size table</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Following the methodology proposed by Munoz et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Munoz E, Espuña A, Puigjaner L (2010) Towards an ontological infrastructure for chemical batch process management. In: Computers &amp; Chemical Engineering, 34(5), 668–682, Selected paper of symposium ESCAPE 19, Krakow, Poland, June 14–17, 2009" href="/article/10.1007/s10055-019-00421-w#ref-CR23" id="ref-link-section-d82258e514">2010</a>), a domain ontology was developed. Such methodology is based on the cycle plan, do, check/study and act (PDSA), which results in an ordered sequence of steps, easy to understand and track for the ontology design. Protégé™ software was used in the design process.</p><p>A first intuitive and broad partition of the interaction cues can be in verbal and nonverbal interaction. Just to illustrate verbal interaction, the utterance was used as a unit. The definition of the rest of the ontology’s taxonomy elements for the interaction cues in CVEs uses the Knapp and Hall (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" href="/article/10.1007/s10055-019-00421-w#ref-CR20" id="ref-link-section-d82258e521">2010</a>) breakdown of nonverbal primary units of study. The nonverbal interaction cues were then grouped in: Kinesics, environmental factors, Paralanguage and Proxemics, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig1">1</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig1_HTML.png?as=webp"></source><img aria-describedby="figure-1-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig1_HTML.png" alt="figure1" loading="lazy" width="685" height="454" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>Main units for interaction cues taxonomy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/1" data-track-dest="link:Figure1 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>This taxonomy is not exhaustive, but it specifically comprises the most common interaction cues for the collaborative process of actors in a CVE. To automatize the cue retrieval, we propose to distinguish its process. Each cue represents a flow of states that changes according to what is going on in the CVE session, which constitutes its process. Each state represents the start, peak, and final time of a cue, and they can be retrieved from the log files of a CVE session. In the next sections are discussed such processes. The selected cues suggested are those that were considered as suitable for automation retrieval.</p><h3 class="c-article__sub-heading" id="Sec3">Kinesics cues</h3><p>For Kinesics, it is necessary to make a clear distinction of the CVE actor, that is, IVA or user. An IVA can display an effective number of nonverbal cues since they are reproduced by animations. On the other hand, a constraint on the number and spontaneity of the nonverbal cues is found as a result of the avatar control performed by the user (Capin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Capin TK, Pandzic IS, Thalmann NM, Thalmann D (1997) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: From desktop to webtop: virtual environments on the internet, WWW and networks, international conference, Bradford, UK" href="/article/10.1007/s10055-019-00421-w#ref-CR5" id="ref-link-section-d82258e544">1997</a>).</p><p>According to Capin et al. (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Capin TK, Pandzic IS, Thalmann NM, Thalmann D (1997) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: From desktop to webtop: virtual environments on the internet, WWW and networks, international conference, Bradford, UK" href="/article/10.1007/s10055-019-00421-w#ref-CR5" id="ref-link-section-d82258e550">1997</a>), a user can control the avatars using three different approaches:</p><ol class="u-list-style-none"><li><span class="u-custom-list-number">1.</span><p><i>Directly controlled</i> through body sensors (Jovanov et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Jovanov E, Hanish N, Courson V, Stidham J, Stinson H, Webb C, Denny K (2009) Avatar—a multi-sensory system for real time body position monitoring. In: Annual international conference of the IEEE engineering in medicine and biology society, 2009. EMBC 2009, pp 2462–2465" href="/article/10.1007/s10055-019-00421-w#ref-CR18" id="ref-link-section-d82258e561">2009</a>; Lange et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Lange B, Rizzo A, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC), Las Vegas, NV, pp 1–8" href="/article/10.1007/s10055-019-00421-w#ref-CR21" id="ref-link-section-d82258e564">2011</a>).</p></li><li><span class="u-custom-list-number">2.</span><p><i>User-guided</i>, when the user guides the avatar defining tasks and movements, usually through a computer input device such as the mouse or the keyboard.</p></li><li><span class="u-custom-list-number">3.</span><p>In a <i>semi-autonomous</i> way, when the avatar has an internal state that depends on its goals and its environment, and the user can modify this state. The semi-autonomous display of nonverbal cue is then achieved by animation.</p></li></ol><p>According to the employed approach to control de avatar, a gesture or a body posture is determined in different ways. A gesture is the communication of a message achieved by moving a body part or parts, but even though it starts with a body part movement, it does not necessarily will engender a gesture. In the directly or user-guided control approaches, the most common body parts of an avatar with movement are one arm and the head (Wolff et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Wolff R, Roberts D, Steed A, Otto O (2005) A review of tele-collaboration technologies with respect to closely coupled collaboration. In: International Journal of Computer Applications in Technology" href="/article/10.1007/s10055-019-00421-w#ref-CR30" id="ref-link-section-d82258e585">2005</a>). Using these cases as example, two states can be established, a simple body movement or a gesture display. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig2">2</a> presents a detailed diagram UML of these states.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig2_HTML.png?as=webp"></source><img aria-describedby="figure-2-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig2_HTML.png" alt="figure2" loading="lazy" width="685" height="358" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>A body part movements</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/2" data-track-dest="link:Figure2 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Each 3D object can be located in the virtual environment through its pivot axes, information that can be retrieved at any moment form the CVE. In the case of a HCI with the environment, a log file can be generated. Also identified from the log files, a body part movement establishes a starting point; this state is kept until a gesture is detected or the movement stops. In the diagram UML (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig2">2</a>), the record of the log file with a timestamp accompanies the messages sent from one state to another <i>(i.e., ^Record.MovementCue Time_Start</i> = <i>TimeLog</i>).</p><p>However, gesture detection is not a straightforward activity; it requires the understanding of the gesture’s distinctive characteristics. As an example, a common arm gesture is a deictic gesture, useful to point something. For this particular case, Nickel and Stiefelhagen (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Nickel K, Stiefelhagen R (2007) Visual recognition of pointing gestures for human–robot interaction. Image Vis Comput 25(12):1875–1884" href="/article/10.1007/s10055-019-00421-w#ref-CR24" id="ref-link-section-d82258e614">2007</a>) determined that the time a person holds a pointing gesture is usually only until attention from others is drawn to the pointed object, around a couple of seconds. Therefore, for automation purposes, the sustained selection of an object, for a couple of seconds can be considered as a pointing gesture. As of head movements, two gestures that can clearly be distinguished. Nodding, to show agreement or comprehension, and headshake, to indicate disagreement or incomprehension. Both are characterized in Cerrato and Skhiri (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Cerrato L, Skhiri M (2003) Analysis and measurement of head movements signalling feedback in face-to-face human dialogues. In: First nordic symposium on multimodal communication, pp 43–52" href="/article/10.1007/s10055-019-00421-w#ref-CR7" id="ref-link-section-d82258e617">2003</a>) using the number and intensity of the movement, which will support automation.</p><p>When the avatar is semi-automatically controlled, cue detection is different. In the log file can be detected the input that triggers the linked animation. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig3">3</a> shows a detailed state diagram UML for this situation. The animation can be either a gesture or a body action like jump or sitting. In this case, the message from one state to another represents the trigger input.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig3_HTML.png?as=webp"></source><img aria-describedby="figure-3-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig3_HTML.png" alt="figure3" loading="lazy" width="685" height="292" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>States of a user’s input that triggers an avatar gesture or body action</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/3" data-track-dest="link:Figure3 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>A body action might lead to a body posture. The user typically achieves body postures through a keys combination. Thus, they can be read directly from the log files and determined the same way as the semi-autonomous controlled actions as can be observed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig3">3</a>.</p><h3 class="c-article__sub-heading" id="Sec4">Environmental factor cues</h3><p>The different objects in the CVE correspond to the communication environment. The objects in it shape the architectural design of the scenario. Hall (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1968" title="Hall E (1968) Proxemics. Curr Anthropol 9:83–108" href="/article/10.1007/s10055-019-00421-w#ref-CR11" id="ref-link-section-d82258e647">1968</a>) differentiated three Proxemics features related to objects:</p><ol class="u-list-style-none"><li><span class="u-custom-list-number">1.</span><p><i>Fixed features</i> the space organized by unmoving boundaries such as a room.</p></li><li><span class="u-custom-list-number">2.</span><p><i>Semi-fixed features</i> movable objects that can change the space organization.</p></li><li><span class="u-custom-list-number">3.</span><p><i>Dynamics</i> movable objects.</p></li></ol><p>While the fixed features cannot be modified during the session, the modification of semi-fixed features and dynamics in the workspace can take a role during collaborative interaction, especially in a CVE object-task-oriented session.</p><p>In the virtual world, for the actors to interact with an object, they have to select it first, which denotes the action of pointing or grabbing that object. After selecting the object, it can be either deselected or manipulated. The manipulation of an object represents its state modification, generally by moving or rotating it (Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Mine M, Brook FP, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. Comput Graphics Proc ACM SIGGRAPH 1997:19–26" href="/article/10.1007/s10055-019-00421-w#ref-CR22" id="ref-link-section-d82258e678">1997</a>). The manipulation of the objects process is presented in a states diagram UML in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig4">4</a>. The interaction starts with the selection of the object; if the object is not deselected, then it can be moved, rotated, or transformed in other ways such as resized or colored. The object can go from one transformation state to another as long as it remains selected. The manipulation ends when the actor deselects the object.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig4_HTML.png?as=webp"></source><img aria-describedby="figure-4-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig4_HTML.png" alt="figure4" loading="lazy" width="685" height="417" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Object manipulation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/4" data-track-dest="link:Figure4 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec5">Proxemics cues</h3><p>Proxemics studies, territory and personal space are represented by the actors’ navigation in the CVE. It is important to highlight that in virtual environments (VEs), there are no certain physical restrictions; for example, navigation can be performed by flying or using techniques such as tele-transportation.</p><p>As shown in the diagram UML in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>, navigation starts once the avatar is moved from its location. An avatar’s change in position will be reflected with a new <i>X</i>, <i>Y</i> and/or <i>Z</i> axes position in the CVE. If the avatar goes in an ascending direction (e.g. <i>Y</i> ax), air navigation is established, connector {1} in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>. If the avatar is moved in the ground (e.g. <i>X</i> or <i>Z</i> axes), land navigation is established, connector {2} in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>. Either ground or ascending movements can have a direction change, which is represented in the reorientation connector {3} in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>. Next the connectors {1}, {2} and {3} of the diagram are explained.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig5_HTML.png?as=webp"></source><img aria-describedby="figure-5-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig5_HTML.png" alt="figure5" loading="lazy" width="685" height="622" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>Navigation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/5" data-track-dest="link:Figure5 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Air navigation process, connector {1} from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>, is detailed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig6">6</a>. When the avatar disrupts the ascending movement, it might descend, stay in the air floating, or fly and make interchanges from those states. When the avatar descends to the ground, the air navigation ends, connector {4} that returns to a new “last location” in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/6" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig6_HTML.png?as=webp"></source><img aria-describedby="figure-6-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig6_HTML.png" alt="figure6" loading="lazy" width="685" height="481" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Air navigation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/6" data-track-dest="link:Figure6 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>For land navigation, connector {2} in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>, the starting point is a standing position. From the standing position, the avatar can move or move faster than regular, which is understood as running, and make interchanges from those states. The land navigation ends with a switch to air navigation or when the session ends, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig7">7</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig7_HTML.png?as=webp"></source><img aria-describedby="figure-7-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig7_HTML.png" alt="figure7" loading="lazy" width="685" height="655" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Land navigation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/7" data-track-dest="link:Figure7 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Also, for both air and land navigation, the avatar can be reoriented by changing its facing direction, either to the right or to the left, as depicted in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig8">8</a>, and then return to navigation, connector {4} in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig8_HTML.png?as=webp"></source><img aria-describedby="figure-8-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig8_HTML.png" alt="figure8" loading="lazy" width="685" height="513" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>Reorientation during the navigation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/8" data-track-dest="link:Figure8 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Other types of navigation such as fall, jump, land, or tele-transportation are usually achieved through a combination of keys and animation. In this case, a semi-autonomous control approach following the states presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig3">3</a> can be applied, but in this case ending with an avatar’s new location.</p><h3 class="c-article__sub-heading" id="Sec6">Paralanguage cues</h3><p>Hardly related to verbal communication is Paralanguage, described as the physical mechanisms to produce nonverbal vocal qualities and sounds (Juslin et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Juslin PN, Scherer KR, Harrigan J, Rosenthal R (2005) Vocal expression of affect. In: Harrigan J, Rosenthal R, Scherer KR (eds) The new handbook of methods in nonverbal behavior research. Oxford University Press, Oxford, UK, pp 65–135" href="/article/10.1007/s10055-019-00421-w#ref-CR19" id="ref-link-section-d82258e806">2005</a>). A vocal expression can contain several nonverbal messages such as emotion and intention. Paralanguage includes pitch, rhythm, tempo, articulation, and resonance of the voice, and vocalizations such as laughing, crying, sighing, swallowing, clearing of the throat, or snoring, among others (Knapp and Hall <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" href="/article/10.1007/s10055-019-00421-w#ref-CR20" id="ref-link-section-d82258e809">2010</a>). Several techniques have been developed for their study and comprehension, though its automatic comprehension remains as a challenge (Johar <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Johar S (2015) Emotion, affect and personality in speech: the bias of language and paralanguage. Springer, Berlin" href="/article/10.1007/s10055-019-00421-w#ref-CR17" id="ref-link-section-d82258e812">2015</a>).</p><p>An easy characteristic of the human voice to extract in a computer system is whether the actor is vocalizing or not (making a pause), the <i>Vocalization_pause</i> cue in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig1">1</a>. From this cue, other indicators can be extracted such as frequency and duration of the speech, which are useful tools for the analysis of group interaction (e.g. Brdiczka et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Brdiczka O, Maisonnasse J, Reignier P (2005) Automatic detection of interaction groups. In: ICMI, Trento, Italy" href="/article/10.1007/s10055-019-00421-w#ref-CR3" id="ref-link-section-d82258e824">2005</a>; Dabbs and Ruback <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc Psychol 20:123–165" href="/article/10.1007/s10055-019-00421-w#ref-CR8" id="ref-link-section-d82258e827">1987</a>).</p><p>From this cue, higher-level indicators such as the speech time rate can be obtained, that is, how much an actor speaks in relation to the others in the environment. Also, because we speak to someone in a dialogue interchange, to better understand interaction cues, the talking-turn structure can be obtained from the individual <i>Vocalization_pause</i> cue. Furthermore, a distinction between individual and group cues might lead to higher-level indicators of interaction, a possibility explored in the next section.</p><p>It is worth to mention that the display of a nonverbal cue might not involve other actors in the CVE; these actions are not interaction. However, there is not a straightforward way to distinguish such situations, which complicates its automatic detection. For example, when people collaborate, they might make a statement directed to no one in particular, not requiring an answer (Heath et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1995" title="Heath C, Jirotka M, Luff P, Hindmarsh J (1995) Unpacking collaboration: The interactional organisation of trading in a city dealing room. Comput Supported Coop Work 3(2):147–165" href="/article/10.1007/s10055-019-00421-w#ref-CR14" id="ref-link-section-d82258e839">1995</a>); however, that statement can influence others and therefore it represents an interaction.</p><p>The representation of talking-turns within a group of persons was presented in Dabbs and Ruback (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc Psychol 20:123–165" href="/article/10.1007/s10055-019-00421-w#ref-CR8" id="ref-link-section-d82258e846">1987</a>) to understand the amount and structure of a group conversation. Based on this approach, we create a detailed state diagram UML, first for a dyad conversation, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig9">9</a>. In a dyad conversation, a person vocalizing alone (taking the floor) can make a pause and then maintain the floor. Or, the pause can cause a conversation partner to take the floor, causing a switching pause. We added to Dabbs and Ruback (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc Psychol 20:123–165" href="/article/10.1007/s10055-019-00421-w#ref-CR8" id="ref-link-section-d82258e852">1987</a>) approach, a relation from the pause state to the switching pause state. These situations are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig9">9</a> through three states: vocalization, pause and switching pause. In order to distinguish a pause from a small gap from one vocalization to another, for automatic speech recognition, the end of an utterance is usually determined by a pause in the range from 500 to 2000 ms (Mine et al. <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Mine M, Brook FP, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. Comput Graphics Proc ACM SIGGRAPH 1997:19–26" href="/article/10.1007/s10055-019-00421-w#ref-CR22" id="ref-link-section-d82258e858">1997</a>), and then, a two-second silence can be functional to automatically determine the end of a talking-turn.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig9_HTML.png?as=webp"></source><img aria-describedby="figure-9-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig9_HTML.png" alt="figure9" loading="lazy" width="685" height="553" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>A dyad conversation process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/9" data-track-dest="link:Figure9 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Dabbs and Ruback (<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc Psychol 20:123–165" href="/article/10.1007/s10055-019-00421-w#ref-CR8" id="ref-link-section-d82258e872">1987</a>) main contribution was the identification of group vocalization, with their respective group pause and group switching pause. From a person vocalization pause, several persons might vocalize at the same time generating a group vocalization. The group, same as a person, can make a pause that leads to a group pause, or a group switching pause which means that a distinct group of persons is now vocalizing. In this case, we considered that the group vocalization could occur either during a pause or while a person is vocalizing.</p><p>During group vocalization, a group pause can originate three situations: (1) a different members’ group vocalization; (2) the person with the floor after the group vocalization takes it again; or (3) a different person takes the floor. These situations are extended from the dyad flow and presented in the states diagram UML in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig10">10</a>, with three group states: group vocalization, group pause, or group switching pause.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig10_HTML.png?as=webp"></source><img aria-describedby="figure-10-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig10_HTML.png" alt="figure10" loading="lazy" width="685" height="901" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>Group vocalization process states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/10" data-track-dest="link:Figure10 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>In the same way, a group state should give us a better understanding of the collaborative interaction process, in the cases of navigation, implementation (object manipulation), and body postures such as sitting. Some of these situations will be included in the case of study.</p><h3 class="c-article__sub-heading" id="Sec7">Taxonomy including selected cues for automation</h3><p>Following the discussed processes and considering these cues as suitable for automation, the main units of nonverbal interaction cues (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig1">1</a>) can be broken down in the taxonomy as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig11">11</a>. Because the actor’s interaction triggers the taxonomy, the root name of the domain is <i>Actor_Interaction</i>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig11_HTML.png?as=webp"></source><img aria-describedby="figure-11-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig11_HTML.png" alt="figure11" loading="lazy" width="685" height="1296" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Automatic interaction cues retrieval taxonomy</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/11" data-track-dest="link:Figure11 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The <i>Actor_Interactin</i> can be of two classes, either verbal or nonverbal. In the case of verbal interaction, just as an example, the class was named <i>Verbal_Communication_Utterance</i>. For the nonverbal interaction, the class is identified as <i>Nonverbal_Interaction_Cues</i> broken into four subclasses: (1) <i>Environmental_factors,</i> (2) <i>Kinesics_cues,</i> (3) <i>Paralanguage_cues</i> and (4)<i> Proxemics_cues</i>.</p><p>Within the <i>Environmental_factors</i> subclass in the <i>Object_cues</i> sub-subclass were included the <i>Dynamics_objects</i> and <i>Semi-fixed_objects</i>, those represent objects that can be manipulated within the environment.</p><p>The included body movements in the <i>Kinesics_cues</i> subclass were the <i>Face_Expressions</i> in case they are automatically generated, and <i>Gestures</i> sub-subclass composed of <i>Hand_Movements</i> and <i>Head_Movements</i>, because they are the most common avatars’ body parts that have independent users’ direct movements.</p><p>The second sub-subclass in <i>Kinesics_cues</i> is <i>Body_Positions</i>, and here, four body positions were included: (1) <i>Crouch_Position,</i> (2) <i>Sit_Position,</i> (3) <i>Squat_Position</i> and (4) <i>Stand_Position</i> for stand up avatars, all as an example of the semi-autonomous controlled cues (see Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00421-w#Sec3">2.1</a>).</p><p>In the <i>Paralanguage_cues</i> sub-subclass, only <i>Vocalization_pause</i> was included because of the automation challenge recognition of paralanguages’ cues. And for this sub-subclass, <i>Speech_Time_Rate</i> is merely suggested, as the rate of time of the user speech.</p><p>Finally, for the <i>Proxemics_cues</i>, <i>Navigation_cues</i> derived from actors’ navigation (detailed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00421-w#Sec5">2.3</a>) were included: <i>Ascending, Descending, Fall, Fly, Jump, Land, Run, Teleportation, Turn_left, Turn_right,</i> and <i>Walk</i>.</p></div></div></section><section aria-labelledby="Sec8"><div class="c-article-section" id="Sec8-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec8">Taxonomy application</h2><div class="c-article-section__content" id="Sec8-content"><p>This case study aims to retrieve the processes states to get individual and group nonverbal cues to classify them within the taxonomy. Thus, a session in a CVE to collaborate in an object-oriented task was implemented as follows:</p><p><i>Participants</i> Three undergraduate male students from a computer science school, aged 20, 20 and 25, voluntarily participated in the study.</p><p><i>Materials, devices and experimental situation</i> The session took place in a room where three Internet-connected Dell™ computers model Alienware X51 displayed the desktop CVE. Microphones, earphones, and the TeamSpeak™ application were used for communication. The participants’ manipulation of objects, the avatars’ navigation and movements, and talking time were automatically registered in a text file in each cycle of the application.</p><p>With the OpenSim™ software and the CtrlAltStudio™ viewer applications, a tridimensional (3D) CVE was generated. The session was videotaped using the Fraps™ application, saving the screens as the users see them; all participants’ screens were videotaped.</p><p><i>Design and procedure</i> The task consisted of the assembly of several pieces from a geometric figure like the one shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig12">12</a>. During the sessions, the participants could watch the same model formed by plastic pieces.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig12_HTML.jpg?as=webp"></source><img aria-describedby="figure-12-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig12_HTML.jpg" alt="figure12" loading="lazy" width="685" height="587" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>Geometric figure</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/12" data-track-dest="link:Figure12 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The scenario is an island in which several different color pieces were placed around a rectangular white plane, as can be observed in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig13">13</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig13_HTML.jpg?as=webp"></source><img aria-describedby="figure-13-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig13_HTML.jpg" alt="figure13" loading="lazy" width="685" height="344" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>The scenario for the task session</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/13" data-track-dest="link:Figure13 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The students had a first-person avatar; it means that they could not see themselves in the virtual world. The avatar corresponds to the participant’s gender and the user’s name is placed at the top of its avatar, to facilitate identification and communication.</p><p>The users’ interaction with the CVE was using keys combinations and the mouse. The participants could make land navigation, by walking or running, or air navigation by flying, floating, ascending and descending; and they could select, move or rotate the dynamic objects. Objects could be manipulated from the distance when they were at sight, and the users’ name in the top of the objects helps to know who is handling it, as can be seen in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig14">14</a>.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig14_HTML.jpg?as=webp"></source><img aria-describedby="figure-14-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig14_HTML.jpg" alt="figure14" loading="lazy" width="685" height="399" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>The task session</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/14" data-track-dest="link:Figure14 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>The participants were exposed to a 5-min trial session to familiarize themselves with the environment. Afterward, they started the trial session to perform a specific task. A piece of paper containing instructions of the keys and mouse combinations was placed in each participant’s desk, as additional help. The plastic figure to be assembled was also placed on each desk. There was no time limit to perform the collaborative task.</p><p>The instructions given to them were: “ <i>By working together, assemble a figure like this (the figure in plastic was then shown to them). Use the microphone with the earphones for communication. Not all the pieces are required. Please assemble the figure on the white plane. Let us know when you finish</i>.”</p><p><i>Data</i> The session lasted 6:44.918 min. The software application saves in text files the <i>X</i>, <i>Y</i>, and <i>Z</i> coordinates of the objects and avatars positions, when the pieces or the avatars are moved or rotated, and when the microphone is deactivated or activated by the voice of a user. Data were treated first by an application based on the ontological model by the use of OWL API™, a Java API™ for creating, manipulating, and serializing OWL ontologies.</p><p>The text log file is formed by the application as follows: First, a timestamp is taken from the system, for each cycle of the application; the timestamp has hour, minute, second and ten thousandth second. Then, the user ID is placed in the log, and then, an ID for the type of action performed by the user (e.g. ON to start the session, CAM for walking, V for flying, A for grabbing an object, GD for turn to the right, MO for object manipulation, and so on; the initials are for the Spanish word(s) of the action). Then are incorporated in the text row the <i>X</i>, <i>Y</i> and <i>Z</i> axes placed between the less than symbol ‘ &lt; ’ and the greater than symbol ‘ &gt; ’. The manipulated objects in the VR environment were sequentially numbered, so in the text log file this number is placed at the end of the row for its identification. Next is presented an example of some rows of the text log file:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-a"><figure><div class="c-article-section__figure-content" id="Figa"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figa_HTML.png?as=webp"></source><img aria-describedby="figure-a-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figa_HTML.png" alt="figurea" loading="lazy" width="685" height="671" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-a-desc"></div></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Results</h3><p>Although the log files from the CVE have a classification of the nonverbal cues, they were treated following the processes described in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10055-019-00421-w#Sec2">2</a>, as mentioned using the OWL API™ and a Java API™ with the aim of establishing its generalization for CVEs.</p><p>In this particular CVE, the main nonverbal communication cues available are: <i>Vocalization_pause</i> from the subclass <i>Paralanguage_Cues, Object_cue</i> from <i>Environmental_factors</i>, and different types of <i>Navigation_cue</i> from <i>Proxemics_cues</i> subclass on the taxonomy. Two files are created by the application: one with the classification of each nonverbal interaction cue and another one with a report by interaction.</p><p>For navigation, a series of boolean functions were programed to verify the type of navigation with different parameters such as previous position or actual position of the <i>Z</i> axis, and previous and current position in grades to identify turns. For example, the next code is to characterize <i>Walk</i> (land navigation, see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>):</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-b"><figure><div class="c-article-section__figure-content" id="Figb"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figb_HTML.png?as=webp"></source><img aria-describedby="figure-b-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figb_HTML.png" alt="figureb" loading="lazy" width="685" height="167" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-b-desc"></div></div></figure></div><p>In the code, the walking condition is true when the <i>Z</i> axis of the avatar is equal to any surface and the speed does not correspond to a threshold that represents the running type of navigation. Another example is in the next code; this time to identify a reorientation to the left (see Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig5">5</a>), which corresponds to the <i>Turn_left</i> subclass of the <i>Navigation_cues</i> subclass.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-c"><figure><div class="c-article-section__figure-content" id="Figc"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figc_HTML.png?as=webp"></source><img aria-describedby="figure-c-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figc_HTML.png" alt="figurec" loading="lazy" width="685" height="262" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-c-desc"></div></div></figure></div><p>In this case, in the code is made a comparison with the <i>Z</i> axis angle to establish the turn, which gives the true result. All the navigation states were properly identified by the application. A segment of the output file of the application for the taxonomy classification is next presented. The timestamp is conserved. Then, the cue classification by name is placed, along with the <i>X</i>, <i>Y</i> and <i>Z</i> axes values.</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-d"><figure><div class="c-article-section__figure-content" id="Figd"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figd_HTML.png?as=webp"></source><img aria-describedby="figure-d-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Figd_HTML.png" alt="figured" loading="lazy" width="685" height="248" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-d-desc"></div></div></figure></div><p>Also, a segment of the interaction activity output report of the application, which groups the starting and ending position of each cue, is next transcribed:</p><div class="c-article-section__figure c-article-section__figure--no-border" data-test="figure" data-container-section="figure" id="figure-e"><figure><div class="c-article-section__figure-content" id="Fige"><div class="c-article-section__figure-item"><div class="c-article-section__figure-content"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fige_HTML.png?as=webp"></source><img aria-describedby="figure-e-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fige_HTML.png" alt="figuree" loading="lazy" width="685" height="350" /></picture></div></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-e-desc"></div></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small c-article__sub-heading--light" id="Sec10">Group analysis</h4><p>The automatic identification of nonverbal interaction cues can be applied to understand the individual performance of an actor (intrasubject) within a CVE, for example, to observe how long was the actor taking care of the task in contrast with the navigation. Another comparison can be made among actors, like who have greater interaction with others. Finally, cues can be observed at the group level to analyze groups.</p><p>The three cues retrieved from the environment (<i>Vocalization_pause, Object_cue</i> and <i>Navigation_cue</i>) were treated to understand group cues and their intersection during the session. In the next examples, only minute 5 was analyzed.</p><p><i>Vocalization and pauses</i> From the log files, a list with the starting and ending vocalization time of the participants was classified. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig15">15</a> shows a graph of minute 5 of the session; it shows the change in the states. In the rectangles are, in the upper left corner, the state (in this case vocalization and group vocalization), and in the upper right corner, the person or persons that contributed to that state (<i>P</i>1, <i>P</i>2 and/or <i>P</i>3). In the next row in the rectangle are the starting and ending time in minutes of the state in seconds and milliseconds. And at the bottom of the rectangle is the elapsed time in that state. Pauses conducted to a change in the state. In this particular minute, participants started to talk around the middle of it. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig15_HTML.png?as=webp"></source><img aria-describedby="figure-15-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig15_HTML.png" alt="figure15" loading="lazy" width="685" height="109" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>Vocalization states for the group</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/15" data-track-dest="link:Figure15 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p><i>Navigation</i> Navigation states in the session can be followed as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig16">16</a>. This example is for the three participants (<i>P</i>1, <i>P</i>2 and <i>P</i>3) and from the first half of the minute 5. In the upper left corner of the rectangles are the states as follows: LR for left rotation, RR for right rotation, F for flying. Same as in vocalization, the next rows in the rectangle show the starting and ending time, and at the bottom of the rectangle, the total time in that state. It can be observed, for example, that <i>P</i>3 was static for 23 s at the beginning of the minute 5. </p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig16_HTML.png?as=webp"></source><img aria-describedby="figure-16-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig16_HTML.png" alt="figure16" loading="lazy" width="685" height="249" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Navigation states of the three participants</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/16" data-track-dest="link:Figure16 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p><i>Object manipulation</i> As well, object manipulation can be followed as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig17">17</a>. In this example, the manipulation of only one object is presented. The types of states in the upper left corner of the rectangles are G for grabbing or selecting the object, MO when the object is moved, and RO when the object is rotated, and D for dropping or deselecting the object. On the upper right corner are first the person that manipulated the object (i.e. <i>P</i>2) and then a number used to identify the objects in the scenario, in this case, object number 14 (i.e. O_14). In the following rows of the text in the rectangle are the starting and ending times of the state, and at the bottom of the rectangle is the elapsed time in that state. Object manipulation can be significant to distinguish when all the group members are working at the same time, as an implementation stage.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig17_HTML.png?as=webp"></source><img aria-describedby="figure-17-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig17_HTML.png" alt="figure17" loading="lazy" width="685" height="60" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>Environmental factors states</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/17" data-track-dest="link:Figure17 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Finally, a graphic with the overlapping activities in that 5 minute is presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig18">18</a>. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10055-019-00421-w#Fig18">18</a> is the time in percentage for each type of cues and when two or three of them occur simultaneously. As can be observed, object manipulation in minute 5 was constant; at least one of the participants was at any time moving an object. Vocalization was always accompanied by another type of interaction (i.e. manipulation or navigation), although only 1.05% of the time the three types of interaction occurred simultaneously.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig18_HTML.png?as=webp"></source><img aria-describedby="figure-18-desc" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10055-019-00421-w/MediaObjects/10055_2019_421_Fig18_HTML.png" alt="figure18" loading="lazy" width="685" height="476" /></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>Navigation states for the group</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10055-019-00421-w/figures/18" data-track-dest="link:Figure18 Full size image" rel="nofollow"><span>Full size image</span><svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-chevron-right"></use></svg></a></div></figure></div><p>Group activities are not a part of the taxonomy; however, logical agents can be added in the ontology to make this kind of inferences.</p></div></div></section><section aria-labelledby="Sec11"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Discussions and future work</h2><div class="c-article-section__content" id="Sec11-content"><p>We propose that the comprehension of collaboration flow can be achieved through the interaction that takes place during collaborative task achievement. Nonverbal interaction during the accomplishment of an object-oriented task is the main activity performed while collaborating. In CVEs, these nonverbal cues have particular characteristics due to the users’ avatar constrained body movements, and therefore they need to be distinguished and treated in consequence.</p><p>The processes to extract nonverbal cues from a CVE, as units of study, were specified for a taxonomy model to narrow the domain. Each process was detailed in state diagrams UML, to determine when the cue starts, how long it lasts, and when it ends. Those states form the interaction cues.</p><p>Some of those processes were implemented in an application and applied to a case study focused on a collaborative assembling task, to classify the available cues and prove the model feasibility. Even more, the taxonomy from the ontological model is considered as the backbone for automatic retrieval of nonverbal interaction in CVEs.</p><p>Besides, higher-level indicators can be inferred from the model cues, similar to those presented in the Results section. This process highlights group cues to define group behavior based on the interaction during the accomplishment of the task.</p><p>For future work, the automatic comprehension of collaborative interaction in virtual environments will be implemented. We plan to integrate the ontology to intelligent agents with the aim to get the automation of collaboration definition types, such as division of labor or hierarchical collaboration. Another inference planned to be implemented are collaboration phases, such as planning, implementing, reviewing or control. This process analysis can include individual characteristics such as leadership or their influence on certain stages of the collaboration session. Along with learning or training proposes, this model can be applied for psychometric tests.</p></div></div></section>
                        
                    

                    <section aria-labelledby="Bib1"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references"><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Ammi, BF. Katz, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Ammi M, Katz BF (2015) Intermodal audio–haptic intermodal display: improvement of communication and interperso" /><p class="c-article-references__text" id="ref-CR1">Ammi M, Katz BF (2015) Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks. Virtual Real 19(3–4):235–252</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs10055-015-0273-5" aria-label="View reference 1">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 1 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Intermodal%20audio%E2%80%93haptic%20intermodal%20display%3A%20improvement%20of%20communication%20and%20interpersonal%20awareness%20for%20collaborative%20search%20tasks&amp;journal=Virtual%20Real&amp;volume=19&amp;issue=3%E2%80%934&amp;pages=235-252&amp;publication_year=2015&amp;author=Ammi%2CM&amp;author=Katz%2CBF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Aylett R, Krenn B, Pelachaud C, Shimodaira H (eds.) (2013) Intelligent virtual agents. In: Proceedings of the " /><p class="c-article-references__text" id="ref-CR2">Aylett R, Krenn B, Pelachaud C, Shimodaira H (eds.) (2013) Intelligent virtual agents. In: Proceedings of the 13th international conference on intelligent virtual agents, IVA'13 (Lecture notes on computer science), vol 8108. Springer Verlag</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Brdiczka O, Maisonnasse J, Reignier P (2005) Automatic detection of interaction groups. In: ICMI, Trento, Ital" /><p class="c-article-references__text" id="ref-CR3">Brdiczka O, Maisonnasse J, Reignier P (2005) Automatic detection of interaction groups. In: ICMI, Trento, Italy</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Breazeal C, Kidd CD, Thomaz AL, Hoffman G, Berlin M (2005) Effects of nonverbal communication on efficiency an" /><p class="c-article-references__text" id="ref-CR4">Breazeal C, Kidd CD, Thomaz AL, Hoffman G, Berlin M (2005) Effects of nonverbal communication on efficiency and robustness in human–robot teamwork. In: IEEE/RSJ international conference on intelligent robots and systems (IROS), pp 383–388</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Capin TK, Pandzic IS, Thalmann NM, Thalmann D (1997) Realistic avatars and autonomous virtual humans in VLNET " /><p class="c-article-references__text" id="ref-CR5">Capin TK, Pandzic IS, Thalmann NM, Thalmann D (1997) Realistic avatars and autonomous virtual humans in VLNET networked virtual environments. In: From desktop to webtop: virtual environments on the internet, WWW and networks, international conference, Bradford, UK</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="L. Casillas, A. Peña, A. Gutiérrez, " /><meta itemprop="datePublished" content="2016" /><meta itemprop="headline" content="Casillas L, Peña A, Gutiérrez A (2016) Towards an automated model to evaluate collaboration through non-verbal" /><p class="c-article-references__text" id="ref-CR6">Casillas L, Peña A, Gutiérrez A (2016) Towards an automated model to evaluate collaboration through non-verbal interaction in collaborative virtual environments. Int J e-Collab 12(4):7–23. <a href="https://doi.org/10.4018/IJeC.2016100102">https://doi.org/10.4018/IJeC.2016100102</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.4018%2FIJeC.2016100102" aria-label="View reference 6">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 6 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Towards%20an%20automated%20model%20to%20evaluate%20collaboration%20through%20non-verbal%20interaction%20in%20collaborative%20virtual%20environments&amp;journal=Int%20J%20e-Collab&amp;doi=10.4018%2FIJeC.2016100102&amp;volume=12&amp;issue=4&amp;pages=7-23&amp;publication_year=2016&amp;author=Casillas%2CL&amp;author=Pe%C3%B1a%2CA&amp;author=Guti%C3%A9rrez%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Cerrato L, Skhiri M (2003) Analysis and measurement of head movements signalling feedback in face-to-face huma" /><p class="c-article-references__text" id="ref-CR7">Cerrato L, Skhiri M (2003) Analysis and measurement of head movements signalling feedback in face-to-face human dialogues. In: First nordic symposium on multimodal communication, pp 43–52</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="JM. Dabbs, RB. Ruback, " /><meta itemprop="datePublished" content="1987" /><meta itemprop="headline" content="Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc" /><p class="c-article-references__text" id="ref-CR8">Dabbs JM, Ruback RB (1987) Dimensions of group process: amount and structure of vocal interaction. Adv Exp Soc Psychol 20:123–165</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 8 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Dimensions%20of%20group%20process%3A%20amount%20and%20structure%20of%20vocal%20interaction&amp;journal=Adv%20Exp%20Soc%20Psychol&amp;volume=20&amp;pages=123-165&amp;publication_year=1987&amp;author=Dabbs%2CJM&amp;author=Ruback%2CRB">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Ellis, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Ellis S (1995) Origins and elements of virtual environments. Oxford University Press, New York" /><p class="c-article-references__text" id="ref-CR9">Ellis S (1995) Origins and elements of virtual environments. Oxford University Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 9 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Origins%20and%20elements%20of%20virtual%20environments&amp;publication_year=1995&amp;author=Ellis%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="S. Gobron, J. Ahn, D. Garcia, Q. Silvestre, D. Thalmann, R. Boulic, " /><meta itemprop="datePublished" content="2012" /><meta itemprop="headline" content="Gobron S, Ahn J, Garcia D, Silvestre Q, Thalmann D, Boulic R (2012) An event-based architecture to manage virt" /><p class="c-article-references__text" id="ref-CR10">Gobron S, Ahn J, Garcia D, Silvestre Q, Thalmann D, Boulic R (2012) An event-based architecture to manage virtual human non-verbal communication in 3D chatting environment. Lect Notes Comput Sci 7378:58–68. <a href="https://doi.org/10.1007/978-3-642-31567-1_6">https://doi.org/10.1007/978-3-642-31567-1_6</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2F978-3-642-31567-1_6" aria-label="View reference 10">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 10 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=An%20event-based%20architecture%20to%20manage%20virtual%20human%20non-verbal%20communication%20in%203D%20chatting%20environment&amp;journal=Lect%20Notes%20Comput%20Sci&amp;doi=10.1007%2F978-3-642-31567-1_6&amp;volume=7378&amp;pages=58-68&amp;publication_year=2012&amp;author=Gobron%2CS&amp;author=Ahn%2CJ&amp;author=Garcia%2CD&amp;author=Silvestre%2CQ&amp;author=Thalmann%2CD&amp;author=Boulic%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="E. Hall, " /><meta itemprop="datePublished" content="1968" /><meta itemprop="headline" content="Hall E (1968) Proxemics. Curr Anthropol 9:83–108" /><p class="c-article-references__text" id="ref-CR11">Hall E (1968) Proxemics. Curr Anthropol 9:83–108</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1086%2F200975" aria-label="View reference 11">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 11 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Proxemics&amp;journal=Curr%20Anthropol&amp;volume=9&amp;pages=83-108&amp;publication_year=1968&amp;author=Hall%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="Nader. Hanna, Deborah. Richards, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Hanna N, Richards D (2015) The influence of users’ personality on the perception of intelligent virtual agents" /><p class="c-article-references__text" id="ref-CR12">Hanna N, Richards D (2015) The influence of users’ personality on the perception of intelligent virtual agents’ personality and the trust within a collaborative context. In: International workshop on multiagent foundations of social computing, Springer, Cham, pp 31–47</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 12 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Communications%20in%20Computer%20and%20Information%20Science&amp;pages=31-47&amp;publication_year=2015&amp;author=Hanna%2CNader&amp;author=Richards%2CDeborah">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Y. Hayashi, H. Morita, YI. Nakano, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Hayashi Y, Morita H, Nakano YI (2014) Estimating collaborative attitudes based on non-verbal features in colla" /><p class="c-article-references__text" id="ref-CR13">Hayashi Y, Morita H, Nakano YI (2014) Estimating collaborative attitudes based on non-verbal features in collaborative learning interaction. Procedia Comput Sci 35:986–993</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.procs.2014.08.184" aria-label="View reference 13">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 13 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Estimating%20collaborative%20attitudes%20based%20on%20non-verbal%20features%20in%20collaborative%20learning%20interaction&amp;journal=Procedia%20Comput%20Sci&amp;volume=35&amp;pages=986-993&amp;publication_year=2014&amp;author=Hayashi%2CY&amp;author=Morita%2CH&amp;author=Nakano%2CYI">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="C. Heath, M. Jirotka, P. Luff, J. Hindmarsh, " /><meta itemprop="datePublished" content="1995" /><meta itemprop="headline" content="Heath C, Jirotka M, Luff P, Hindmarsh J (1995) Unpacking collaboration: The interactional organisation of trad" /><p class="c-article-references__text" id="ref-CR14">Heath C, Jirotka M, Luff P, Hindmarsh J (1995) Unpacking collaboration: The interactional organisation of trading in a city dealing room. Comput Supported Coop Work 3(2):147–165</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2FBF00773445" aria-label="View reference 14">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 14 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Unpacking%20collaboration%3A%20The%20interactional%20organisation%20of%20trading%20in%20a%20city%20dealing%20room&amp;journal=Comput%20Supported%20Coop%20Work&amp;volume=3&amp;issue=2&amp;pages=147-165&amp;publication_year=1995&amp;author=Heath%2CC&amp;author=Jirotka%2CM&amp;author=Luff%2CP&amp;author=Hindmarsh%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Honold F, Schüssel F, Panayotova K, Weber M (2012) The nonverbal toolkit: towards a framework for automatic in" /><p class="c-article-references__text" id="ref-CR15">Honold F, Schüssel F, Panayotova K, Weber M (2012) The nonverbal toolkit: towards a framework for automatic integration of nonverbal communication into virtual environments. In: 2012 8th international conference on intelligent environments (IE), IEEE, pp 243–250</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jofré LN, Rodríguez GB, Alvarado YM, Fernández JM, Guerrero RA (2016) Non-verbal communication interface using" /><p class="c-article-references__text" id="ref-CR16">Jofré LN, Rodríguez GB, Alvarado YM, Fernández JM, Guerrero RA (2016) Non-verbal communication interface using a data glove. In: IEEE CACIDI 2016-IEEE conference on computer sciences, IEEE, pp 1–6</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="S. Johar, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Johar S (2015) Emotion, affect and personality in speech: the bias of language and paralanguage. Springer, Ber" /><p class="c-article-references__text" id="ref-CR17">Johar S (2015) Emotion, affect and personality in speech: the bias of language and paralanguage. Springer, Berlin</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 17 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Emotion%2C%20affect%20and%20personality%20in%20speech%3A%20the%20bias%20of%20language%20and%20paralanguage&amp;publication_year=2015&amp;author=Johar%2CS">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Jovanov E, Hanish N, Courson V, Stidham J, Stinson H, Webb C, Denny K (2009) Avatar—a multi-sensory system for" /><p class="c-article-references__text" id="ref-CR18">Jovanov E, Hanish N, Courson V, Stidham J, Stinson H, Webb C, Denny K (2009) Avatar—a multi-sensory system for real time body position monitoring. In: Annual international conference of the IEEE engineering in medicine and biology society, 2009. EMBC 2009, pp 2462–2465</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="PN. Juslin, KR. Scherer, J. Harrigan, R. Rosenthal, " /><meta itemprop="datePublished" content="2005" /><meta itemprop="headline" content="Juslin PN, Scherer KR, Harrigan J, Rosenthal R (2005) Vocal expression of affect. In: Harrigan J, Rosenthal R," /><p class="c-article-references__text" id="ref-CR19">Juslin PN, Scherer KR, Harrigan J, Rosenthal R (2005) Vocal expression of affect. In: Harrigan J, Rosenthal R, Scherer KR (eds) The new handbook of methods in nonverbal behavior research. Oxford University Press, Oxford, UK, pp 65–135</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 19 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20new%20handbook%20of%20methods%20in%20nonverbal%20behavior%20research&amp;pages=65-135&amp;publication_year=2005&amp;author=Juslin%2CPN&amp;author=Scherer%2CKR&amp;author=Harrigan%2CJ&amp;author=Rosenthal%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="M. Knapp, J. Hall, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont" /><p class="c-article-references__text" id="ref-CR20">Knapp M, Hall J (2010) Nonverbal communication in human interaction, 7th edn. Thomson Wadsworth, Belmont</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 20 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nonverbal%20communication%20in%20human%20interaction&amp;publication_year=2010&amp;author=Knapp%2CM&amp;author=Hall%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Lange B, Rizzo A, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology wi" /><p class="c-article-references__text" id="ref-CR21">Lange B, Rizzo A, Chang CY, Suma EA, Bolas M (2011) Markerless full body tracking: depth-sensing technology within virtual environments. In: Interservice/industry training, simulation, and education conference (I/ITSEC), Las Vegas, NV, pp 1–8</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="M. Mine, FP. Brook, CH. Sequin, " /><meta itemprop="datePublished" content="1997" /><meta itemprop="headline" content="Mine M, Brook FP, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment i" /><p class="c-article-references__text" id="ref-CR22">Mine M, Brook FP, Sequin CH (1997) Moving objects in space: exploiting proprioception in virtual-environment interaction. Comput Graphics Proc ACM SIGGRAPH 1997:19–26</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 22 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Moving%20objects%20in%20space%3A%20exploiting%20proprioception%20in%20virtual-environment%20interaction&amp;journal=Comput%20Graphics%20Proc%20ACM%20SIGGRAPH&amp;volume=1997&amp;pages=19-26&amp;publication_year=1997&amp;author=Mine%2CM&amp;author=Brook%2CFP&amp;author=Sequin%2CCH">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Munoz E, Espuña A, Puigjaner L (2010) Towards an ontological infrastructure for chemical batch process managem" /><p class="c-article-references__text" id="ref-CR23">Munoz E, Espuña A, Puigjaner L (2010) Towards an ontological infrastructure for chemical batch process management. In: Computers &amp; Chemical Engineering, 34(5), 668–682, Selected paper of symposium ESCAPE 19, Krakow, Poland, June 14–17, 2009</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="K. Nickel, R. Stiefelhagen, " /><meta itemprop="datePublished" content="2007" /><meta itemprop="headline" content="Nickel K, Stiefelhagen R (2007) Visual recognition of pointing gestures for human–robot interaction. Image Vis" /><p class="c-article-references__text" id="ref-CR24">Nickel K, Stiefelhagen R (2007) Visual recognition of pointing gestures for human–robot interaction. Image Vis Comput 25(12):1875–1884</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1016%2Fj.imavis.2005.12.020" aria-label="View reference 24">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 24 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20recognition%20of%20pointing%20gestures%20for%20human%E2%80%93robot%20interaction&amp;journal=Image%20Vis%20Comput&amp;volume=25&amp;issue=12&amp;pages=1875-1884&amp;publication_year=2007&amp;author=Nickel%2CK&amp;author=Stiefelhagen%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Peña, " /><meta itemprop="datePublished" content="2014" /><meta itemprop="headline" content="Peña A (2014) A collaboration facilitator model for learning virtual environments. Comput Sci Inf Technol 2(2)" /><p class="c-article-references__text" id="ref-CR25">Peña A (2014) A collaboration facilitator model for learning virtual environments. Comput Sci Inf Technol 2(2):100–107</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 25 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20collaboration%20facilitator%20model%20for%20learning%20virtual%20environments&amp;journal=Comput%20Sci%20Inf%20Technol&amp;volume=2&amp;issue=2&amp;pages=100-107&amp;publication_year=2014&amp;author=Pe%C3%B1a%2CA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="A. Peña, N. Rangel, G. Lara, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Peña A, Rangel N, Lara G (2015) Nonverbal interaction contextualized in collaborative virtual environments. J " /><p class="c-article-references__text" id="ref-CR26">Peña A, Rangel N, Lara G (2015) Nonverbal interaction contextualized in collaborative virtual environments. J Multimodal User Interfaces 9(3):253–260. <a href="https://doi.org/10.1007/s12193-015-0193-4">https://doi.org/10.1007/s12193-015-0193-4</a>
</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1007%2Fs12193-015-0193-4" aria-label="View reference 26">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 26 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Nonverbal%20interaction%20contextualized%20in%20collaborative%20virtual%20environments&amp;journal=J%20Multimodal%20User%20Interfaces&amp;doi=10.1007%2Fs12193-015-0193-4&amp;volume=9&amp;issue=3&amp;pages=253-260&amp;publication_year=2015&amp;author=Pe%C3%B1a%2CA&amp;author=Rangel%2CN&amp;author=Lara%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/Book"><meta itemprop="author" content="R. Schroeder, " /><meta itemprop="datePublished" content="2010" /><meta itemprop="headline" content="Schroeder R (2010) Being there together: social interaction in shared virtual environments. Oxford University " /><p class="c-article-references__text" id="ref-CR27">Schroeder R (2010) Being there together: social interaction in shared virtual environments. Oxford University Press, New York</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 27 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Being%20there%20together%3A%20social%20interaction%20in%20shared%20virtual%20environments&amp;publication_year=2010&amp;author=Schroeder%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="GJ. Shahrour, MJ. Russell, " /><meta itemprop="datePublished" content="2015" /><meta itemprop="headline" content="Shahrour GJ, Russell MJ (2015) Recognizing an individual, their topic of conversation, and cultural background" /><p class="c-article-references__text" id="ref-CR28">Shahrour GJ, Russell MJ (2015) Recognizing an individual, their topic of conversation, and cultural background from 3D body movement. World Acad Sci Eng Technol Int J Comput Electr Autom Control Inf Eng 9(1):311–316</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 28 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=Recognizing%20an%20individual%2C%20their%20topic%20of%20conversation%2C%20and%20cultural%20background%20from%203D%20body%20movement&amp;journal=World%20Acad%20Sci%20Eng%20Technol%20Int%20J%20Comput%20Electr%20Autom%20Control%20Inf%20Eng&amp;volume=9&amp;issue=1&amp;pages=311-316&amp;publication_year=2015&amp;author=Shahrour%2CGJ&amp;author=Russell%2CMJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Spante M, Heldal I, Steed A, Axelsson A, Schroeder R (2003) Strangers and friends in networked immersive envir" /><p class="c-article-references__text" id="ref-CR29">Spante M, Heldal I, Steed A, Axelsson A, Schroeder R (2003) Strangers and friends in networked immersive environments: virtual spaces for future living. In: Home oriented informatics and telematics (HOIT), Irvine, California, USA</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wolff R, Roberts D, Steed A, Otto O (2005) A review of tele-collaboration technologies with respect to closely" /><p class="c-article-references__text" id="ref-CR30">Wolff R, Roberts D, Steed A, Otto O (2005) A review of tele-collaboration technologies with respect to closely coupled collaboration. In: International Journal of Computer Applications in Technology</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="headline" content="Wolff R, Roberts D, Murgia A, Murray N, Rae J, Steptoe W, Sharkey P (2008) Communicating eye gaze across a dis" /><p class="c-article-references__text" id="ref-CR31">Wolff R, Roberts D, Murgia A, Murray N, Rae J, Steptoe W, Sharkey P (2008) Communicating eye gaze across a distance without rooting participants to the spot. In: 12th IEEE international symposium on distributed simulation and real-time applications, 2008. DS-RT 2008, Vancouver, British Columbia, Canada</p></li><li class="c-article-references__item js-c-reading-companion-references-item" itemprop="citation" itemscope="itemscope" itemtype="http://schema.org/ScholarlyArticle"><meta itemprop="author" content="Z. Zeng, M. Pantic, GI. Roisman, TS. Huang, " /><meta itemprop="datePublished" content="2009" /><meta itemprop="headline" content="Zeng Z, Pantic M, Roisman GI, Huang TS (2009) A survey of affect recognition methods: audio, visual, and spont" /><p class="c-article-references__text" id="ref-CR32">Zeng Z, Pantic M, Roisman GI, Huang TS (2009) A survey of affect recognition methods: audio, visual, and spontaneous expressions. IEEE Trans Pattern Anal Mach Intell 31:39–58</p><p class="c-article-references__links u-hide-print"><a data-track="click" data-track-action="outbound reference" data-track-label="link" href="https://doi.org/10.1109%2FTPAMI.2008.52" aria-label="View reference 32">Article</a> 
    <a data-track="click" data-track-action="outbound reference" data-track-label="link" aria-label="Search for reference 32 on Google Scholar" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20survey%20of%20affect%20recognition%20methods%3A%20audio%2C%20visual%2C%20and%20spontaneous%20expressions&amp;journal=IEEE%20Trans%20Pattern%20Anal%20Mach%20Intell&amp;volume=31&amp;pages=39-58&amp;publication_year=2009&amp;author=Zeng%2CZ&amp;author=Pantic%2CM&amp;author=Roisman%2CGI&amp;author=Huang%2CTS">
                    Google Scholar</a> 
                </p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" href="/article/10.1007/s10055-019-00421-w-references.ris">Download references<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p></div></div></div></section><section aria-labelledby="Ack1"><div class="c-article-section" id="Ack1-section"><div class="c-article-section__content" id="Ack1-content"></div></div></section><section aria-labelledby="author-information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">CUCEI, Universidad de Guadalajara, Blvd. Marcelino García Barragán #1421, C.P. 44430, Guadalajara, Jalisco, Mexico</p><p class="c-article-author-affiliation__authors-list">Adriana Peña Pérez Negrón &amp; Graciela Lara López</p></li><li id="Aff2"><p class="c-article-author-affiliation__address">Centro de Investigación en Matemáticas, A.C., Unidad Zacatecas, Parque Quantum, Ciudad del Conocimiento. Av. Lassec, Andador Galileo Galilei, Manzana 3, Lote 7, 98160, Zacatecas, Mexico</p><p class="c-article-author-affiliation__authors-list">Edrisi Muñoz</p></li></ol><div class="js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Adriana-Pe_a_P_rez_Negr_n"><span class="c-article-authors-search__title u-h3 js-search-name">Adriana Peña Pérez Negrón</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Adriana+Pe%C3%B1a P%C3%A9rez Negr%C3%B3n&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Adriana+Pe%C3%B1a P%C3%A9rez Negr%C3%B3n" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Adriana+Pe%C3%B1a P%C3%A9rez Negr%C3%B3n%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Edrisi-Mu_oz"><span class="c-article-authors-search__title u-h3 js-search-name">Edrisi Muñoz</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Edrisi+Mu%C3%B1oz&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Edrisi+Mu%C3%B1oz" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Edrisi+Mu%C3%B1oz%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li><li id="auth-Graciela-Lara_L_pez"><span class="c-article-authors-search__title u-h3 js-search-name">Graciela Lara López</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=&#34;Graciela+Lara L%C3%B3pez&#34;" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Graciela+Lara L%C3%B3pez" data-track="click" data-track-action="author link - pubmed" data-track-label="link">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Graciela+Lara L%C3%B3pez%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" rel="nofollow" href="/article/10.1007/s10055-019-00421-w/email/correspondent/c1/new">Graciela Lara López</a>.</p></div></div></section><section aria-labelledby="additional-information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section aria-labelledby="rightslink"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20model%20for%20nonverbal%20interaction%20cues%20in%20collaborative%20virtual%20environments&amp;author=Adriana%20Pe%C3%B1a%20P%C3%A9rez%20Negr%C3%B3n%20et%20al&amp;contentID=10.1007%2Fs10055-019-00421-w&amp;publication=1359-4338&amp;publicationDate=2019-12-21&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10055-019-00421-w" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10055-019-00421-w" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>" /></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Peña Pérez Negrón, A., Muñoz, E. &amp; Lara López, G. A model for nonverbal interaction cues in collaborative virtual environments.
                    <i>Virtual Reality</i>  (2019). https://doi.org/10.1007/s10055-019-00421-w</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" href="/article/10.1007/s10055-019-00421-w.ris">Download citation<svg width="16" height="16" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#global-icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-01-17">17 January 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-11">11 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2019-12-21">21 December 2019</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value"><a href="https://doi.org/10.1007/s10055-019-00421-w" data-track="click" data-track-action="view doi" data-track-label="link" itemprop="sameAs">https://doi.org/10.1007/s10055-019-00421-w</a></span></p></li></ul><div data-component="share-box"></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span itemprop="about">CVE</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Avatars</span></li><li class="c-article-subject-list__subject"><span itemprop="about">Nonverbal communication</span></li><li class="c-article-subject-list__subject"><span itemprop="about">HCI</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                </div>
            </article>
        </main>

        <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion" data-track-component="reading companion">
            <aside>
                <div data-test="download-article-link-wrapper">
                    
    
    <div class="c-pdf-download u-clear-both">
        <a href="https://link.springer.com/content/pdf/10.1007/s10055-019-00421-w.pdf" class="c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button">
            
                <span>Download PDF</span>
                <svg width="16" height="16" class="u-icon"><use xlink:href="#global-icon-download"/></svg>
            
        </a>
    </div>
    

                </div>

                <div data-test="collections">
                    
                </div>

                <div data-test="editorial-summary">
                    
                </div>

                <div class="c-reading-companion">
                    <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
                        

                        <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                            <div class="js-ad">
    <aside class="c-ad c-ad--300x250">
        <div class="c-ad__inner">
            <p class="c-ad__label">Advertisement</p>
            <div id="div-gpt-ad-MPU1" data-gpt-unitpath="/270604982/springerlink/10055/article" data-gpt-sizes="300x250" data-gpt-targeting="pos=MPU1;articleid=421;"></div>
        </div>
    </aside>
</div>

                        </div>
                        <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
                        <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
                    </div>
                </div>
            </aside>
        </div>
    </div>


        
    <footer class="app-footer" role="contentinfo">
        <div class="app-footer__aside-wrapper u-hide-print">
            <div class="app-footer__container">
                <p class="app-footer__strapline">Over 10 million scientific documents at your fingertips</p>
                
                    <div class="app-footer__edition" data-component="SV.EditionSwitcher">
                        <span class="u-visually-hidden" data-role="button-dropdown__title" data-btn-text="Switch between Academic & Corporate Edition">Switch Edition</span>
                        <ul class="app-footer-edition-list" data-role="button-dropdown__content" data-test="footer-edition-switcher-list">
                            <li class="selected">
                                <a data-test="footer-academic-link"
                                   href="/siteEdition/link"
                                   id="siteedition-academic-link">Academic Edition</a>
                            </li>
                            <li>
                                <a data-test="footer-corporate-link"
                                   href="/siteEdition/rd"
                                   id="siteedition-corporate-link">Corporate Edition</a>
                            </li>
                        </ul>
                    </div>
                
            </div>
        </div>
        <div class="app-footer__container">
            <ul class="app-footer__nav u-hide-print">
                <li><a href="/">Home</a></li>
                <li><a href="/impressum">Impressum</a></li>
                <li><a href="/termsandconditions">Legal information</a></li>
                <li><a href="/privacystatement">Privacy statement</a></li>
                <li><a href="https://www.springernature.com/ccpa">California Privacy Statement</a></li>
                <li><a href="/cookiepolicy">How we use cookies</a></li>
                
                <li><a class="optanon-toggle-display" href="javascript:void(0);">Manage cookies/Do not sell my data</a></li>
                
                <li><a href="/accessibility">Accessibility</a></li>
                <li><a id="contactus-footer-link" href="/contactus">Contact us</a></li>
            </ul>
            <div class="c-user-metadata">
    
        <p class="c-user-metadata__item">
            <span data-test="footer-user-login-status">Not logged in</span>
            <span data-test="footer-user-ip"> - 130.225.98.201</span>
        </p>
        <p class="c-user-metadata__item" data-test="footer-business-partners">
            Copenhagen University Library Royal Danish Library Copenhagen (1600006921)  - DEFF Consortium Danish National Library Authority (2000421697)  - Det Kongelige Bibliotek The Royal Library (3000092219)  - DEFF Danish Agency for Culture (3000209025)  - 1236 DEFF LNCS (3000236495) 
        </p>

        
    
</div>

            <a class="app-footer__parent-logo" target="_blank" rel="noopener" href="//www.springernature.com"  title="Go to Springer Nature">
                <span class="u-visually-hidden">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12" alt="Springer Nature logo"
                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href=/oscar-static/images/springerlink/svg/springernature-ecf01c77dd.svg>
                    </image>
                </svg>
            </a>
            <p class="app-footer__copyright">&copy; 2020 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
            
        </div>
        
    <svg class="u-hide hide">
        <symbol id="global-icon-chevron-right" viewBox="0 0 16 16">
            <path d="M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-download" viewBox="0 0 16 16">
            <path d="M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-email" viewBox="0 0 18 18">
            <path d="M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-institution" viewBox="0 0 18 18">
            <path d="M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z" fill-rule="evenodd"/>
        </symbol>
        <symbol id="global-icon-search" viewBox="0 0 22 22">
            <path fill-rule="evenodd" d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z"/>
        </symbol>
    </svg>

    </footer>



    </div>
    
    
</body>
</html>

